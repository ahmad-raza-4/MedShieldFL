nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/25/2025 03:53:14:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
01/25/2025 03:53:14:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/25/2025 03:53:14:DEBUG:ChannelConnectivity.IDLE
01/25/2025 03:53:14:DEBUG:ChannelConnectivity.CONNECTING
01/25/2025 03:53:14:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/25/2025 03:53:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 03:53:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 88c81256-bec6-4c7a-9bb4-81d31e7c93d6
01/25/2025 03:53:14:INFO:Received: get_parameters message 88c81256-bec6-4c7a-9bb4-81d31e7c93d6
[92mINFO [0m:      Sent reply
01/25/2025 03:53:19:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 03:53:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 03:53:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30f0d95f-0ab3-4350-9326-7f97252941f2
01/25/2025 03:53:48:INFO:Received: train message 30f0d95f-0ab3-4350-9326-7f97252941f2
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 03:58:44:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 03:59:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 03:59:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62391bd6-9649-4d66-9f5d-1efe4727e3b6
01/25/2025 03:59:24:INFO:Received: evaluate message 62391bd6-9649-4d66-9f5d-1efe4727e3b6
[92mINFO [0m:      Sent reply
01/25/2025 04:00:00:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 04:00:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 04:00:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1c419286-f571-4c48-950e-780161027d0d
01/25/2025 04:00:33:INFO:Received: train message 1c419286-f571-4c48-950e-780161027d0d
Epoch [1/5], Batch [10/225], Loss: 1.1400
Epoch [1/5], Batch [20/225], Loss: 1.0578
Epoch [1/5], Batch [30/225], Loss: 0.9094
Epoch [1/5], Batch [40/225], Loss: 0.9828
Epoch [1/5], Batch [50/225], Loss: 1.0022
Epoch [1/5], Batch [60/225], Loss: 1.1211
Epoch [1/5], Batch [70/225], Loss: 1.0710
Epoch [1/5], Batch [80/225], Loss: 1.1261
Epoch [1/5], Batch [90/225], Loss: 1.2845
Epoch [1/5], Batch [100/225], Loss: 1.1203
Epoch [1/5], Batch [110/225], Loss: 1.0029
Epoch [1/5], Batch [120/225], Loss: 0.8933
Epoch [1/5], Batch [130/225], Loss: 1.1278
Epoch [1/5], Batch [140/225], Loss: 1.1265
Epoch [1/5], Batch [150/225], Loss: 1.0248
Epoch [1/5], Batch [160/225], Loss: 1.1384
Epoch [1/5], Batch [170/225], Loss: 0.7740
Epoch [1/5], Batch [180/225], Loss: 1.0530
Epoch [1/5], Batch [190/225], Loss: 1.0213
Epoch [1/5], Batch [200/225], Loss: 0.9665
Epoch [1/5], Batch [210/225], Loss: 1.1811
Epoch [1/5], Batch [220/225], Loss: 0.9332
Epoch [2/5], Batch [10/225], Loss: 0.8989
Epoch [2/5], Batch [20/225], Loss: 0.8194
Epoch [2/5], Batch [30/225], Loss: 0.9386
Epoch [2/5], Batch [40/225], Loss: 1.1271
Epoch [2/5], Batch [50/225], Loss: 1.0429
Epoch [2/5], Batch [60/225], Loss: 1.1422
Epoch [2/5], Batch [70/225], Loss: 0.8279
Epoch [2/5], Batch [80/225], Loss: 0.8810
Epoch [2/5], Batch [90/225], Loss: 1.1661
Epoch [2/5], Batch [100/225], Loss: 1.1502
Epoch [2/5], Batch [110/225], Loss: 0.9827
Epoch [2/5], Batch [120/225], Loss: 0.7202
Epoch [2/5], Batch [130/225], Loss: 1.0767
Epoch [2/5], Batch [140/225], Loss: 0.8556
Epoch [2/5], Batch [150/225], Loss: 1.1203
Epoch [2/5], Batch [160/225], Loss: 1.0644
Epoch [2/5], Batch [170/225], Loss: 1.0484
Epoch [2/5], Batch [180/225], Loss: 1.2456
Epoch [2/5], Batch [190/225], Loss: 0.9289
Epoch [2/5], Batch [200/225], Loss: 1.1139
Epoch [2/5], Batch [210/225], Loss: 1.2192
Epoch [2/5], Batch [220/225], Loss: 0.9185
Epoch [3/5], Batch [10/225], Loss: 0.9084
Epoch [3/5], Batch [20/225], Loss: 0.8601
Epoch [3/5], Batch [30/225], Loss: 1.0725
Epoch [3/5], Batch [40/225], Loss: 0.9402
Epoch [3/5], Batch [50/225], Loss: 0.9337
Epoch [3/5], Batch [60/225], Loss: 0.9670
Epoch [3/5], Batch [70/225], Loss: 1.2018
Epoch [3/5], Batch [80/225], Loss: 0.9233
Epoch [3/5], Batch [90/225], Loss: 1.0108
Epoch [3/5], Batch [100/225], Loss: 1.0526
Epoch [3/5], Batch [110/225], Loss: 1.0120
Epoch [3/5], Batch [120/225], Loss: 0.9745
Epoch [3/5], Batch [130/225], Loss: 0.9205
Epoch [3/5], Batch [140/225], Loss: 0.7747
Epoch [3/5], Batch [150/225], Loss: 1.2053
Epoch [3/5], Batch [160/225], Loss: 0.9346
Epoch [3/5], Batch [170/225], Loss: 0.9836
Epoch [3/5], Batch [180/225], Loss: 0.7748
Epoch [3/5], Batch [190/225], Loss: 0.8823
Epoch [3/5], Batch [200/225], Loss: 0.8251
Epoch [3/5], Batch [210/225], Loss: 1.1784
Epoch [3/5], Batch [220/225], Loss: 0.9213
Epoch [4/5], Batch [10/225], Loss: 1.0468
Epoch [4/5], Batch [20/225], Loss: 0.9058
Epoch [4/5], Batch [30/225], Loss: 1.1963
Epoch [4/5], Batch [40/225], Loss: 0.9819
Epoch [4/5], Batch [50/225], Loss: 0.9552
Epoch [4/5], Batch [60/225], Loss: 1.1174
Epoch [4/5], Batch [70/225], Loss: 1.0559
Epoch [4/5], Batch [80/225], Loss: 0.6842
Epoch [4/5], Batch [90/225], Loss: 1.0672
Epoch [4/5], Batch [100/225], Loss: 0.8709
Epoch [4/5], Batch [110/225], Loss: 0.8027
Epoch [4/5], Batch [120/225], Loss: 0.9953
Epoch [4/5], Batch [130/225], Loss: 0.9587
Epoch [4/5], Batch [140/225], Loss: 1.4454
Epoch [4/5], Batch [150/225], Loss: 0.9354
Epoch [4/5], Batch [160/225], Loss: 0.7889
Epoch [4/5], Batch [170/225], Loss: 1.1597
Epoch [4/5], Batch [180/225], Loss: 0.9880
Epoch [4/5], Batch [190/225], Loss: 1.1388
Epoch [4/5], Batch [200/225], Loss: 0.6909
Epoch [4/5], Batch [210/225], Loss: 1.1533
Epoch [4/5], Batch [220/225], Loss: 0.9144
Epoch [5/5], Batch [10/225], Loss: 0.6963
Epoch [5/5], Batch [20/225], Loss: 1.0836
Epoch [5/5], Batch [30/225], Loss: 1.3842
Epoch [5/5], Batch [40/225], Loss: 1.6476
Epoch [5/5], Batch [50/225], Loss: 0.6816
Epoch [5/5], Batch [60/225], Loss: 0.9290
Epoch [5/5], Batch [70/225], Loss: 0.9213
Epoch [5/5], Batch [80/225], Loss: 0.8263
Epoch [5/5], Batch [90/225], Loss: 1.0009
Epoch [5/5], Batch [100/225], Loss: 1.3537
Epoch [5/5], Batch [110/225], Loss: 0.7295
Epoch [5/5], Batch [120/225], Loss: 0.7102
Epoch [5/5], Batch [130/225], Loss: 1.3264
Epoch [5/5], Batch [140/225], Loss: 0.7323
Epoch [5/5], Batch [150/225], Loss: 1.1908
Epoch [5/5], Batch [160/225], Loss: 0.8906
Epoch [5/5], Batch [170/225], Loss: 1.0374
Epoch [5/5], Batch [180/225], Loss: 1.0937
Epoch [5/5], Batch [190/225], Loss: 1.0670
Epoch [5/5], Batch [200/225], Loss: 1.4712
Epoch [5/5], Batch [210/225], Loss: 0.9560
Epoch [5/5], Batch [220/225], Loss: 0.9646
Epsilon = 0.48

{'loss': [175.82146698236465], 'accuracy': [0.49960906958561374], 'auc': [0.38544397756630056]}

Epoch [1/5], Batch [10/225], Loss: 0.9251
Epoch [1/5], Batch [20/225], Loss: 1.0383
Epoch [1/5], Batch [30/225], Loss: 0.9067
Epoch [1/5], Batch [40/225], Loss: 0.9579
Epoch [1/5], Batch [50/225], Loss: 1.1544
Epoch [1/5], Batch [60/225], Loss: 0.9537
Epoch [1/5], Batch [70/225], Loss: 0.8788
Epoch [1/5], Batch [80/225], Loss: 0.8742
Epoch [1/5], Batch [90/225], Loss: 0.9083
Epoch [1/5], Batch [100/225], Loss: 0.8863
Epoch [1/5], Batch [110/225], Loss: 0.8445
Epoch [1/5], Batch [120/225], Loss: 0.8797
Epoch [1/5], Batch [130/225], Loss: 0.9312
Epoch [1/5], Batch [140/225], Loss: 0.9100
Epoch [1/5], Batch [150/225], Loss: 0.9170
Epoch [1/5], Batch [160/225], Loss: 0.8960
Epoch [1/5], Batch [170/225], Loss: 1.0728
Epoch [1/5], Batch [180/225], Loss: 1.0444
Epoch [1/5], Batch [190/225], Loss: 0.8476
Epoch [1/5], Batch [200/225], Loss: 1.0005
Epoch [1/5], Batch [210/225], Loss: 0.8715
Epoch [1/5], Batch [220/225], Loss: 1.0431
Epoch [2/5], Batch [10/225], Loss: 0.8833
Epoch [2/5], Batch [20/225], Loss: 0.8315
Epoch [2/5], Batch [30/225], Loss: 0.8823
Epoch [2/5], Batch [40/225], Loss: 0.8718
Epoch [2/5], Batch [50/225], Loss: 0.8894
Epoch [2/5], Batch [60/225], Loss: 1.0682
Epoch [2/5], Batch [70/225], Loss: 0.8150
Epoch [2/5], Batch [80/225], Loss: 0.8166
Epoch [2/5], Batch [90/225], Loss: 0.8427
Epoch [2/5], Batch [100/225], Loss: 0.8612
Epoch [2/5], Batch [110/225], Loss: 0.8838
Epoch [2/5], Batch [120/225], Loss: 0.8591
Epoch [2/5], Batch [130/225], Loss: 0.8516
Epoch [2/5], Batch [140/225], Loss: 0.8907
Epoch [2/5], Batch [150/225], Loss: 0.8572
Epoch [2/5], Batch [160/225], Loss: 0.8383
Epoch [2/5], Batch [170/225], Loss: 0.8452
Epoch [2/5], Batch [180/225], Loss: 0.8884
Epoch [2/5], Batch [190/225], Loss: 0.8336
Epoch [2/5], Batch [200/225], Loss: 0.8586
Epoch [2/5], Batch [210/225], Loss: 1.2494
Epoch [2/5], Batch [220/225], Loss: 1.1282
Epoch [3/5], Batch [10/225], Loss: 0.8046
Epoch [3/5], Batch [20/225], Loss: 0.8439
Epoch [3/5], Batch [30/225], Loss: 1.0151
Epoch [3/5], Batch [40/225], Loss: 1.0587
Epoch [3/5], Batch [50/225], Loss: 0.8238
Epoch [3/5], Batch [60/225], Loss: 0.8471
Epoch [3/5], Batch [70/225], Loss: 0.8529
Epoch [3/5], Batch [80/225], Loss: 0.9400
Epoch [3/5], Batch [90/225], Loss: 0.8482
Epoch [3/5], Batch [100/225], Loss: 0.8010
Epoch [3/5], Batch [110/225], Loss: 0.8913
Epoch [3/5], Batch [120/225], Loss: 0.8139
Epoch [3/5], Batch [130/225], Loss: 0.8227
Epoch [3/5], Batch [140/225], Loss: 1.2950
Epoch [3/5], Batch [150/225], Loss: 0.8146
Epoch [3/5], Batch [160/225], Loss: 1.0515
Epoch [3/5], Batch [170/225], Loss: 0.9622
Epoch [3/5], Batch [180/225], Loss: 0.7989
Epoch [3/5], Batch [190/225], Loss: 0.8375
Epoch [3/5], Batch [200/225], Loss: 0.8121
Epoch [3/5], Batch [210/225], Loss: 1.1664
Epoch [3/5], Batch [220/225], Loss: 0.9760
Epoch [4/5], Batch [10/225], Loss: 1.0816
Epoch [4/5], Batch [20/225], Loss: 0.8748
Epoch [4/5], Batch [30/225], Loss: 0.8395
Epoch [4/5], Batch [40/225], Loss: 0.8395
Epoch [4/5], Batch [50/225], Loss: 0.7857
Epoch [4/5], Batch [60/225], Loss: 1.1651
Epoch [4/5], Batch [70/225], Loss: 0.7745
Epoch [4/5], Batch [80/225], Loss: 1.0483
Epoch [4/5], Batch [90/225], Loss: 0.8043
Epoch [4/5], Batch [100/225], Loss: 0.8182
Epoch [4/5], Batch [110/225], Loss: 0.6993
Epoch [4/5], Batch [120/225], Loss: 1.0079
Epoch [4/5], Batch [130/225], Loss: 1.3167
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 04:04:44:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 04:05:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 04:05:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 509b806f-27d7-4d55-8650-5e10ac30c125
01/25/2025 04:05:27:INFO:Received: evaluate message 509b806f-27d7-4d55-8650-5e10ac30c125
[92mINFO [0m:      Sent reply
01/25/2025 04:06:03:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 04:06:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 04:06:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 474b7037-cea0-4db4-b0cb-f41c82213e62
01/25/2025 04:06:41:INFO:Received: train message 474b7037-cea0-4db4-b0cb-f41c82213e62
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 04:10:00:INFO:Sent reply
