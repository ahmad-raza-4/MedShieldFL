nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/21/2025 10:04:46:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
01/21/2025 10:04:46:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.IDLE
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.CONNECTING
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/21/2025 10:05:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:05:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c3ffeb93-cda7-4426-8d71-851d4be252e5
01/21/2025 10:05:06:INFO:Received: train message c3ffeb93-cda7-4426-8d71-851d4be252e5
Error importing huggingface_hub.hf_api: No module named 'tqdm'
Error importing huggingface_hub.hf_api: No module named 'tqdm'
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:14:17:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:14:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:14:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2a4e758-c1a6-42f3-8dd2-daa4a1df8966
01/21/2025 10:14:49:INFO:Received: evaluate message f2a4e758-c1a6-42f3-8dd2-daa4a1df8966
Epsilon = 0.65
[92mINFO [0m:      Sent reply
01/21/2025 10:16:38:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:17:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:17:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6beb8160-b584-4a31-806f-fffa9d1c0323
01/21/2025 10:17:04:INFO:Received: train message 6beb8160-b584-4a31-806f-fffa9d1c0323

{'loss': [183.56197291612625], 'accuracy': [0.49648162627052383], 'auc': [0.5590802922012008]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:26:05:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:26:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:26:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 445236e8-6160-442d-87f7-24cfed240c82
01/21/2025 10:26:24:INFO:Received: evaluate message 445236e8-6160-442d-87f7-24cfed240c82
Epsilon = 0.89
[92mINFO [0m:      Sent reply
01/21/2025 10:28:11:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:28:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:28:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 801c2128-ddb8-4a3b-b07f-786ccc52fdba
01/21/2025 10:28:52:INFO:Received: train message 801c2128-ddb8-4a3b-b07f-786ccc52fdba

{'loss': [183.56197291612625, 174.04597514867783], 'accuracy': [0.49648162627052383, 0.49569976544175137], 'auc': [0.5590802922012008, 0.5831169002518957]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:37:49:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:38:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:38:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de224b03-a0c5-45d9-9921-653bbb99509b
01/21/2025 10:38:15:INFO:Received: evaluate message de224b03-a0c5-45d9-9921-653bbb99509b
Epsilon = 1.08
[92mINFO [0m:      Sent reply
01/21/2025 10:40:03:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:40:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:40:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dae102d6-bc3c-4ae6-a49e-c5634eac5df9
01/21/2025 10:40:33:INFO:Received: train message dae102d6-bc3c-4ae6-a49e-c5634eac5df9

{'loss': [183.56197291612625, 174.04597514867783, 171.55733466148376], 'accuracy': [0.49648162627052383, 0.49569976544175137, 0.4910086004691165], 'auc': [0.5590802922012008, 0.5831169002518957, 0.5971922107095]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:49:38:INFO:Sent reply
