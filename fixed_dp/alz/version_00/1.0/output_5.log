nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/21/2025 10:04:46:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
01/21/2025 10:04:46:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.IDLE
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.CONNECTING
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/21/2025 10:05:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:05:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 89346360-c56d-485a-8f30-4d99328ce58e
01/21/2025 10:05:17:INFO:Received: train message 89346360-c56d-485a-8f30-4d99328ce58e
Error importing huggingface_hub.hf_api: No module named 'tqdm'
Error importing huggingface_hub.hf_api: No module named 'tqdm'
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:11:45:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:14:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:14:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cd58af40-a915-40fe-afa1-228919b97680
01/21/2025 10:14:45:INFO:Received: evaluate message cd58af40-a915-40fe-afa1-228919b97680
Epsilon = 0.89
[92mINFO [0m:      Sent reply
01/21/2025 10:16:31:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:16:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:16:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 634c18e4-c64c-463f-8597-a2718bb0d495
01/21/2025 10:16:47:INFO:Received: train message 634c18e4-c64c-463f-8597-a2718bb0d495

{'loss': [183.56197291612625], 'accuracy': [0.49648162627052383], 'auc': [0.5590802922012008]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:22:53:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:26:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:26:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0af91b3c-cd42-4ec6-bf55-54a3bc82ba01
01/21/2025 10:26:25:INFO:Received: evaluate message 0af91b3c-cd42-4ec6-bf55-54a3bc82ba01
Epsilon = 1.20
[92mINFO [0m:      Sent reply
01/21/2025 10:28:12:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:28:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:28:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5b1b73c-163b-4beb-8e24-c2d63776154c
01/21/2025 10:28:40:INFO:Received: train message f5b1b73c-163b-4beb-8e24-c2d63776154c

{'loss': [183.56197291612625, 174.04597514867783], 'accuracy': [0.49648162627052383, 0.49569976544175137], 'auc': [0.5590802922012008, 0.5831169002518957]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:35:07:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:38:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:38:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7b0bf387-74c1-443c-808f-45e71cbcdd9d
01/21/2025 10:38:19:INFO:Received: evaluate message 7b0bf387-74c1-443c-808f-45e71cbcdd9d
Epsilon = 1.45
[92mINFO [0m:      Sent reply
01/21/2025 10:40:07:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:40:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:40:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ec82819-cf79-4e2b-8cba-8b2ef1dda4dd
01/21/2025 10:40:32:INFO:Received: train message 4ec82819-cf79-4e2b-8cba-8b2ef1dda4dd

{'loss': [183.56197291612625, 174.04597514867783, 171.55733466148376], 'accuracy': [0.49648162627052383, 0.49569976544175137, 0.4910086004691165], 'auc': [0.5590802922012008, 0.5831169002518957, 0.5971922107095]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:46:50:INFO:Sent reply
