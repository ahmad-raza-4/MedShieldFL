nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/22/2025 11:29:16:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
01/22/2025 11:29:16:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/22/2025 11:29:16:DEBUG:ChannelConnectivity.IDLE
01/22/2025 11:29:16:DEBUG:ChannelConnectivity.CONNECTING
01/22/2025 11:29:16:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/22/2025 11:29:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/22/2025 11:29:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 3f97a46e-7e22-426f-b458-cc4c7aba8193
01/22/2025 11:29:16:INFO:Received: get_parameters message 3f97a46e-7e22-426f-b458-cc4c7aba8193
[92mINFO [0m:      Sent reply
01/22/2025 11:29:19:INFO:Sent reply
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/22/2025 11:30:20:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
01/22/2025 11:30:20:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/22/2025 11:30:20:DEBUG:ChannelConnectivity.IDLE
01/22/2025 11:30:20:DEBUG:ChannelConnectivity.CONNECTING
01/22/2025 11:30:20:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/22/2025 11:30:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/22/2025 11:30:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message abb285f3-65df-4ef5-8ed4-de9c21685a80
01/22/2025 11:30:53:INFO:Received: train message abb285f3-65df-4ef5-8ed4-de9c21685a80
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/22/2025 11:35:16:INFO:Sent reply
[92mINFO [0m:      
01/22/2025 11:35:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/22/2025 11:35:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 04f096d2-c7aa-4e61-bbf9-a9b52d92c978
01/22/2025 11:35:46:INFO:Received: evaluate message 04f096d2-c7aa-4e61-bbf9-a9b52d92c978
[92mINFO [0m:      Sent reply
01/22/2025 11:36:30:INFO:Sent reply
[92mINFO [0m:      
01/22/2025 11:36:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/22/2025 11:36:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5664e32-c3b9-4593-b15c-79beb5ad8717
01/22/2025 11:36:57:INFO:Received: train message f5664e32-c3b9-4593-b15c-79beb5ad8717
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/22/2025 11:41:20:INFO:Sent reply
[92mINFO [0m:      
01/22/2025 11:41:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/22/2025 11:41:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dd28fe66-28e9-4262-ac46-4a5813d7bac0
01/22/2025 11:41:42:INFO:Received: evaluate message dd28fe66-28e9-4262-ac46-4a5813d7bac0
[92mINFO [0m:      Sent reply
01/22/2025 11:42:11:INFO:Sent reply
[92mINFO [0m:      
01/22/2025 11:42:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/22/2025 11:42:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 17c63eca-e7a6-4f8a-8a81-c16cf2d01e09
01/22/2025 11:42:50:INFO:Received: train message 17c63eca-e7a6-4f8a-8a81-c16cf2d01e09
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[91mERROR [0m:     Client raised an exception.
Traceback (most recent call last):
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 496, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "client_4.py", line 147, in fit
    epsilon = train(
  File "client_4.py", line 85, in train
    optimizer.step()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 518, in step
    if self.pre_step():
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 499, in pre_step
    self.clip_and_accumulate()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 412, in clip_and_accumulate
    grad = contract("i,i...", per_sample_clip_factor, grad_sample)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 507, in contract
    return _core_contract(operands, contraction_list, backend=backend, **einsum_kwargs)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 573, in _core_contract
    new_view = _tensordot(*tmp_operands, axes=(tuple(left_pos), tuple(right_pos)), backend=backend)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/sharing.py", line 131, in cached_tensordot
    return tensordot(x, y, axes, backend=backend)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 374, in _tensordot
    return fn(x, y, axes=axes)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/backends/torch.py", line 54, in tensordot
    return torch.tensordot(x, y, dims=axes)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/functional.py", line 1193, in tensordot
    return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)
01/22/2025 11:45:21:ERROR:Client raised an exception.
Traceback (most recent call last):
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 496, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "client_4.py", line 147, in fit
    epsilon = train(
  File "client_4.py", line 85, in train
    optimizer.step()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 518, in step
    if self.pre_step():
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 499, in pre_step
    self.clip_and_accumulate()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 412, in clip_and_accumulate
    grad = contract("i,i...", per_sample_clip_factor, grad_sample)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 507, in contract
    return _core_contract(operands, contraction_list, backend=backend, **einsum_kwargs)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 573, in _core_contract
    new_view = _tensordot(*tmp_operands, axes=(tuple(left_pos), tuple(right_pos)), backend=backend)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/sharing.py", line 131, in cached_tensordot
    return tensordot(x, y, axes, backend=backend)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 374, in _tensordot
    return fn(x, y, axes=axes)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/backends/torch.py", line 54, in tensordot
    return torch.tensordot(x, y, dims=axes)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/functional.py", line 1193, in tensordot
    return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)
01/22/2025 11:45:21:DEBUG:gRPC channel closed
Error importing huggingface_hub.hf_api: No module named 'tqdm'
Error importing huggingface_hub.hf_api: No module named 'tqdm'
Epsilon = 0.89

{'loss': [183.98990601301193], 'accuracy': [0.49335418295543393], 'auc': [0.5195222601061145]}

Epsilon = 1.24

{'loss': [183.98990601301193, 172.18215227127075], 'accuracy': [0.49335418295543393, 0.4988272087568413], 'auc': [0.5195222601061145, 0.5699639102911909]}

Traceback (most recent call last):
  File "client_4.py", line 183, in <module>
    fl.client.start_client(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 291, in start_client
    _start_client_internal(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 503, in _start_client_internal
    raise ex
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 496, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "client_4.py", line 147, in fit
    epsilon = train(
  File "client_4.py", line 85, in train
    optimizer.step()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 518, in step
    if self.pre_step():
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 499, in pre_step
    self.clip_and_accumulate()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 412, in clip_and_accumulate
    grad = contract("i,i...", per_sample_clip_factor, grad_sample)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 507, in contract
    return _core_contract(operands, contraction_list, backend=backend, **einsum_kwargs)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 573, in _core_contract
    new_view = _tensordot(*tmp_operands, axes=(tuple(left_pos), tuple(right_pos)), backend=backend)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/sharing.py", line 131, in cached_tensordot
    return tensordot(x, y, axes, backend=backend)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/contract.py", line 374, in _tensordot
    return fn(x, y, axes=axes)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/opt_einsum/backends/torch.py", line 54, in tensordot
    return torch.tensordot(x, y, dims=axes)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/functional.py", line 1193, in tensordot
    return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)
