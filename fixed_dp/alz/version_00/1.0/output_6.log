nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/21/2025 10:04:46:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
01/21/2025 10:04:46:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.IDLE
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.CONNECTING
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/21/2025 10:05:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:05:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 438d9cfa-101a-4ba5-a547-241a7e4d8bb6
01/21/2025 10:05:06:INFO:Received: train message 438d9cfa-101a-4ba5-a547-241a7e4d8bb6
Error importing huggingface_hub.hf_api: No module named 'tqdm'
Error importing huggingface_hub.hf_api: No module named 'tqdm'
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:08:51:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:14:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:14:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7b1c3d66-065d-4a07-9666-b8a33e4a2241
01/21/2025 10:14:40:INFO:Received: evaluate message 7b1c3d66-065d-4a07-9666-b8a33e4a2241
Epsilon = 1.32
[92mINFO [0m:      Sent reply
01/21/2025 10:15:56:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:17:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:17:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 216df484-4f29-4769-8e56-89439ecc4447
01/21/2025 10:17:03:INFO:Received: train message 216df484-4f29-4769-8e56-89439ecc4447

{'loss': [183.56197291612625], 'accuracy': [0.49648162627052383], 'auc': [0.5590802922012008]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:20:51:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:26:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:26:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5837a72d-24eb-4386-884f-df2e0c2f2083
01/21/2025 10:26:35:INFO:Received: evaluate message 5837a72d-24eb-4386-884f-df2e0c2f2083
Epsilon = 1.76
[92mINFO [0m:      Sent reply
01/21/2025 10:28:23:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:28:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:28:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4383f2af-555c-436f-b1f9-4a7aca32bfff
01/21/2025 10:28:40:INFO:Received: train message 4383f2af-555c-436f-b1f9-4a7aca32bfff

{'loss': [183.56197291612625, 174.04597514867783], 'accuracy': [0.49648162627052383, 0.49569976544175137], 'auc': [0.5590802922012008, 0.5831169002518957]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:32:22:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:38:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:38:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7c0e4bb7-800b-44c5-b37a-decb7d019967
01/21/2025 10:38:19:INFO:Received: evaluate message 7c0e4bb7-800b-44c5-b37a-decb7d019967
Epsilon = 2.11
[92mINFO [0m:      Sent reply
01/21/2025 10:40:08:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:40:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:40:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b2e1a957-0733-4a67-9364-ca648418ab8f
01/21/2025 10:40:26:INFO:Received: train message b2e1a957-0733-4a67-9364-ca648418ab8f

{'loss': [183.56197291612625, 174.04597514867783, 171.55733466148376], 'accuracy': [0.49648162627052383, 0.49569976544175137, 0.4910086004691165], 'auc': [0.5590802922012008, 0.5831169002518957, 0.5971922107095]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:44:07:INFO:Sent reply
