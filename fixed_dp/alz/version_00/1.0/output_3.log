nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/21/2025 10:04:46:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
01/21/2025 10:04:46:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.IDLE
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.CONNECTING
01/21/2025 10:04:46:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/21/2025 10:05:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:05:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 24a9a1bc-29b7-4881-8b4c-791e24ea14f0
01/21/2025 10:05:19:INFO:Received: train message 24a9a1bc-29b7-4881-8b4c-791e24ea14f0
Error importing huggingface_hub.hf_api: No module named 'tqdm'
Error importing huggingface_hub.hf_api: No module named 'tqdm'
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:09:08:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:14:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:14:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d4e196f-e607-4524-921d-4ccac107f16e
01/21/2025 10:14:51:INFO:Received: evaluate message 3d4e196f-e607-4524-921d-4ccac107f16e
Epsilon = 1.32
[92mINFO [0m:      Sent reply
01/21/2025 10:16:37:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:17:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:17:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1ffba7c8-5be6-4f9c-be67-eb2eddbb1304
01/21/2025 10:17:07:INFO:Received: train message 1ffba7c8-5be6-4f9c-be67-eb2eddbb1304

{'loss': [183.56197291612625], 'accuracy': [0.49648162627052383], 'auc': [0.5590802922012008]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:20:55:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:26:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:26:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee2e3bbd-5098-44f0-b835-ecbcaa39c412
01/21/2025 10:26:37:INFO:Received: evaluate message ee2e3bbd-5098-44f0-b835-ecbcaa39c412
Epsilon = 1.76
[92mINFO [0m:      Sent reply
01/21/2025 10:28:25:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:28:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:28:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 42ebe65f-4c47-41a1-a341-89a264801684
01/21/2025 10:28:51:INFO:Received: train message 42ebe65f-4c47-41a1-a341-89a264801684

{'loss': [183.56197291612625, 174.04597514867783], 'accuracy': [0.49648162627052383, 0.49569976544175137], 'auc': [0.5590802922012008, 0.5831169002518957]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:32:38:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:38:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:38:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 14be852d-5690-4989-b4bd-1289f946e737
01/21/2025 10:38:21:INFO:Received: evaluate message 14be852d-5690-4989-b4bd-1289f946e737
Epsilon = 2.11
[92mINFO [0m:      Sent reply
01/21/2025 10:40:09:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 10:40:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 10:40:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f26e5afe-cc10-48da-9122-97fea91ca9e3
01/21/2025 10:40:34:INFO:Received: train message f26e5afe-cc10-48da-9122-97fea91ca9e3

{'loss': [183.56197291612625, 174.04597514867783, 171.55733466148376], 'accuracy': [0.49648162627052383, 0.49569976544175137, 0.4910086004691165], 'auc': [0.5590802922012008, 0.5831169002518957, 0.5971922107095]}

/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 10:44:20:INFO:Sent reply
