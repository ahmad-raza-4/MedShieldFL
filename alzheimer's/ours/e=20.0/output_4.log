nohup: ignoring input
02/05/2025 10:04:55:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:04:55:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:04:55:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778695.188846 1883526 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:05:36:INFO:
[92mINFO [0m:      Received: train message 3aedb698-abc1-4540-a246-d2ee8672a61e
02/05/2025 10:05:36:INFO:Received: train message 3aedb698-abc1-4540-a246-d2ee8672a61e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:06:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:22:INFO:
[92mINFO [0m:      Received: evaluate message 1505ee7b-6884-4067-ab74-bdb974909ae4
02/05/2025 10:07:22:INFO:Received: evaluate message 1505ee7b-6884-4067-ab74-bdb974909ae4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:07:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:03:INFO:
[92mINFO [0m:      Received: train message 98995b47-a138-4705-9ea3-d0d0b054e718
02/05/2025 10:08:03:INFO:Received: train message 98995b47-a138-4705-9ea3-d0d0b054e718
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:09:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:57:INFO:
[92mINFO [0m:      Received: evaluate message 42d1145d-aeb2-4ccc-88f6-ed3008fbf5a4
02/05/2025 10:09:57:INFO:Received: evaluate message 42d1145d-aeb2-4ccc-88f6-ed3008fbf5a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:10:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:10:45:INFO:
[92mINFO [0m:      Received: train message ef7e5b1c-cfa8-48c4-b2dc-4f312cb06d73
02/05/2025 10:10:45:INFO:Received: train message ef7e5b1c-cfa8-48c4-b2dc-4f312cb06d73
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:11:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:12:32:INFO:
[92mINFO [0m:      Received: evaluate message e642df21-c30d-44d9-99fd-0e5cfdc090f6
02/05/2025 10:12:32:INFO:Received: evaluate message e642df21-c30d-44d9-99fd-0e5cfdc090f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:12:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:17:INFO:
[92mINFO [0m:      Received: train message 5b82fb71-b2c9-47bd-b3d4-d967bf992d2e
02/05/2025 10:13:17:INFO:Received: train message 5b82fb71-b2c9-47bd-b3d4-d967bf992d2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:05:INFO:
[92mINFO [0m:      Received: evaluate message 171d6cc6-b3f2-4812-9e5a-6531600be9b9
02/05/2025 10:15:05:INFO:Received: evaluate message 171d6cc6-b3f2-4812-9e5a-6531600be9b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:15:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:38:INFO:
[92mINFO [0m:      Received: train message b12c07ef-3103-4ee2-941b-325894df2606
02/05/2025 10:15:38:INFO:Received: train message b12c07ef-3103-4ee2-941b-325894df2606
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:16:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:23:INFO:
[92mINFO [0m:      Received: evaluate message cbe65a53-83bb-4f30-963c-ae592bdbb8b7
02/05/2025 10:17:23:INFO:Received: evaluate message cbe65a53-83bb-4f30-963c-ae592bdbb8b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:17:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:14:INFO:
[92mINFO [0m:      Received: train message 9b140a75-1d16-4be6-a83d-cf6f839df3b9
02/05/2025 10:18:14:INFO:Received: train message 9b140a75-1d16-4be6-a83d-cf6f839df3b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:07:INFO:
[92mINFO [0m:      Received: evaluate message 98761dac-19b5-47ed-a8b3-05ec59438a40
02/05/2025 10:21:07:INFO:Received: evaluate message 98761dac-19b5-47ed-a8b3-05ec59438a40
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:21:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:52:INFO:
[92mINFO [0m:      Received: train message dda0df09-3692-4c86-bdb1-e74ea2888ed0
02/05/2025 10:21:52:INFO:Received: train message dda0df09-3692-4c86-bdb1-e74ea2888ed0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:41:INFO:
[92mINFO [0m:      Received: evaluate message ae0e6010-4645-4a9d-a262-1dc633a3b0ff
02/05/2025 10:23:41:INFO:Received: evaluate message ae0e6010-4645-4a9d-a262-1dc633a3b0ff
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814], 'accuracy': [0.5175918686473807], 'auc': [0.714457107011283], 'precision': [0.4117264737633955], 'recall': [0.5175918686473807], 'f1': [0.42093829387716764]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771], 'accuracy': [0.5175918686473807, 0.5363565285379203], 'auc': [0.714457107011283, 0.7407919892413165], 'precision': [0.4117264737633955, 0.4429501750516807], 'recall': [0.5175918686473807, 0.5363565285379203], 'f1': [0.42093829387716764, 0.4824075932878562]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:24:INFO:
[92mINFO [0m:      Received: train message d497f8ad-0aa8-4586-906b-52cf2411f90a
02/05/2025 10:24:24:INFO:Received: train message d497f8ad-0aa8-4586-906b-52cf2411f90a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:25:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:13:INFO:
[92mINFO [0m:      Received: evaluate message 366d7a5d-e837-4187-af5a-b93fd7ff2b1f
02/05/2025 10:26:13:INFO:Received: evaluate message 366d7a5d-e837-4187-af5a-b93fd7ff2b1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:26:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:39:INFO:
[92mINFO [0m:      Received: train message 654447f9-8be2-48d0-955f-564ea2f99b95
02/05/2025 10:26:39:INFO:Received: train message 654447f9-8be2-48d0-955f-564ea2f99b95
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:27:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:36:INFO:
[92mINFO [0m:      Received: evaluate message 02bb3306-5eb7-494e-9974-7b6e0d2820ad
02/05/2025 10:28:36:INFO:Received: evaluate message 02bb3306-5eb7-494e-9974-7b6e0d2820ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:28:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:04:INFO:
[92mINFO [0m:      Received: train message 58f343a6-ffd4-45fb-9235-e2b566778905
02/05/2025 10:29:04:INFO:Received: train message 58f343a6-ffd4-45fb-9235-e2b566778905
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:15:INFO:
[92mINFO [0m:      Received: evaluate message 91fb19fc-b1b3-4ec7-a15b-65092cf9ac27
02/05/2025 10:31:15:INFO:Received: evaluate message 91fb19fc-b1b3-4ec7-a15b-65092cf9ac27
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:31:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:52:INFO:
[92mINFO [0m:      Received: train message 8d95a25a-5472-4425-9b87-256ca60fa9df
02/05/2025 10:31:52:INFO:Received: train message 8d95a25a-5472-4425-9b87-256ca60fa9df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:32:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:22:INFO:
[92mINFO [0m:      Received: evaluate message aee599d2-9599-4971-bc88-0d31a740934f
02/05/2025 10:33:22:INFO:Received: evaluate message aee599d2-9599-4971-bc88-0d31a740934f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:33:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:22:INFO:
[92mINFO [0m:      Received: train message 8a63dc4c-a70c-48a8-8e87-440341960d53
02/05/2025 10:34:22:INFO:Received: train message 8a63dc4c-a70c-48a8-8e87-440341960d53

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:35:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:07:INFO:
[92mINFO [0m:      Received: evaluate message a0f8b293-c3d6-416f-a7a0-d4135da2afb2
02/05/2025 10:36:07:INFO:Received: evaluate message a0f8b293-c3d6-416f-a7a0-d4135da2afb2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:36:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:37:INFO:
[92mINFO [0m:      Received: train message c58b537c-be3b-45e2-b18e-c07f25880d70
02/05/2025 10:36:37:INFO:Received: train message c58b537c-be3b-45e2-b18e-c07f25880d70
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:37:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:24:INFO:
[92mINFO [0m:      Received: evaluate message 6bfeafb9-7b62-4e17-bd5d-99bc25961897
02/05/2025 10:38:24:INFO:Received: evaluate message 6bfeafb9-7b62-4e17-bd5d-99bc25961897
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:38:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:15:INFO:
[92mINFO [0m:      Received: train message ba9cd53c-d63f-4042-8765-526c3b477189
02/05/2025 10:39:15:INFO:Received: train message ba9cd53c-d63f-4042-8765-526c3b477189
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:40:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:00:INFO:
[92mINFO [0m:      Received: evaluate message 6d1cf8bc-04b8-483b-a857-672243e38180
02/05/2025 10:41:00:INFO:Received: evaluate message 6d1cf8bc-04b8-483b-a857-672243e38180
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:41:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:42:INFO:
[92mINFO [0m:      Received: train message d928c0fe-7b9b-4c14-a8c2-d2146726386b
02/05/2025 10:41:42:INFO:Received: train message d928c0fe-7b9b-4c14-a8c2-d2146726386b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:23:INFO:
[92mINFO [0m:      Received: evaluate message 613f3744-41a1-427b-a7a9-db48d823a5f3
02/05/2025 10:43:23:INFO:Received: evaluate message 613f3744-41a1-427b-a7a9-db48d823a5f3
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:13:INFO:
[92mINFO [0m:      Received: train message 6ba482ec-73c6-492a-aad7-b262a6bd030e
02/05/2025 10:44:13:INFO:Received: train message 6ba482ec-73c6-492a-aad7-b262a6bd030e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:45:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:58:INFO:
[92mINFO [0m:      Received: evaluate message a5388192-b10c-48e7-8b71-3e5056fbfd30
02/05/2025 10:45:58:INFO:Received: evaluate message a5388192-b10c-48e7-8b71-3e5056fbfd30
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:46:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:54:INFO:
[92mINFO [0m:      Received: train message 647c7d83-3e86-4bc8-8afb-8785c84601a3
02/05/2025 10:46:54:INFO:Received: train message 647c7d83-3e86-4bc8-8afb-8785c84601a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:47:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:45:INFO:
[92mINFO [0m:      Received: evaluate message 64d9a6da-ee09-4fd4-8653-a8c0bab874c9
02/05/2025 10:48:46:INFO:Received: evaluate message 64d9a6da-ee09-4fd4-8653-a8c0bab874c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:48:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:51:INFO:
[92mINFO [0m:      Received: train message 2dc7a9f3-0887-4917-8121-eba16474834b
02/05/2025 10:49:51:INFO:Received: train message 2dc7a9f3-0887-4917-8121-eba16474834b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:50:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:08:INFO:
[92mINFO [0m:      Received: evaluate message a66b0ce5-bd27-47fd-a2a6-c75788752a4e
02/05/2025 10:52:08:INFO:Received: evaluate message a66b0ce5-bd27-47fd-a2a6-c75788752a4e

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:52:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:17:INFO:
[92mINFO [0m:      Received: train message a7ca4999-19f6-4bb3-a665-bc3f1d47ec30
02/05/2025 10:53:17:INFO:Received: train message a7ca4999-19f6-4bb3-a665-bc3f1d47ec30
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:54:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:55:15:INFO:
[92mINFO [0m:      Received: evaluate message b214f4f7-4fe8-4b3d-9374-46fa757efe2a
02/05/2025 10:55:15:INFO:Received: evaluate message b214f4f7-4fe8-4b3d-9374-46fa757efe2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:55:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:01:INFO:
[92mINFO [0m:      Received: train message c14d2ccf-74a7-42d4-b7cd-02f70dea890c
02/05/2025 10:57:01:INFO:Received: train message c14d2ccf-74a7-42d4-b7cd-02f70dea890c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:58:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:27:INFO:
[92mINFO [0m:      Received: evaluate message fa50cb8c-36c6-43f6-b743-512bc6dcb62b
02/05/2025 10:59:27:INFO:Received: evaluate message fa50cb8c-36c6-43f6-b743-512bc6dcb62b

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:59:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:35:INFO:
[92mINFO [0m:      Received: train message 02056b10-b53a-403a-a33c-a6d7741c41cb
02/05/2025 11:00:35:INFO:Received: train message 02056b10-b53a-403a-a33c-a6d7741c41cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:02:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:01:INFO:
[92mINFO [0m:      Received: evaluate message 03bfda48-4da6-4e5b-ae35-0c59ab3e185e
02/05/2025 11:03:01:INFO:Received: evaluate message 03bfda48-4da6-4e5b-ae35-0c59ab3e185e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:58:INFO:
[92mINFO [0m:      Received: train message b30fc871-1c92-425b-bc65-e2f031f76c9b
02/05/2025 11:04:58:INFO:Received: train message b30fc871-1c92-425b-bc65-e2f031f76c9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:06:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:07:41:INFO:
[92mINFO [0m:      Received: evaluate message 9d44a394-3ce9-4bed-a147-c6f82ecf64a8
02/05/2025 11:07:41:INFO:Received: evaluate message 9d44a394-3ce9-4bed-a147-c6f82ecf64a8

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:07:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:09:44:INFO:
[92mINFO [0m:      Received: train message 97e9cf5b-6a03-431f-ab0c-306a46a455e9
02/05/2025 11:09:44:INFO:Received: train message 97e9cf5b-6a03-431f-ab0c-306a46a455e9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:00:INFO:
[92mINFO [0m:      Received: evaluate message 44eed7ad-684e-4b2d-b5d4-9c1489eb1505
02/05/2025 11:13:00:INFO:Received: evaluate message 44eed7ad-684e-4b2d-b5d4-9c1489eb1505
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:31:INFO:
[92mINFO [0m:      Received: train message f96deaa8-b400-42bc-8e13-f5703ea6b987
02/05/2025 11:14:31:INFO:Received: train message f96deaa8-b400-42bc-8e13-f5703ea6b987
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:08:INFO:
[92mINFO [0m:      Received: evaluate message a1d7f6e0-cf25-4a73-9d50-0b006db22f16
02/05/2025 11:17:08:INFO:Received: evaluate message a1d7f6e0-cf25-4a73-9d50-0b006db22f16

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:51:INFO:
[92mINFO [0m:      Received: train message 2cf781c3-79db-48fb-821e-2c1a76c36331
02/05/2025 11:18:51:INFO:Received: train message 2cf781c3-79db-48fb-821e-2c1a76c36331
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:20:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:56:INFO:
[92mINFO [0m:      Received: evaluate message 6cb6a3e1-9b66-400e-938b-632f1d1ea3f6
02/05/2025 11:20:56:INFO:Received: evaluate message 6cb6a3e1-9b66-400e-938b-632f1d1ea3f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:21:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:24:INFO:
[92mINFO [0m:      Received: train message 5da20c95-62fe-4cdb-b4da-91325e259340
02/05/2025 11:21:24:INFO:Received: train message 5da20c95-62fe-4cdb-b4da-91325e259340
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:32:INFO:
[92mINFO [0m:      Received: evaluate message ac844f68-e350-4092-a070-48d434cf59f7
02/05/2025 11:23:32:INFO:Received: evaluate message ac844f68-e350-4092-a070-48d434cf59f7

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:21:INFO:
[92mINFO [0m:      Received: train message 291f79c3-523a-4058-8796-e60df8794a91
02/05/2025 11:24:21:INFO:Received: train message 291f79c3-523a-4058-8796-e60df8794a91
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:30:INFO:
[92mINFO [0m:      Received: evaluate message 54cf5d13-ed02-4351-a52c-2638d3db0c62
02/05/2025 11:26:30:INFO:Received: evaluate message 54cf5d13-ed02-4351-a52c-2638d3db0c62
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:25:INFO:
[92mINFO [0m:      Received: train message 5505418e-71b2-48ed-85f7-55444dfef768
02/05/2025 11:27:25:INFO:Received: train message 5505418e-71b2-48ed-85f7-55444dfef768
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:28:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:03:INFO:
[92mINFO [0m:      Received: evaluate message 1ac95691-e66c-4465-8020-89efdf459b8a
02/05/2025 11:29:03:INFO:Received: evaluate message 1ac95691-e66c-4465-8020-89efdf459b8a

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:30:10:INFO:
[92mINFO [0m:      Received: train message ba7e9940-4b2d-4411-84be-2ea4a48fdc57
02/05/2025 11:30:10:INFO:Received: train message ba7e9940-4b2d-4411-84be-2ea4a48fdc57
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:31:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:58:INFO:
[92mINFO [0m:      Received: evaluate message 3118c238-732e-4c9e-b9e4-883d42f7db14
02/05/2025 11:31:58:INFO:Received: evaluate message 3118c238-732e-4c9e-b9e4-883d42f7db14
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:32:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:54:INFO:
[92mINFO [0m:      Received: train message c8a3484c-0068-4a7f-92a7-67aadc3f0ff3
02/05/2025 11:32:54:INFO:Received: train message c8a3484c-0068-4a7f-92a7-67aadc3f0ff3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:33:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:48:INFO:
[92mINFO [0m:      Received: evaluate message 548dc3ad-c451-46ab-b36a-335ebc4400b8
02/05/2025 11:34:48:INFO:Received: evaluate message 548dc3ad-c451-46ab-b36a-335ebc4400b8

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:34:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:56:INFO:
[92mINFO [0m:      Received: reconnect message 921e2440-38df-4c9c-915c-63f6c539def5
02/05/2025 11:34:56:INFO:Received: reconnect message 921e2440-38df-4c9c-915c-63f6c539def5
02/05/2025 11:34:56:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:34:56:INFO:Disconnect and shut down

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}



Final client history:
{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}

