nohup: ignoring input
01/27/2025 02:55:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 02:55:31:DEBUG:ChannelConnectivity.IDLE
01/27/2025 02:55:31:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 02:55:32:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 02:56:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:56:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64d84c01-5f6d-432b-8722-e1a4c3b7e4a7
01/27/2025 02:56:00:INFO:Received: train message 64d84c01-5f6d-432b-8722-e1a4c3b7e4a7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:56:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82f90a5d-4a2b-40e9-8ffa-62954ab0ce66
01/27/2025 02:57:20:INFO:Received: evaluate message 82f90a5d-4a2b-40e9-8ffa-62954ab0ce66
[92mINFO [0m:      Sent reply
01/27/2025 02:57:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:58:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:58:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca8baede-687e-4d96-b9f4-e53893c827b0
01/27/2025 02:58:00:INFO:Received: train message ca8baede-687e-4d96-b9f4-e53893c827b0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:58:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5094be2b-9cea-4330-83f6-8ea6a287f968
01/27/2025 02:59:10:INFO:Received: evaluate message 5094be2b-9cea-4330-83f6-8ea6a287f968
[92mINFO [0m:      Sent reply
01/27/2025 02:59:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 20c44db2-b8f9-4463-b2aa-8e9b28d3119d
01/27/2025 02:59:44:INFO:Received: train message 20c44db2-b8f9-4463-b2aa-8e9b28d3119d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:00:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1c26b281-b5a5-43f6-958b-f97acdd0ff4c
01/27/2025 03:01:10:INFO:Received: evaluate message 1c26b281-b5a5-43f6-958b-f97acdd0ff4c
[92mINFO [0m:      Sent reply
01/27/2025 03:01:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ccbaf2e6-648f-4163-aa0d-09163ead260c
01/27/2025 03:01:49:INFO:Received: train message ccbaf2e6-648f-4163-aa0d-09163ead260c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:02:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:02:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:02:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bcba5433-df84-476b-ba0d-7a2425ac2297
01/27/2025 03:02:55:INFO:Received: evaluate message bcba5433-df84-476b-ba0d-7a2425ac2297
[92mINFO [0m:      Sent reply
01/27/2025 03:02:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2603ee1a-0fa3-4f5f-894f-3d30bd7fc97a
01/27/2025 03:03:38:INFO:Received: train message 2603ee1a-0fa3-4f5f-894f-3d30bd7fc97a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:04:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:04:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:04:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4aba3189-1b16-4419-96c4-84bd39982f7d
01/27/2025 03:04:42:INFO:Received: evaluate message 4aba3189-1b16-4419-96c4-84bd39982f7d
[92mINFO [0m:      Sent reply
01/27/2025 03:04:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f271bb5-05af-451f-ae4b-94ecc7e528a6
01/27/2025 03:05:27:INFO:Received: train message 7f271bb5-05af-451f-ae4b-94ecc7e528a6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:05:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:06:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:06:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5c271bbb-a62d-4e60-bd24-261fb1d83cc3
01/27/2025 03:06:55:INFO:Received: evaluate message 5c271bbb-a62d-4e60-bd24-261fb1d83cc3
[92mINFO [0m:      Sent reply
01/27/2025 03:07:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:07:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:07:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bff35d6c-6163-41d6-9a0b-8814126fcd47
01/27/2025 03:07:22:INFO:Received: train message bff35d6c-6163-41d6-9a0b-8814126fcd47
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:07:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:08:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:08:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eaf6c437-fce6-4d59-8ff6-0f06a1b513e9
01/27/2025 03:08:45:INFO:Received: evaluate message eaf6c437-fce6-4d59-8ff6-0f06a1b513e9
[92mINFO [0m:      Sent reply
01/27/2025 03:08:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:09:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:09:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94b18b8c-35e3-469b-b88f-829820aacb24
01/27/2025 03:09:18:INFO:Received: train message 94b18b8c-35e3-469b-b88f-829820aacb24
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597], 'accuracy': [0.5160281469898358], 'auc': [0.7330179566813546]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973], 'accuracy': [0.5160281469898358, 0.5191555903049258], 'auc': [0.7330179566813546, 0.750850648076995]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:09:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:10:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:10:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e434187a-7c7e-4e29-acb7-c34f7dda7557
01/27/2025 03:10:24:INFO:Received: evaluate message e434187a-7c7e-4e29-acb7-c34f7dda7557
[92mINFO [0m:      Sent reply
01/27/2025 03:10:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:11:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:11:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message edadd474-a0eb-4ebd-86c9-f677498bdfdd
01/27/2025 03:11:37:INFO:Received: train message edadd474-a0eb-4ebd-86c9-f677498bdfdd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:12:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:13:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:13:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3a94409b-92e2-4970-9ade-a526fd215c52
01/27/2025 03:13:18:INFO:Received: evaluate message 3a94409b-92e2-4970-9ade-a526fd215c52
[92mINFO [0m:      Sent reply
01/27/2025 03:13:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:13:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:13:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6c9cfe1f-ac06-4871-a70e-635202231ff3
01/27/2025 03:13:45:INFO:Received: train message 6c9cfe1f-ac06-4871-a70e-635202231ff3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:14:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fb2a9a2e-b550-4291-8375-bdc25f281c56
01/27/2025 03:15:21:INFO:Received: evaluate message fb2a9a2e-b550-4291-8375-bdc25f281c56
[92mINFO [0m:      Sent reply
01/27/2025 03:15:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:16:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:16:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5f4742f2-c300-417b-9bdc-420f75e75eb6
01/27/2025 03:16:01:INFO:Received: train message 5f4742f2-c300-417b-9bdc-420f75e75eb6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:16:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 332e2564-b217-4cb0-ba6a-5cd6c42a44ba
01/27/2025 03:17:14:INFO:Received: evaluate message 332e2564-b217-4cb0-ba6a-5cd6c42a44ba
[92mINFO [0m:      Sent reply
01/27/2025 03:17:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 601de09c-c89b-4423-adae-f8df2614e2fe
01/27/2025 03:17:46:INFO:Received: train message 601de09c-c89b-4423-adae-f8df2614e2fe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:18:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ca188c3-394c-4267-849e-109d05d87b59
01/27/2025 03:19:03:INFO:Received: evaluate message 5ca188c3-394c-4267-849e-109d05d87b59
[92mINFO [0m:      Sent reply
01/27/2025 03:19:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2daa6994-8a1f-4d7b-ba9c-3350251a6a72
01/27/2025 03:19:33:INFO:Received: train message 2daa6994-8a1f-4d7b-ba9c-3350251a6a72
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:20:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:20:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:20:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 279cbc8b-205b-4332-aafc-8eae37a42a9a
01/27/2025 03:20:43:INFO:Received: evaluate message 279cbc8b-205b-4332-aafc-8eae37a42a9a
[92mINFO [0m:      Sent reply
01/27/2025 03:20:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:21:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:21:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7821a6dd-df9e-4a9a-9c3d-f9b1d444f18c
01/27/2025 03:21:09:INFO:Received: train message 7821a6dd-df9e-4a9a-9c3d-f9b1d444f18c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:21:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:22:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:22:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c4a26816-1345-4a41-9435-79e11f9ccc2c
01/27/2025 03:22:31:INFO:Received: evaluate message c4a26816-1345-4a41-9435-79e11f9ccc2c
[92mINFO [0m:      Sent reply
01/27/2025 03:22:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:23:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:23:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 75fa9e54-8995-4a0c-b85a-bb7302da8a77
01/27/2025 03:23:17:INFO:Received: train message 75fa9e54-8995-4a0c-b85a-bb7302da8a77
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:23:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:24:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:24:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 289aad6c-3a5b-4ffd-92eb-1877a7554906
01/27/2025 03:24:27:INFO:Received: evaluate message 289aad6c-3a5b-4ffd-92eb-1877a7554906
[92mINFO [0m:      Sent reply
01/27/2025 03:24:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:25:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:25:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ee0dc7e1-69b3-419a-b2c9-15242624b1da
01/27/2025 03:25:06:INFO:Received: train message ee0dc7e1-69b3-419a-b2c9-15242624b1da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:25:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aebde0bd-bc9f-4722-8cac-83a4d31179be
01/27/2025 03:26:22:INFO:Received: evaluate message aebde0bd-bc9f-4722-8cac-83a4d31179be
[92mINFO [0m:      Sent reply
01/27/2025 03:26:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2131c841-d7a2-452a-84b6-3f65207dc289
01/27/2025 03:26:51:INFO:Received: train message 2131c841-d7a2-452a-84b6-3f65207dc289
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:27:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6dfd3d22-3436-4d9e-8920-1136367b691c
01/27/2025 03:28:10:INFO:Received: evaluate message 6dfd3d22-3436-4d9e-8920-1136367b691c
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:28:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b5671107-4257-4d13-b024-499db6167a19
01/27/2025 03:28:31:INFO:Received: train message b5671107-4257-4d13-b024-499db6167a19
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:29:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:29:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:29:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80152c40-4bae-4edc-b0c8-cee6e0d27ae3
01/27/2025 03:29:53:INFO:Received: evaluate message 80152c40-4bae-4edc-b0c8-cee6e0d27ae3
[92mINFO [0m:      Sent reply
01/27/2025 03:29:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:30:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:30:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4315c123-b0b7-40b4-9b6d-2086e32ae725
01/27/2025 03:30:18:INFO:Received: train message 4315c123-b0b7-40b4-9b6d-2086e32ae725
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:30:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:31:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:31:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a76c24be-96aa-480e-b6e5-b64f1698f865
01/27/2025 03:31:40:INFO:Received: evaluate message a76c24be-96aa-480e-b6e5-b64f1698f865
[92mINFO [0m:      Sent reply
01/27/2025 03:31:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:32:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:32:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c0fe3d8-d9fe-43f9-81cc-3c37e068a0a6
01/27/2025 03:32:06:INFO:Received: train message 0c0fe3d8-d9fe-43f9-81cc-3c37e068a0a6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:32:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fc368dcb-b095-4cfd-9ab7-e9facc4d1762
01/27/2025 03:33:28:INFO:Received: evaluate message fc368dcb-b095-4cfd-9ab7-e9facc4d1762
[92mINFO [0m:      Sent reply
01/27/2025 03:33:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:34:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:34:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77142d95-3831-47a3-8b2e-b595ea5c31b5
01/27/2025 03:34:14:INFO:Received: train message 77142d95-3831-47a3-8b2e-b595ea5c31b5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:34:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fbfae7f6-e4df-4ecc-891c-2d2d84308837
01/27/2025 03:35:13:INFO:Received: evaluate message fbfae7f6-e4df-4ecc-891c-2d2d84308837

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:35:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:36:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:36:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a2b43e4f-20ee-4a6a-b275-a225075eea86
01/27/2025 03:36:02:INFO:Received: train message a2b43e4f-20ee-4a6a-b275-a225075eea86
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:36:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:36:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:36:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 06e23997-0e44-4c05-8a9c-e84c191d526e
01/27/2025 03:36:59:INFO:Received: evaluate message 06e23997-0e44-4c05-8a9c-e84c191d526e
[92mINFO [0m:      Sent reply
01/27/2025 03:37:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 83db684e-972b-48ef-833b-dfb01b417ad8
01/27/2025 03:37:35:INFO:Received: train message 83db684e-972b-48ef-833b-dfb01b417ad8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:38:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:39:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:39:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47223b61-fd17-46eb-a2ec-cd733496902e
01/27/2025 03:39:28:INFO:Received: evaluate message 47223b61-fd17-46eb-a2ec-cd733496902e
[92mINFO [0m:      Sent reply
01/27/2025 03:39:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:40:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:40:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7e4c6daa-6589-40a0-b888-e8387b850166
01/27/2025 03:40:14:INFO:Received: train message 7e4c6daa-6589-40a0-b888-e8387b850166
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:40:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:41:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:41:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 912bf1dd-5f89-43d7-a6bb-d22693576d84
01/27/2025 03:41:39:INFO:Received: evaluate message 912bf1dd-5f89-43d7-a6bb-d22693576d84
[92mINFO [0m:      Sent reply
01/27/2025 03:41:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:42:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:42:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e7277a06-23b7-4031-92aa-fb03c040a863
01/27/2025 03:42:25:INFO:Received: train message e7277a06-23b7-4031-92aa-fb03c040a863

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:43:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:43:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:43:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4f66372c-0ebb-4dee-9fd6-b8f365c3160e
01/27/2025 03:43:21:INFO:Received: evaluate message 4f66372c-0ebb-4dee-9fd6-b8f365c3160e
[92mINFO [0m:      Sent reply
01/27/2025 03:43:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:44:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:44:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 170cd15c-efde-4b80-adf6-cebdc97ed16a
01/27/2025 03:44:06:INFO:Received: train message 170cd15c-efde-4b80-adf6-cebdc97ed16a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:44:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c736eaf-6289-485a-8a26-7d73f6a900f6
01/27/2025 03:45:12:INFO:Received: evaluate message 6c736eaf-6289-485a-8a26-7d73f6a900f6
[92mINFO [0m:      Sent reply
01/27/2025 03:45:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 784069f1-d0e0-471e-8315-ac9fd922c8ea
01/27/2025 03:45:56:INFO:Received: train message 784069f1-d0e0-471e-8315-ac9fd922c8ea
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:46:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:46:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:46:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 26cd92b6-7ed4-4443-89a4-f70361a27440
01/27/2025 03:46:48:INFO:Received: evaluate message 26cd92b6-7ed4-4443-89a4-f70361a27440
[92mINFO [0m:      Sent reply
01/27/2025 03:46:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1e24fd9e-96cb-4bd1-9881-c576a04db770
01/27/2025 03:47:40:INFO:Received: train message 1e24fd9e-96cb-4bd1-9881-c576a04db770
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:48:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:48:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:48:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0e4a304c-1e66-4ccf-9ea9-d88de08399da
01/27/2025 03:48:50:INFO:Received: evaluate message 0e4a304c-1e66-4ccf-9ea9-d88de08399da
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:48:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:49:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:49:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5afa5e57-bca8-46de-8d0c-a5a2b9daaaeb
01/27/2025 03:49:25:INFO:Received: train message 5afa5e57-bca8-46de-8d0c-a5a2b9daaaeb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:50:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5388642-9343-4643-8d9a-39ab4509449c
01/27/2025 03:50:29:INFO:Received: evaluate message e5388642-9343-4643-8d9a-39ab4509449c
[92mINFO [0m:      Sent reply
01/27/2025 03:50:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:51:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:51:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f1624eb-33ad-4b4b-896a-3bf875fccbd7
01/27/2025 03:51:11:INFO:Received: train message 7f1624eb-33ad-4b4b-896a-3bf875fccbd7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:51:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ce27499-866d-455f-84c3-3bba26c71e79
01/27/2025 03:52:23:INFO:Received: evaluate message 0ce27499-866d-455f-84c3-3bba26c71e79
[92mINFO [0m:      Sent reply
01/27/2025 03:52:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 9a9421ac-b9d0-450d-b811-2cdcf825ed38
01/27/2025 03:52:29:INFO:Received: reconnect message 9a9421ac-b9d0-450d-b811-2cdcf825ed38
01/27/2025 03:52:31:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 03:52:31:INFO:Disconnect and shut down

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5303955078125
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00985483918339014, 0.0029788902029395103, 0.03978271782398224, 0.04054459556937218]
Noise Multiplier after list and tensor:  0.023290260694921017
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}



Final client history:
{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}

