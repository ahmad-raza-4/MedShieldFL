nohup: ignoring input
01/27/2025 00:41:16:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 00:41:16:DEBUG:ChannelConnectivity.IDLE
01/27/2025 00:41:16:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 00:41:16:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 00:42:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:42:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9289c95-8758-4924-a6ee-996fd9027ad1
01/27/2025 00:42:09:INFO:Received: train message b9289c95-8758-4924-a6ee-996fd9027ad1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:42:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:43:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:43:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b1b7f31-b786-479e-9fb1-6bd04a6e2325
01/27/2025 00:43:40:INFO:Received: evaluate message 4b1b7f31-b786-479e-9fb1-6bd04a6e2325
[92mINFO [0m:      Sent reply
01/27/2025 00:43:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:44:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:44:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88606993-d8fb-42d8-9f10-0dbec288517d
01/27/2025 00:44:38:INFO:Received: train message 88606993-d8fb-42d8-9f10-0dbec288517d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:44:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:45:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:45:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 928f1f32-c380-4b4d-b3f4-89ebfe1ae305
01/27/2025 00:45:44:INFO:Received: evaluate message 928f1f32-c380-4b4d-b3f4-89ebfe1ae305
[92mINFO [0m:      Sent reply
01/27/2025 00:45:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:46:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:46:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88a9a2c2-7e05-4630-8b32-b2e55388e5c3
01/27/2025 00:46:16:INFO:Received: train message 88a9a2c2-7e05-4630-8b32-b2e55388e5c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:46:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2ceb621a-a2f8-4573-b523-5cf287b4eceb
01/27/2025 00:47:24:INFO:Received: evaluate message 2ceb621a-a2f8-4573-b523-5cf287b4eceb
[92mINFO [0m:      Sent reply
01/27/2025 00:47:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:48:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:48:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9e14131-9b43-486c-993c-b14121442932
01/27/2025 00:48:02:INFO:Received: train message b9e14131-9b43-486c-993c-b14121442932
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:48:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:48:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:48:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f428433-fe1c-42fd-97d9-5220709e90cf
01/27/2025 00:48:55:INFO:Received: evaluate message 3f428433-fe1c-42fd-97d9-5220709e90cf
[92mINFO [0m:      Sent reply
01/27/2025 00:48:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0f7eb6df-74b8-4e29-bfc8-dc03b3941461
01/27/2025 00:49:32:INFO:Received: train message 0f7eb6df-74b8-4e29-bfc8-dc03b3941461
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:49:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:50:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:50:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 96869c69-927a-44a4-b7e9-df3f6f7d445e
01/27/2025 00:50:39:INFO:Received: evaluate message 96869c69-927a-44a4-b7e9-df3f6f7d445e
[92mINFO [0m:      Sent reply
01/27/2025 00:50:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:51:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:51:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 519810f0-0b93-48a7-b6af-4598dd64d95a
01/27/2025 00:51:11:INFO:Received: train message 519810f0-0b93-48a7-b6af-4598dd64d95a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:51:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fc8eaf5e-395b-454a-a08c-e705938bc493
01/27/2025 00:52:05:INFO:Received: evaluate message fc8eaf5e-395b-454a-a08c-e705938bc493
[92mINFO [0m:      Sent reply
01/27/2025 00:52:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c149c176-69a8-4e67-bb06-a4c5187a25d0
01/27/2025 00:52:55:INFO:Received: train message c149c176-69a8-4e67-bb06-a4c5187a25d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:53:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:53:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:53:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4e8d5ca0-b564-4a59-93f7-d60300748b3b
01/27/2025 00:53:56:INFO:Received: evaluate message 4e8d5ca0-b564-4a59-93f7-d60300748b3b
[92mINFO [0m:      Sent reply
01/27/2025 00:54:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:54:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:54:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bafbd2a6-78cf-43c3-9edb-193ebd96b951
01/27/2025 00:54:30:INFO:Received: train message bafbd2a6-78cf-43c3-9edb-193ebd96b951
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406], 'accuracy': [0.5160281469898358], 'auc': [0.7097259125281268]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157], 'accuracy': [0.5160281469898358, 0.506645817044566], 'auc': [0.7097259125281268, 0.7279838714938965]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:54:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:55:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:55:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2c6b01be-7be7-4d96-995f-e30b278658b5
01/27/2025 00:55:40:INFO:Received: evaluate message 2c6b01be-7be7-4d96-995f-e30b278658b5
[92mINFO [0m:      Sent reply
01/27/2025 00:55:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:55:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:55:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 31439d84-42a7-4b9a-8cb4-11cf6477cc12
01/27/2025 00:55:58:INFO:Received: train message 31439d84-42a7-4b9a-8cb4-11cf6477cc12
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:56:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 142f96e6-6125-4f95-b923-34c8f55a7019
01/27/2025 00:57:17:INFO:Received: evaluate message 142f96e6-6125-4f95-b923-34c8f55a7019
[92mINFO [0m:      Sent reply
01/27/2025 00:57:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a670972d-933a-492b-adf6-4f3fe18503ac
01/27/2025 00:57:39:INFO:Received: train message a670972d-933a-492b-adf6-4f3fe18503ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:57:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:58:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:58:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 275e0af0-38e4-41a7-91fc-8ee4393cdf6d
01/27/2025 00:58:34:INFO:Received: evaluate message 275e0af0-38e4-41a7-91fc-8ee4393cdf6d
[92mINFO [0m:      Sent reply
01/27/2025 00:58:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:59:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:59:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2a55549d-04d2-43ad-be63-1e194c95e89d
01/27/2025 00:59:25:INFO:Received: train message 2a55549d-04d2-43ad-be63-1e194c95e89d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:59:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cdb48df1-8a6b-4307-ac36-a4e092663d1b
01/27/2025 01:00:05:INFO:Received: evaluate message cdb48df1-8a6b-4307-ac36-a4e092663d1b
[92mINFO [0m:      Sent reply
01/27/2025 01:00:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64f99945-53a2-4f31-8875-66878dfe48de
01/27/2025 01:00:56:INFO:Received: train message 64f99945-53a2-4f31-8875-66878dfe48de
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:01:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:01:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:01:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b518975-490f-48de-ad17-a5100b1334be
01/27/2025 01:01:40:INFO:Received: evaluate message 4b518975-490f-48de-ad17-a5100b1334be
[92mINFO [0m:      Sent reply
01/27/2025 01:01:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:02:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:02:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6011a801-65dc-4993-8ba1-d4c8becaaddb
01/27/2025 01:02:18:INFO:Received: train message 6011a801-65dc-4993-8ba1-d4c8becaaddb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:02:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1780dc9b-d02c-445d-9d8f-6fb02cf6f5e2
01/27/2025 01:03:13:INFO:Received: evaluate message 1780dc9b-d02c-445d-9d8f-6fb02cf6f5e2
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:03:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9f221019-c83c-428a-88d6-2a6595fc4d68
01/27/2025 01:03:26:INFO:Received: train message 9f221019-c83c-428a-88d6-2a6595fc4d68
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:03:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c2326c5-5611-408f-9b3d-9fd89dd093de
01/27/2025 01:04:14:INFO:Received: evaluate message 4c2326c5-5611-408f-9b3d-9fd89dd093de
[92mINFO [0m:      Sent reply
01/27/2025 01:04:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 835c3c81-14c5-4eca-a773-a952bfaf30bc
01/27/2025 01:04:56:INFO:Received: train message 835c3c81-14c5-4eca-a773-a952bfaf30bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:05:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c729a3e8-2eec-4271-a7a5-946addb0ff11
01/27/2025 01:05:39:INFO:Received: evaluate message c729a3e8-2eec-4271-a7a5-946addb0ff11
[92mINFO [0m:      Sent reply
01/27/2025 01:05:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:06:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:06:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7c45de99-7e02-406b-bcf9-f3dd3eed906e
01/27/2025 01:06:24:INFO:Received: train message 7c45de99-7e02-406b-bcf9-f3dd3eed906e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:06:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a6088c8b-2668-4430-a832-1f95dea3faf5
01/27/2025 01:07:13:INFO:Received: evaluate message a6088c8b-2668-4430-a832-1f95dea3faf5
[92mINFO [0m:      Sent reply
01/27/2025 01:07:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ae4a909-55fe-48a7-83d1-9597b19afe74
01/27/2025 01:07:57:INFO:Received: train message 4ae4a909-55fe-48a7-83d1-9597b19afe74
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:08:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f3326b9-3346-4b57-80cf-ed34c40ce810
01/27/2025 01:08:47:INFO:Received: evaluate message 3f3326b9-3346-4b57-80cf-ed34c40ce810
[92mINFO [0m:      Sent reply
01/27/2025 01:08:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:09:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:09:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d3156d6b-a1f1-457c-91cc-cbfc9ae9fe7e
01/27/2025 01:09:22:INFO:Received: train message d3156d6b-a1f1-457c-91cc-cbfc9ae9fe7e

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:09:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64a79483-2b21-48a0-860d-8e3440dd7378
01/27/2025 01:10:13:INFO:Received: evaluate message 64a79483-2b21-48a0-860d-8e3440dd7378
[92mINFO [0m:      Sent reply
01/27/2025 01:10:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 86ea84f9-f8dd-425c-8d32-820ecabea12f
01/27/2025 01:10:40:INFO:Received: train message 86ea84f9-f8dd-425c-8d32-820ecabea12f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:10:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:11:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:11:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45d64882-d05e-4e89-89d5-4061c729b738
01/27/2025 01:11:39:INFO:Received: evaluate message 45d64882-d05e-4e89-89d5-4061c729b738
[92mINFO [0m:      Sent reply
01/27/2025 01:11:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:12:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:12:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5f0560c1-fd39-4c0e-8758-f8c41fc12739
01/27/2025 01:12:04:INFO:Received: train message 5f0560c1-fd39-4c0e-8758-f8c41fc12739
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:12:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:13:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:13:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c395dff3-1bbb-4504-8ac2-cf5fba936ad2
01/27/2025 01:13:26:INFO:Received: evaluate message c395dff3-1bbb-4504-8ac2-cf5fba936ad2
[92mINFO [0m:      Sent reply
01/27/2025 01:13:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:14:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:14:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f012abfc-c224-4d5f-b904-03af53787c97
01/27/2025 01:14:06:INFO:Received: train message f012abfc-c224-4d5f-b904-03af53787c97
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:14:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 297b3f50-0be8-425b-9850-7509fcbe2687
01/27/2025 01:15:13:INFO:Received: evaluate message 297b3f50-0be8-425b-9850-7509fcbe2687
[92mINFO [0m:      Sent reply
01/27/2025 01:15:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0f451565-9706-4108-a776-d2689b2238aa
01/27/2025 01:15:41:INFO:Received: train message 0f451565-9706-4108-a776-d2689b2238aa
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:15:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:16:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:16:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee23bca8-b34d-417a-b00f-7121e90a075e
01/27/2025 01:16:45:INFO:Received: evaluate message ee23bca8-b34d-417a-b00f-7121e90a075e
[92mINFO [0m:      Sent reply
01/27/2025 01:16:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca8dc87e-1530-41e1-afaa-3ef5885ecf13
01/27/2025 01:17:02:INFO:Received: train message ca8dc87e-1530-41e1-afaa-3ef5885ecf13
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:17:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 342e0407-d228-4ccf-92e4-3e737c56fece
01/27/2025 01:17:59:INFO:Received: evaluate message 342e0407-d228-4ccf-92e4-3e737c56fece
[92mINFO [0m:      Sent reply
01/27/2025 01:18:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e698b97e-86ab-4b54-8e77-7fa422884cd9
01/27/2025 01:18:32:INFO:Received: train message e698b97e-86ab-4b54-8e77-7fa422884cd9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:18:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5cdeb0e4-8031-4b78-9583-3051e01b2b38
01/27/2025 01:19:22:INFO:Received: evaluate message 5cdeb0e4-8031-4b78-9583-3051e01b2b38
[92mINFO [0m:      Sent reply
01/27/2025 01:19:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e32f517c-bafc-4d56-9ae5-e67b100d4f02
01/27/2025 01:19:52:INFO:Received: train message e32f517c-bafc-4d56-9ae5-e67b100d4f02
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:20:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:20:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:20:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fae85d7d-b216-4ea2-aa44-888f7a546d6a
01/27/2025 01:20:30:INFO:Received: evaluate message fae85d7d-b216-4ea2-aa44-888f7a546d6a
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:20:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f370e9fd-6623-4001-bb81-7fbfa2f77f38
01/27/2025 01:21:06:INFO:Received: train message f370e9fd-6623-4001-bb81-7fbfa2f77f38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:21:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 72392027-853a-4023-b68e-0b66ca90a8f8
01/27/2025 01:21:58:INFO:Received: evaluate message 72392027-853a-4023-b68e-0b66ca90a8f8
[92mINFO [0m:      Sent reply
01/27/2025 01:22:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f2daa54-3753-499d-a2c8-a8c723c0329f
01/27/2025 01:22:46:INFO:Received: train message 7f2daa54-3753-499d-a2c8-a8c723c0329f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:22:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f5a4237-25a2-47f9-9a43-16cc7448213b
01/27/2025 01:23:29:INFO:Received: evaluate message 0f5a4237-25a2-47f9-9a43-16cc7448213b
[92mINFO [0m:      Sent reply
01/27/2025 01:23:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b6975525-046f-49f7-876b-2b6c7b6f095c
01/27/2025 01:24:09:INFO:Received: train message b6975525-046f-49f7-876b-2b6c7b6f095c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:24:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ed4e4672-5561-4c03-a368-156d2ed0a1e1
01/27/2025 01:24:45:INFO:Received: evaluate message ed4e4672-5561-4c03-a368-156d2ed0a1e1

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:24:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b54b6490-6d79-47ee-ac46-bb8a597d662a
01/27/2025 01:25:32:INFO:Received: train message b54b6490-6d79-47ee-ac46-bb8a597d662a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:25:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3fa39179-7386-4351-b009-eb8846e264b2
01/27/2025 01:26:26:INFO:Received: evaluate message 3fa39179-7386-4351-b009-eb8846e264b2
[92mINFO [0m:      Sent reply
01/27/2025 01:26:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0971dd37-639e-439c-9626-ec9f479aa6b7
01/27/2025 01:26:53:INFO:Received: train message 0971dd37-639e-439c-9626-ec9f479aa6b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:27:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4f8b584e-d798-4a95-ac4a-e5a9bde99548
01/27/2025 01:27:40:INFO:Received: evaluate message 4f8b584e-d798-4a95-ac4a-e5a9bde99548
[92mINFO [0m:      Sent reply
01/27/2025 01:27:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 476b7b2d-d42c-44c3-8ec5-8fddd3ce7d33
01/27/2025 01:27:53:INFO:Received: reconnect message 476b7b2d-d42c-44c3-8ec5-8fddd3ce7d33
01/27/2025 01:27:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 01:27:53:INFO:Disconnect and shut down

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.05302295461297035, 0.03289469704031944, 0.08506232500076294, 0.2705058455467224]
Noise Multiplier after list and tensor:  0.11037145555019379
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}



Final client history:
{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}

