nohup: ignoring input
01/27/2025 01:39:24:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 01:39:24:DEBUG:ChannelConnectivity.IDLE
01/27/2025 01:39:24:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 01:39:24:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 01:39:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:39:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 72d6b413-d153-4e5d-8161-efbd8576295c
01/27/2025 01:39:51:INFO:Received: train message 72d6b413-d153-4e5d-8161-efbd8576295c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:40:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:41:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:41:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dfba094e-3a0a-4b80-b0f2-234df626ae81
01/27/2025 01:41:48:INFO:Received: evaluate message dfba094e-3a0a-4b80-b0f2-234df626ae81
[92mINFO [0m:      Sent reply
01/27/2025 01:41:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:42:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:42:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7da7825b-2767-431e-8f68-9ff241ada9b8
01/27/2025 01:42:53:INFO:Received: train message 7da7825b-2767-431e-8f68-9ff241ada9b8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:43:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9d053719-444e-41de-b63d-caf7eede1f7e
01/27/2025 01:44:08:INFO:Received: evaluate message 9d053719-444e-41de-b63d-caf7eede1f7e
[92mINFO [0m:      Sent reply
01/27/2025 01:44:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ea6d8897-b121-45c7-9f9f-93e54b331ccd
01/27/2025 01:44:30:INFO:Received: train message ea6d8897-b121-45c7-9f9f-93e54b331ccd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:44:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:45:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:45:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6a2a8290-748a-44d9-9bb6-d5b893958e75
01/27/2025 01:45:33:INFO:Received: evaluate message 6a2a8290-748a-44d9-9bb6-d5b893958e75
[92mINFO [0m:      Sent reply
01/27/2025 01:45:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:45:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:45:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0649cb0e-4a74-43bc-98e2-dc53e3df417f
01/27/2025 01:45:51:INFO:Received: train message 0649cb0e-4a74-43bc-98e2-dc53e3df417f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:46:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:46:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:46:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1945e80-1ab3-4607-a8f4-401dce946e73
01/27/2025 01:46:52:INFO:Received: evaluate message e1945e80-1ab3-4607-a8f4-401dce946e73
[92mINFO [0m:      Sent reply
01/27/2025 01:46:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4dd3c258-c326-431a-81a0-745fd3ddab92
01/27/2025 01:47:40:INFO:Received: train message 4dd3c258-c326-431a-81a0-745fd3ddab92
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:48:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:48:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:48:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3ccb9cb0-12a4-415d-aeee-b05f0608f6c6
01/27/2025 01:48:37:INFO:Received: evaluate message 3ccb9cb0-12a4-415d-aeee-b05f0608f6c6
[92mINFO [0m:      Sent reply
01/27/2025 01:48:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:49:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:49:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 633dbf7c-b7e2-4a67-be26-8a94dc103b65
01/27/2025 01:49:07:INFO:Received: train message 633dbf7c-b7e2-4a67-be26-8a94dc103b65
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:49:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7e8ccd9b-1dee-4afc-b9dd-91fe96717e5f
01/27/2025 01:50:13:INFO:Received: evaluate message 7e8ccd9b-1dee-4afc-b9dd-91fe96717e5f
[92mINFO [0m:      Sent reply
01/27/2025 01:50:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 331c9a37-9fa8-4803-ae21-0caf2b0e697b
01/27/2025 01:50:36:INFO:Received: train message 331c9a37-9fa8-4803-ae21-0caf2b0e697b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:51:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:51:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:51:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0e4d6af9-6b41-4aa4-b4f6-977b9aec2456
01/27/2025 01:51:45:INFO:Received: evaluate message 0e4d6af9-6b41-4aa4-b4f6-977b9aec2456
[92mINFO [0m:      Sent reply
01/27/2025 01:51:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:52:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:52:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 241eb5a0-1a75-453e-bd07-8cc2637966e4
01/27/2025 01:52:22:INFO:Received: train message 241eb5a0-1a75-453e-bd07-8cc2637966e4
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379], 'accuracy': [0.5215011727912432], 'auc': [0.7229562347935747]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828], 'accuracy': [0.5215011727912432, 0.5175918686473807], 'auc': [0.7229562347935747, 0.7423497364044646]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:52:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 08ee8b20-f60c-437b-9541-e556ac82cdea
01/27/2025 01:53:14:INFO:Received: evaluate message 08ee8b20-f60c-437b-9541-e556ac82cdea
[92mINFO [0m:      Sent reply
01/27/2025 01:53:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9da81531-1f72-41f7-923e-0f0520b2f706
01/27/2025 01:53:32:INFO:Received: train message 9da81531-1f72-41f7-923e-0f0520b2f706
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:53:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:54:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:54:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 04ccbab6-9737-4f68-b706-f2b4640ebd55
01/27/2025 01:54:32:INFO:Received: evaluate message 04ccbab6-9737-4f68-b706-f2b4640ebd55
[92mINFO [0m:      Sent reply
01/27/2025 01:54:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:55:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:55:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3faeb674-4ca4-4d16-9eb6-51432657bc8e
01/27/2025 01:55:10:INFO:Received: train message 3faeb674-4ca4-4d16-9eb6-51432657bc8e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:55:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c55e1b6c-9096-4356-a2af-3f7867f03ad6
01/27/2025 01:56:11:INFO:Received: evaluate message c55e1b6c-9096-4356-a2af-3f7867f03ad6
[92mINFO [0m:      Sent reply
01/27/2025 01:56:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c9a57bc7-602d-41a7-b000-688b6bff2453
01/27/2025 01:56:50:INFO:Received: train message c9a57bc7-602d-41a7-b000-688b6bff2453
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:57:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:57:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:57:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 286747f7-d77f-4cff-b6c4-341c3fb9a60b
01/27/2025 01:57:41:INFO:Received: evaluate message 286747f7-d77f-4cff-b6c4-341c3fb9a60b
[92mINFO [0m:      Sent reply
01/27/2025 01:57:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:58:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:58:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b6e0da8-62a5-469b-afa7-413389ab18b2
01/27/2025 01:58:25:INFO:Received: train message 1b6e0da8-62a5-469b-afa7-413389ab18b2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:58:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0945fab-c9c9-440b-a6b2-588ae272a2bb
01/27/2025 01:59:24:INFO:Received: evaluate message f0945fab-c9c9-440b-a6b2-588ae272a2bb
[92mINFO [0m:      Sent reply
01/27/2025 01:59:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1235c1f5-1097-4440-8552-4f66a2b73fd8
01/27/2025 01:59:45:INFO:Received: train message 1235c1f5-1097-4440-8552-4f66a2b73fd8
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:00:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:00:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:00:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 88637b81-0b64-4aba-a1fc-4c700ea968a0
01/27/2025 02:00:52:INFO:Received: evaluate message 88637b81-0b64-4aba-a1fc-4c700ea968a0
[92mINFO [0m:      Sent reply
01/27/2025 02:00:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:01:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:01:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 56dfa5af-822d-454f-acbf-b124f174d78a
01/27/2025 02:01:37:INFO:Received: train message 56dfa5af-822d-454f-acbf-b124f174d78a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:02:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:02:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:02:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6524c687-bfce-4be2-a118-d6eb63106f57
01/27/2025 02:02:52:INFO:Received: evaluate message 6524c687-bfce-4be2-a118-d6eb63106f57
[92mINFO [0m:      Sent reply
01/27/2025 02:02:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:03:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:03:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c56bc036-2595-401f-bd37-874aa1bd9415
01/27/2025 02:03:17:INFO:Received: train message c56bc036-2595-401f-bd37-874aa1bd9415
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:03:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:04:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:04:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 094a6d8a-362d-49ce-a347-b7787cffcd5c
01/27/2025 02:04:27:INFO:Received: evaluate message 094a6d8a-362d-49ce-a347-b7787cffcd5c
[92mINFO [0m:      Sent reply
01/27/2025 02:04:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:05:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:05:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3ba5a2b0-cdad-4c99-ace8-141de59344c6
01/27/2025 02:05:08:INFO:Received: train message 3ba5a2b0-cdad-4c99-ace8-141de59344c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:05:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:06:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:06:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ebcbfe4-42c7-4212-8042-28a5344d3ab0
01/27/2025 02:06:33:INFO:Received: evaluate message 0ebcbfe4-42c7-4212-8042-28a5344d3ab0
[92mINFO [0m:      Sent reply
01/27/2025 02:06:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:07:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:07:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 50372ee4-bf65-4d12-be66-50e56edbaba9
01/27/2025 02:07:22:INFO:Received: train message 50372ee4-bf65-4d12-be66-50e56edbaba9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:07:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:08:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:08:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 73ce9306-fffb-4241-963e-5a850c0d70bb
01/27/2025 02:08:36:INFO:Received: evaluate message 73ce9306-fffb-4241-963e-5a850c0d70bb
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:08:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:09:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:09:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30d5ab60-ab8e-444d-b507-5e1d0d27957c
01/27/2025 02:09:50:INFO:Received: train message 30d5ab60-ab8e-444d-b507-5e1d0d27957c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:10:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:11:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:11:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aa9652a5-761a-4564-95c8-6c6ad09f5627
01/27/2025 02:11:26:INFO:Received: evaluate message aa9652a5-761a-4564-95c8-6c6ad09f5627
[92mINFO [0m:      Sent reply
01/27/2025 02:11:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:11:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:11:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0f54177f-2f65-4afd-ba6b-69a71421ceea
01/27/2025 02:11:58:INFO:Received: train message 0f54177f-2f65-4afd-ba6b-69a71421ceea
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:12:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c5bc725-2823-4dd1-8c1c-c5af18de4634
01/27/2025 02:13:15:INFO:Received: evaluate message 6c5bc725-2823-4dd1-8c1c-c5af18de4634
[92mINFO [0m:      Sent reply
01/27/2025 02:13:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9a3c34e-3c5d-48b4-8293-114938e9cb9f
01/27/2025 02:13:33:INFO:Received: train message b9a3c34e-3c5d-48b4-8293-114938e9cb9f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:14:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:15:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:15:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 77375149-d49e-4a04-bcac-be465a586326
01/27/2025 02:15:01:INFO:Received: evaluate message 77375149-d49e-4a04-bcac-be465a586326
[92mINFO [0m:      Sent reply
01/27/2025 02:15:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:15:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:15:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf3ddece-985e-47d3-bb7b-9a2220563430
01/27/2025 02:15:25:INFO:Received: train message cf3ddece-985e-47d3-bb7b-9a2220563430
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:15:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:16:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:16:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 44e8beef-af47-4f40-8784-e814bc1281dc
01/27/2025 02:16:41:INFO:Received: evaluate message 44e8beef-af47-4f40-8784-e814bc1281dc

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:16:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:17:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:17:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 41eed1d8-88f4-4586-9379-2e4522fecb67
01/27/2025 02:17:16:INFO:Received: train message 41eed1d8-88f4-4586-9379-2e4522fecb67
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:17:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:18:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:18:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 252755ca-74e6-43b1-99cc-a7d42c15b440
01/27/2025 02:18:24:INFO:Received: evaluate message 252755ca-74e6-43b1-99cc-a7d42c15b440
[92mINFO [0m:      Sent reply
01/27/2025 02:18:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:19:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:19:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78967867-0670-401b-a2f1-6851be99208f
01/27/2025 02:19:01:INFO:Received: train message 78967867-0670-401b-a2f1-6851be99208f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:19:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 78d0ab19-5983-4e75-bd1a-240aef572683
01/27/2025 02:20:03:INFO:Received: evaluate message 78d0ab19-5983-4e75-bd1a-240aef572683
[92mINFO [0m:      Sent reply
01/27/2025 02:20:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5144fb6f-5cf9-4fcd-947e-f999f8b6749f
01/27/2025 02:20:57:INFO:Received: train message 5144fb6f-5cf9-4fcd-947e-f999f8b6749f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:21:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ebe0b20-25b8-4d5b-a23c-60dddab56012
01/27/2025 02:22:13:INFO:Received: evaluate message 8ebe0b20-25b8-4d5b-a23c-60dddab56012
[92mINFO [0m:      Sent reply
01/27/2025 02:22:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bee11192-e1ea-4ccd-a185-9327f56a4fef
01/27/2025 02:22:36:INFO:Received: train message bee11192-e1ea-4ccd-a185-9327f56a4fef

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:23:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:23:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:23:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3fd2432b-0407-4e33-87ee-8086c016bc5a
01/27/2025 02:23:55:INFO:Received: evaluate message 3fd2432b-0407-4e33-87ee-8086c016bc5a
[92mINFO [0m:      Sent reply
01/27/2025 02:24:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:24:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:24:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 76eb899a-ba74-42f3-87df-502051b58b79
01/27/2025 02:24:33:INFO:Received: train message 76eb899a-ba74-42f3-87df-502051b58b79
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:25:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:25:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:25:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 101336cf-fa79-4187-89fa-20db1f154fb9
01/27/2025 02:25:31:INFO:Received: evaluate message 101336cf-fa79-4187-89fa-20db1f154fb9
[92mINFO [0m:      Sent reply
01/27/2025 02:25:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:26:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:26:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bfa13809-be64-4eb6-ba36-56d682fd6706
01/27/2025 02:26:08:INFO:Received: train message bfa13809-be64-4eb6-ba36-56d682fd6706
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:26:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4119a0a4-f277-4e6e-bed3-198cc19ca7d5
01/27/2025 02:27:17:INFO:Received: evaluate message 4119a0a4-f277-4e6e-bed3-198cc19ca7d5
[92mINFO [0m:      Sent reply
01/27/2025 02:27:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 11b2597e-8c65-4cc4-b438-0c5ba68c0ad0
01/27/2025 02:27:53:INFO:Received: train message 11b2597e-8c65-4cc4-b438-0c5ba68c0ad0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:28:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cdf2f722-4a86-471d-8b0f-e3b111b18e31
01/27/2025 02:29:02:INFO:Received: evaluate message cdf2f722-4a86-471d-8b0f-e3b111b18e31
0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:29:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c855c047-8017-4d51-8a9a-64cf035b0971
01/27/2025 02:29:37:INFO:Received: train message c855c047-8017-4d51-8a9a-64cf035b0971
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:30:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:30:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:30:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec2b7c95-120e-4277-a3fb-45d2f06c20ff
01/27/2025 02:30:51:INFO:Received: evaluate message ec2b7c95-120e-4277-a3fb-45d2f06c20ff
[92mINFO [0m:      Sent reply
01/27/2025 02:30:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:31:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:31:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d84d9704-2eab-41e3-a856-74ebc4807f28
01/27/2025 02:31:07:INFO:Received: train message d84d9704-2eab-41e3-a856-74ebc4807f28
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:31:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34cbd744-42a4-487b-8dd8-9f802ff0d2e6
01/27/2025 02:32:31:INFO:Received: evaluate message 34cbd744-42a4-487b-8dd8-9f802ff0d2e6
[92mINFO [0m:      Sent reply
01/27/2025 02:32:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message f7c0edf5-688f-4901-93bc-baebfbf589e1
01/27/2025 02:32:40:INFO:Received: reconnect message f7c0edf5-688f-4901-93bc-baebfbf589e1
01/27/2025 02:32:40:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 02:32:40:INFO:Disconnect and shut down

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.012501605786383152, 0.007755820173770189, 0.02005576156079769, 0.06377912312746048]
Noise Multiplier after list and tensor:  0.026023077662102878
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}



Final client history:
{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}

