nohup: ignoring input
01/27/2025 01:39:21:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 01:39:21:DEBUG:ChannelConnectivity.IDLE
01/27/2025 01:39:21:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 01:39:21:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 01:39:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:39:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message cd4add45-3edf-4f26-99d8-bafefbfabeb8
01/27/2025 01:39:21:INFO:Received: get_parameters message cd4add45-3edf-4f26-99d8-bafefbfabeb8
[92mINFO [0m:      Sent reply
01/27/2025 01:39:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:40:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:40:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f9da22a9-249f-42c9-9631-66eb4f835950
01/27/2025 01:40:01:INFO:Received: train message f9da22a9-249f-42c9-9631-66eb4f835950
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:40:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:41:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:41:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 18d5f03f-92bb-4665-997d-45c0c463e724
01/27/2025 01:41:56:INFO:Received: evaluate message 18d5f03f-92bb-4665-997d-45c0c463e724
[92mINFO [0m:      Sent reply
01/27/2025 01:42:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:42:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:42:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 37ca54b5-33a1-482a-8db4-1ad98e17bcd3
01/27/2025 01:42:47:INFO:Received: train message 37ca54b5-33a1-482a-8db4-1ad98e17bcd3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:43:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 241337f9-4bdf-41c4-b074-319cb5387c7d
01/27/2025 01:44:06:INFO:Received: evaluate message 241337f9-4bdf-41c4-b074-319cb5387c7d
[92mINFO [0m:      Sent reply
01/27/2025 01:44:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5e9eaa6-5f06-4ecb-b2dc-153f137a618a
01/27/2025 01:44:26:INFO:Received: train message f5e9eaa6-5f06-4ecb-b2dc-153f137a618a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:44:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:45:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:45:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c52d8a6-7a3e-4e45-8aa4-5501db41f305
01/27/2025 01:45:22:INFO:Received: evaluate message 4c52d8a6-7a3e-4e45-8aa4-5501db41f305
[92mINFO [0m:      Sent reply
01/27/2025 01:45:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:46:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:46:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f2341539-c9ee-42ad-ab18-230a4b634bbd
01/27/2025 01:46:12:INFO:Received: train message f2341539-c9ee-42ad-ab18-230a4b634bbd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:46:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 910a0c25-8071-4c6b-88e3-3cefd9c2355d
01/27/2025 01:47:09:INFO:Received: evaluate message 910a0c25-8071-4c6b-88e3-3cefd9c2355d
[92mINFO [0m:      Sent reply
01/27/2025 01:47:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 224733e8-2765-4508-8a7b-94b037437bac
01/27/2025 01:47:39:INFO:Received: train message 224733e8-2765-4508-8a7b-94b037437bac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:47:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:48:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:48:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5faae9f0-e4b2-4fc9-a2f7-b1f0af2bc2e9
01/27/2025 01:48:26:INFO:Received: evaluate message 5faae9f0-e4b2-4fc9-a2f7-b1f0af2bc2e9
[92mINFO [0m:      Sent reply
01/27/2025 01:48:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:48:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:48:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1e55943-66d7-4195-9153-27db4d2747ef
01/27/2025 01:48:55:INFO:Received: train message c1e55943-66d7-4195-9153-27db4d2747ef
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:49:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f5dcbb72-f0fd-4c3c-9fc4-24adb9af4ce4
01/27/2025 01:50:07:INFO:Received: evaluate message f5dcbb72-f0fd-4c3c-9fc4-24adb9af4ce4
[92mINFO [0m:      Sent reply
01/27/2025 01:50:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4d266152-c35f-4afe-b266-d2abcec19880
01/27/2025 01:50:29:INFO:Received: train message 4d266152-c35f-4afe-b266-d2abcec19880
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:50:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:51:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:51:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3e4230c2-f9de-43fa-be8e-2ed0e20cd052
01/27/2025 01:51:44:INFO:Received: evaluate message 3e4230c2-f9de-43fa-be8e-2ed0e20cd052
[92mINFO [0m:      Sent reply
01/27/2025 01:51:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:52:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:52:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0ec437a0-630e-4f53-ab39-10c57575efd4
01/27/2025 01:52:13:INFO:Received: train message 0ec437a0-630e-4f53-ab39-10c57575efd4
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379], 'accuracy': [0.5215011727912432], 'auc': [0.7229562347935747]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828], 'accuracy': [0.5215011727912432, 0.5175918686473807], 'auc': [0.7229562347935747, 0.7423497364044646]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:52:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 890bd59c-3ff0-408d-88a9-e81435356cda
01/27/2025 01:53:13:INFO:Received: evaluate message 890bd59c-3ff0-408d-88a9-e81435356cda
[92mINFO [0m:      Sent reply
01/27/2025 01:53:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4faf2110-b7d8-4cef-b58a-69851e5b1b17
01/27/2025 01:53:47:INFO:Received: train message 4faf2110-b7d8-4cef-b58a-69851e5b1b17
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:54:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:54:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:54:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2f9d684e-0897-4563-97cc-fc3905747bdf
01/27/2025 01:54:45:INFO:Received: evaluate message 2f9d684e-0897-4563-97cc-fc3905747bdf
[92mINFO [0m:      Sent reply
01/27/2025 01:54:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:55:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:55:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 74e430b8-4606-4990-8f5d-faddecc3df24
01/27/2025 01:55:19:INFO:Received: train message 74e430b8-4606-4990-8f5d-faddecc3df24
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:55:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a3f15de0-3f11-4e9a-90a0-246543ae9dc0
01/27/2025 01:56:11:INFO:Received: evaluate message a3f15de0-3f11-4e9a-90a0-246543ae9dc0
[92mINFO [0m:      Sent reply
01/27/2025 01:56:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f41888b9-8eb6-435c-a927-dc0c43afb4e9
01/27/2025 01:56:52:INFO:Received: train message f41888b9-8eb6-435c-a927-dc0c43afb4e9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:57:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:57:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:57:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message affb19fb-f353-4c44-800b-9ef56f26fbbf
01/27/2025 01:57:49:INFO:Received: evaluate message affb19fb-f353-4c44-800b-9ef56f26fbbf
[92mINFO [0m:      Sent reply
01/27/2025 01:57:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:58:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:58:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ec332c1-1d81-4664-9eb5-1c0e69133f05
01/27/2025 01:58:17:INFO:Received: train message 6ec332c1-1d81-4664-9eb5-1c0e69133f05
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:58:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 69b533d5-7034-4fb3-9a85-16be86edcb09
01/27/2025 01:59:04:INFO:Received: evaluate message 69b533d5-7034-4fb3-9a85-16be86edcb09
[92mINFO [0m:      Sent reply
01/27/2025 01:59:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 24e10b19-92e8-4735-897e-d85b6014313d
01/27/2025 01:59:48:INFO:Received: train message 24e10b19-92e8-4735-897e-d85b6014313d
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:00:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:00:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:00:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 487b0159-f021-462f-852b-55e053b47e06
01/27/2025 02:00:55:INFO:Received: evaluate message 487b0159-f021-462f-852b-55e053b47e06
[92mINFO [0m:      Sent reply
01/27/2025 02:01:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:01:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:01:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f78dc6e1-b35f-496e-8480-c291406d5522
01/27/2025 02:01:46:INFO:Received: train message f78dc6e1-b35f-496e-8480-c291406d5522
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:02:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:02:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:02:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d7f480af-1596-458b-9674-7e009e86d1a2
01/27/2025 02:02:53:INFO:Received: evaluate message d7f480af-1596-458b-9674-7e009e86d1a2
[92mINFO [0m:      Sent reply
01/27/2025 02:02:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:03:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:03:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f8342ba-750f-4437-98d5-281ed9576e9f
01/27/2025 02:03:31:INFO:Received: train message 3f8342ba-750f-4437-98d5-281ed9576e9f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:03:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:04:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:04:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1048fcc6-7b8e-4bf1-a03c-5e4b19cd46c7
01/27/2025 02:04:23:INFO:Received: evaluate message 1048fcc6-7b8e-4bf1-a03c-5e4b19cd46c7
[92mINFO [0m:      Sent reply
01/27/2025 02:04:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:05:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:05:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c6b9d39c-d0b7-40a3-b586-14b1197a4acf
01/27/2025 02:05:03:INFO:Received: train message c6b9d39c-d0b7-40a3-b586-14b1197a4acf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:05:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:06:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:06:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9a0f1c2-f4b2-47ea-a080-235ab8fa826d
01/27/2025 02:06:31:INFO:Received: evaluate message b9a0f1c2-f4b2-47ea-a080-235ab8fa826d
[92mINFO [0m:      Sent reply
01/27/2025 02:06:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:07:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:07:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 15441843-92a1-4b13-992a-0241a1c4af8c
01/27/2025 02:07:30:INFO:Received: train message 15441843-92a1-4b13-992a-0241a1c4af8c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:07:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:08:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:08:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cfd44c16-532a-4914-b281-08f685a9b22c
01/27/2025 02:08:50:INFO:Received: evaluate message cfd44c16-532a-4914-b281-08f685a9b22c
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:08:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:09:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:09:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9a87bbd-a433-4e74-ad13-ebceef5706ef
01/27/2025 02:09:43:INFO:Received: train message b9a87bbd-a433-4e74-ad13-ebceef5706ef
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:10:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:11:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:11:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4f88fef0-fb5d-4c62-a88b-a1c8883a6502
01/27/2025 02:11:25:INFO:Received: evaluate message 4f88fef0-fb5d-4c62-a88b-a1c8883a6502
[92mINFO [0m:      Sent reply
01/27/2025 02:11:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:11:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:11:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 56c496bb-5d5b-46e9-a442-35b3ac55e38b
01/27/2025 02:11:57:INFO:Received: train message 56c496bb-5d5b-46e9-a442-35b3ac55e38b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:12:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2240a1b4-83f8-4d08-87a0-ca7ed0a40be2
01/27/2025 02:13:16:INFO:Received: evaluate message 2240a1b4-83f8-4d08-87a0-ca7ed0a40be2
[92mINFO [0m:      Sent reply
01/27/2025 02:13:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e11ea320-bc58-4601-b39e-d6b89ce37895
01/27/2025 02:13:33:INFO:Received: train message e11ea320-bc58-4601-b39e-d6b89ce37895
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:13:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:14:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:14:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0da98e94-fd60-4d5e-b3e2-0d308a0b12f9
01/27/2025 02:14:44:INFO:Received: evaluate message 0da98e94-fd60-4d5e-b3e2-0d308a0b12f9
[92mINFO [0m:      Sent reply
01/27/2025 02:14:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:15:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:15:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c9599919-bc32-4605-aac6-d164416818e2
01/27/2025 02:15:19:INFO:Received: train message c9599919-bc32-4605-aac6-d164416818e2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:15:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:16:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:16:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dac7aa84-befd-4340-839d-c458b2729bf5
01/27/2025 02:16:44:INFO:Received: evaluate message dac7aa84-befd-4340-839d-c458b2729bf5

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:16:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:17:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:17:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7046d364-9f41-4b08-88b9-994c5416060e
01/27/2025 02:17:20:INFO:Received: train message 7046d364-9f41-4b08-88b9-994c5416060e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:17:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:18:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:18:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 707770ba-a98a-446f-86a2-1656d1c39294
01/27/2025 02:18:21:INFO:Received: evaluate message 707770ba-a98a-446f-86a2-1656d1c39294
[92mINFO [0m:      Sent reply
01/27/2025 02:18:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:19:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:19:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f856fc9a-949a-4454-87a1-3768e76e3a30
01/27/2025 02:19:01:INFO:Received: train message f856fc9a-949a-4454-87a1-3768e76e3a30
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:19:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2dcd0d57-4fef-472c-ab8d-49136a73505f
01/27/2025 02:20:07:INFO:Received: evaluate message 2dcd0d57-4fef-472c-ab8d-49136a73505f
[92mINFO [0m:      Sent reply
01/27/2025 02:20:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 388ded5e-5307-4bdc-91de-9fd078683ffc
01/27/2025 02:20:50:INFO:Received: train message 388ded5e-5307-4bdc-91de-9fd078683ffc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:21:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 59c89307-0dbc-404b-b0db-9e71274609d2
01/27/2025 02:22:10:INFO:Received: evaluate message 59c89307-0dbc-404b-b0db-9e71274609d2
[92mINFO [0m:      Sent reply
01/27/2025 02:22:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d9f4ba09-019a-4a49-9067-9800eefcbd0e
01/27/2025 02:22:43:INFO:Received: train message d9f4ba09-019a-4a49-9067-9800eefcbd0e

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:23:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:23:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:23:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0a418035-3555-4848-9287-ed770cf304b4
01/27/2025 02:23:42:INFO:Received: evaluate message 0a418035-3555-4848-9287-ed770cf304b4
[92mINFO [0m:      Sent reply
01/27/2025 02:23:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:24:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:24:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff1e5182-65b5-472f-b809-78059f853da7
01/27/2025 02:24:30:INFO:Received: train message ff1e5182-65b5-472f-b809-78059f853da7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:24:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:25:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:25:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f97a6213-bf7e-44d1-b034-0e109018e488
01/27/2025 02:25:37:INFO:Received: evaluate message f97a6213-bf7e-44d1-b034-0e109018e488
[92mINFO [0m:      Sent reply
01/27/2025 02:25:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:26:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:26:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8956811-76e3-42b3-8e63-fd9d6bd92253
01/27/2025 02:26:04:INFO:Received: train message c8956811-76e3-42b3-8e63-fd9d6bd92253
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:26:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 09e5fd89-6abc-40a6-8346-390ec16d9a5e
01/27/2025 02:27:07:INFO:Received: evaluate message 09e5fd89-6abc-40a6-8346-390ec16d9a5e
[92mINFO [0m:      Sent reply
01/27/2025 02:27:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6f0b4b8c-7553-42ff-a7b7-26478aed4211
01/27/2025 02:27:51:INFO:Received: train message 6f0b4b8c-7553-42ff-a7b7-26478aed4211
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:28:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:28:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:28:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d50ee5fb-68cb-4158-964d-add07a87aa01
01/27/2025 02:28:59:INFO:Received: evaluate message d50ee5fb-68cb-4158-964d-add07a87aa01
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:29:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 67315321-fc97-4b83-928e-55556727fcba
01/27/2025 02:29:18:INFO:Received: train message 67315321-fc97-4b83-928e-55556727fcba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:29:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:30:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:30:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d2936b4-f875-4a51-83d7-bddc0e51c0ff
01/27/2025 02:30:44:INFO:Received: evaluate message 1d2936b4-f875-4a51-83d7-bddc0e51c0ff
[92mINFO [0m:      Sent reply
01/27/2025 02:30:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:31:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:31:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1aeafdea-d3d6-4aa6-858c-7384601bde5c
01/27/2025 02:31:14:INFO:Received: train message 1aeafdea-d3d6-4aa6-858c-7384601bde5c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:31:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 46112f3a-c5aa-427a-a13d-f3aab34f5b38
01/27/2025 02:32:18:INFO:Received: evaluate message 46112f3a-c5aa-427a-a13d-f3aab34f5b38
[92mINFO [0m:      Sent reply
01/27/2025 02:32:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 59eebb5e-6139-43a3-91fe-01f24d5777f5
01/27/2025 02:32:40:INFO:Received: reconnect message 59eebb5e-6139-43a3-91fe-01f24d5777f5
01/27/2025 02:32:40:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 02:32:40:INFO:Disconnect and shut down

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6463623046875
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.013656758703291416, 0.004164603538811207, 0.026038192212581635, 0.004981085192412138]
Noise Multiplier after list and tensor:  0.012210159911774099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}



Final client history:
{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}

