nohup: ignoring input
01/27/2025 01:39:26:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 01:39:26:DEBUG:ChannelConnectivity.IDLE
01/27/2025 01:39:26:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 01:39:26:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 01:40:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:40:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6727cd9a-fb18-4855-bdf3-70bebca50cee
01/27/2025 01:40:14:INFO:Received: train message 6727cd9a-fb18-4855-bdf3-70bebca50cee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:40:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:41:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:41:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6a9f8866-1f13-4f38-954f-dc70591a934e
01/27/2025 01:41:34:INFO:Received: evaluate message 6a9f8866-1f13-4f38-954f-dc70591a934e
[92mINFO [0m:      Sent reply
01/27/2025 01:41:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:42:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:42:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce775ea2-a2f8-4f0c-87dd-96fde357c647
01/27/2025 01:42:46:INFO:Received: train message ce775ea2-a2f8-4f0c-87dd-96fde357c647
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:43:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:43:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:43:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cf9b1c23-e843-4dc9-8a57-a63d7b87b48c
01/27/2025 01:43:56:INFO:Received: evaluate message cf9b1c23-e843-4dc9-8a57-a63d7b87b48c
[92mINFO [0m:      Sent reply
01/27/2025 01:44:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 48d85685-5467-4a68-a902-5f65dd78c60c
01/27/2025 01:44:38:INFO:Received: train message 48d85685-5467-4a68-a902-5f65dd78c60c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:45:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:45:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:45:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 709310b3-c141-48b9-96bd-3d3ecdbe54b7
01/27/2025 01:45:36:INFO:Received: evaluate message 709310b3-c141-48b9-96bd-3d3ecdbe54b7
[92mINFO [0m:      Sent reply
01/27/2025 01:45:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:46:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:46:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c52c2c0b-1f38-4d0a-b251-c784a27178d2
01/27/2025 01:46:00:INFO:Received: train message c52c2c0b-1f38-4d0a-b251-c784a27178d2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:46:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:46:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:46:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 230dd409-d397-4574-97a5-08ea93148b23
01/27/2025 01:46:58:INFO:Received: evaluate message 230dd409-d397-4574-97a5-08ea93148b23
[92mINFO [0m:      Sent reply
01/27/2025 01:47:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a72f6013-2d1f-46f5-a980-62089005d2d0
01/27/2025 01:47:42:INFO:Received: train message a72f6013-2d1f-46f5-a980-62089005d2d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:48:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:48:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:48:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2072cf47-cc61-4e86-a5ba-462b73c58266
01/27/2025 01:48:36:INFO:Received: evaluate message 2072cf47-cc61-4e86-a5ba-462b73c58266
[92mINFO [0m:      Sent reply
01/27/2025 01:48:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:49:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:49:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ce6a3c3-a968-477c-b9c3-0837a8e1b685
01/27/2025 01:49:13:INFO:Received: train message 8ce6a3c3-a968-477c-b9c3-0837a8e1b685
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:49:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45a37bc6-4f00-4f0f-9bf9-97df3fe734fb
01/27/2025 01:50:00:INFO:Received: evaluate message 45a37bc6-4f00-4f0f-9bf9-97df3fe734fb
[92mINFO [0m:      Sent reply
01/27/2025 01:50:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c69c3166-a890-4960-b130-5fbad294647e
01/27/2025 01:50:39:INFO:Received: train message c69c3166-a890-4960-b130-5fbad294647e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:51:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:51:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:51:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 44d1f966-888e-4799-8c77-18e135fd6d5e
01/27/2025 01:51:49:INFO:Received: evaluate message 44d1f966-888e-4799-8c77-18e135fd6d5e
[92mINFO [0m:      Sent reply
01/27/2025 01:51:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:52:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:52:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9673e230-0e1e-42e9-b2b5-197e38abdc79
01/27/2025 01:52:15:INFO:Received: train message 9673e230-0e1e-42e9-b2b5-197e38abdc79
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379], 'accuracy': [0.5215011727912432], 'auc': [0.7229562347935747]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828], 'accuracy': [0.5215011727912432, 0.5175918686473807], 'auc': [0.7229562347935747, 0.7423497364044646]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:52:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ba5ae41-ff79-4ccd-b8d9-43e1c5de4fef
01/27/2025 01:53:05:INFO:Received: evaluate message 1ba5ae41-ff79-4ccd-b8d9-43e1c5de4fef
[92mINFO [0m:      Sent reply
01/27/2025 01:53:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dd2e42b8-6f1b-4905-9c8e-305edf143e49
01/27/2025 01:53:31:INFO:Received: train message dd2e42b8-6f1b-4905-9c8e-305edf143e49
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:53:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:54:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:54:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64c845e9-a07c-4ba3-a983-6764f54764ef
01/27/2025 01:54:45:INFO:Received: evaluate message 64c845e9-a07c-4ba3-a983-6764f54764ef
[92mINFO [0m:      Sent reply
01/27/2025 01:54:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:55:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:55:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e37c5fb6-70d6-48bb-ac84-ff7d8f22742f
01/27/2025 01:55:19:INFO:Received: train message e37c5fb6-70d6-48bb-ac84-ff7d8f22742f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:55:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2056b0a4-5654-4152-a02d-d65c9b308005
01/27/2025 01:56:13:INFO:Received: evaluate message 2056b0a4-5654-4152-a02d-d65c9b308005
[92mINFO [0m:      Sent reply
01/27/2025 01:56:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e1985f3d-b663-428a-85a3-c399420c8478
01/27/2025 01:56:35:INFO:Received: train message e1985f3d-b663-428a-85a3-c399420c8478
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:56:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:57:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:57:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4cf9319b-196c-4d1f-9fb0-ff24cc6aa58b
01/27/2025 01:57:51:INFO:Received: evaluate message 4cf9319b-196c-4d1f-9fb0-ff24cc6aa58b
[92mINFO [0m:      Sent reply
01/27/2025 01:57:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:58:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:58:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db8f53c8-0aaf-412d-ac1c-cada790ee20f
01/27/2025 01:58:23:INFO:Received: train message db8f53c8-0aaf-412d-ac1c-cada790ee20f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:58:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ed1ccef-8298-4c44-91e1-e022bb377aab
01/27/2025 01:59:12:INFO:Received: evaluate message 6ed1ccef-8298-4c44-91e1-e022bb377aab
[92mINFO [0m:      Sent reply
01/27/2025 01:59:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4fc7efe4-3278-40d2-80c7-a685e04fbc14
01/27/2025 01:59:56:INFO:Received: train message 4fc7efe4-3278-40d2-80c7-a685e04fbc14
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:00:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:00:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:00:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2b424c02-b56a-44a6-98fa-466af762677d
01/27/2025 02:00:54:INFO:Received: evaluate message 2b424c02-b56a-44a6-98fa-466af762677d
[92mINFO [0m:      Sent reply
01/27/2025 02:01:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:01:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:01:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 36571db8-219b-4cdc-8473-167bbedc9569
01/27/2025 02:01:11:INFO:Received: train message 36571db8-219b-4cdc-8473-167bbedc9569
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:01:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:02:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:02:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 49cca126-06d6-4cb8-98cb-3a4c8f0bea79
01/27/2025 02:02:46:INFO:Received: evaluate message 49cca126-06d6-4cb8-98cb-3a4c8f0bea79
[92mINFO [0m:      Sent reply
01/27/2025 02:02:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:03:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:03:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c25b535-c038-49f0-bf7e-ed09581dcbe3
01/27/2025 02:03:26:INFO:Received: train message 0c25b535-c038-49f0-bf7e-ed09581dcbe3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:03:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:04:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:04:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0c97f62c-6550-4a95-a3a9-99d1a6faf1bc
01/27/2025 02:04:28:INFO:Received: evaluate message 0c97f62c-6550-4a95-a3a9-99d1a6faf1bc
[92mINFO [0m:      Sent reply
01/27/2025 02:04:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:05:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:05:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c937ebf2-7f0e-4c5d-a05d-bd16549a24eb
01/27/2025 02:05:00:INFO:Received: train message c937ebf2-7f0e-4c5d-a05d-bd16549a24eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:05:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:06:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:06:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e0e90d55-1133-4928-b305-ef230967d115
01/27/2025 02:06:28:INFO:Received: evaluate message e0e90d55-1133-4928-b305-ef230967d115
[92mINFO [0m:      Sent reply
01/27/2025 02:06:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:07:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:07:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6253680f-9fda-4b20-b99e-1bd9e5f6ad0f
01/27/2025 02:07:32:INFO:Received: train message 6253680f-9fda-4b20-b99e-1bd9e5f6ad0f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:08:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:08:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:08:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4eea4534-6c54-4c12-bf7a-0fc50eb15907
01/27/2025 02:08:41:INFO:Received: evaluate message 4eea4534-6c54-4c12-bf7a-0fc50eb15907
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:08:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:09:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:09:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3a8acd98-3b80-4d4e-8a0a-7d2fb1855966
01/27/2025 02:09:48:INFO:Received: train message 3a8acd98-3b80-4d4e-8a0a-7d2fb1855966
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:10:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:10:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:10:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3b4f0602-3191-434c-855e-31c7417c55c0
01/27/2025 02:10:56:INFO:Received: evaluate message 3b4f0602-3191-434c-855e-31c7417c55c0
[92mINFO [0m:      Sent reply
01/27/2025 02:11:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:12:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:12:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 882bab1a-82af-4384-8d4d-18e6467bc2e2
01/27/2025 02:12:04:INFO:Received: train message 882bab1a-82af-4384-8d4d-18e6467bc2e2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:12:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1c408f39-82ac-4b0d-8667-99c840824b1c
01/27/2025 02:13:11:INFO:Received: evaluate message 1c408f39-82ac-4b0d-8667-99c840824b1c
[92mINFO [0m:      Sent reply
01/27/2025 02:13:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 606dcd85-9a74-48d5-819f-a7234f5dfd3c
01/27/2025 02:13:49:INFO:Received: train message 606dcd85-9a74-48d5-819f-a7234f5dfd3c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:14:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:14:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:14:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aa952da7-c1dd-46c8-a242-dc5dcaec7983
01/27/2025 02:14:53:INFO:Received: evaluate message aa952da7-c1dd-46c8-a242-dc5dcaec7983
[92mINFO [0m:      Sent reply
01/27/2025 02:14:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:15:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:15:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30eb5a9a-ad94-448c-b35f-b8c79aa8e7d2
01/27/2025 02:15:19:INFO:Received: train message 30eb5a9a-ad94-448c-b35f-b8c79aa8e7d2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:15:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:16:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:16:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5262d05c-7ab0-4d5d-b399-e48dcecc7b77
01/27/2025 02:16:46:INFO:Received: evaluate message 5262d05c-7ab0-4d5d-b399-e48dcecc7b77

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:16:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:17:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:17:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7e8f9b31-4077-48c1-add3-9839fc7ca0da
01/27/2025 02:17:22:INFO:Received: train message 7e8f9b31-4077-48c1-add3-9839fc7ca0da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:17:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7ebec9b8-6a49-4933-9a3d-50cbd91f2232
01/27/2025 02:18:32:INFO:Received: evaluate message 7ebec9b8-6a49-4933-9a3d-50cbd91f2232
[92mINFO [0m:      Sent reply
01/27/2025 02:18:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:19:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:19:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bd43190a-9a6a-4ec0-829c-77141e0076b0
01/27/2025 02:19:03:INFO:Received: train message bd43190a-9a6a-4ec0-829c-77141e0076b0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:19:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aa705146-cd18-44d3-97f3-cd30c9d91c32
01/27/2025 02:20:21:INFO:Received: evaluate message aa705146-cd18-44d3-97f3-cd30c9d91c32
[92mINFO [0m:      Sent reply
01/27/2025 02:20:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 06636547-31a4-478d-aeaa-9e4118035c4d
01/27/2025 02:20:48:INFO:Received: train message 06636547-31a4-478d-aeaa-9e4118035c4d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:21:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:21:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:21:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 65d14a97-0728-46ab-a206-f92a2d3c6153
01/27/2025 02:21:59:INFO:Received: evaluate message 65d14a97-0728-46ab-a206-f92a2d3c6153
[92mINFO [0m:      Sent reply
01/27/2025 02:22:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3852c45-31b1-4ce4-b869-4dd6872f271b
01/27/2025 02:22:50:INFO:Received: train message a3852c45-31b1-4ce4-b869-4dd6872f271b

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:23:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:23:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:23:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6d7e4f1-8313-4f41-9b5d-49fe2f902dd9
01/27/2025 02:23:48:INFO:Received: evaluate message b6d7e4f1-8313-4f41-9b5d-49fe2f902dd9
[92mINFO [0m:      Sent reply
01/27/2025 02:23:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:24:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:24:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message debb77d5-55ef-48c2-9888-a531f2d77fe4
01/27/2025 02:24:14:INFO:Received: train message debb77d5-55ef-48c2-9888-a531f2d77fe4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:24:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:25:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:25:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1e9bee38-4af6-4e63-b1f8-1b85bafed479
01/27/2025 02:25:26:INFO:Received: evaluate message 1e9bee38-4af6-4e63-b1f8-1b85bafed479
[92mINFO [0m:      Sent reply
01/27/2025 02:25:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:25:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:25:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dbaff21f-f395-47ce-85ae-b9147bb5e8fc
01/27/2025 02:25:56:INFO:Received: train message dbaff21f-f395-47ce-85ae-b9147bb5e8fc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:26:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a69aeeaa-e11a-460d-9a73-d91bbcd7cae4
01/27/2025 02:27:06:INFO:Received: evaluate message a69aeeaa-e11a-460d-9a73-d91bbcd7cae4
[92mINFO [0m:      Sent reply
01/27/2025 02:27:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e7136962-a139-4406-b315-bd63ab9f513b
01/27/2025 02:27:53:INFO:Received: train message e7136962-a139-4406-b315-bd63ab9f513b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:28:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message feb8faa8-0659-4522-b6f1-aeed7a888673
01/27/2025 02:29:01:INFO:Received: evaluate message feb8faa8-0659-4522-b6f1-aeed7a888673
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:29:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d110a816-f89d-49d7-ba19-bb8f7afdba03
01/27/2025 02:29:39:INFO:Received: train message d110a816-f89d-49d7-ba19-bb8f7afdba03
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:30:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:30:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:30:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efcef32d-4d5e-4be9-94ac-bd2230120151
01/27/2025 02:30:40:INFO:Received: evaluate message efcef32d-4d5e-4be9-94ac-bd2230120151
[92mINFO [0m:      Sent reply
01/27/2025 02:30:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:31:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:31:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 65b4942a-d3a6-457f-8477-557220ce5dd3
01/27/2025 02:31:22:INFO:Received: train message 65b4942a-d3a6-457f-8477-557220ce5dd3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:31:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1a047b11-7198-43b1-bec7-b2f32ec49ae5
01/27/2025 02:32:35:INFO:Received: evaluate message 1a047b11-7198-43b1-bec7-b2f32ec49ae5
[92mINFO [0m:      Sent reply
01/27/2025 02:32:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message b059d000-0827-43d3-b8c2-406ef8917244
01/27/2025 02:32:40:INFO:Received: reconnect message b059d000-0827-43d3-b8c2-406ef8917244
01/27/2025 02:32:40:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 02:32:40:INFO:Disconnect and shut down

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7666015625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.0186251699924469, 0.01241789385676384, 0.08830023556947708, 0.0126039395108819]
Noise Multiplier after list and tensor:  0.03298680973239243
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}



Final client history:
{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}

