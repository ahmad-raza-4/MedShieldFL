nohup: ignoring input
01/27/2025 02:55:25:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 02:55:25:DEBUG:ChannelConnectivity.IDLE
01/27/2025 02:55:25:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 02:55:25:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 02:56:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:56:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b1ab3e2-386a-4805-b5f3-1f9d78e50359
01/27/2025 02:56:05:INFO:Received: train message 6b1ab3e2-386a-4805-b5f3-1f9d78e50359
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:56:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c58a94da-09df-4940-8eb5-756fd316c276
01/27/2025 02:57:09:INFO:Received: evaluate message c58a94da-09df-4940-8eb5-756fd316c276
[92mINFO [0m:      Sent reply
01/27/2025 02:57:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9e2b2c92-3898-446c-b9eb-0b567982ab35
01/27/2025 02:57:53:INFO:Received: train message 9e2b2c92-3898-446c-b9eb-0b567982ab35
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:58:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f4699c1-968a-413b-8949-9ed22b09b186
01/27/2025 02:59:03:INFO:Received: evaluate message 9f4699c1-968a-413b-8949-9ed22b09b186
[92mINFO [0m:      Sent reply
01/27/2025 02:59:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ade9c9a3-a292-4620-acae-e3740ca9b692
01/27/2025 02:59:56:INFO:Received: train message ade9c9a3-a292-4620-acae-e3740ca9b692
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:00:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7700b60-5932-49eb-9f51-caa97817ff72
01/27/2025 03:01:10:INFO:Received: evaluate message b7700b60-5932-49eb-9f51-caa97817ff72
[92mINFO [0m:      Sent reply
01/27/2025 03:01:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ceb6596d-c28a-4d30-9297-c8ad16754247
01/27/2025 03:01:41:INFO:Received: train message ceb6596d-c28a-4d30-9297-c8ad16754247
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:02:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3fa89f1e-4ef8-4989-a169-01e298e7f84a
01/27/2025 03:03:06:INFO:Received: evaluate message 3fa89f1e-4ef8-4989-a169-01e298e7f84a
[92mINFO [0m:      Sent reply
01/27/2025 03:03:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e3870434-8e4f-49a8-9902-a08ab5e3e669
01/27/2025 03:03:45:INFO:Received: train message e3870434-8e4f-49a8-9902-a08ab5e3e669
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:04:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 554c0773-4993-4639-962d-bd955f9cd3d5
01/27/2025 03:05:04:INFO:Received: evaluate message 554c0773-4993-4639-962d-bd955f9cd3d5
[92mINFO [0m:      Sent reply
01/27/2025 03:05:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88f09bc1-1c3c-472d-98f6-6076fd62f4cd
01/27/2025 03:05:36:INFO:Received: train message 88f09bc1-1c3c-472d-98f6-6076fd62f4cd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:06:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:06:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:06:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message adce10d9-6c7e-4605-9197-00751385bf59
01/27/2025 03:06:53:INFO:Received: evaluate message adce10d9-6c7e-4605-9197-00751385bf59
[92mINFO [0m:      Sent reply
01/27/2025 03:06:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:07:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:07:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 46fc12d5-ac1e-45d6-abd7-42d599239bee
01/27/2025 03:07:37:INFO:Received: train message 46fc12d5-ac1e-45d6-abd7-42d599239bee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:08:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:08:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:08:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cf328b8c-537e-42fe-8377-bbcf9c59b059
01/27/2025 03:08:42:INFO:Received: evaluate message cf328b8c-537e-42fe-8377-bbcf9c59b059
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597], 'accuracy': [0.5160281469898358], 'auc': [0.7330179566813546]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973], 'accuracy': [0.5160281469898358, 0.5191555903049258], 'auc': [0.7330179566813546, 0.750850648076995]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:08:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:09:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:09:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7e106278-fc8a-460c-b372-eadee3611f67
01/27/2025 03:09:04:INFO:Received: train message 7e106278-fc8a-460c-b372-eadee3611f67
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:09:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:10:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:10:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0315f7d4-d776-4c24-aae3-657eaaceafe7
01/27/2025 03:10:32:INFO:Received: evaluate message 0315f7d4-d776-4c24-aae3-657eaaceafe7
[92mINFO [0m:      Sent reply
01/27/2025 03:10:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:11:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:11:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message debf1b87-b598-4982-b286-da49e75a22b6
01/27/2025 03:11:37:INFO:Received: train message debf1b87-b598-4982-b286-da49e75a22b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:12:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:13:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:13:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 58d5b6af-5832-43fd-a036-4d99221c0e26
01/27/2025 03:13:20:INFO:Received: evaluate message 58d5b6af-5832-43fd-a036-4d99221c0e26
[92mINFO [0m:      Sent reply
01/27/2025 03:13:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:13:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:13:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 73d26d57-5ae4-4d00-be49-052378790939
01/27/2025 03:13:40:INFO:Received: train message 73d26d57-5ae4-4d00-be49-052378790939
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:14:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76544171-0386-4b2c-8526-938d9aefcc94
01/27/2025 03:15:08:INFO:Received: evaluate message 76544171-0386-4b2c-8526-938d9aefcc94
[92mINFO [0m:      Sent reply
01/27/2025 03:15:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:16:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:16:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e9244c9f-5e0c-4baf-9e47-3e291bd5652d
01/27/2025 03:16:03:INFO:Received: train message e9244c9f-5e0c-4baf-9e47-3e291bd5652d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:16:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8099db98-67bf-4889-82ce-a74ee6141f14
01/27/2025 03:17:04:INFO:Received: evaluate message 8099db98-67bf-4889-82ce-a74ee6141f14
[92mINFO [0m:      Sent reply
01/27/2025 03:17:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 89a5a1de-9145-4dfe-b8cb-a5064efd95a9
01/27/2025 03:17:46:INFO:Received: train message 89a5a1de-9145-4dfe-b8cb-a5064efd95a9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:18:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f5711e3-06aa-4b2d-aa0f-bfac5a43aea5
01/27/2025 03:19:07:INFO:Received: evaluate message 9f5711e3-06aa-4b2d-aa0f-bfac5a43aea5
[92mINFO [0m:      Sent reply
01/27/2025 03:19:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 26daeab6-1d6f-4f9d-9f56-f8f753540270
01/27/2025 03:19:24:INFO:Received: train message 26daeab6-1d6f-4f9d-9f56-f8f753540270

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:19:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:20:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:20:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 146ac5d9-e07d-4bd2-ae25-4f9a603710c4
01/27/2025 03:20:50:INFO:Received: evaluate message 146ac5d9-e07d-4bd2-ae25-4f9a603710c4
[92mINFO [0m:      Sent reply
01/27/2025 03:20:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:21:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:21:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message abb550c3-d905-4747-acf9-8140d1a15bd9
01/27/2025 03:21:15:INFO:Received: train message abb550c3-d905-4747-acf9-8140d1a15bd9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:21:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:22:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:22:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 67b2c8be-d04c-47ab-a8b1-9ba00ddb5f8e
01/27/2025 03:22:48:INFO:Received: evaluate message 67b2c8be-d04c-47ab-a8b1-9ba00ddb5f8e
[92mINFO [0m:      Sent reply
01/27/2025 03:22:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:23:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:23:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6d1a301a-5ccb-4558-bc2e-344c431ef604
01/27/2025 03:23:27:INFO:Received: train message 6d1a301a-5ccb-4558-bc2e-344c431ef604
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:23:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:24:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:24:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 27a45611-00c6-41b1-8229-4ce4bfb8a79a
01/27/2025 03:24:33:INFO:Received: evaluate message 27a45611-00c6-41b1-8229-4ce4bfb8a79a
[92mINFO [0m:      Sent reply
01/27/2025 03:24:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:25:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:25:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 39831b9a-c3a7-45ab-aa44-3f025cfb758f
01/27/2025 03:25:00:INFO:Received: train message 39831b9a-c3a7-45ab-aa44-3f025cfb758f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:25:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ba45cf4-6e36-419c-9f25-fb0ddd2c085e
01/27/2025 03:26:22:INFO:Received: evaluate message 4ba45cf4-6e36-419c-9f25-fb0ddd2c085e
[92mINFO [0m:      Sent reply
01/27/2025 03:26:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77be5505-1575-43d1-bec2-04f4ed5773c0
01/27/2025 03:26:44:INFO:Received: train message 77be5505-1575-43d1-bec2-04f4ed5773c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:27:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:27:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:27:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 851ecf85-91c0-4244-badf-97d90b5955de
01/27/2025 03:27:57:INFO:Received: evaluate message 851ecf85-91c0-4244-badf-97d90b5955de
[0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:28:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 44143e17-b0c2-4492-a71f-dccc44a9eb95
01/27/2025 03:28:29:INFO:Received: train message 44143e17-b0c2-4492-a71f-dccc44a9eb95
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:28:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:29:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:29:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 064b734f-bf0d-4504-aafb-c5e69748252d
01/27/2025 03:29:51:INFO:Received: evaluate message 064b734f-bf0d-4504-aafb-c5e69748252d
[92mINFO [0m:      Sent reply
01/27/2025 03:29:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:30:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:30:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d1b7230-e26b-4dad-8686-14570f1d938b
01/27/2025 03:30:24:INFO:Received: train message 3d1b7230-e26b-4dad-8686-14570f1d938b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:30:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:31:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:31:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c65be383-8723-4359-9113-730fbb5e6ef7
01/27/2025 03:31:34:INFO:Received: evaluate message c65be383-8723-4359-9113-730fbb5e6ef7
[92mINFO [0m:      Sent reply
01/27/2025 03:31:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:32:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:32:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 35670cd5-635c-4a3f-934c-c4349bc96059
01/27/2025 03:32:15:INFO:Received: train message 35670cd5-635c-4a3f-934c-c4349bc96059
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:32:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2e23def0-fac7-42cd-b8e2-71c0cd6be11c
01/27/2025 03:33:10:INFO:Received: evaluate message 2e23def0-fac7-42cd-b8e2-71c0cd6be11c
[92mINFO [0m:      Sent reply
01/27/2025 03:33:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db1970af-e199-440e-9965-362d15176329
01/27/2025 03:33:55:INFO:Received: train message db1970af-e199-440e-9965-362d15176329
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:34:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a30763e-1f19-4673-9d5b-f1bd3fa07e0d
01/27/2025 03:35:07:INFO:Received: evaluate message 2a30763e-1f19-4673-9d5b-f1bd3fa07e0d

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:35:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b3132b24-0e42-421a-9f41-d54fa6d4aee7
01/27/2025 03:35:46:INFO:Received: train message b3132b24-0e42-421a-9f41-d54fa6d4aee7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:36:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c2db99c-7685-498e-8e23-2a4c38bd7b24
01/27/2025 03:37:01:INFO:Received: evaluate message 4c2db99c-7685-498e-8e23-2a4c38bd7b24
[92mINFO [0m:      Sent reply
01/27/2025 03:37:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 31f802de-17e4-449e-9f38-6721086f7b09
01/27/2025 03:37:50:INFO:Received: train message 31f802de-17e4-449e-9f38-6721086f7b09
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:38:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:39:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:39:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c7eb3b3e-d3be-4658-9ec7-e6419c8afc18
01/27/2025 03:39:26:INFO:Received: evaluate message c7eb3b3e-d3be-4658-9ec7-e6419c8afc18
[92mINFO [0m:      Sent reply
01/27/2025 03:39:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:39:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:39:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2981a220-c573-40ec-afbe-d9acb8579c35
01/27/2025 03:39:51:INFO:Received: train message 2981a220-c573-40ec-afbe-d9acb8579c35
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:40:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:41:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:41:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 88c864f7-276f-4b04-8ba5-d712052566da
01/27/2025 03:41:46:INFO:Received: evaluate message 88c864f7-276f-4b04-8ba5-d712052566da
[92mINFO [0m:      Sent reply
01/27/2025 03:41:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:42:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:42:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d5ccc9a7-3b6f-450e-8e79-c5b56da9bde6
01/27/2025 03:42:21:INFO:Received: train message d5ccc9a7-3b6f-450e-8e79-c5b56da9bde6

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:42:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:43:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:43:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 662f5540-253f-4eeb-908d-ff6aa4d1bb83
01/27/2025 03:43:35:INFO:Received: evaluate message 662f5540-253f-4eeb-908d-ff6aa4d1bb83
[92mINFO [0m:      Sent reply
01/27/2025 03:43:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:44:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:44:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3b7a4121-6027-446c-ad1e-2f5a12801159
01/27/2025 03:44:04:INFO:Received: train message 3b7a4121-6027-446c-ad1e-2f5a12801159
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:44:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 96d42b7c-efbc-4321-84b7-88bd79c6cd4d
01/27/2025 03:45:03:INFO:Received: evaluate message 96d42b7c-efbc-4321-84b7-88bd79c6cd4d
[92mINFO [0m:      Sent reply
01/27/2025 03:45:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 99fbdd68-2a5e-4e95-9d16-2d8e97f5ad49
01/27/2025 03:45:52:INFO:Received: train message 99fbdd68-2a5e-4e95-9d16-2d8e97f5ad49
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:46:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:46:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:46:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message edc56b36-4fc6-45d2-9a9e-4361c3290cbf
01/27/2025 03:46:50:INFO:Received: evaluate message edc56b36-4fc6-45d2-9a9e-4361c3290cbf
[92mINFO [0m:      Sent reply
01/27/2025 03:46:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a922ecbb-4e4c-46b2-b38b-d9eaa8a69512
01/27/2025 03:47:44:INFO:Received: train message a922ecbb-4e4c-46b2-b38b-d9eaa8a69512
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:48:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:48:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:48:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fbac0d5b-3255-4ea5-a61a-bb5ad0715908
01/27/2025 03:48:53:INFO:Received: evaluate message fbac0d5b-3255-4ea5-a61a-bb5ad0715908
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:48:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:49:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:49:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 65a163de-5aae-4a8d-97aa-f52d9348ea00
01/27/2025 03:49:27:INFO:Received: train message 65a163de-5aae-4a8d-97aa-f52d9348ea00
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:49:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 87b3edbb-1a7b-4d51-b50c-d4d0c02ab2d7
01/27/2025 03:50:35:INFO:Received: evaluate message 87b3edbb-1a7b-4d51-b50c-d4d0c02ab2d7
[92mINFO [0m:      Sent reply
01/27/2025 03:50:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:51:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:51:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 010ff733-1f9d-4923-bf3e-ebc0e6bae3a8
01/27/2025 03:51:05:INFO:Received: train message 010ff733-1f9d-4923-bf3e-ebc0e6bae3a8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:51:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7ad79990-7d55-449d-92e8-b22cb701f042
01/27/2025 03:52:20:INFO:Received: evaluate message 7ad79990-7d55-449d-92e8-b22cb701f042
[92mINFO [0m:      Sent reply
01/27/2025 03:52:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 66687a2e-4188-4d2d-ac68-9ce234c350c6
01/27/2025 03:52:29:INFO:Received: reconnect message 66687a2e-4188-4d2d-ac68-9ce234c350c6
01/27/2025 03:52:29:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 03:52:29:INFO:Disconnect and shut down

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4737091064453125
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010008830577135086, 0.0030521745793521404, 0.019082993268966675, 0.003650561673566699]
Noise Multiplier after list and tensor:  0.00894864002475515
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}



Final client history:
{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}

