nohup: ignoring input
01/27/2025 02:55:30:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 02:55:30:DEBUG:ChannelConnectivity.IDLE
01/27/2025 02:55:30:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 02:55:30:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 02:56:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:56:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 84f495a0-56cb-495b-8846-2e3234f26296
01/27/2025 02:56:00:INFO:Received: train message 84f495a0-56cb-495b-8846-2e3234f26296
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:56:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:56:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:56:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8597d3e-1501-4996-ad87-fe79030c432a
01/27/2025 02:56:57:INFO:Received: evaluate message a8597d3e-1501-4996-ad87-fe79030c432a
[92mINFO [0m:      Sent reply
01/27/2025 02:57:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f2989f6-1ca3-4ae7-aafd-94db5f71febe
01/27/2025 02:57:58:INFO:Received: train message 1f2989f6-1ca3-4ae7-aafd-94db5f71febe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:58:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 146081db-446a-4d3f-8168-ae3f7f14b83a
01/27/2025 02:59:10:INFO:Received: evaluate message 146081db-446a-4d3f-8168-ae3f7f14b83a
[92mINFO [0m:      Sent reply
01/27/2025 02:59:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33de735e-66c5-40be-aae6-15673b601468
01/27/2025 02:59:58:INFO:Received: train message 33de735e-66c5-40be-aae6-15673b601468
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:00:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:00:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:00:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 636fdbc8-2554-4453-9a01-9f1b41e838a0
01/27/2025 03:00:59:INFO:Received: evaluate message 636fdbc8-2554-4453-9a01-9f1b41e838a0
[92mINFO [0m:      Sent reply
01/27/2025 03:01:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8fbff73-7e51-4d5f-b040-ec506a8b4340
01/27/2025 03:01:50:INFO:Received: train message c8fbff73-7e51-4d5f-b040-ec506a8b4340
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:02:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f3b55df-d1ee-4dee-9126-4945090ec724
01/27/2025 03:03:06:INFO:Received: evaluate message 0f3b55df-d1ee-4dee-9126-4945090ec724
[92mINFO [0m:      Sent reply
01/27/2025 03:03:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 13297859-3855-44b3-bdef-b0c52fcd1536
01/27/2025 03:03:35:INFO:Received: train message 13297859-3855-44b3-bdef-b0c52fcd1536
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:04:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b2dbb63f-7495-4165-bc40-de7087f495d2
01/27/2025 03:05:06:INFO:Received: evaluate message b2dbb63f-7495-4165-bc40-de7087f495d2
[92mINFO [0m:      Sent reply
01/27/2025 03:05:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a5e45364-9b19-43f7-8a92-5f6e554486e0
01/27/2025 03:05:45:INFO:Received: train message a5e45364-9b19-43f7-8a92-5f6e554486e0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:06:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:06:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:06:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f806f4c-7e26-4207-94e2-8ef6141e266d
01/27/2025 03:06:57:INFO:Received: evaluate message 3f806f4c-7e26-4207-94e2-8ef6141e266d
[92mINFO [0m:      Sent reply
01/27/2025 03:07:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:07:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:07:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 54777289-4689-4445-af00-f06576db9a4f
01/27/2025 03:07:37:INFO:Received: train message 54777289-4689-4445-af00-f06576db9a4f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:08:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:08:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:08:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33aa33f5-7b25-4b9f-8c25-b50609e27e6b
01/27/2025 03:08:44:INFO:Received: evaluate message 33aa33f5-7b25-4b9f-8c25-b50609e27e6b
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597], 'accuracy': [0.5160281469898358], 'auc': [0.7330179566813546]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973], 'accuracy': [0.5160281469898358, 0.5191555903049258], 'auc': [0.7330179566813546, 0.750850648076995]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:08:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:09:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:09:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 857af82d-9c7e-46ea-8967-b8da253ee216
01/27/2025 03:09:26:INFO:Received: train message 857af82d-9c7e-46ea-8967-b8da253ee216
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:09:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:10:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:10:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c798a53c-e960-4486-bfc1-00dfeb9de148
01/27/2025 03:10:48:INFO:Received: evaluate message c798a53c-e960-4486-bfc1-00dfeb9de148
[92mINFO [0m:      Sent reply
01/27/2025 03:10:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:11:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:11:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1549c508-5b87-4426-b2db-1168461ea09c
01/27/2025 03:11:39:INFO:Received: train message 1549c508-5b87-4426-b2db-1168461ea09c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:12:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:13:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:13:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 018c4e1b-9397-4a56-9de5-bec3c2b51825
01/27/2025 03:13:21:INFO:Received: evaluate message 018c4e1b-9397-4a56-9de5-bec3c2b51825
[92mINFO [0m:      Sent reply
01/27/2025 03:13:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:14:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:14:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4e58076f-918c-49d1-90f2-091a4aed4ac2
01/27/2025 03:14:08:INFO:Received: train message 4e58076f-918c-49d1-90f2-091a4aed4ac2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:14:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bb73d24c-537e-4ebc-b52c-d036c49b2a16
01/27/2025 03:15:12:INFO:Received: evaluate message bb73d24c-537e-4ebc-b52c-d036c49b2a16
[92mINFO [0m:      Sent reply
01/27/2025 03:15:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 83b4e427-9bca-4b8a-9351-c7775b9ab651
01/27/2025 03:15:51:INFO:Received: train message 83b4e427-9bca-4b8a-9351-c7775b9ab651
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:16:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message caa39ca0-3ab4-46ba-b670-2fa3f3a22807
01/27/2025 03:17:07:INFO:Received: evaluate message caa39ca0-3ab4-46ba-b670-2fa3f3a22807
[92mINFO [0m:      Sent reply
01/27/2025 03:17:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 85cce2e0-d774-441f-ac99-66e7ec0fabd4
01/27/2025 03:17:50:INFO:Received: train message 85cce2e0-d774-441f-ac99-66e7ec0fabd4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:18:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 83847100-c332-4f78-91db-37e8fe52e4cb
01/27/2025 03:19:08:INFO:Received: evaluate message 83847100-c332-4f78-91db-37e8fe52e4cb
[92mINFO [0m:      Sent reply
01/27/2025 03:19:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ab72702-9ed9-48fe-aa5d-3b3903f5c305
01/27/2025 03:19:35:INFO:Received: train message 4ab72702-9ed9-48fe-aa5d-3b3903f5c305

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:20:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:20:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:20:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6eab4821-965b-4718-8055-704c98df61fb
01/27/2025 03:20:52:INFO:Received: evaluate message 6eab4821-965b-4718-8055-704c98df61fb
[92mINFO [0m:      Sent reply
01/27/2025 03:20:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:21:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:21:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fee08b4b-632c-4df7-8aba-5262a5a6d3c7
01/27/2025 03:21:28:INFO:Received: train message fee08b4b-632c-4df7-8aba-5262a5a6d3c7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:22:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:22:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:22:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c932707e-4b87-4c4f-9018-cb7ccbc2de1a
01/27/2025 03:22:48:INFO:Received: evaluate message c932707e-4b87-4c4f-9018-cb7ccbc2de1a
[92mINFO [0m:      Sent reply
01/27/2025 03:22:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:23:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:23:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 60072576-d584-4323-8007-be6936be1f4e
01/27/2025 03:23:06:INFO:Received: train message 60072576-d584-4323-8007-be6936be1f4e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:23:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:24:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:24:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d7e8ec08-64b3-4b93-aeca-bab8464757e8
01/27/2025 03:24:29:INFO:Received: evaluate message d7e8ec08-64b3-4b93-aeca-bab8464757e8
[92mINFO [0m:      Sent reply
01/27/2025 03:24:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:25:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:25:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 89726c58-1aec-4d38-b4a5-cf24013a06fa
01/27/2025 03:25:12:INFO:Received: train message 89726c58-1aec-4d38-b4a5-cf24013a06fa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:25:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d2d85a05-6d30-419a-af1a-4a256463ab98
01/27/2025 03:26:17:INFO:Received: evaluate message d2d85a05-6d30-419a-af1a-4a256463ab98
[92mINFO [0m:      Sent reply
01/27/2025 03:26:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c560e6b-eeb4-4af1-ab03-5aaebcbe80c2
01/27/2025 03:26:56:INFO:Received: train message 0c560e6b-eeb4-4af1-ab03-5aaebcbe80c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:27:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 691d1ea2-51bd-4dec-96f9-aadf3b206434
01/27/2025 03:28:05:INFO:Received: evaluate message 691d1ea2-51bd-4dec-96f9-aadf3b206434
[0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:28:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0ba738f0-bd9f-4475-ba37-41fa2e3814d9
01/27/2025 03:28:46:INFO:Received: train message 0ba738f0-bd9f-4475-ba37-41fa2e3814d9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:29:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:29:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:29:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0216b526-b5f5-4127-9f27-4beac5295a9c
01/27/2025 03:29:50:INFO:Received: evaluate message 0216b526-b5f5-4127-9f27-4beac5295a9c
[92mINFO [0m:      Sent reply
01/27/2025 03:29:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:30:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:30:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c498c12-a717-46ba-85e5-d36ae701ba79
01/27/2025 03:30:33:INFO:Received: train message 0c498c12-a717-46ba-85e5-d36ae701ba79
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:31:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:31:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:31:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 05b1dedd-4634-416c-808b-f14cd991e8b3
01/27/2025 03:31:42:INFO:Received: evaluate message 05b1dedd-4634-416c-808b-f14cd991e8b3
[92mINFO [0m:      Sent reply
01/27/2025 03:31:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:32:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:32:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8d31e370-6644-4aa5-ad09-7e41c3d5dcfa
01/27/2025 03:32:16:INFO:Received: train message 8d31e370-6644-4aa5-ad09-7e41c3d5dcfa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:32:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 60bb85b7-c95e-433d-aa28-5b11d2d2073e
01/27/2025 03:33:17:INFO:Received: evaluate message 60bb85b7-c95e-433d-aa28-5b11d2d2073e
[92mINFO [0m:      Sent reply
01/27/2025 03:33:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:34:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:34:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 18bfbd81-d534-49fb-89c1-3e8bf27d9111
01/27/2025 03:34:07:INFO:Received: train message 18bfbd81-d534-49fb-89c1-3e8bf27d9111
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:34:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a278ff22-5021-4321-bd3b-bcfaf877af45
01/27/2025 03:35:24:INFO:Received: evaluate message a278ff22-5021-4321-bd3b-bcfaf877af45

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:35:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5fcace28-73ca-4721-b21c-40344fea5e24
01/27/2025 03:35:49:INFO:Received: train message 5fcace28-73ca-4721-b21c-40344fea5e24
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:36:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f595bcf7-787a-49c4-9d8b-394a41c02fb3
01/27/2025 03:37:12:INFO:Received: evaluate message f595bcf7-787a-49c4-9d8b-394a41c02fb3
[92mINFO [0m:      Sent reply
01/27/2025 03:37:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8d8d40f0-6301-48a5-b59a-588c1fa54a02
01/27/2025 03:37:54:INFO:Received: train message 8d8d40f0-6301-48a5-b59a-588c1fa54a02
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:38:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:39:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:39:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 003964ae-2f54-4760-a64b-9a026ef334bf
01/27/2025 03:39:13:INFO:Received: evaluate message 003964ae-2f54-4760-a64b-9a026ef334bf
[92mINFO [0m:      Sent reply
01/27/2025 03:39:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:40:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:40:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9c758f6-c941-470d-8ea5-6e2d5201711e
01/27/2025 03:40:11:INFO:Received: train message b9c758f6-c941-470d-8ea5-6e2d5201711e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:40:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:41:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:41:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f80a745b-0667-484b-b181-be4a310947a1
01/27/2025 03:41:45:INFO:Received: evaluate message f80a745b-0667-484b-b181-be4a310947a1
[92mINFO [0m:      Sent reply
01/27/2025 03:41:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:42:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:42:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f6415e04-2134-43f2-86a2-4c89dcf5abdd
01/27/2025 03:42:21:INFO:Received: train message f6415e04-2134-43f2-86a2-4c89dcf5abdd

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:43:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:43:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:43:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2fd1a4c-981a-49dd-bdb2-811fb7a7cb4d
01/27/2025 03:43:35:INFO:Received: evaluate message c2fd1a4c-981a-49dd-bdb2-811fb7a7cb4d
[92mINFO [0m:      Sent reply
01/27/2025 03:43:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:43:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:43:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88be0ff8-ca13-4794-809c-54e2b0764d69
01/27/2025 03:43:55:INFO:Received: train message 88be0ff8-ca13-4794-809c-54e2b0764d69
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:44:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bf80af2d-4b22-44e6-8e1b-d847d46e484a
01/27/2025 03:45:16:INFO:Received: evaluate message bf80af2d-4b22-44e6-8e1b-d847d46e484a
[92mINFO [0m:      Sent reply
01/27/2025 03:45:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a8ad7244-38c9-4708-ba3c-515b0a7287e7
01/27/2025 03:45:56:INFO:Received: train message a8ad7244-38c9-4708-ba3c-515b0a7287e7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:46:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8a7bd0c-81ac-4b8a-89d4-0b7c66d0cb15
01/27/2025 03:47:07:INFO:Received: evaluate message a8a7bd0c-81ac-4b8a-89d4-0b7c66d0cb15
[92mINFO [0m:      Sent reply
01/27/2025 03:47:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6a8099c-2e64-4e92-ba59-74e2b803854e
01/27/2025 03:47:40:INFO:Received: train message a6a8099c-2e64-4e92-ba59-74e2b803854e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:48:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:48:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:48:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ce4386ce-41a9-4187-a1a2-1ff93938605a
01/27/2025 03:48:36:INFO:Received: evaluate message ce4386ce-41a9-4187-a1a2-1ff93938605a
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:48:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:49:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:49:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a478f5d-ee3a-4963-88fd-f15f59defc80
01/27/2025 03:49:08:INFO:Received: train message 5a478f5d-ee3a-4963-88fd-f15f59defc80
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:49:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b1bcd9b4-c5f2-4006-9c35-b5578529469e
01/27/2025 03:50:32:INFO:Received: evaluate message b1bcd9b4-c5f2-4006-9c35-b5578529469e
[92mINFO [0m:      Sent reply
01/27/2025 03:50:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:51:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:51:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message faa62476-8e87-4582-83f8-6f18586e31c8
01/27/2025 03:51:10:INFO:Received: train message faa62476-8e87-4582-83f8-6f18586e31c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:51:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 81404d10-790e-4e86-ae4a-26e4af7005d7
01/27/2025 03:52:20:INFO:Received: evaluate message 81404d10-790e-4e86-ae4a-26e4af7005d7
[92mINFO [0m:      Sent reply
01/27/2025 03:52:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 97a63fb4-6351-43ee-b129-1ea06deb86cb
01/27/2025 03:52:29:INFO:Received: reconnect message 97a63fb4-6351-43ee-b129-1ea06deb86cb
01/27/2025 03:52:29:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 03:52:29:INFO:Disconnect and shut down

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.526275634765625
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.012786268256604671, 0.008524943143129349, 0.06061853468418121, 0.008652664721012115]
Noise Multiplier after list and tensor:  0.022645602701231837
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}



Final client history:
{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}

