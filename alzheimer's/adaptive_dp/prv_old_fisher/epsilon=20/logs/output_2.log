nohup: ignoring input
01/27/2025 02:55:27:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 02:55:27:DEBUG:ChannelConnectivity.IDLE
01/27/2025 02:55:27:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 02:55:27:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 02:55:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:55:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 91a9ca52-1061-4757-9553-c165c901681f
01/27/2025 02:55:48:INFO:Received: train message 91a9ca52-1061-4757-9553-c165c901681f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:56:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b451b0dc-76f3-4af5-b28c-a0aa24ad946b
01/27/2025 02:57:21:INFO:Received: evaluate message b451b0dc-76f3-4af5-b28c-a0aa24ad946b
[92mINFO [0m:      Sent reply
01/27/2025 02:57:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8006225-eba4-4f85-bb56-bada4f4839a6
01/27/2025 02:57:53:INFO:Received: train message b8006225-eba4-4f85-bb56-bada4f4839a6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:58:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d3e3481-8d4a-498c-bef7-d72563bc5556
01/27/2025 02:59:07:INFO:Received: evaluate message 3d3e3481-8d4a-498c-bef7-d72563bc5556
[92mINFO [0m:      Sent reply
01/27/2025 02:59:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 580f9db9-c6ce-466c-9530-dfd2f3a67b4c
01/27/2025 02:59:51:INFO:Received: train message 580f9db9-c6ce-466c-9530-dfd2f3a67b4c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:00:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ce73b83b-aeb7-4bbe-99e4-bc2e36178800
01/27/2025 03:01:07:INFO:Received: evaluate message ce73b83b-aeb7-4bbe-99e4-bc2e36178800
[92mINFO [0m:      Sent reply
01/27/2025 03:01:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b4cfa250-f506-4b45-abc3-d860ff51c5b7
01/27/2025 03:01:46:INFO:Received: train message b4cfa250-f506-4b45-abc3-d860ff51c5b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:02:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 386bb72f-bb9b-4d5a-9d19-3019f8898f41
01/27/2025 03:03:08:INFO:Received: evaluate message 386bb72f-bb9b-4d5a-9d19-3019f8898f41
[92mINFO [0m:      Sent reply
01/27/2025 03:03:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 55d42fb5-8685-4f60-8e38-e74c6b3a4b23
01/27/2025 03:03:45:INFO:Received: train message 55d42fb5-8685-4f60-8e38-e74c6b3a4b23
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:04:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 332a02fb-a129-4833-9477-e3c9de321172
01/27/2025 03:05:07:INFO:Received: evaluate message 332a02fb-a129-4833-9477-e3c9de321172
[92mINFO [0m:      Sent reply
01/27/2025 03:05:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ecb8e19f-3644-42b6-9c07-b7877933beb5
01/27/2025 03:05:49:INFO:Received: train message ecb8e19f-3644-42b6-9c07-b7877933beb5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:06:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:06:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:06:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 655e91aa-1263-49db-9d2a-bd618498cbcf
01/27/2025 03:06:50:INFO:Received: evaluate message 655e91aa-1263-49db-9d2a-bd618498cbcf
[92mINFO [0m:      Sent reply
01/27/2025 03:06:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:07:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:07:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 54747a03-c4ac-4c1a-8daf-1d2fb142955f
01/27/2025 03:07:35:INFO:Received: train message 54747a03-c4ac-4c1a-8daf-1d2fb142955f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:07:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:08:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:08:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 324615c9-3b27-4379-87b8-c5fcd5d7da4c
01/27/2025 03:08:33:INFO:Received: evaluate message 324615c9-3b27-4379-87b8-c5fcd5d7da4c
[92mINFO [0m:      Sent reply
01/27/2025 03:08:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:09:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:09:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 46254e90-b747-4790-b155-eeb7097ca1a1
01/27/2025 03:09:13:INFO:Received: train message 46254e90-b747-4790-b155-eeb7097ca1a1
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597], 'accuracy': [0.5160281469898358], 'auc': [0.7330179566813546]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973], 'accuracy': [0.5160281469898358, 0.5191555903049258], 'auc': [0.7330179566813546, 0.750850648076995]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:09:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:10:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:10:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a2adc1db-aa2c-47df-9011-ed85dba205fc
01/27/2025 03:10:47:INFO:Received: evaluate message a2adc1db-aa2c-47df-9011-ed85dba205fc
[92mINFO [0m:      Sent reply
01/27/2025 03:10:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:11:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:11:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 36b79f79-1ade-42f9-8c13-f0466b7ceeb1
01/27/2025 03:11:16:INFO:Received: train message 36b79f79-1ade-42f9-8c13-f0466b7ceeb1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:11:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:13:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:13:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6b23ba85-88e0-46c1-b872-602a9317702e
01/27/2025 03:13:18:INFO:Received: evaluate message 6b23ba85-88e0-46c1-b872-602a9317702e
[92mINFO [0m:      Sent reply
01/27/2025 03:13:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:14:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:14:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7abcf289-48fe-4a19-9088-4e6f59797c98
01/27/2025 03:14:11:INFO:Received: train message 7abcf289-48fe-4a19-9088-4e6f59797c98
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:14:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1de9491c-a9b9-4938-9a22-df915fb2767f
01/27/2025 03:15:21:INFO:Received: evaluate message 1de9491c-a9b9-4938-9a22-df915fb2767f
[92mINFO [0m:      Sent reply
01/27/2025 03:15:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:16:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:16:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5c378063-8939-4cf0-b6cb-75de6f259305
01/27/2025 03:16:01:INFO:Received: train message 5c378063-8939-4cf0-b6cb-75de6f259305
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:16:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee256d29-53b8-43d9-a321-5fd7b7cb5791
01/27/2025 03:17:10:INFO:Received: evaluate message ee256d29-53b8-43d9-a321-5fd7b7cb5791
[92mINFO [0m:      Sent reply
01/27/2025 03:17:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4e9159ec-18b3-4e7b-bae9-6eb80d755f8d
01/27/2025 03:17:51:INFO:Received: train message 4e9159ec-18b3-4e7b-bae9-6eb80d755f8d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:18:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:18:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:18:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 65e0b5d1-3f67-4951-b9f4-6fa3921e3149
01/27/2025 03:18:53:INFO:Received: evaluate message 65e0b5d1-3f67-4951-b9f4-6fa3921e3149
[92mINFO [0m:      Sent reply
01/27/2025 03:18:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ab3d9a4d-af2b-4c30-b7f9-96d1cec6db52
01/27/2025 03:19:46:INFO:Received: train message ab3d9a4d-af2b-4c30-b7f9-96d1cec6db52
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:20:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:20:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:20:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 01502edc-b57b-411f-8e80-f9849df68502
01/27/2025 03:20:53:INFO:Received: evaluate message 01502edc-b57b-411f-8e80-f9849df68502
[92mINFO [0m:      Sent reply
01/27/2025 03:20:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:21:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:21:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e9e8cc89-5509-4561-8059-77bdc1601529
01/27/2025 03:21:15:INFO:Received: train message e9e8cc89-5509-4561-8059-77bdc1601529
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:21:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:22:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:22:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ce05fa4-bd75-4b9c-9247-7c24b9cfed6e
01/27/2025 03:22:47:INFO:Received: evaluate message 4ce05fa4-bd75-4b9c-9247-7c24b9cfed6e
[92mINFO [0m:      Sent reply
01/27/2025 03:22:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:23:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:23:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 38fe8598-bde5-427f-b5f0-0db4e4a9111b
01/27/2025 03:23:27:INFO:Received: train message 38fe8598-bde5-427f-b5f0-0db4e4a9111b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:23:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:24:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:24:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fb139732-46db-4a9d-8245-209523f1ff3c
01/27/2025 03:24:21:INFO:Received: evaluate message fb139732-46db-4a9d-8245-209523f1ff3c
[92mINFO [0m:      Sent reply
01/27/2025 03:24:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:25:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:25:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3aabb5ec-1f15-43e8-8d07-53d21ba036d4
01/27/2025 03:25:06:INFO:Received: train message 3aabb5ec-1f15-43e8-8d07-53d21ba036d4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:25:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f8341a4f-1da2-4885-97cf-b1ed0e770ae8
01/27/2025 03:26:14:INFO:Received: evaluate message f8341a4f-1da2-4885-97cf-b1ed0e770ae8
[92mINFO [0m:      Sent reply
01/27/2025 03:26:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d19f304d-37d8-4be7-973a-4b47fdd9e030
01/27/2025 03:26:51:INFO:Received: train message d19f304d-37d8-4be7-973a-4b47fdd9e030
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:27:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5898e581-e408-430c-9acc-4aae4205c607
01/27/2025 03:28:08:INFO:Received: evaluate message 5898e581-e408-430c-9acc-4aae4205c607
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:28:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40632d16-90ce-4bce-8a19-0323ee277141
01/27/2025 03:28:37:INFO:Received: train message 40632d16-90ce-4bce-8a19-0323ee277141
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:29:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:29:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:29:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 98b65264-53ac-488e-ae6f-23efea9e7df5
01/27/2025 03:29:53:INFO:Received: evaluate message 98b65264-53ac-488e-ae6f-23efea9e7df5
[92mINFO [0m:      Sent reply
01/27/2025 03:29:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:30:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:30:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3657bc1-a7af-4da4-81a9-c307eacf5577
01/27/2025 03:30:31:INFO:Received: train message a3657bc1-a7af-4da4-81a9-c307eacf5577
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:30:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:31:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:31:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 39697020-428c-499e-a8c8-4e195d75a937
01/27/2025 03:31:28:INFO:Received: evaluate message 39697020-428c-499e-a8c8-4e195d75a937
[92mINFO [0m:      Sent reply
01/27/2025 03:31:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:32:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:32:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d3c223c5-352b-42a1-8956-537a7efe9653
01/27/2025 03:32:18:INFO:Received: train message d3c223c5-352b-42a1-8956-537a7efe9653
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:32:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 81da40aa-3562-40df-8c5f-4cdd252272bd
01/27/2025 03:33:28:INFO:Received: evaluate message 81da40aa-3562-40df-8c5f-4cdd252272bd
[92mINFO [0m:      Sent reply
01/27/2025 03:33:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 52d9476e-f985-4b03-a6e3-303d4fa383fc
01/27/2025 03:33:59:INFO:Received: train message 52d9476e-f985-4b03-a6e3-303d4fa383fc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:34:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0d12fb00-d3ad-4ddb-9444-43479bdea27d
01/27/2025 03:35:17:INFO:Received: evaluate message 0d12fb00-d3ad-4ddb-9444-43479bdea27d

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:35:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 321497d8-4ff2-4045-b79b-a443eca8c784
01/27/2025 03:35:55:INFO:Received: train message 321497d8-4ff2-4045-b79b-a443eca8c784
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:36:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2def2bfb-5b76-410b-96a3-fbb026aa9586
01/27/2025 03:37:06:INFO:Received: evaluate message 2def2bfb-5b76-410b-96a3-fbb026aa9586
[92mINFO [0m:      Sent reply
01/27/2025 03:37:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7e0487f-6b83-4147-b181-885a7de60345
01/27/2025 03:37:34:INFO:Received: train message b7e0487f-6b83-4147-b181-885a7de60345
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:38:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:39:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:39:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 30b9b879-fc98-48f7-af7e-4f3779d04c37
01/27/2025 03:39:19:INFO:Received: evaluate message 30b9b879-fc98-48f7-af7e-4f3779d04c37
[92mINFO [0m:      Sent reply
01/27/2025 03:39:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:40:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:40:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 233e9668-0efb-459c-bd1e-a428b0add052
01/27/2025 03:40:04:INFO:Received: train message 233e9668-0efb-459c-bd1e-a428b0add052
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:40:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:41:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:41:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66e0bf43-5335-4d2c-9455-3d6478dce9f1
01/27/2025 03:41:49:INFO:Received: evaluate message 66e0bf43-5335-4d2c-9455-3d6478dce9f1
[92mINFO [0m:      Sent reply
01/27/2025 03:41:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:42:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:42:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fe3de78f-c67a-4f7f-99f4-bed1738907d0
01/27/2025 03:42:25:INFO:Received: train message fe3de78f-c67a-4f7f-99f4-bed1738907d0

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:42:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:43:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:43:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aaf2c025-bf03-4427-b35c-159cfa412cf0
01/27/2025 03:43:33:INFO:Received: evaluate message aaf2c025-bf03-4427-b35c-159cfa412cf0
[92mINFO [0m:      Sent reply
01/27/2025 03:43:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:44:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:44:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a4ce4ace-160a-497f-b67a-7e102908ad2b
01/27/2025 03:44:13:INFO:Received: train message a4ce4ace-160a-497f-b67a-7e102908ad2b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:44:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2ff7b877-b39f-4336-9e2e-b7809e787ace
01/27/2025 03:45:19:INFO:Received: evaluate message 2ff7b877-b39f-4336-9e2e-b7809e787ace
[92mINFO [0m:      Sent reply
01/27/2025 03:45:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 28dbe6f4-32f6-4d1c-bcb4-a13702251804
01/27/2025 03:45:50:INFO:Received: train message 28dbe6f4-32f6-4d1c-bcb4-a13702251804
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:46:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21c6d205-35ae-4252-983b-db3efeefa9d6
01/27/2025 03:47:01:INFO:Received: evaluate message 21c6d205-35ae-4252-983b-db3efeefa9d6
[92mINFO [0m:      Sent reply
01/27/2025 03:47:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bb172ad5-979a-4b35-81d2-39fe6915fc1e
01/27/2025 03:47:44:INFO:Received: train message bb172ad5-979a-4b35-81d2-39fe6915fc1e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:48:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:48:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:48:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message caddbb0c-4180-437f-8ea7-5fab4697c738
01/27/2025 03:48:53:INFO:Received: evaluate message caddbb0c-4180-437f-8ea7-5fab4697c738
0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:48:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:49:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:49:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c73eee4e-1e13-4232-b9e6-350718afc4ed
01/27/2025 03:49:16:INFO:Received: train message c73eee4e-1e13-4232-b9e6-350718afc4ed
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:49:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b0fda91e-9822-41d0-85c4-af7d0ed31e14
01/27/2025 03:50:32:INFO:Received: evaluate message b0fda91e-9822-41d0-85c4-af7d0ed31e14
[92mINFO [0m:      Sent reply
01/27/2025 03:50:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:51:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:51:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 90ba9773-57af-4ffd-b66a-2b88d7857e33
01/27/2025 03:51:06:INFO:Received: train message 90ba9773-57af-4ffd-b66a-2b88d7857e33
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:51:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0166cd92-a51b-4fc3-b5e7-c0e7b10bd867
01/27/2025 03:52:03:INFO:Received: evaluate message 0166cd92-a51b-4fc3-b5e7-c0e7b10bd867
[92mINFO [0m:      Sent reply
01/27/2025 03:52:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message dbc8d31c-f052-44e0-affd-8183b9d2108d
01/27/2025 03:52:29:INFO:Received: reconnect message dbc8d31c-f052-44e0-affd-8183b9d2108d
01/27/2025 03:52:30:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 03:52:30:INFO:Disconnect and shut down

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4901123046875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.017860490828752518, 0.002161738695576787, 0.004579490050673485, 0.013450022786855698]
Noise Multiplier after list and tensor:  0.009512935590464622
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}



Final client history:
{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}

