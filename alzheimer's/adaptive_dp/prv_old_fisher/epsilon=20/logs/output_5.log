nohup: ignoring input
01/27/2025 02:55:28:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 02:55:28:DEBUG:ChannelConnectivity.IDLE
01/27/2025 02:55:28:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 02:55:28:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 02:56:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:56:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6fc2361f-67d7-4cf4-8107-0232993b592b
01/27/2025 02:56:05:INFO:Received: train message 6fc2361f-67d7-4cf4-8107-0232993b592b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:56:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ab49da9-2a00-4882-a98f-1c0bcaf059a0
01/27/2025 02:57:17:INFO:Received: evaluate message 9ab49da9-2a00-4882-a98f-1c0bcaf059a0
[92mINFO [0m:      Sent reply
01/27/2025 02:57:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f74982d7-c898-42ad-952f-fbbf2068776d
01/27/2025 02:57:47:INFO:Received: train message f74982d7-c898-42ad-952f-fbbf2068776d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:58:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f779d991-5b2b-400d-8477-3462b4880a22
01/27/2025 02:59:18:INFO:Received: evaluate message f779d991-5b2b-400d-8477-3462b4880a22
[92mINFO [0m:      Sent reply
01/27/2025 02:59:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 75c92d4c-e4a2-4cc7-8fe0-8e8fcade0c67
01/27/2025 02:59:53:INFO:Received: train message 75c92d4c-e4a2-4cc7-8fe0-8e8fcade0c67
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:00:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:00:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:00:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dae10925-7b92-4fdc-b79d-e80a6eabd73e
01/27/2025 03:00:53:INFO:Received: evaluate message dae10925-7b92-4fdc-b79d-e80a6eabd73e
[92mINFO [0m:      Sent reply
01/27/2025 03:00:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a7a50da-8bfa-4a31-87ea-b4b6689f95b1
01/27/2025 03:01:49:INFO:Received: train message 5a7a50da-8bfa-4a31-87ea-b4b6689f95b1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:02:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 796bf314-edcd-4d6b-88bc-c86fcb6d9a56
01/27/2025 03:03:08:INFO:Received: evaluate message 796bf314-edcd-4d6b-88bc-c86fcb6d9a56
[92mINFO [0m:      Sent reply
01/27/2025 03:03:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c9dd6d86-514c-4997-b468-22cae9f7e4a3
01/27/2025 03:03:46:INFO:Received: train message c9dd6d86-514c-4997-b468-22cae9f7e4a3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:04:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2733bbf7-ad69-4f0f-9922-87a5514b776a
01/27/2025 03:05:09:INFO:Received: evaluate message 2733bbf7-ad69-4f0f-9922-87a5514b776a
[92mINFO [0m:      Sent reply
01/27/2025 03:05:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 924f32ae-719b-4ab4-b179-93d757972c5c
01/27/2025 03:05:46:INFO:Received: train message 924f32ae-719b-4ab4-b179-93d757972c5c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:06:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:06:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:06:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2411f118-9265-458f-82d6-bb8e4f9ce863
01/27/2025 03:06:54:INFO:Received: evaluate message 2411f118-9265-458f-82d6-bb8e4f9ce863
[92mINFO [0m:      Sent reply
01/27/2025 03:07:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:07:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:07:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cbc62894-1787-4141-86d9-02666b540cb6
01/27/2025 03:07:22:INFO:Received: train message cbc62894-1787-4141-86d9-02666b540cb6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:07:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:08:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:08:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b14f770d-5696-423b-b09c-e283e7dcfd2c
01/27/2025 03:08:41:INFO:Received: evaluate message b14f770d-5696-423b-b09c-e283e7dcfd2c
[92mINFO [0m:      Sent reply
01/27/2025 03:08:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:09:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:09:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 04e9baf3-486b-400f-9084-a5ffdc7f43dd
01/27/2025 03:09:14:INFO:Received: train message 04e9baf3-486b-400f-9084-a5ffdc7f43dd
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597], 'accuracy': [0.5160281469898358], 'auc': [0.7330179566813546]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973], 'accuracy': [0.5160281469898358, 0.5191555903049258], 'auc': [0.7330179566813546, 0.750850648076995]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:09:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:10:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:10:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d76c550-4a8f-4f29-9933-0a8c6f8d3daa
01/27/2025 03:10:22:INFO:Received: evaluate message 1d76c550-4a8f-4f29-9933-0a8c6f8d3daa
[92mINFO [0m:      Sent reply
01/27/2025 03:10:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:11:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:11:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a33908a-5fce-4364-a301-0b344e05026c
01/27/2025 03:11:27:INFO:Received: train message 9a33908a-5fce-4364-a301-0b344e05026c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:12:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:12:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:12:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f531ee68-b101-4f98-8e5e-bcb16344ce15
01/27/2025 03:12:56:INFO:Received: evaluate message f531ee68-b101-4f98-8e5e-bcb16344ce15
[92mINFO [0m:      Sent reply
01/27/2025 03:13:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:14:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:14:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cce80336-b6a0-4615-91b4-fb02b4d44eb3
01/27/2025 03:14:02:INFO:Received: train message cce80336-b6a0-4615-91b4-fb02b4d44eb3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:14:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 564b7f3d-7f89-4c44-96b1-8516ee4a724f
01/27/2025 03:15:24:INFO:Received: evaluate message 564b7f3d-7f89-4c44-96b1-8516ee4a724f
[92mINFO [0m:      Sent reply
01/27/2025 03:15:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2923da2a-dcec-4706-80cb-9c73cec50a52
01/27/2025 03:15:41:INFO:Received: train message 2923da2a-dcec-4706-80cb-9c73cec50a52
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:16:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1da48529-d2a3-4eac-baca-9bd3ae108192
01/27/2025 03:17:13:INFO:Received: evaluate message 1da48529-d2a3-4eac-baca-9bd3ae108192
[92mINFO [0m:      Sent reply
01/27/2025 03:17:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6fd53ead-e22f-423a-915f-ce6c46727890
01/27/2025 03:17:48:INFO:Received: train message 6fd53ead-e22f-423a-915f-ce6c46727890
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:18:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f9d775f-da00-4c3c-bfac-0a81758f0a9f
01/27/2025 03:19:01:INFO:Received: evaluate message 9f9d775f-da00-4c3c-bfac-0a81758f0a9f
[92mINFO [0m:      Sent reply
01/27/2025 03:19:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 371a9b1c-f634-4073-9ed0-25f8c3586727
01/27/2025 03:19:44:INFO:Received: train message 371a9b1c-f634-4073-9ed0-25f8c3586727
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:20:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:20:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:20:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 412fd5fa-ec92-4bc6-8e4d-47642b67857a
01/27/2025 03:20:39:INFO:Received: evaluate message 412fd5fa-ec92-4bc6-8e4d-47642b67857a
[92mINFO [0m:      Sent reply
01/27/2025 03:20:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:21:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:21:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 776ab8bf-7f87-4c8e-a262-111abde93d31
01/27/2025 03:21:30:INFO:Received: train message 776ab8bf-7f87-4c8e-a262-111abde93d31
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:21:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:22:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:22:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f1dd3bc-ae33-4680-a9f5-fb000fa54df5
01/27/2025 03:22:37:INFO:Received: evaluate message 1f1dd3bc-ae33-4680-a9f5-fb000fa54df5
[92mINFO [0m:      Sent reply
01/27/2025 03:22:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:23:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:23:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c86ddb4-3b49-461e-9241-f237293149ab
01/27/2025 03:23:28:INFO:Received: train message 0c86ddb4-3b49-461e-9241-f237293149ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:23:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:24:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:24:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 098ec767-1476-4713-8565-3652d707ef99
01/27/2025 03:24:19:INFO:Received: evaluate message 098ec767-1476-4713-8565-3652d707ef99
[92mINFO [0m:      Sent reply
01/27/2025 03:24:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:25:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:25:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ae5a396-51d3-46c8-8e6e-037c173a88bb
01/27/2025 03:25:12:INFO:Received: train message 5ae5a396-51d3-46c8-8e6e-037c173a88bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:25:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 32b893ab-e86e-4219-b4dc-3898034921b9
01/27/2025 03:26:19:INFO:Received: evaluate message 32b893ab-e86e-4219-b4dc-3898034921b9
[92mINFO [0m:      Sent reply
01/27/2025 03:26:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 35d6cae6-fa86-4844-b9ac-a3372b3e22c8
01/27/2025 03:26:49:INFO:Received: train message 35d6cae6-fa86-4844-b9ac-a3372b3e22c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:27:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a4f520e2-a0ff-41ed-93e6-51b098a9e065
01/27/2025 03:28:05:INFO:Received: evaluate message a4f520e2-a0ff-41ed-93e6-51b098a9e065
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:28:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fd362f1d-429a-4a31-b838-d957abe94c7b
01/27/2025 03:28:47:INFO:Received: train message fd362f1d-429a-4a31-b838-d957abe94c7b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:29:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:29:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:29:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d567597-4f05-4209-9ffc-53781cbdde2c
01/27/2025 03:29:49:INFO:Received: evaluate message 3d567597-4f05-4209-9ffc-53781cbdde2c
[92mINFO [0m:      Sent reply
01/27/2025 03:29:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:30:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:30:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 47096833-616b-4347-8209-9898931d5baf
01/27/2025 03:30:33:INFO:Received: train message 47096833-616b-4347-8209-9898931d5baf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:31:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:31:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:31:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dda818d1-42ca-4df7-b398-7105cc38c49c
01/27/2025 03:31:37:INFO:Received: evaluate message dda818d1-42ca-4df7-b398-7105cc38c49c
[92mINFO [0m:      Sent reply
01/27/2025 03:31:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:32:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:32:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2546785-78a3-4775-81e1-95b44b0b11ea
01/27/2025 03:32:06:INFO:Received: train message e2546785-78a3-4775-81e1-95b44b0b11ea
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:32:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 398d5e17-3a84-439d-9a7f-b0677e357b79
01/27/2025 03:33:28:INFO:Received: evaluate message 398d5e17-3a84-439d-9a7f-b0677e357b79
[92mINFO [0m:      Sent reply
01/27/2025 03:33:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:34:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:34:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 87f4f60d-8032-4c5d-a04a-af58556ba7a1
01/27/2025 03:34:11:INFO:Received: train message 87f4f60d-8032-4c5d-a04a-af58556ba7a1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:34:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2fe8f5c4-01c1-422e-ac05-5a48dbdfaf15
01/27/2025 03:35:17:INFO:Received: evaluate message 2fe8f5c4-01c1-422e-ac05-5a48dbdfaf15

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:35:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:36:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:36:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b2a9fca2-adc4-4076-ab31-23b73404b356
01/27/2025 03:36:03:INFO:Received: train message b2a9fca2-adc4-4076-ab31-23b73404b356
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:36:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 208631fd-7ce9-48bb-a58b-ece5229e4dda
01/27/2025 03:37:13:INFO:Received: evaluate message 208631fd-7ce9-48bb-a58b-ece5229e4dda
[92mINFO [0m:      Sent reply
01/27/2025 03:37:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a72f2d6-2e8f-40a8-85b6-ea6047be93b4
01/27/2025 03:37:54:INFO:Received: train message 4a72f2d6-2e8f-40a8-85b6-ea6047be93b4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:38:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:39:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:39:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 604e3245-d446-4d3d-bcf9-9ba15fbe6028
01/27/2025 03:39:06:INFO:Received: evaluate message 604e3245-d446-4d3d-bcf9-9ba15fbe6028
[92mINFO [0m:      Sent reply
01/27/2025 03:39:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:40:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:40:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3808d8f0-bf6b-4091-87f8-ed67402bb861
01/27/2025 03:40:00:INFO:Received: train message 3808d8f0-bf6b-4091-87f8-ed67402bb861
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:40:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:41:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:41:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8e986920-f87f-4c9b-9e1f-e3d3f166035c
01/27/2025 03:41:50:INFO:Received: evaluate message 8e986920-f87f-4c9b-9e1f-e3d3f166035c
[92mINFO [0m:      Sent reply
01/27/2025 03:41:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:42:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:42:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 22457e5b-c373-470c-9d8e-0bd7b79b9b07
01/27/2025 03:42:21:INFO:Received: train message 22457e5b-c373-470c-9d8e-0bd7b79b9b07

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:42:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:43:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:43:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c72b2bd0-8ee2-479c-8c75-9d908994d150
01/27/2025 03:43:31:INFO:Received: evaluate message c72b2bd0-8ee2-479c-8c75-9d908994d150
[92mINFO [0m:      Sent reply
01/27/2025 03:43:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:44:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:44:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c2743446-3849-423a-ada4-6f27254923be
01/27/2025 03:44:11:INFO:Received: train message c2743446-3849-423a-ada4-6f27254923be
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:44:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3c00bc20-1bdd-42a3-a55f-207d04181183
01/27/2025 03:45:17:INFO:Received: evaluate message 3c00bc20-1bdd-42a3-a55f-207d04181183
[92mINFO [0m:      Sent reply
01/27/2025 03:45:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8777eaaf-affa-4c06-89e7-cf141f53497f
01/27/2025 03:45:51:INFO:Received: train message 8777eaaf-affa-4c06-89e7-cf141f53497f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:46:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 09e02485-bfe0-4d98-b656-4cbbe20c2e75
01/27/2025 03:47:04:INFO:Received: evaluate message 09e02485-bfe0-4d98-b656-4cbbe20c2e75
[92mINFO [0m:      Sent reply
01/27/2025 03:47:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 97cc7de8-7cbe-41d7-a112-6f6ace89acbd
01/27/2025 03:47:32:INFO:Received: train message 97cc7de8-7cbe-41d7-a112-6f6ace89acbd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:48:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:48:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:48:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 843bca2e-89cf-49a2-9c1d-ff1bcbaa15ee
01/27/2025 03:48:50:INFO:Received: evaluate message 843bca2e-89cf-49a2-9c1d-ff1bcbaa15ee
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:48:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:49:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:49:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b5ea1d4b-7965-4d04-8a5b-5b8f753f8d1c
01/27/2025 03:49:21:INFO:Received: train message b5ea1d4b-7965-4d04-8a5b-5b8f753f8d1c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:49:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3a782de8-108f-4f9f-b90c-7125a9f7dd82
01/27/2025 03:50:33:INFO:Received: evaluate message 3a782de8-108f-4f9f-b90c-7125a9f7dd82
[92mINFO [0m:      Sent reply
01/27/2025 03:50:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 68438eac-4531-477d-95f6-2934f5673429
01/27/2025 03:50:54:INFO:Received: train message 68438eac-4531-477d-95f6-2934f5673429
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:51:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8fb613fa-8466-4889-91a9-8eb37cd334ed
01/27/2025 03:52:03:INFO:Received: evaluate message 8fb613fa-8466-4889-91a9-8eb37cd334ed
[92mINFO [0m:      Sent reply
01/27/2025 03:52:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 4c2e898f-2329-4be0-add3-790a6169ba7e
01/27/2025 03:52:29:INFO:Received: reconnect message 4c2e898f-2329-4be0-add3-790a6169ba7e
01/27/2025 03:52:29:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 03:52:29:INFO:Disconnect and shut down

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.5063629150390625
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.008811932057142258, 0.005466798786073923, 0.014136585406959057, 0.04495561122894287]
Noise Multiplier after list and tensor:  0.018342731869779527
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}



Final client history:
{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}

