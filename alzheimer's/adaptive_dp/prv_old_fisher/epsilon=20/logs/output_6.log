nohup: ignoring input
01/27/2025 02:55:25:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 02:55:25:DEBUG:ChannelConnectivity.IDLE
01/27/2025 02:55:25:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 02:55:25:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 02:55:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:55:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message c96f0b2e-820d-4274-a56a-63cb73e9d583
01/27/2025 02:55:25:INFO:Received: get_parameters message c96f0b2e-820d-4274-a56a-63cb73e9d583
[92mINFO [0m:      Sent reply
01/27/2025 02:55:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:55:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:55:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 144f7086-35f6-4405-ab1b-a9a454059ab3
01/27/2025 02:55:48:INFO:Received: train message 144f7086-35f6-4405-ab1b-a9a454059ab3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:56:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message befc8dd4-6056-40d8-8def-43ad6a619683
01/27/2025 02:57:12:INFO:Received: evaluate message befc8dd4-6056-40d8-8def-43ad6a619683
[92mINFO [0m:      Sent reply
01/27/2025 02:57:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:57:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:57:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b2a9210-2b2f-4c58-9f2b-467863cb4986
01/27/2025 02:57:56:INFO:Received: train message 6b2a9210-2b2f-4c58-9f2b-467863cb4986
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:58:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1c964512-d2ba-4f87-bf14-a212575107af
01/27/2025 02:59:18:INFO:Received: evaluate message 1c964512-d2ba-4f87-bf14-a212575107af
[92mINFO [0m:      Sent reply
01/27/2025 02:59:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:59:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:59:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3249f962-6165-4d69-a7ad-0851e85561dc
01/27/2025 02:59:50:INFO:Received: train message 3249f962-6165-4d69-a7ad-0851e85561dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:00:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0e821a88-e10f-45bf-87bd-d210f8910274
01/27/2025 03:01:10:INFO:Received: evaluate message 0e821a88-e10f-45bf-87bd-d210f8910274
[92mINFO [0m:      Sent reply
01/27/2025 03:01:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:01:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:01:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce3b8bd1-9642-458a-95e7-1035de89f09f
01/27/2025 03:01:50:INFO:Received: train message ce3b8bd1-9642-458a-95e7-1035de89f09f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:02:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cb2fc53c-3b38-4ea8-9d59-f0410754c268
01/27/2025 03:03:03:INFO:Received: evaluate message cb2fc53c-3b38-4ea8-9d59-f0410754c268
[92mINFO [0m:      Sent reply
01/27/2025 03:03:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:03:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:03:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 34778f18-1485-46cb-bef1-ff3e8f507409
01/27/2025 03:03:44:INFO:Received: train message 34778f18-1485-46cb-bef1-ff3e8f507409
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:04:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:04:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:04:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f304abab-30ff-4482-88b7-94bbf4cf5525
01/27/2025 03:04:56:INFO:Received: evaluate message f304abab-30ff-4482-88b7-94bbf4cf5525
[92mINFO [0m:      Sent reply
01/27/2025 03:05:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:05:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:05:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e5fa19a-33f7-45ff-9a53-ff8da84f8951
01/27/2025 03:05:40:INFO:Received: train message 0e5fa19a-33f7-45ff-9a53-ff8da84f8951
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:06:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:06:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:06:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7ccd302b-68b8-433e-b42c-aaa949e85777
01/27/2025 03:06:55:INFO:Received: evaluate message 7ccd302b-68b8-433e-b42c-aaa949e85777
[92mINFO [0m:      Sent reply
01/27/2025 03:07:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:07:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:07:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1e72887-001c-4f7a-9b81-9175c41e54d1
01/27/2025 03:07:31:INFO:Received: train message c1e72887-001c-4f7a-9b81-9175c41e54d1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:07:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:08:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:08:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 838e4c3d-b36e-4ee2-92e6-9e815033dea2
01/27/2025 03:08:45:INFO:Received: evaluate message 838e4c3d-b36e-4ee2-92e6-9e815033dea2
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597], 'accuracy': [0.5160281469898358], 'auc': [0.7330179566813546]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973], 'accuracy': [0.5160281469898358, 0.5191555903049258], 'auc': [0.7330179566813546, 0.750850648076995]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:08:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:09:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:09:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 91a817bd-3891-4b1a-97c0-244b2ebe6406
01/27/2025 03:09:22:INFO:Received: train message 91a817bd-3891-4b1a-97c0-244b2ebe6406
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:09:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:10:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:10:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 67b4a3b5-06b4-4554-9203-02ccc2a321ca
01/27/2025 03:10:46:INFO:Received: evaluate message 67b4a3b5-06b4-4554-9203-02ccc2a321ca
[92mINFO [0m:      Sent reply
01/27/2025 03:10:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:11:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:11:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 175e859c-66f2-4820-94bc-fde412e2010b
01/27/2025 03:11:19:INFO:Received: train message 175e859c-66f2-4820-94bc-fde412e2010b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:11:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:13:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:13:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b1603eb3-37e5-42a3-8a0f-93e2009963b9
01/27/2025 03:13:14:INFO:Received: evaluate message b1603eb3-37e5-42a3-8a0f-93e2009963b9
[92mINFO [0m:      Sent reply
01/27/2025 03:13:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:14:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:14:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 040a6178-e53c-4d9a-9107-22898e53f015
01/27/2025 03:14:02:INFO:Received: train message 040a6178-e53c-4d9a-9107-22898e53f015
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:14:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6268b3c7-4245-4027-96d5-c90092a70ef6
01/27/2025 03:15:24:INFO:Received: evaluate message 6268b3c7-4245-4027-96d5-c90092a70ef6
[92mINFO [0m:      Sent reply
01/27/2025 03:15:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:15:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:15:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7e383f9-1d96-48bf-ab1b-f53cbe4d7cc0
01/27/2025 03:15:51:INFO:Received: train message f7e383f9-1d96-48bf-ab1b-f53cbe4d7cc0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:16:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 42955e91-9c52-467a-8489-c223c881486b
01/27/2025 03:17:04:INFO:Received: evaluate message 42955e91-9c52-467a-8489-c223c881486b
[92mINFO [0m:      Sent reply
01/27/2025 03:17:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:17:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:17:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff5af4a4-2359-4501-b54e-6bb5713b3849
01/27/2025 03:17:51:INFO:Received: train message ff5af4a4-2359-4501-b54e-6bb5713b3849
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:18:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f8fc0c0a-b4c9-4bc3-b736-1fdf295e958b
01/27/2025 03:19:06:INFO:Received: evaluate message f8fc0c0a-b4c9-4bc3-b736-1fdf295e958b
[92mINFO [0m:      Sent reply
01/27/2025 03:19:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:19:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:19:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a36ffd25-55d8-439c-b06f-88648b11372a
01/27/2025 03:19:48:INFO:Received: train message a36ffd25-55d8-439c-b06f-88648b11372a

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:20:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:20:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:20:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6165aa96-d440-404e-8123-003648ec44b5
01/27/2025 03:20:51:INFO:Received: evaluate message 6165aa96-d440-404e-8123-003648ec44b5
[92mINFO [0m:      Sent reply
01/27/2025 03:20:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:21:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:21:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b775f947-a01b-4d7b-9ddd-fcfff555e0ff
01/27/2025 03:21:28:INFO:Received: train message b775f947-a01b-4d7b-9ddd-fcfff555e0ff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:21:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:22:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:22:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2fddd5e0-0731-4e70-a14c-0893fba663a5
01/27/2025 03:22:48:INFO:Received: evaluate message 2fddd5e0-0731-4e70-a14c-0893fba663a5
[92mINFO [0m:      Sent reply
01/27/2025 03:22:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:23:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:23:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c260c63d-eef5-4836-8439-00ec052d23a9
01/27/2025 03:23:20:INFO:Received: train message c260c63d-eef5-4836-8439-00ec052d23a9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:23:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:24:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:24:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1208fa28-6a12-4f44-98f5-a9510c11b093
01/27/2025 03:24:33:INFO:Received: evaluate message 1208fa28-6a12-4f44-98f5-a9510c11b093
[92mINFO [0m:      Sent reply
01/27/2025 03:24:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:24:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:24:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a13b44f0-0a4b-4527-8754-00eaa1b73bf5
01/27/2025 03:24:57:INFO:Received: train message a13b44f0-0a4b-4527-8754-00eaa1b73bf5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:25:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6a335195-5084-475a-abfc-e9cdc541e46d
01/27/2025 03:26:08:INFO:Received: evaluate message 6a335195-5084-475a-abfc-e9cdc541e46d
[92mINFO [0m:      Sent reply
01/27/2025 03:26:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:26:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:26:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9e0270d8-ffc5-4508-b29d-7966fd9d9bc8
01/27/2025 03:26:58:INFO:Received: train message 9e0270d8-ffc5-4508-b29d-7966fd9d9bc8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:27:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a6acdd7a-ae33-4ed7-ab27-31a67a9f3076
01/27/2025 03:28:08:INFO:Received: evaluate message a6acdd7a-ae33-4ed7-ab27-31a67a9f3076
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:28:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:28:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:28:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8662594a-ec62-4846-b554-c8e80fc58111
01/27/2025 03:28:47:INFO:Received: train message 8662594a-ec62-4846-b554-c8e80fc58111
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:29:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:29:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:29:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3bc3e184-db5a-491a-aa15-460fa7e9c0d9
01/27/2025 03:29:55:INFO:Received: evaluate message 3bc3e184-db5a-491a-aa15-460fa7e9c0d9
[92mINFO [0m:      Sent reply
01/27/2025 03:30:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:30:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:30:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6996473e-9b39-496e-b41c-711400e9fc38
01/27/2025 03:30:11:INFO:Received: train message 6996473e-9b39-496e-b41c-711400e9fc38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:30:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:31:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:31:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ffae5471-ede4-4f90-85e4-e1ca2e6bb404
01/27/2025 03:31:25:INFO:Received: evaluate message ffae5471-ede4-4f90-85e4-e1ca2e6bb404
[92mINFO [0m:      Sent reply
01/27/2025 03:31:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:32:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:32:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c3c6071b-4fec-494d-acd4-abe18a79d59a
01/27/2025 03:32:09:INFO:Received: train message c3c6071b-4fec-494d-acd4-abe18a79d59a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:32:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:33:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:33:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f8334a3f-d778-4c94-97c9-73946ce13293
01/27/2025 03:33:20:INFO:Received: evaluate message f8334a3f-d778-4c94-97c9-73946ce13293
[92mINFO [0m:      Sent reply
01/27/2025 03:33:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:34:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:34:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b29ac98-ffd8-4827-ad26-15028dc06795
01/27/2025 03:34:14:INFO:Received: train message 4b29ac98-ffd8-4827-ad26-15028dc06795
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:34:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 304daa07-366e-49ed-911b-778e4cf6f440
01/27/2025 03:35:25:INFO:Received: evaluate message 304daa07-366e-49ed-911b-778e4cf6f440

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:35:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:35:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:35:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 06187a73-9a8e-4186-9bb0-e3df5735af79
01/27/2025 03:35:58:INFO:Received: train message 06187a73-9a8e-4186-9bb0-e3df5735af79
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:36:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7246a3e4-025d-4a9b-823e-76a1d58914bc
01/27/2025 03:37:06:INFO:Received: evaluate message 7246a3e4-025d-4a9b-823e-76a1d58914bc
[92mINFO [0m:      Sent reply
01/27/2025 03:37:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:37:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:37:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cdac3ce3-5f56-4832-b595-bb5d6e159b3c
01/27/2025 03:37:31:INFO:Received: train message cdac3ce3-5f56-4832-b595-bb5d6e159b3c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:38:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:39:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:39:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0ba754b-cb9d-416a-9fdb-b666732ad35e
01/27/2025 03:39:17:INFO:Received: evaluate message f0ba754b-cb9d-416a-9fdb-b666732ad35e
[92mINFO [0m:      Sent reply
01/27/2025 03:39:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:40:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:40:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d1094080-8816-4be0-aea6-933ca842cf32
01/27/2025 03:40:07:INFO:Received: train message d1094080-8816-4be0-aea6-933ca842cf32
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:40:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:41:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:41:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dddd30d1-883c-496d-8210-dd604bd4df6a
01/27/2025 03:41:30:INFO:Received: evaluate message dddd30d1-883c-496d-8210-dd604bd4df6a
[92mINFO [0m:      Sent reply
01/27/2025 03:41:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:42:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:42:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c03c7227-65e1-49b7-9846-3f5bdb794ae4
01/27/2025 03:42:14:INFO:Received: train message c03c7227-65e1-49b7-9846-3f5bdb794ae4

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:42:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:43:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:43:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 08bc433e-2759-4466-8ebf-7fe60ed6bb6b
01/27/2025 03:43:38:INFO:Received: evaluate message 08bc433e-2759-4466-8ebf-7fe60ed6bb6b
[92mINFO [0m:      Sent reply
01/27/2025 03:43:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:44:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:44:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ff61e34-fe50-49b5-9bed-4475a040a0a8
01/27/2025 03:44:13:INFO:Received: train message 4ff61e34-fe50-49b5-9bed-4475a040a0a8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:44:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b039fcac-e880-4a94-84e9-2b068e768208
01/27/2025 03:45:06:INFO:Received: evaluate message b039fcac-e880-4a94-84e9-2b068e768208
[92mINFO [0m:      Sent reply
01/27/2025 03:45:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:45:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:45:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 56aa6df2-3a49-47f0-8ee3-bbd54d2cc475
01/27/2025 03:45:48:INFO:Received: train message 56aa6df2-3a49-47f0-8ee3-bbd54d2cc475
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:46:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 91f8a6eb-5f78-4816-824c-c7af6ed6a181
01/27/2025 03:47:07:INFO:Received: evaluate message 91f8a6eb-5f78-4816-824c-c7af6ed6a181
[92mINFO [0m:      Sent reply
01/27/2025 03:47:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:47:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:47:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b936d26e-c7d0-4487-90bc-731b8818a5cb
01/27/2025 03:47:29:INFO:Received: train message b936d26e-c7d0-4487-90bc-731b8818a5cb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:47:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:48:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:48:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4241f9ab-4b8f-46ce-ba95-6d3c53fcfc76
01/27/2025 03:48:40:INFO:Received: evaluate message 4241f9ab-4b8f-46ce-ba95-6d3c53fcfc76
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 03:48:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:49:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:49:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 81feaa11-4b4a-4fe2-8ad6-2b72a7af0927
01/27/2025 03:49:29:INFO:Received: train message 81feaa11-4b4a-4fe2-8ad6-2b72a7af0927
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:49:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efbe62eb-caaf-4f81-bc66-2cf48da00a41
01/27/2025 03:50:15:INFO:Received: evaluate message efbe62eb-caaf-4f81-bc66-2cf48da00a41
[92mINFO [0m:      Sent reply
01/27/2025 03:50:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:50:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:50:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f041e164-b4ea-49fc-9868-c89399feed67
01/27/2025 03:50:51:INFO:Received: train message f041e164-b4ea-49fc-9868-c89399feed67
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 03:51:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8a58b733-becd-4b1c-97ec-f9d1cf5fbb1b
01/27/2025 03:52:25:INFO:Received: evaluate message 8a58b733-becd-4b1c-97ec-f9d1cf5fbb1b
[92mINFO [0m:      Sent reply
01/27/2025 03:52:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 03:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 03:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 7c0093fb-66bf-4a04-8700-a4b45bf4ae75
01/27/2025 03:52:29:INFO:Received: reconnect message 7c0093fb-66bf-4a04-8700-a4b45bf4ae75
01/27/2025 03:52:29:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 03:52:29:INFO:Disconnect and shut down

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4741668701171875
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0033772496972233057, 0.0028396956622600555, 0.018534529954195023, 0.0064943390898406506]
Noise Multiplier after list and tensor:  0.007811453600879759
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}



Final client history:
{'loss': [1.0662722773995597, 1.1490683297424973, 1.1504581312913276, 1.0952356473089104, 1.067511607838199, 1.088321950353618, 1.0863164109210655, 1.0831082892473831, 1.0827887868676174, 1.0806894781442244, 1.0171061018838652, 1.058793097105317, 1.0433001176652916, 1.043168607076879, 1.0514106531456358, 1.0479999810946556, 1.0757014013250141, 1.029314816743718, 1.0481533218492654, 1.1144001157699477, 1.0473009730056453, 1.0472267286399084, 1.050090495686311, 1.049231067963929, 1.0025508782656702, 1.09585794449971, 1.038341565184336, 1.0159656852367243, 1.0108209826034713, 1.018126366099312], 'accuracy': [0.5160281469898358, 0.5191555903049258, 0.5324472243940579, 0.5433932759968726, 0.5535574667709148, 0.5574667709147771, 0.5574667709147771, 0.5527756059421423, 0.565285379202502, 0.5574667709147771, 0.5684128225175918, 0.5715402658326818, 0.5762314308053167, 0.5699765441751369, 0.5746677091477717, 0.5824863174354965, 0.5731039874902267, 0.5832681782642689, 0.5809225957779516, 0.5715402658326818, 0.5793588741204065, 0.584831899921814, 0.5856137607505864, 0.5918686473807663, 0.5918686473807663, 0.5785770132916341, 0.5973416731821736, 0.5910867865519938, 0.5942142298670836, 0.5926505082095387], 'auc': [0.7330179566813546, 0.750850648076995, 0.7597964508585066, 0.7686641291508115, 0.7741690309982179, 0.777482465772379, 0.7805046197368739, 0.7829714172045317, 0.7843886517453874, 0.7871431393117945, 0.7913764565747703, 0.792713383931654, 0.7930032336822541, 0.7929069973026031, 0.794088761295928, 0.7957992500846616, 0.7971492714621615, 0.798826897844212, 0.7983936258644918, 0.7991072350776287, 0.8021014284089532, 0.8025763880687812, 0.8028593715029614, 0.8036410953300436, 0.8066637170054862, 0.8063193481327393, 0.8081266112412239, 0.809400643700373, 0.8081338495421652, 0.8101340498118591]}

