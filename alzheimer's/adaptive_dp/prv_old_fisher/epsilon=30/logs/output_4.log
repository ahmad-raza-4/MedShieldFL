nohup: ignoring input
01/26/2025 22:40:16:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/26/2025 22:40:16:DEBUG:ChannelConnectivity.IDLE
01/26/2025 22:40:16:DEBUG:ChannelConnectivity.CONNECTING
01/26/2025 22:40:16:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/26/2025 22:40:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ddc06fd8-4d5b-4989-bc66-0c99d84b1a98
01/26/2025 22:40:45:INFO:Received: train message ddc06fd8-4d5b-4989-bc66-0c99d84b1a98
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:41:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 63b225b4-68ce-46a5-9b73-333f96991250
01/26/2025 22:42:04:INFO:Received: evaluate message 63b225b4-68ce-46a5-9b73-333f96991250
[92mINFO [0m:      Sent reply
01/26/2025 22:42:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 84ad11fe-84e2-4f75-a921-9fd821c1ca9e
01/26/2025 22:42:50:INFO:Received: train message 84ad11fe-84e2-4f75-a921-9fd821c1ca9e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:43:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f47aa7c9-1bfd-43ec-b8a9-a523a8823151
01/26/2025 22:44:08:INFO:Received: evaluate message f47aa7c9-1bfd-43ec-b8a9-a523a8823151
[92mINFO [0m:      Sent reply
01/26/2025 22:44:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f624ec1-2bb5-42a9-9eeb-2a9afc70999b
01/26/2025 22:44:49:INFO:Received: train message 8f624ec1-2bb5-42a9-9eeb-2a9afc70999b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:45:23:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:45:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:45:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6041a79e-b25d-4a51-985d-b80f3259a55f
01/26/2025 22:45:45:INFO:Received: evaluate message 6041a79e-b25d-4a51-985d-b80f3259a55f
[92mINFO [0m:      Sent reply
01/26/2025 22:45:51:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d85718a8-797e-4a2e-b1c6-36d8b96a10d4
01/26/2025 22:46:54:INFO:Received: train message d85718a8-797e-4a2e-b1c6-36d8b96a10d4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:47:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:47:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:47:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d36fd2a8-bfd7-4d61-9a90-8b5ab02c0b19
01/26/2025 22:47:56:INFO:Received: evaluate message d36fd2a8-bfd7-4d61-9a90-8b5ab02c0b19
[92mINFO [0m:      Sent reply
01/26/2025 22:47:59:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 583ab501-d8c5-442a-8c59-e3422cd3d79a
01/26/2025 22:48:47:INFO:Received: train message 583ab501-d8c5-442a-8c59-e3422cd3d79a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:49:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b10dc835-30c3-400a-88e1-1e26d487e7b7
01/26/2025 22:50:16:INFO:Received: evaluate message b10dc835-30c3-400a-88e1-1e26d487e7b7
[92mINFO [0m:      Sent reply
01/26/2025 22:50:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 48f05a06-5f3e-4e83-9367-bb48077b94b2
01/26/2025 22:50:55:INFO:Received: train message 48f05a06-5f3e-4e83-9367-bb48077b94b2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:51:30:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cd74164d-d98e-4f37-9412-b4f6932cc5f7
01/26/2025 22:52:11:INFO:Received: evaluate message cd74164d-d98e-4f37-9412-b4f6932cc5f7
[92mINFO [0m:      Sent reply
01/26/2025 22:52:16:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a199f75-bfce-4f89-99ec-1cbd6baedf30
01/26/2025 22:52:43:INFO:Received: train message 5a199f75-bfce-4f89-99ec-1cbd6baedf30
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:53:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a20bd5c-a9ad-4e2e-bfe4-43e51a90fa79
01/26/2025 22:54:20:INFO:Received: evaluate message 7a20bd5c-a9ad-4e2e-bfe4-43e51a90fa79
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544], 'accuracy': [0.5199374511336982], 'auc': [0.7254494785945842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378], 'accuracy': [0.5199374511336982, 0.5238467552775606], 'auc': [0.7254494785945842, 0.7453794430136471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 22:54:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb1d593d-f120-4d7d-b355-22db61873b3a
01/26/2025 22:54:51:INFO:Received: train message fb1d593d-f120-4d7d-b355-22db61873b3a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:55:33:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:56:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:56:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 828e54c9-1d14-41c1-be02-393a6146e6e3
01/26/2025 22:56:33:INFO:Received: evaluate message 828e54c9-1d14-41c1-be02-393a6146e6e3
[92mINFO [0m:      Sent reply
01/26/2025 22:56:36:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:57:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:57:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cba5c2de-53e9-40a5-8c7b-08b300fdf09c
01/26/2025 22:57:11:INFO:Received: train message cba5c2de-53e9-40a5-8c7b-08b300fdf09c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:57:54:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:58:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:58:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b3928f04-324c-4298-b5e3-8d1be64088bf
01/26/2025 22:58:52:INFO:Received: evaluate message b3928f04-324c-4298-b5e3-8d1be64088bf
[92mINFO [0m:      Sent reply
01/26/2025 22:58:56:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:59:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:59:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 23fa21a0-de7e-47bf-b1a0-1e44d9ed7e19
01/26/2025 22:59:36:INFO:Received: train message 23fa21a0-de7e-47bf-b1a0-1e44d9ed7e19
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:00:14:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b4c1f98-2ecc-46fc-9c6f-87f25d34a3dd
01/26/2025 23:01:03:INFO:Received: evaluate message 9b4c1f98-2ecc-46fc-9c6f-87f25d34a3dd
[92mINFO [0m:      Sent reply
01/26/2025 23:01:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ba915d1e-65eb-4ad2-9b27-8b05b70dbe80
01/26/2025 23:01:33:INFO:Received: train message ba915d1e-65eb-4ad2-9b27-8b05b70dbe80
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:02:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e9b10d94-837e-41d2-ad15-836dd64fd16d
01/26/2025 23:03:20:INFO:Received: evaluate message e9b10d94-837e-41d2-ad15-836dd64fd16d
[92mINFO [0m:      Sent reply
01/26/2025 23:03:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:04:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:04:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8b59c3f9-e13f-4f4f-8b9c-02c1aebac4b9
01/26/2025 23:04:00:INFO:Received: train message 8b59c3f9-e13f-4f4f-8b9c-02c1aebac4b9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:04:36:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13683397-8ad3-4e5a-85ed-74182739e431
01/26/2025 23:05:17:INFO:Received: evaluate message 13683397-8ad3-4e5a-85ed-74182739e431
[92mINFO [0m:      Sent reply
01/26/2025 23:05:22:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ce09561-b26a-469f-bed7-3936c7614de9
01/26/2025 23:05:49:INFO:Received: train message 6ce09561-b26a-469f-bed7-3936c7614de9

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:06:29:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4d03afb4-6319-466c-b9fd-9ce8c760d06a
01/26/2025 23:07:00:INFO:Received: evaluate message 4d03afb4-6319-466c-b9fd-9ce8c760d06a
[92mINFO [0m:      Sent reply
01/26/2025 23:07:04:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 799494e4-24e5-4f4a-a481-35c87a3bae09
01/26/2025 23:07:44:INFO:Received: train message 799494e4-24e5-4f4a-a481-35c87a3bae09
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:08:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f485b58-c436-4b27-a7f1-0ffd09772e32
01/26/2025 23:09:12:INFO:Received: evaluate message 1f485b58-c436-4b27-a7f1-0ffd09772e32
[92mINFO [0m:      Sent reply
01/26/2025 23:09:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 45819044-611c-4153-9580-136161773b20
01/26/2025 23:09:52:INFO:Received: train message 45819044-611c-4153-9580-136161773b20
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:10:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e912254a-0db6-4cf0-9f45-c2cd619a36ce
01/26/2025 23:11:15:INFO:Received: evaluate message e912254a-0db6-4cf0-9f45-c2cd619a36ce
[92mINFO [0m:      Sent reply
01/26/2025 23:11:19:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9e7d956b-b30b-44da-8747-50e190e74cfa
01/26/2025 23:11:59:INFO:Received: train message 9e7d956b-b30b-44da-8747-50e190e74cfa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:12:30:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 157ee83a-a6f1-40b4-b7ba-02370f5825b7
01/26/2025 23:13:23:INFO:Received: evaluate message 157ee83a-a6f1-40b4-b7ba-02370f5825b7
[92mINFO [0m:      Sent reply
01/26/2025 23:13:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:14:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:14:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c99d7c9d-7789-428c-a4bb-4a93274f2583
01/26/2025 23:14:08:INFO:Received: train message c99d7c9d-7789-428c-a4bb-4a93274f2583
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:14:47:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:15:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:15:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b02b1c27-4a09-465d-9a05-fc09f4e15d72
01/26/2025 23:15:31:INFO:Received: evaluate message b02b1c27-4a09-465d-9a05-fc09f4e15d72
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:15:37:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:16:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:16:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2938960c-2eba-493a-9da3-ca4d5948286c
01/26/2025 23:16:08:INFO:Received: train message 2938960c-2eba-493a-9da3-ca4d5948286c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:16:46:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e31f19cc-847f-4484-81cd-5dea1eabd735
01/26/2025 23:18:03:INFO:Received: evaluate message e31f19cc-847f-4484-81cd-5dea1eabd735
[92mINFO [0m:      Sent reply
01/26/2025 23:18:07:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 054f19a4-cde3-48d7-93a3-b731546b9c0a
01/26/2025 23:18:37:INFO:Received: train message 054f19a4-cde3-48d7-93a3-b731546b9c0a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:19:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f7439613-b8e4-4f46-8c81-f481a493a74e
01/26/2025 23:20:23:INFO:Received: evaluate message f7439613-b8e4-4f46-8c81-f481a493a74e
[92mINFO [0m:      Sent reply
01/26/2025 23:20:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:21:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:21:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 944ddca4-170d-4ae1-97ae-202dba82845b
01/26/2025 23:21:17:INFO:Received: train message 944ddca4-170d-4ae1-97ae-202dba82845b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:21:54:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:22:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:22:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f7e907f-927b-4ce1-814e-04dc9a2878ef
01/26/2025 23:22:45:INFO:Received: evaluate message 7f7e907f-927b-4ce1-814e-04dc9a2878ef
[92mINFO [0m:      Sent reply
01/26/2025 23:22:50:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:23:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:23:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dfd9a425-71b1-4b55-b810-6743556e9838
01/26/2025 23:23:45:INFO:Received: train message dfd9a425-71b1-4b55-b810-6743556e9838
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:24:29:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62afcbe4-53f9-41c5-bb33-a534b477a96c
01/26/2025 23:25:12:INFO:Received: evaluate message 62afcbe4-53f9-41c5-bb33-a534b477a96c

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:25:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9e58a15c-300d-42bf-972f-2f9eb2fdbef9
01/26/2025 23:25:39:INFO:Received: train message 9e58a15c-300d-42bf-972f-2f9eb2fdbef9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:26:11:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:26:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:26:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ec2d372-6759-49b4-ae5e-3bcdadc04987
01/26/2025 23:26:53:INFO:Received: evaluate message 5ec2d372-6759-49b4-ae5e-3bcdadc04987
[92mINFO [0m:      Sent reply
01/26/2025 23:26:58:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e4596e7a-b97c-4719-a550-cf469e14679a
01/26/2025 23:27:36:INFO:Received: train message e4596e7a-b97c-4719-a550-cf469e14679a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:28:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:28:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:28:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9bab3376-a5cf-4620-8d47-865c968ba444
01/26/2025 23:28:55:INFO:Received: evaluate message 9bab3376-a5cf-4620-8d47-865c968ba444
[92mINFO [0m:      Sent reply
01/26/2025 23:29:00:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:29:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:29:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dde12a7c-2727-4f34-97f0-18845ab42375
01/26/2025 23:29:31:INFO:Received: train message dde12a7c-2727-4f34-97f0-18845ab42375
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:30:07:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:30:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:30:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e321b34b-d657-4831-b668-b7dc38faf606
01/26/2025 23:30:44:INFO:Received: evaluate message e321b34b-d657-4831-b668-b7dc38faf606
[92mINFO [0m:      Sent reply
01/26/2025 23:30:47:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:31:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:31:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8feb85f2-4a6b-4b41-8336-61b71de55b2d
01/26/2025 23:31:21:INFO:Received: train message 8feb85f2-4a6b-4b41-8336-61b71de55b2d

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:31:59:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:32:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:32:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 337ce92f-0181-4b34-9b43-c1393d3d0af8
01/26/2025 23:32:38:INFO:Received: evaluate message 337ce92f-0181-4b34-9b43-c1393d3d0af8
[92mINFO [0m:      Sent reply
01/26/2025 23:32:41:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:33:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:33:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 39dec4e9-76ad-4896-b65e-a093fc0066dc
01/26/2025 23:33:03:INFO:Received: train message 39dec4e9-76ad-4896-b65e-a093fc0066dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:33:45:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:34:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:34:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8e9ade7d-416a-4591-8b6a-09a737350755
01/26/2025 23:34:35:INFO:Received: evaluate message 8e9ade7d-416a-4591-8b6a-09a737350755
[92mINFO [0m:      Sent reply
01/26/2025 23:34:38:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:35:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:35:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33acd076-fd9d-48ae-b276-ebf55455d7cc
01/26/2025 23:35:10:INFO:Received: train message 33acd076-fd9d-48ae-b276-ebf55455d7cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:35:46:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82f31c36-4d2f-4c1f-8d5c-0ab5fa2f920d
01/26/2025 23:36:26:INFO:Received: evaluate message 82f31c36-4d2f-4c1f-8d5c-0ab5fa2f920d
[92mINFO [0m:      Sent reply
01/26/2025 23:36:29:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6e5e7166-3ddb-45c1-9a25-7da7a97f5f66
01/26/2025 23:36:49:INFO:Received: train message 6e5e7166-3ddb-45c1-9a25-7da7a97f5f66
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:37:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ffda817a-ee7a-4ccf-a23c-1d01ba5dc196
01/26/2025 23:38:09:INFO:Received: evaluate message ffda817a-ee7a-4ccf-a23c-1d01ba5dc196
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:38:12:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1baee9d8-448f-46fb-bd1a-e5eaf8a97246
01/26/2025 23:38:52:INFO:Received: train message 1baee9d8-448f-46fb-bd1a-e5eaf8a97246
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:39:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2c7f543-4593-4abc-a306-107efcaf3ea6
01/26/2025 23:40:03:INFO:Received: evaluate message f2c7f543-4593-4abc-a306-107efcaf3ea6
[92mINFO [0m:      Sent reply
01/26/2025 23:40:06:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 14911d58-efd7-41fe-bbf5-3c69f40dcce4
01/26/2025 23:40:58:INFO:Received: train message 14911d58-efd7-41fe-bbf5-3c69f40dcce4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:41:41:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 54c333f3-c448-4cf3-8d57-a167f7f6265d
01/26/2025 23:42:30:INFO:Received: evaluate message 54c333f3-c448-4cf3-8d57-a167f7f6265d
[92mINFO [0m:      Sent reply
01/26/2025 23:42:34:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message bbb0bd5a-25ec-41ce-bb43-e99f8d43f8d2
01/26/2025 23:42:42:INFO:Received: reconnect message bbb0bd5a-25ec-41ce-bb43-e99f8d43f8d2
01/26/2025 23:42:42:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/26/2025 23:42:42:INFO:Disconnect and shut down

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010328368283808231, 0.006886196322739124, 0.048965856432914734, 0.0069893659092485905]
Noise Multiplier after list and tensor:  0.01829244673717767
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}



Final client history:
{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}

