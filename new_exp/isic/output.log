nohup: ignoring input
nohup: ignoring input
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/new_exp/isic', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/new_exp/isic']
BaseNM 0.38177490234375
noise multiplier 0.26758588943630457
Noise multiplier before  adjustment: 0.005351717788726091
BaseNM 0.38177490234375
noise multiplier 0.28254565922543406
Noise multiplier before  adjustment: 0.005650913184508681
BaseNM 0.38177490234375
noise multiplier 0.2737503373064101
Noise multiplier before  adjustment: 0.005475006746128202
BaseNM 0.38177490234375
noise multiplier 0.2677649292163551
Noise multiplier before  adjustment: 0.005355298584327102
BaseNM 0.38177490234375
noise multiplier 0.3015311569906771
Noise multiplier before  adjustment: 0.006030623139813542
BaseNM 0.38177490234375
noise multiplier 0.2653410444036126
Noise multiplier before  adjustment: 0.005306820888072252
BaseNM 0.42724609375
noise multiplier 0.3083798894658685
Noise multiplier before  adjustment: 0.00616759778931737
BaseNM 0.42724609375
noise multiplier 0.3231919500976801
Noise multiplier before  adjustment: 0.006463839001953602
BaseNM 0.42724609375
noise multiplier 0.3072072803042829
Noise multiplier before  adjustment: 0.006144145606085658
BaseNM 0.42724609375
noise multiplier 0.310944105964154
Noise multiplier before  adjustment: 0.00621888211928308
BaseNM 0.42724609375
noise multiplier 0.2864773345645517
Noise multiplier before  adjustment: 0.005729546691291034
BaseNM 0.42724609375
noise multiplier 0.3132916644681245
Noise multiplier before  adjustment: 0.00626583328936249
BaseNM 0.46192169189453125
noise multiplier 0.3409183546900749
Noise multiplier before  adjustment: 0.006818367093801498
BaseNM 0.46192169189453125
noise multiplier 0.3135190249886364
Noise multiplier before  adjustment: 0.006270380499772727
BaseNM 0.46192169189453125
noise multiplier 0.3439145530574024
Noise multiplier before  adjustment: 0.0068782910611480475
BaseNM 0.46192169189453125
noise multiplier 0.341826394200325
Noise multiplier before  adjustment: 0.0068365278840065
BaseNM 0.46192169189453125
noise multiplier 0.3186589847318828
Noise multiplier before  adjustment: 0.006373179694637656
BaseNM 0.46192169189453125
noise multiplier 0.33545104623772204
Noise multiplier before  adjustment: 0.006709020924754441
BaseNM 0.4913330078125
noise multiplier 0.34452851209789515
Noise multiplier before  adjustment: 0.006890570241957903
BaseNM 0.4913330078125
noise multiplier 0.37580014555715024
Noise multiplier before  adjustment: 0.007516002911143005
BaseNM 0.4913330078125
noise multiplier 0.34606175031512976
Noise multiplier before  adjustment: 0.006921235006302595
BaseNM 0.4913330078125
noise multiplier 0.34531418327242136
Noise multiplier before  adjustment: 0.006906283665448427
BaseNM 0.4913330078125
noise multiplier 0.3486335228662938
Noise multiplier before  adjustment: 0.006972670457325875
BaseNM 0.4913330078125
noise multiplier 0.3497470417059958
Noise multiplier before  adjustment: 0.006994940834119916
BaseNM 0.5175018310546875
noise multiplier 0.3418303322978318
Noise multiplier before  adjustment: 0.006836606645956636
BaseNM 0.5175018310546875
noise multiplier 0.38013049960136414
Noise multiplier before  adjustment: 0.007602609992027283
BaseNM 0.5175018310546875
noise multiplier 0.3770466591231525
Noise multiplier before  adjustment: 0.0075409331824630495
BaseNM 0.5175018310546875
noise multiplier 0.36539625050500035
Noise multiplier before  adjustment: 0.0073079250101000075
BaseNM 0.5175018310546875
noise multiplier 0.356189479585737
Noise multiplier before  adjustment: 0.00712378959171474
BaseNM 0.5175018310546875
noise multiplier 0.35674496251158416
Noise multiplier before  adjustment: 0.007134899250231684
BaseNM 0.5413436889648438
noise multiplier 0.3961847620084882
Noise multiplier before  adjustment: 0.007923695240169764
BaseNM 0.5413436889648438
noise multiplier 0.38814849266782403
Noise multiplier before  adjustment: 0.007762969853356481
BaseNM 0.5413436889648438
noise multiplier 0.38408813206478953
Noise multiplier before  adjustment: 0.007681762641295791
BaseNM 0.5413436889648438
noise multiplier 0.36395029490813613
Noise multiplier before  adjustment: 0.007279005898162723
BaseNM 0.5413436889648438
noise multiplier 0.38751486456021667
Noise multiplier before  adjustment: 0.007750297291204333
BaseNM 0.5413436889648438
noise multiplier 0.3462200169451535
Noise multiplier before  adjustment: 0.0069244003389030695
BaseNM 0.4621124267578125
noise multiplier 0.3242458994500339
Noise multiplier before  adjustment: 0.010808196648334463
BaseNM 0.4621124267578125
noise multiplier 0.33641293551772833
Noise multiplier before  adjustment: 0.011213764517257611
BaseNM 0.4621124267578125
noise multiplier 0.3311909958720207
Noise multiplier before  adjustment: 0.01103969986240069
BaseNM 0.4621124267578125
noise multiplier 0.32495427783578634
Noise multiplier before  adjustment: 0.010831809261192878
BaseNM 0.4621124267578125
noise multiplier 0.30968115013092756
Noise multiplier before  adjustment: 0.010322705004364252
BaseNM 0.4621124267578125
noise multiplier 0.34058208391070366
Noise multiplier before  adjustment: 0.011352736130356788
BaseNM 0.525970458984375
noise multiplier 0.36812905967235565
Noise multiplier before  adjustment: 0.01227096865574519
BaseNM 0.525970458984375
noise multiplier 0.3804333135485649
Noise multiplier before  adjustment: 0.01268111045161883
BaseNM 0.525970458984375
noise multiplier 0.36030684504657984
Noise multiplier before  adjustment: 0.012010228168219329
BaseNM 0.525970458984375
noise multiplier 0.38224871177226305
Noise multiplier before  adjustment: 0.012741623725742102
BaseNM 0.525970458984375
noise multiplier 0.360746955499053
Noise multiplier before  adjustment: 0.0120248985166351
BaseNM 0.525970458984375
noise multiplier 0.39193141739815474
Noise multiplier before  adjustment: 0.013064380579938492
BaseNM 0.575408935546875
noise multiplier 0.41239448729902506
Noise multiplier before  adjustment: 0.013746482909967501
BaseNM 0.575408935546875
noise multiplier 0.35370317683555186
Noise multiplier before  adjustment: 0.011790105894518395
BaseNM 0.575408935546875
noise multiplier 0.4229139320086688
Noise multiplier before  adjustment: 0.014097131066955626
BaseNM 0.575408935546875
noise multiplier 0.42226309748366475
Noise multiplier before  adjustment: 0.014075436582788824
BaseNM 0.575408935546875
noise multiplier 0.40886844671331346
Noise multiplier before  adjustment: 0.013628948223777115
BaseNM 0.575408935546875
noise multiplier 0.40848435484804213
Noise multiplier before  adjustment: 0.013616145161601404
BaseNM 0.61767578125
noise multiplier 0.46811168268322945
Noise multiplier before  adjustment: 0.015603722756107647
BaseNM 0.61767578125
noise multiplier 0.41059977537952363
Noise multiplier before  adjustment: 0.013686659179317455
BaseNM 0.61767578125
noise multiplier 0.4412964414805174
Noise multiplier before  adjustment: 0.014709881382683913
BaseNM 0.61767578125
noise multiplier 0.45172166731208563
Noise multiplier before  adjustment: 0.015057388910402853
BaseNM 0.61767578125
noise multiplier 0.42703763116151094
Noise multiplier before  adjustment: 0.014234587705383698
BaseNM 0.61767578125
noise multiplier 0.4368829829618335
Noise multiplier before  adjustment: 0.014562766098727782
BaseNM 0.655364990234375
noise multiplier 0.45160231832414865
Noise multiplier before  adjustment: 0.015053410610804955
BaseNM 0.655364990234375
noise multiplier 0.4706632732413709
Noise multiplier before  adjustment: 0.015688775774712362
BaseNM 0.655364990234375
noise multiplier 0.43131470028311014
Noise multiplier before  adjustment: 0.014377156676103672
BaseNM 0.655364990234375
noise multiplier 0.45768171083182096
Noise multiplier before  adjustment: 0.015256057027727365
BaseNM 0.655364990234375
noise multiplier 0.45770636899396777
Noise multiplier before  adjustment: 0.015256878966465593
BaseNM ['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/new_exp/isic', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/new_exp/isic']
BaseNM 0.38177490234375
noise multiplier 0.2672695843502879
Noise multiplier before  adjustment: 0.005345391687005758
BaseNM 0.38177490234375
noise multiplier 0.28314518090337515
Noise multiplier before  adjustment: 0.005662903618067503
BaseNM 0.38177490234375
noise multiplier 0.27346497448161244
Noise multiplier before  adjustment: 0.005469299489632249
BaseNM 0.38177490234375
noise multiplier 0.26796825462952256
Noise multiplier before  adjustment: 0.005359365092590451
BaseNM 0.38177490234375
noise multiplier 0.30075803375802934
Noise multiplier before  adjustment: 0.006015160675160587
BaseNM 0.38177490234375
noise multiplier 0.265380528755486
Noise multiplier before  adjustment: 0.00530761057510972
BaseNM 0.42724609375
noise multiplier 0.3081935839727521
Noise multiplier before  adjustment: 0.006163871679455042
BaseNM 0.42724609375
noise multiplier 0.32299181912094355
Noise multiplier before  adjustment: 0.006459836382418871
BaseNM 0.42724609375
noise multiplier 0.30689815478399396
Noise multiplier before  adjustment: 0.00613796309567988
BaseNM 0.42724609375
noise multiplier 0.3104719687253237
Noise multiplier before  adjustment: 0.006209439374506473
BaseNM 0.42724609375
noise multiplier 0.28609372884966433
Noise multiplier before  adjustment: 0.0057218745769932865
BaseNM 0.42724609375
noise multiplier 0.31444580340757966
Noise multiplier before  adjustment: 0.006288916068151593
BaseNM 0.46192169189453125
noise multiplier 0.3409524760209024
Noise multiplier before  adjustment: 0.006819049520418048
BaseNM 0.46192169189453125
noise multiplier 0.3157638676930219
Noise multiplier before  adjustment: 0.006315277353860438
BaseNM 0.46192169189453125
noise multiplier 0.3443006561137736
Noise multiplier before  adjustment: 0.006886013122275472
BaseNM 0.46192169189453125
noise multiplier 0.3412074614316225
Noise multiplier before  adjustment: 0.0068241492286324505
BaseNM 0.46192169189453125
noise multiplier 0.3179788524284959
Noise multiplier before  adjustment: 0.006359577048569918
BaseNM 0.46192169189453125
noise multiplier 0.33442032895982265
Noise multiplier before  adjustment: 0.006688406579196453
BaseNM 0.4913330078125
noise multiplier 0.34452467039227486
Noise multiplier before  adjustment: 0.006890493407845497
BaseNM 0.4913330078125
noise multiplier 0.3766395431011915
Noise multiplier before  adjustment: 0.00753279086202383
BaseNM 0.4913330078125
noise multiplier 0.3464747532270849
Noise multiplier before  adjustment: 0.006929495064541697
BaseNM 0.4913330078125
noise multiplier 0.34530393593013287
Noise multiplier before  adjustment: 0.006906078718602657
BaseNM 0.4913330078125
noise multiplier 0.3492654140572995
Noise multiplier before  adjustment: 0.00698530828114599
BaseNM 0.4913330078125
noise multiplier 0.35178561694920063
Noise multiplier before  adjustment: 0.007035712338984012
BaseNM 0.5175018310546875
noise multiplier 0.34200060879811645
Noise multiplier before  adjustment: 0.006840012175962329
BaseNM 0.5175018310546875
noise multiplier 0.37903452874161303
Noise multiplier before  adjustment: 0.007580690574832261
BaseNM 0.5175018310546875
noise multiplier 0.377303646877408
Noise multiplier before  adjustment: 0.007546072937548161
BaseNM 0.5175018310546875
noise multiplier 0.3661945224739611
Noise multiplier before  adjustment: 0.0073238904494792225
BaseNM 0.5175018310546875
noise multiplier 0.35679881321266294
Noise multiplier before  adjustment: 0.007135976264253259
BaseNM 0.5175018310546875
noise multiplier 0.3569336524233222
Noise multiplier before  adjustment: 0.007138673048466444
BaseNM 0.5413436889648438
noise multiplier 0.3966395268216729
Noise multiplier before  adjustment: 0.007932790536433459
BaseNM 0.5413436889648438
noise multiplier 0.3862420436926186
Noise multiplier before  adjustment: 0.007724840873852373
BaseNM 0.5413436889648438
noise multiplier 0.3838771996088326
Noise multiplier before  adjustment: 0.007677543992176652
BaseNM 0.5413436889648438
noise multiplier 0.36451737070456147
Noise multiplier before  adjustment: 0.007290347414091229
BaseNM 0.5413436889648438
noise multiplier 0.3865915625356138
Noise multiplier before  adjustment: 0.007731831250712275
BaseNM 0.5413436889648438
noise multiplier 0.3488467843271792
Noise multiplier before  adjustment: 0.006976935686543584
BaseNM 0.4621124267578125
noise multiplier 0.3238947759382427
Noise multiplier before  adjustment: 0.010796492531274756
BaseNM 0.4621124267578125
noise multiplier 0.33536669542081654
Noise multiplier before  adjustment: 0.01117888984736055
BaseNM 0.4621124267578125
noise multiplier 0.33117035683244467
Noise multiplier before  adjustment: 0.011039011894414823
BaseNM 0.4621124267578125
noise multiplier 0.325961765833199
Noise multiplier before  adjustment: 0.010865392194439967
BaseNM 0.4621124267578125
noise multiplier 0.30972381588071585
Noise multiplier before  adjustment: 0.010324127196023861
BaseNM 0.4621124267578125
noise multiplier 0.34224984236061573
Noise multiplier before  adjustment: 0.011408328078687191
BaseNM 0.525970458984375
noise multiplier 0.3683805614709854
Noise multiplier before  adjustment: 0.012279352049032848
BaseNM 0.525970458984375
noise multiplier 0.3828468290157616
Noise multiplier before  adjustment: 0.012761560967192054
BaseNM 0.525970458984375
noise multiplier 0.3603115491569042
Noise multiplier before  adjustment: 0.012010384971896807
BaseNM 0.525970458984375
noise multiplier 0.3813832588493824
Noise multiplier before  adjustment: 0.012712775294979413
BaseNM 0.525970458984375
noise multiplier 0.36068299668841064
Noise multiplier before  adjustment: 0.012022766556280354
BaseNM 0.525970458984375
noise multiplier 0.3919165786355734
Noise multiplier before  adjustment: 0.013063885954519112
BaseNM 0.575408935546875
noise multiplier 0.41283680871129036
Noise multiplier before  adjustment: 0.013761226957043012
BaseNM 0.575408935546875
noise multiplier 0.3534889845177531
Noise multiplier before  adjustment: 0.01178296615059177
BaseNM 0.575408935546875
noise multiplier 0.4224556025583297
Noise multiplier before  adjustment: 0.01408185341861099
BaseNM 0.575408935546875
noise multiplier 0.42221658071503043
Noise multiplier before  adjustment: 0.014073886023834348
BaseNM 0.575408935546875
noise multiplier 0.40779058262705803
Noise multiplier before  adjustment: 0.013593019420901934
BaseNM 0.575408935546875
noise multiplier 0.4080695614684373
Noise multiplier before  adjustment: 0.013602318715614577
BaseNM 0.61767578125
noise multiplier 0.4683640617877245
Noise multiplier before  adjustment: 0.015612135392924149
BaseNM 0.61767578125
noise multiplier 0.4083084047306329
Noise multiplier before  adjustment: 0.013610280157687763
BaseNM 0.61767578125
noise multiplier 0.4407764598727226
Noise multiplier before  adjustment: 0.014692548662424088
BaseNM 0.61767578125
noise multiplier 0.4525753017514944
Noise multiplier before  adjustment: 0.01508584339171648
BaseNM 0.61767578125
noise multiplier 0.42574350629001856
Noise multiplier before  adjustment: 0.014191450209667285
BaseNM 0.61767578125
noise multiplier 0.43730634078383446
Noise multiplier before  adjustment: 0.014576878026127815
BaseNM 0.655364990234375
noise multiplier 0.4518042076379061
Noise multiplier before  adjustment: 0.01506014025459687
BaseNM 0.655364990234375
noise multiplier 0.4709813352674246
Noise multiplier before  adjustment: 0.015699377842247488
BaseNM 0.655364990234375
noise multiplier 0.4308026498183608
Noise multiplier before  adjustment: 0.014360088327278694
BaseNM 0.655364990234375
noise multiplier 0.45723970141261816
Noise multiplier before  adjustment: 0.015241323380420606
BaseNM 0.655364990234375
noise multiplier 0.4566084682010114
Noise multiplier before  adjustment: 0.015220282273367048
BaseNM 0.655364990234375
noise multiplier 0.4789568888954818
Noise multiplier before  adjustment: 0.015965229629849396
BaseNM 0.69000244140625
noise multiplier 0.4810234042815864
Noise multiplier before  adjustment: 0.01603411347605288
BaseNM 0.69000244140625
noise multiplier 0.48915223381482065
Noise multiplier before  adjustment: 0.01630507446049402
BaseNM 0.69000244140625
noise multiplier 0.4760226118378341
Noise multiplier before  adjustment: 0.015867420394594472
BaseNM 0.69000244140625
noise multiplier 0.48697515530511737
Noise multiplier before  adjustment: 0.016232505176837244
BaseNM 0.69000244140625
noise multiplier 0.4866916255559772
Noise multiplier before  adjustment: 0.016223054185199242
BaseNM 0.69000244140625
noise multiplier 0.5041676769033074
Noise multiplier before  adjustment: 0.01680558923011025
BaseNM 0.5389404296875
noise multiplier 0.3911371426656842
Noise multiplier before  adjustment: 0.01955685713328421
BaseNM 0.5389404296875
noise multiplier 0.4178833917248994
Noise multiplier before  adjustment: 0.02089416958624497
BaseNM 0.5389404296875
noise multiplier 0.40107616083696485
Noise multiplier before  adjustment: 0.020053808041848243
BaseNM 0.5389404296875
noise multiplier 0.38269774871878326
Noise multiplier before  adjustment: 0.019134887435939163
BaseNM 0.5389404296875
noise multiplier 0.37237345217727125
Noise multiplier before  adjustment: 0.018618672608863563
BaseNM 0.5389404296875
noise multiplier 0.41732500679790974
Noise multiplier before  adjustment: 0.020866250339895486
BaseNM 0.6243896484375
noise multiplier 0.4596051601693034
Noise multiplier before  adjustment: 0.02298025800846517
BaseNM 0.6243896484375
noise multiplier 0.3755874240305275
Noise multiplier before  adjustment: 0.018779371201526375
BaseNM 0.6243896484375
noise multiplier 0.4402503836899996
Noise multiplier before  adjustment: 0.02201251918449998
BaseNM 0.6243896484375
noise multiplier 0.4491003528237343
Noise multiplier before  adjustment: 0.022455017641186716
BaseNM 0.6243896484375
noise multiplier 0.4693427844904363
Noise multiplier before  adjustment: 0.023467139224521816
BaseNM 0.6243896484375
noise multiplier 0.45783187728375196
Noise multiplier before  adjustment: 0.022891593864187598
BaseNM 0.691375732421875
noise multiplier 0.4883885383605957
Noise multiplier before  adjustment: 0.024419426918029785
BaseNM 0.691375732421875
noise multiplier 0.43604156374931335
Noise multiplier before  adjustment: 0.021802078187465667
BaseNM 0.691375732421875
noise multiplier 0.49930175114423037
Noise multiplier before  adjustment: 0.024965087557211517
BaseNM 0.691375732421875
noise multiplier 0.4866982763633132
Noise multiplier before  adjustment: 0.02433491381816566
BaseNM 0.691375732421875
noise multiplier 0.5066206834744662
Noise multiplier before  adjustment: 0.02533103417372331
BaseNM 0.691375732421875
noise multiplier 0.4719691714271903
Noise multiplier before  adjustment: 0.023598458571359514
BaseNM 0.749053955078125
noise multiplier 0.5480505442246795
Noise multiplier before  adjustment: 0.027402527211233974
BaseNM 0.749053955078125
noise multiplier 0.5191915850155056
Noise multiplier before  adjustment: 0.02595957925077528
BaseNM 0.749053955078125
noise multiplier 0.5715375654399395
Noise multiplier before  adjustment: 0.028576878271996974
BaseNM 0.749053955078125
noise multiplier 0.5273593999445438
Noise multiplier before  adjustment: 0.02636796999722719
BaseNM 0.749053955078125
noise multiplier 0.5431207325309515
Noise multiplier before  adjustment: 0.027156036626547576
BaseNM 0.749053955078125
noise multiplier 0.5205512344837189
Noise multiplier before  adjustment: 0.026027561724185945
BaseNM 0.80078125
noise multiplier 0.5719382194802165
Noise multiplier before  adjustment: 0.028596910974010827
BaseNM 0.80078125
noise multiplier 0.6015139608643949
Noise multiplier before  adjustment: 0.030075698043219745
BaseNM 0.80078125
noise multiplier 0.6178926434367895
Noise multiplier before  adjustment: 0.030894632171839475
BaseNM 0.80078125
noise multiplier 0.5538948820903897
Noise multiplier before  adjustment: 0.027694744104519487
BaseNM 0.80078125
noise multiplier 0.502925596665591
Noise multiplier before  adjustment: 0.02514627983327955
BaseNM 0.80078125
noise multiplier 0.6505971681326628
Noise multiplier before  adjustment: 0.03252985840663314
BaseNM 0.848236083984375
noise multiplier 0.6000510975718498
Noise multiplier before  adjustment: 0.03000255487859249
BaseNM 0.848236083984375
noise multiplier 0.6247850591316819
Noise multiplier before  adjustment: 0.031239252956584095
BaseNM 0.848236083984375
noise multiplier 0.6248449636623263
Noise multiplier before  adjustment: 0.031242248183116317
BaseNM 0.848236083984375
noise multiplier 0.6111816614866257
Noise multiplier before  adjustment: 0.030559083074331285
BaseNM 0.848236083984375
noise multiplier 0.5628455830737948
Noise multiplier before  adjustment: 0.02814227915368974
BaseNM 0.848236083984375
noise multiplier 0.5659333476796746
Noise multiplier before  adjustment: 0.02829666738398373
BaseNM 0.7086181640625
noise multiplier 0.493345033377409
Noise multiplier before  adjustment: 0.0493345033377409
BaseNM 0.7086181640625
noise multiplier 0.5247769025154412
Noise multiplier before  adjustment: 0.05247769025154412
BaseNM 0.7086181640625
noise multiplier 0.4872213276103139
Noise multiplier before  adjustment: 0.04872213276103139
BaseNM 0.7086181640625
noise multiplier 0.5221030423417687
Noise multiplier before  adjustment: 0.05221030423417687
BaseNM 0.7086181640625
noise multiplier 0.508241682080552
Noise multiplier before  adjustment: 0.0508241682080552
BaseNM 0.7086181640625
noise multiplier 0.44194001983851194
Noise multiplier before  adjustment: 0.04419400198385119
BaseNM 0.85723876953125
noise multiplier 0.5809757150709629
Noise multiplier before  adjustment: 0.05809757150709629
BaseNM 0.85723876953125
noise multiplier 0.5680006537586451
Noise multiplier before  adjustment: 0.0568000653758645
BaseNM 0.85723876953125
noise multiplier 0.6107625467702746
Noise multiplier before  adjustment: 0.061076254677027465
BaseNM 0.85723876953125
noise multiplier 0.5695745539851487
Noise multiplier before  adjustment: 0.056957455398514864
BaseNM 0.85723876953125
noise multiplier 0.5794319557026029
Noise multiplier before  adjustment: 0.057943195570260286
BaseNM 0.85723876953125
noise multiplier 0.5834087203256786
Noise multiplier before  adjustment: 0.05834087203256786
BaseNM 0.9771728515625
noise multiplier 0.7191835511475801
Noise multiplier before  adjustment: 0.07191835511475801
BaseNM 0.9771728515625
noise multiplier 0.6794827096164227
Noise multiplier before  adjustment: 0.06794827096164227
BaseNM 0.9771728515625
noise multiplier 0.6697221379727125
Noise multiplier before  adjustment: 0.06697221379727125
BaseNM 0.9771728515625
noise multiplier 0.7009047707542777
Noise multiplier before  adjustment: 0.07009047707542777
BaseNM 0.9771728515625
noise multiplier 0.7178616668097675
Noise multiplier before  adjustment: 0.07178616668097675
BaseNM 0.9771728515625
noise multiplier 0.6556918490678072
Noise multiplier before  adjustment: 0.06556918490678072
BaseNM 1.08154296875
noise multiplier 0.7502238163724542
Noise multiplier before  adjustment: 0.07502238163724542
BaseNM 1.08154296875
noise multiplier 0.7027110606431961
Noise multiplier before  adjustment: 0.0702711060643196
BaseNM 1.08154296875
noise multiplier 0.7809876538813114
Noise multiplier before  adjustment: 0.07809876538813114
BaseNM 1.08154296875
noise multiplier 0.7643294502049685
Noise multiplier before  adjustment: 0.07643294502049684
BaseNM 1.08154296875
noise multiplier 0.7904113228432834
Noise multiplier before  adjustment: 0.07904113228432834
BaseNM 1.08154296875
noise multiplier 0.7359314262866974
Noise multiplier before  adjustment: 0.07359314262866974
BaseNM 1.1761474609375
noise multiplier 0.8600656893104315
Noise multiplier before  adjustment: 0.08600656893104315
BaseNM 1.1761474609375
noise multiplier 0.8491814555600286
Noise multiplier before  adjustment: 0.08491814555600286
BaseNM 1.1761474609375
noise multiplier 0.8051338363438845
Noise multiplier before  adjustment: 0.08051338363438845
BaseNM 1.1761474609375
0.655364990234375
noise multiplier 0.47980860783718526
Noise multiplier before  adjustment: 0.01599362026123951
BaseNM 0.69000244140625
noise multiplier 0.4806219395250082
Noise multiplier before  adjustment: 0.016020731317500272
BaseNM 0.69000244140625
noise multiplier 0.4862214846070856
Noise multiplier before  adjustment: 0.016207382820236187
BaseNM 0.69000244140625
noise multiplier 0.4767428352497518
Noise multiplier before  adjustment: 0.015891427841658392
BaseNM 0.69000244140625
noise multiplier 0.48795644845813513
Noise multiplier before  adjustment: 0.016265214948604504
BaseNM 0.69000244140625
noise multiplier 0.4915811992250383
Noise multiplier before  adjustment: 0.016386039974167942
BaseNM 0.69000244140625
noise multiplier 0.5023844158276916
Noise multiplier before  adjustment: 0.016746147194256385
BaseNM 0.5389404296875
noise multiplier 0.3913940545171499
Noise multiplier before  adjustment: 0.019569702725857497
BaseNM 0.5389404296875
noise multiplier 0.4164690007455647
Noise multiplier before  adjustment: 0.020823450037278236
BaseNM 0.5389404296875
noise multiplier 0.4001394878141582
Noise multiplier before  adjustment: 0.02000697439070791
BaseNM 0.5389404296875
noise multiplier 0.38348551094532013
Noise multiplier before  adjustment: 0.019174275547266008
BaseNM 0.5389404296875
noise multiplier 0.37340483628213406
Noise multiplier before  adjustment: 0.018670241814106703
BaseNM 0.5389404296875
noise multiplier 0.4179624642711133
Noise multiplier before  adjustment: 0.020898123213555664
BaseNM 0.6243896484375
noise multiplier 0.45971056167036295
Noise multiplier before  adjustment: 0.02298552808351815
BaseNM 0.6243896484375
noise multiplier 0.374198054196313
Noise multiplier before  adjustment: 0.01870990270981565
BaseNM 0.6243896484375
noise multiplier 0.44048925768584013
Noise multiplier before  adjustment: 0.022024462884292006
BaseNM 0.6243896484375
noise multiplier 0.4490621732547879
Noise multiplier before  adjustment: 0.022453108662739397
BaseNM 0.6243896484375
noise multiplier 0.4695881581865251
Noise multiplier before  adjustment: 0.023479407909326254
BaseNM 0.6243896484375
noise multiplier 0.4603763595223427
Noise multiplier before  adjustment: 0.023018817976117135
BaseNM 0.691375732421875
noise multiplier 0.4888837728649378
Noise multiplier before  adjustment: 0.02444418864324689
BaseNM 0.691375732421875
noise multiplier 0.43668877333402634
Noise multiplier before  adjustment: 0.021834438666701317
BaseNM 0.691375732421875
noise multiplier 0.4992306628264487
Noise multiplier before  adjustment: 0.024961533141322434
BaseNM 0.691375732421875
noise multiplier 0.4886655732989311
Noise multiplier before  adjustment: 0.024433278664946555
BaseNM 0.691375732421875
noise multiplier 0.5077810790389776
Noise multiplier before  adjustment: 0.025389053951948883
BaseNM 0.691375732421875
noise multiplier 0.4741500671952963
Noise multiplier before  adjustment: 0.023707503359764813
BaseNM 0.749053955078125
noise multiplier 0.5480318935588002
Noise multiplier before  adjustment: 0.02740159467794001
BaseNM 0.749053955078125
noise multiplier 0.5203454461880028
Noise multiplier before  adjustment: 0.026017272309400142
BaseNM 0.749053955078125
noise multiplier 0.5705880112946033
Noise multiplier before  adjustment: 0.02852940056473017
BaseNM 0.749053955078125
noise multiplier 0.5274427449330688
Noise multiplier before  adjustment: 0.026372137246653438
BaseNM 0.749053955078125
noise multiplier 0.5455391551367939
Noise multiplier before  adjustment: 0.027276957756839693
BaseNM 0.749053955078125
noise multiplier 0.5216668825596571
Noise multiplier before  adjustment: 0.026083344127982855
BaseNM 0.80078125
noise multiplier 0.5713969822973013
Noise multiplier before  adjustment: 0.028569849114865063
BaseNM 0.80078125
noise multiplier 0.5991303832270205
Noise multiplier before  adjustment: 0.029956519161351024
BaseNM 0.80078125
noise multiplier 0.6176216648891568
Noise multiplier before  adjustment: 0.03088108324445784
BaseNM 0.80078125
noise multiplier 0.5536386203020811
Noise multiplier before  adjustment: 0.027681931015104057
BaseNM 0.80078125
noise multiplier 0.5034847466740757
Noise multiplier before  adjustment: 0.025174237333703785
BaseNM 0.80078125
noise multiplier 0.6523410975933075
Noise multiplier before  adjustment: 0.032617054879665375
BaseNM 0.848236083984375
noise multiplier 0.6002543736249208
Noise multiplier before  adjustment: 0.030012718681246043
BaseNM 0.848236083984375
noise multiplier 0.6261718813329935
Noise multiplier before  adjustment: 0.031308594066649674
BaseNM 0.848236083984375
noise multiplier 0.6247303001582623
Noise multiplier before  adjustment: 0.031236515007913114
BaseNM 0.848236083984375
noise multiplier 0.6111396076157689
Noise multiplier before  adjustment: 0.030556980380788447
BaseNM 0.848236083984375
noise multiplier 0.5630811494775116
Noise multiplier before  adjustment: 0.02815405747387558
BaseNM 0.848236083984375
noise multiplier 0.5621451502665877
Noise multiplier before  adjustment: 0.028107257513329386
BaseNM 0.7086181640625
noise multiplier 0.49351801723241806
Noise multiplier before  adjustment: 0.04935180172324181
BaseNM 0.7086181640625
noise multiplier 0.5270726359449327
Noise multiplier before  adjustment: 0.05270726359449327
BaseNM 0.7086181640625
noise multiplier 0.48690836410969496
Noise multiplier before  adjustment: 0.048690836410969496
BaseNM 0.7086181640625
noise multiplier 0.5211326284334064
Noise multiplier before  adjustment: 0.052113262843340634
BaseNM 0.7086181640625
noise multiplier 0.5099219959229231
Noise multiplier before  adjustment: 0.05099219959229231
BaseNM 0.7086181640625
noise multiplier 0.4426000942476094
Noise multiplier before  adjustment: 0.04426000942476094
BaseNM 0.85723876953125
noise multiplier 0.580522995442152
Noise multiplier before  adjustment: 0.0580522995442152
BaseNM 0.85723876953125
noise multiplier 0.5668079340830445
Noise multiplier before  adjustment: 0.05668079340830445
BaseNM 0.85723876953125
noise multiplier 0.6098162671551108
Noise multiplier before  adjustment: 0.060981626715511085
BaseNM 0.85723876953125
noise multiplier 0.567661676555872
Noise multiplier before  adjustment: 0.056766167655587195
BaseNM 0.85723876953125
noise multiplier 0.5800249711610377
Noise multiplier before  adjustment: 0.058002497116103766
BaseNM 0.85723876953125
noise multiplier 0.5855532386340201
Noise multiplier before  adjustment: 0.05855532386340201
BaseNM 0.9771728515625
noise multiplier 0.7198468130081892
Noise multiplier before  adjustment: 0.07198468130081892
BaseNM 0.9771728515625
noise multiplier 0.6808896493166685
Noise multiplier before  adjustment: 0.06808896493166686
BaseNM 0.9771728515625
noise multiplier 0.669170456007123
Noise multiplier before  adjustment: 0.0669170456007123
BaseNM 0.9771728515625
noise multiplier 0.6974243810400367
Noise multiplier before  adjustment: 0.06974243810400367
BaseNM 0.9771728515625
noise multiplier 0.7173932548612356
Noise multiplier before  adjustment: 0.07173932548612356
BaseNM 0.9771728515625
noise multiplier 0.6560235768556595
Noise multiplier before  adjustment: 0.06560235768556595
BaseNM 1.08154296875
noise multiplier 0.7502713277935982
Noise multiplier before  adjustment: 0.07502713277935982
BaseNM 1.08154296875
noise multiplier 0.7066502375528216
Noise multiplier before  adjustment: 0.07066502375528216
BaseNM 1.08154296875
noise multiplier 0.7810115069150925
Noise multiplier before  adjustment: 0.07810115069150925
BaseNM 1.08154296875
noise multiplier 0.7644946910440922
Noise multiplier before  adjustment: 0.07644946910440922
BaseNM 1.08154296875
noise multiplier 0.793876264244318
Noise multiplier before  adjustment: 0.0793876264244318
BaseNM 1.08154296875
noise multiplier 0.7399263046681881
Noise multiplier before  adjustment: 0.07399263046681881
BaseNM 1.1761474609375
noise multiplier 0.8600258063524961
Noise multiplier before  adjustment: 0.08600258063524961
BaseNM 1.1761474609375
noise multiplier 0.8470747144892812
Noise multiplier before  adjustment: 0.08470747144892812
BaseNM 1.1761474609375
noise multiplier 0.8042703233659267
Noise multiplier before  adjustment: 0.08042703233659268
BaseNM 1.1761474609375
noise multiplier 0.7998201753944159
Noise multiplier before  adjustment: 0.07998201753944159
BaseNM 1.1761474609375
noise multiplier 0.8292707898654044
Noise multiplier before  adjustment: 0.08292707898654043
BaseNM 1.1761474609375
noise multiplier 0.7759294332936406
Noise multiplier before  adjustment: 0.07759294332936406
BaseNM 1.263427734375
noise multiplier 0.9381746109575033
Noise multiplier before  adjustment: 0.09381746109575033
BaseNM 1.263427734375
noise multiplier 0.8486929386854172
Noise multiplier before  adjustment: 0.08486929386854172
BaseNM 1.263427734375
noise multiplier 0.9505474455654621
Noise multiplier before  adjustment: 0.09505474455654621
BaseNM 1.263427734375
noise multiplier 0.8956356514245272
Noise multiplier before  adjustment: 0.08956356514245271
BaseNM 1.263427734375
noise multiplier 0.8929569008760154
Noise multiplier before  adjustment: 0.08929569008760155
BaseNM 1.263427734375
noise multiplier 0.8099793158471584
Noise multiplier before  adjustment: 0.08099793158471584
BaseNM 0.953369140625
noise multiplier 0.6691450364887714
Noise multiplier before  adjustment: 0.1338290072977543
BaseNM 0.953369140625
noise multiplier 0.729557198472321
Noise multiplier before  adjustment: 0.1459114396944642
BaseNM 0.953369140625
noise multiplier 0.6659494917839766
Noise multiplier before  adjustment: 0.13318989835679532
BaseNM 0.953369140625
noise multiplier 0.7296643052250147
Noise multiplier before  adjustment: 0.14593286104500294
BaseNM 0.953369140625
noise multiplier 0.6783098801970482
Noise multiplier before  adjustment: 0.13566197603940963
BaseNM 0.953369140625
noise multiplier 0.7018094602972269
Noise multiplier before  adjustment: 0.14036189205944538
BaseNM 1.234130859375
noise multiplier 0.8717179428786039
Noise multiplier before  adjustment: 0.1743435885757208
BaseNM 1.234130859375
noise multiplier 0.9176003294996917
Noise multiplier before  adjustment: 0.18352006589993836
BaseNM 1.234130859375
noise multiplier 0.7849213276058435
Noise multiplier before  adjustment: 0.1569842655211687
BaseNM 1.234130859375
noise multiplier 0.8856059601530433
Noise multiplier before  adjustment: 0.17712119203060867
BaseNM 1.234130859375
noise multiplier 0.8402340961620212
Noise multiplier before  adjustment: 0.16804681923240422
BaseNM 1.234130859375
noise multiplier 0.8705004481598735
Noise multiplier before  adjustment: 0.1741000896319747
BaseNM 1.46728515625
noise multiplier 1.0491044782102108
Noise multiplier before  adjustment: 0.20982089564204215
BaseNM 1.46728515625
noise multiplier 1.0870867087505758
Noise multiplier before  adjustment: 0.21741734175011515
BaseNM 1.46728515625
noise multiplier 1.0459508001804352
Noise multiplier before  adjustment: 0.20919016003608704
BaseNM 1.46728515625
noise multiplier 1.0772266555577517
Noise multiplier before  adjustment: 0.21544533111155034
BaseNM 1.46728515625
noise multiplier 1.0130156353116035
Noise multiplier before  adjustment: 0.2026031270623207
BaseNM 1.46728515625
noise multiplier 1.036713213659823
Noise multiplier before  adjustment: 0.2073426427319646
BaseNM 1.6748046875
noise multiplier 1.155093789100647
Noise multiplier before  adjustment: 0.2310187578201294
BaseNM 1.6748046875
noise multiplier 1.224799639545381
Noise multiplier before  adjustment: 0.24495992790907622
BaseNM 1.6748046875
noise multiplier 1.1333526447415352
Noise multiplier before  adjustment: 0.22667052894830703
BaseNM 1.6748046875
noise multiplier 1.2064994433894753
Noise multiplier before  adjustment: 0.24129988867789506
BaseNM 1.6748046875
noise multiplier 1.1278581549413502
Noise multiplier before  adjustment: 0.22557163098827004
BaseNM 1.6748046875
noise multiplier 1.142480231821537
Noise multiplier before  adjustment: 0.2284960463643074
BaseNM 1.86279296875
noise multiplier 1.3842772841453552
Noise multiplier before  adjustment: 0.27685545682907103
BaseNM 1.86279296875
noise multiplier 1.269241776317358
Noise multiplier before  adjustment: 0.2538483552634716
BaseNM 1.86279296875
noise multiplier 1.2678305078297853
Noise multiplier before  adjustment: 0.25356610156595705
BaseNM 1.86279296875
noise multiplier 1.2702700709924102
Noise multiplier before  adjustment: 0.25405401419848206
BaseNM 1.86279296875
noise multiplier 1.3188754664734006
Noise multiplier before  adjustment: 0.26377509329468013
BaseNM 1.86279296875
noise multiplier 1.2345933597534895
Noise multiplier before  adjustment: 0.2469186719506979
BaseNM 2.0361328125
noise multiplier 1.453930938616395
Noise multiplier before  adjustment: 0.290786187723279
BaseNM 2.0361328125
noise multiplier 1.5575189907103777
Noise multiplier before  adjustment: 0.3115037981420755
BaseNM 2.0361328125
noise multiplier 1.473515471443534
Noise multiplier before  adjustment: 0.2947030942887068
BaseNM 2.0361328125
noise multiplier 1.3889658357948065
Noise multiplier before  adjustment: 0.2777931671589613
BaseNM 2.0361328125
noise multiplier 1.3871984872967005
Noise multiplier before  adjustment: 0.2774396974593401
BaseNM 2.0361328125
noise multiplier 1.4164390796795487
Noise multiplier before  adjustment: 0.28328781593590974
BaseNM 2.099609375
noise multiplier 1.5437698736786842
Noise multiplier before  adjustment: 1.5437698736786842
BaseNM 2.099609375
noise multiplier 1.2925661765038967
Noise multiplier before  adjustment: 1.2925661765038967
BaseNM 2.099609375
noise multiplier 1.430607594549656
Noise multiplier before  adjustment: 1.430607594549656
BaseNM 2.099609375
noise multiplier 1.5146120265126228
Noise multiplier before  adjustment: 1.5146120265126228
BaseNM 2.099609375
noise multiplier 1.5311255566775799
Noise multiplier before  adjustment: 1.5311255566775799
BaseNM 2.099609375
noise multiplier 1.4534997586160898
Noise multiplier before  adjustment: 1.4534997586160898
BaseNM 3.57421875
noise multiplier 2.628499962389469
Noise multiplier before  adjustment: 2.628499962389469
BaseNM 3.57421875
noise multiplier 2.7162830270826817
Noise multiplier before  adjustment: 2.7162830270826817
BaseNM 3.57421875
noise multiplier 2.524609327316284
Noise multiplier before  adjustment: 2.524609327316284
BaseNM 3.57421875
noise multiplier 2.5740360021591187
Noise multiplier before  adjustment: 2.5740360021591187
BaseNM 3.57421875
noise multiplier 2.659778654575348
Noise multiplier before  adjustment: 2.659778654575348
BaseNM 3.57421875
noise multiplier 2.717584842815995
Noise multiplier before  adjustment: 2.717584842815995
BaseNM 4.8828125
noise multiplier 3.420351631939411
Noise multiplier before  adjustment: 3.420351631939411
BaseNM 4.8828125
noise multiplier 3.0953084882348776
Noise multiplier before  adjustment: 3.0953084882348776
BaseNM 4.8828125
noise multiplier 3.1977933421730995
Noise multiplier before  adjustment: 3.1977933421730995
BaseNM 4.8828125
noise multiplier 3.479092739522457
Noise multiplier before  adjustment: 3.479092739522457
BaseNM 4.8828125
noise multiplier 3.139929009601474
Noise multiplier before  adjustment: 3.139929009601474
BaseNM 4.8828125
noise multiplier 3.6483695209026337
Noise multiplier before  adjustment: 3.6483695209026337
BaseNM 6.015625
noise multiplier 4.466718979179859
Noise multiplier before  adjustment: 4.466718979179859
BaseNM 6.015625
noise multiplier 4.485148683190346
Noise multiplier before  adjustment: 4.485148683190346
BaseNM 6.015625
noise multiplier 4.146159775555134
Noise multiplier before  adjustment: 4.146159775555134
BaseNM 6.015625
noise multiplier 4.087289109826088
Noise multiplier before  adjustment: 4.087289109826088
BaseNM 6.015625
noise multiplier 4.398937035351992
Noise multiplier before  adjustment: 4.398937035351992
BaseNM 6.015625
noise multiplier 4.708793096244335
Noise multiplier before  adjustment: 4.708793096244335
BaseNM 7.0703125
noise multiplier 5.1021477952599525
Noise multiplier before  adjustment: 5.1021477952599525
BaseNM 7.0703125
noise multiplier 5.167387027293444
Noise multiplier before  adjustment: 5.167387027293444
BaseNM 7.0703125
noise multiplier 5.0790499821305275
Noise multiplier before  adjustment: 5.0790499821305275
BaseNM 7.0703125
noise multiplier 4.814572259783745
Noise multiplier before  adjustment: 4.814572259783745
BaseNM 7.0703125
noise multiplier 5.019820351153612
noise multiplier 0.7991119716316462
Noise multiplier before  adjustment: 0.07991119716316461
BaseNM 1.1761474609375
noise multiplier 0.8309340821579099
Noise multiplier before  adjustment: 0.08309340821579099
BaseNM 1.1761474609375
noise multiplier 0.7744692703709006
Noise multiplier before  adjustment: 0.07744692703709007
BaseNM 1.263427734375
noise multiplier 0.9378429688513279
Noise multiplier before  adjustment: 0.09378429688513279
BaseNM 1.263427734375
noise multiplier 0.8467303840443492
Noise multiplier before  adjustment: 0.08467303840443492
BaseNM 1.263427734375
noise multiplier 0.9506769236177206
Noise multiplier before  adjustment: 0.09506769236177207
BaseNM 1.263427734375
noise multiplier 0.8975365981459618
Noise multiplier before  adjustment: 0.08975365981459618
BaseNM 1.263427734375
noise multiplier 0.8950391695834696
Noise multiplier before  adjustment: 0.08950391695834696
BaseNM 1.263427734375
noise multiplier 0.8099138280376792
Noise multiplier before  adjustment: 0.08099138280376791
BaseNM 0.953369140625
noise multiplier 0.6683658566325903
Noise multiplier before  adjustment: 0.13367317132651807
BaseNM 0.953369140625
noise multiplier 0.7301962170749903
Noise multiplier before  adjustment: 0.14603924341499805
BaseNM 0.953369140625
noise multiplier 0.666421826928854
Noise multiplier before  adjustment: 0.1332843653857708
BaseNM 0.953369140625
noise multiplier 0.7319273017346859
Noise multiplier before  adjustment: 0.14638546034693717
BaseNM 0.953369140625
noise multiplier 0.6781666418537498
Noise multiplier before  adjustment: 0.13563332837074996
BaseNM 0.953369140625
noise multiplier 0.7022631065919995
Noise multiplier before  adjustment: 0.14045262131839992
BaseNM 1.234130859375
noise multiplier 0.8715500142425299
Noise multiplier before  adjustment: 0.17431000284850598
BaseNM 1.234130859375
noise multiplier 0.9182843891903758
Noise multiplier before  adjustment: 0.18365687783807516
BaseNM 1.234130859375
noise multiplier 0.7856053775176406
Noise multiplier before  adjustment: 0.1571210755035281
BaseNM 1.234130859375
noise multiplier 0.8823441490530968
Noise multiplier before  adjustment: 0.17646882981061934
BaseNM 1.234130859375
noise multiplier 0.8384151919744909
Noise multiplier before  adjustment: 0.16768303839489818
BaseNM 1.234130859375
noise multiplier 0.8769573718309402
Noise multiplier before  adjustment: 0.17539147436618804
BaseNM 1.46728515625
noise multiplier 1.0500540230423212
Noise multiplier before  adjustment: 0.21001080460846425
BaseNM 1.46728515625
noise multiplier 1.0861054016277194
Noise multiplier before  adjustment: 0.21722108032554388
BaseNM 1.46728515625
noise multiplier 1.0473192818462849
Noise multiplier before  adjustment: 0.20946385636925696
BaseNM 1.46728515625
noise multiplier 1.074648318812251
Noise multiplier before  adjustment: 0.2149296637624502
BaseNM 1.46728515625
noise multiplier 1.0148479295894504
Noise multiplier before  adjustment: 0.20296958591789008
BaseNM 1.46728515625
noise multiplier 1.0317019196227193
Noise multiplier before  adjustment: 0.20634038392454385
BaseNM 1.6748046875
noise multiplier 1.1553248390555382
Noise multiplier before  adjustment: 0.23106496781110764
BaseNM 1.6748046875
noise multiplier 1.2260056883096695
Noise multiplier before  adjustment: 0.2452011376619339
BaseNM 1.6748046875
noise multiplier 1.1330959927290678
Noise multiplier before  adjustment: 0.22661919854581355
BaseNM 1.6748046875
noise multiplier 1.2077192505821586
Noise multiplier before  adjustment: 0.2415438501164317
BaseNM 1.6748046875
noise multiplier 1.133832428837195
Noise multiplier before  adjustment: 0.22676648576743902
BaseNM 1.6748046875
noise multiplier 1.1373220290988684
Noise multiplier before  adjustment: 0.22746440581977367
BaseNM 1.86279296875
noise multiplier 1.384886261075735
Noise multiplier before  adjustment: 0.276977252215147
BaseNM 1.86279296875
noise multiplier 1.2802640069276094
Noise multiplier before  adjustment: 0.2560528013855219
BaseNM 1.86279296875
noise multiplier 1.2654530452564359
Noise multiplier before  adjustment: 0.25309060905128716
BaseNM 1.86279296875
noise multiplier 1.2695769341662526
Noise multiplier before  adjustment: 0.25391538683325054
BaseNM 1.86279296875
noise multiplier 1.3161388956941664
Noise multiplier before  adjustment: 0.2632277791388333
BaseNM 1.86279296875
noise multiplier 1.2390924682840705
Noise multiplier before  adjustment: 0.2478184936568141
BaseNM 2.0361328125
noise multiplier 1.4529641475528479
Noise multiplier before  adjustment: 0.2905928295105696
BaseNM 2.0361328125
noise multiplier 1.5585778299719095
Noise multiplier before  adjustment: 0.3117155659943819
BaseNM 2.0361328125
noise multiplier 1.4734186120331287
Noise multiplier before  adjustment: 0.29468372240662577
BaseNM 2.0361328125
noise multiplier 1.3851308729499578
Noise multiplier before  adjustment: 0.27702617458999157
BaseNM 2.0361328125
noise multiplier 1.3895376734435558
Noise multiplier before  adjustment: 0.27790753468871116
BaseNM 2.0361328125
noise multiplier 1.4190377909690142
Noise multiplier before  adjustment: 0.2838075581938028
BaseNM 2.099609375
noise multiplier 1.5415890030562878
Noise multiplier before  adjustment: 1.5415890030562878
BaseNM 2.099609375
noise multiplier 1.2841744115576148
Noise multiplier before  adjustment: 1.2841744115576148
BaseNM 2.099609375
noise multiplier 1.4334860388189554
Noise multiplier before  adjustment: 1.4334860388189554
BaseNM 2.099609375
noise multiplier 1.5152257587760687
Noise multiplier before  adjustment: 1.5152257587760687
BaseNM 2.099609375
noise multiplier 1.5343495216220617
Noise multiplier before  adjustment: 1.5343495216220617
BaseNM 2.099609375
noise multiplier 1.4445033501833677
Noise multiplier before  adjustment: 1.4445033501833677
BaseNM 3.57421875
noise multiplier 2.627587202936411
Noise multiplier before  adjustment: 2.627587202936411
BaseNM 3.57421875
noise multiplier 2.720919609069824
Noise multiplier before  adjustment: 2.720919609069824
BaseNM 3.57421875
noise multiplier 2.5226364322006702
Noise multiplier before  adjustment: 2.5226364322006702
BaseNM 3.57421875
noise multiplier 2.5741473734378815
Noise multiplier before  adjustment: 2.5741473734378815
BaseNM 3.57421875
noise multiplier 2.658325783908367
Noise multiplier before  adjustment: 2.658325783908367
BaseNM 3.57421875
noise multiplier 2.721839811652899
Noise multiplier before  adjustment: 2.721839811652899
BaseNM 4.8828125
noise multiplier 3.421082839369774
Noise multiplier before  adjustment: 3.421082839369774
BaseNM 4.8828125
noise multiplier 3.090689020231366
Noise multiplier before  adjustment: 3.090689020231366
BaseNM 4.8828125
noise multiplier 3.206667110323906
Noise multiplier before  adjustment: 3.206667110323906
BaseNM 4.8828125
noise multiplier 3.4744753316044807
Noise multiplier before  adjustment: 3.4744753316044807
BaseNM 4.8828125
noise multiplier 3.128475297242403
Noise multiplier before  adjustment: 3.128475297242403
BaseNM 4.8828125
noise multiplier 3.642494659870863
Noise multiplier before  adjustment: 3.642494659870863
BaseNM 6.015625
noise multiplier 4.461805492639542
Noise multiplier before  adjustment: 4.461805492639542
BaseNM 6.015625
noise multiplier 4.482452105730772
Noise multiplier before  adjustment: 4.482452105730772
BaseNM 6.015625
noise multiplier 4.143915310502052
Noise multiplier before  adjustment: 4.143915310502052
BaseNM 6.015625
noise multiplier 4.101403146982193
Noise multiplier before  adjustment: 4.101403146982193
BaseNM 6.015625
noise multiplier 4.413890331983566
Noise multiplier before  adjustment: 4.413890331983566
BaseNM 6.015625
noise multiplier 4.679280459880829
Noise multiplier before  adjustment: 4.679280459880829
BaseNM 7.0703125
noise multiplier 5.108227364718914
Noise multiplier before  adjustment: 5.108227364718914
BaseNM 7.0703125
noise multiplier 5.156101502478123
Noise multiplier before  adjustment: 5.156101502478123
BaseNM 7.0703125
noise multiplier 5.0832371562719345
Noise multiplier before  adjustment: 5.0832371562719345
BaseNM 7.0703125
noise multiplier 4.821921266615391
Noise multiplier before  adjustment: 4.821921266615391
BaseNM 7.0703125
noise multiplier 5.027998711913824
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
Noise multiplier before  adjustment: 5.019820351153612
BaseNM 7.0703125
noise multiplier 4.768834501504898
Noise multiplier before  adjustment: 4.768834501504898
BaseNM 7.96875
noise multiplier 5.484271124005318
Noise multiplier before  adjustment: 5.484271124005318
BaseNM 7.96875
noise multiplier 6.0979882925748825
Noise multiplier before  adjustment: 6.0979882925748825
BaseNM 7.96875
noise multiplier 5.570707514882088
Noise multiplier before  adjustment: 5.570707514882088
BaseNM 7.96875
noise multiplier 5.508421007543802
Noise multiplier before  adjustment: 5.508421007543802
BaseNM 7.96875
noise multiplier 5.12787470407784
Noise multiplier before  adjustment: 5.12787470407784
BaseNM 7.96875
noise multiplier 5.543196938931942
Noise multiplier before  adjustment: 5.543196938931942
BaseNM 5.46875
noise multiplier 3.822577454149723
Noise multiplier before  adjustment: 38.22577454149723
BaseNM 5.46875
noise multiplier 3.7773530893027782
Noise multiplier before  adjustment: 37.77353089302778
BaseNM 5.46875
noise multiplier 3.8590469136834145
Noise multiplier before  adjustment: 38.590469136834145
BaseNM 5.46875
noise multiplier 3.758256748318672
Noise multiplier before  adjustment: 37.58256748318672
BaseNM 5.46875
noise multiplier 3.8569727204740047
Noise multiplier before  adjustment: 38.56972720474005
BaseNM 5.46875
noise multiplier 3.637423787266016
Noise multiplier before  adjustment: 36.37423787266016
BaseNM 20
noise multiplier 13.651939928531647
Noise multiplier before  adjustment: 136.51939928531647
BaseNM 20
noise multiplier 12.626735240221024
Noise multiplier before  adjustment: 126.26735240221024
BaseNM 20
noise multiplier 13.997327476739883
Noise multiplier before  adjustment: 139.97327476739883
BaseNM 20
noise multiplier 15.24497789144516
Noise multiplier before  adjustment: 152.4497789144516
BaseNM 20
noise multiplier 13.723641008138657
Noise multiplier before  adjustment: 137.23641008138657
BaseNM 20
noise multiplier 14.096521973609924
Noise multiplier before  adjustment: 140.96521973609924
BaseNM 35.0
noise multiplier 24.34347712993622
Noise multiplier before  adjustment: 243.43477129936218
BaseNM 35.0
noise multiplier 25.527857780456543
Noise multiplier before  adjustment: 255.27857780456543
BaseNM 35.0
noise multiplier 24.50250154733658
Noise multiplier before  adjustment: 245.02501547336578
BaseNM 35.0
noise multiplier 23.821054935455322
Noise multiplier before  adjustment: 238.21054935455322
BaseNM 35.0
noise multiplier 24.48401141166687
Noise multiplier before  adjustment: 244.8401141166687
BaseNM 35.0
noise multiplier 25.975251078605652
Noise multiplier before  adjustment: 259.7525107860565
BaseNM 50.0
noise multiplier 34.90041881799698
Noise multiplier before  adjustment: 349.0041881799698
BaseNM 50.0
noise multiplier 37.05041301250458
Noise multiplier before  adjustment: 370.5041301250458
BaseNM 50.0
noise multiplier 34.95123356580734
Noise multiplier before  adjustment: 349.5123356580734
BaseNM 50.0
noise multiplier 35.35971450805664
Noise multiplier before  adjustment: 353.5971450805664
BaseNM 50.0
noise multiplier 33.56427377462387
Noise multiplier before  adjustment: 335.6427377462387
BaseNM 50.0
noise multiplier 34.732240438461304
Noise multiplier before  adjustment: 347.32240438461304
BaseNM 65.0
noise multiplier 44.60355877876282
Noise multiplier before  adjustment: 446.0355877876282
BaseNM 65.0
noise multiplier 43.04498916864395
Noise multiplier before  adjustment: 430.4498916864395
BaseNM 65.0
noise multiplier 47.22615838050842
Noise multiplier before  adjustment: 472.26158380508423
BaseNM 65.0
noise multiplier 46.887245774269104
Noise multiplier before  adjustment: 468.87245774269104
BaseNM 65.0
noise multiplier 45.10173246264458
Noise multiplier before  adjustment: 451.01732462644577
BaseNM 65.0
noise multiplier 44.678421914577484
Noise multiplier before  adjustment: 446.78421914577484
BaseNM 80
noise multiplier 56.458980441093445
Noise multiplier before  adjustment: 564.5898044109344
BaseNM 80
noise multiplier 61.75543189048767
Noise multiplier before  adjustment: 617.5543189048767
BaseNM 80
noise multiplier 55.72676861286163
Noise multiplier before  adjustment: 557.2676861286163
BaseNM 80
noise multiplier 56.88166058063507
Noise multiplier before  adjustment: 568.8166058063507
BaseNM 80
noise multiplier 55.645524084568024
Noise multiplier before  adjustment: 556.4552408456802
BaseNM 80
noise multiplier 46.18661642074585
Noise multiplier before  adjustment: 461.8661642074585
1...
1...
1...
1...
1...
1...
  0%|          | 0/9 [00:00<?, ?it/s] 11%|         | 1/9 [35:06<4:40:51, 2106.50s/it] 22%|       | 2/9 [1:09:51<4:04:17, 2093.99s/it] 33%|      | 3/9 [1:45:31<3:31:28, 2114.70s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
Noise multiplier before  adjustment: 5.027998711913824
BaseNM 7.0703125
noise multiplier 4.7704774513840675
Noise multiplier before  adjustment: 4.7704774513840675
BaseNM 7.96875
noise multiplier 5.481477931141853
Noise multiplier before  adjustment: 5.481477931141853
BaseNM 7.96875
noise multiplier 6.11719411611557
Noise multiplier before  adjustment: 6.11719411611557
BaseNM 7.96875
noise multiplier 5.5698197260499
Noise multiplier before  adjustment: 5.5698197260499
BaseNM 7.96875
noise multiplier 5.514516226947308
Noise multiplier before  adjustment: 5.514516226947308
BaseNM 7.96875
noise multiplier 5.161237578839064
Noise multiplier before  adjustment: 5.161237578839064
BaseNM 7.96875
noise multiplier 5.5280702486634254
Noise multiplier before  adjustment: 5.5280702486634254
BaseNM 5.46875
noise multiplier 3.8257139399647713
Noise multiplier before  adjustment: 38.25713939964771
BaseNM 5.46875
noise multiplier 3.7995514571666718
Noise multiplier before  adjustment: 37.99551457166672
BaseNM 5.46875
noise multiplier 3.8672204986214638
Noise multiplier before  adjustment: 38.67220498621464
BaseNM 5.46875
noise multiplier 3.757803723216057
Noise multiplier before  adjustment: 37.57803723216057
BaseNM 5.46875
noise multiplier 3.8532571382820606
Noise multiplier before  adjustment: 38.532571382820606
BaseNM 5.46875
noise multiplier 3.6482766158878803
Noise multiplier before  adjustment: 36.4827661588788
BaseNM 20
noise multiplier 13.64149385690689
Noise multiplier before  adjustment: 136.4149385690689
BaseNM 20
noise multiplier 12.661930084228516
Noise multiplier before  adjustment: 126.61930084228516
BaseNM 20
noise multiplier 13.969720333814621
Noise multiplier before  adjustment: 139.6972033381462
BaseNM 20
noise multiplier 15.282067358493805
Noise multiplier before  adjustment: 152.82067358493805
BaseNM 20
noise multiplier 13.722661145031452
Noise multiplier before  adjustment: 137.22661145031452
BaseNM 20
noise multiplier 14.16971006244421
Noise multiplier before  adjustment: 141.6971006244421
BaseNM 35.0
noise multiplier 24.339267164468765
Noise multiplier before  adjustment: 243.39267164468765
BaseNM 35.0
noise multiplier 25.41064316034317
Noise multiplier before  adjustment: 254.1064316034317
BaseNM 35.0
noise multiplier 24.567549973726273
Noise multiplier before  adjustment: 245.67549973726273
BaseNM 35.0
noise multiplier 23.87878030538559
Noise multiplier before  adjustment: 238.7878030538559
BaseNM 35.0
noise multiplier 24.566188409924507
Noise multiplier before  adjustment: 245.66188409924507
BaseNM 35.0
noise multiplier 26.035061568021774
Noise multiplier before  adjustment: 260.35061568021774
BaseNM 50.0
noise multiplier 34.894227147102356
Noise multiplier before  adjustment: 348.94227147102356
BaseNM 50.0
noise multiplier 37.053029000759125
Noise multiplier before  adjustment: 370.53029000759125
BaseNM 50.0
noise multiplier 34.98593366146088
Noise multiplier before  adjustment: 349.85933661460876
BaseNM 50.0
noise multiplier 35.344198346138
Noise multiplier before  adjustment: 353.44198346138
BaseNM 50.0
noise multiplier 33.71829229593277
Noise multiplier before  adjustment: 337.1829229593277
BaseNM 50.0
noise multiplier 34.81866240501404
Noise multiplier before  adjustment: 348.1866240501404
BaseNM 65.0
noise multiplier 44.597814321517944
Noise multiplier before  adjustment: 445.97814321517944
BaseNM 65.0
noise multiplier 42.82950121164322
Noise multiplier before  adjustment: 428.2950121164322
BaseNM 65.0
noise multiplier 47.358774065971375
Noise multiplier before  adjustment: 473.58774065971375
BaseNM 65.0
noise multiplier 46.98072791099548
Noise multiplier before  adjustment: 469.80727910995483
BaseNM 65.0
noise multiplier 44.818352431058884
Noise multiplier before  adjustment: 448.18352431058884
BaseNM 65.0
noise multiplier 45.05565491318703
Noise multiplier before  adjustment: 450.55654913187027
BaseNM 80
noise multiplier 56.40558588504791
Noise multiplier before  adjustment: 564.0558588504791
BaseNM 80
noise multiplier 61.822437942028046
Noise multiplier before  adjustment: 618.2243794202805
BaseNM 80
noise multiplier 55.70241618156433
Noise multiplier before  adjustment: 557.0241618156433
BaseNM 80
noise multiplier 57.02427613735199
Noise multiplier before  adjustment: 570.2427613735199
BaseNM 80
noise multiplier 55.74059599637985
Noise multiplier before  adjustment: 557.4059599637985
BaseNM 80
noise multiplier 45.92552000284195
Noise multiplier before  adjustment: 459.2552000284195
1...
1...
1...
1...
1...
1...
  0%|          | 0/9 [00:00<?, ?it/s] 44%|     | 4/9 [2:20:21<2:55:26, 2105.23s/it] 11%|         | 1/9 [34:37<4:36:59, 2077.49s/it] 56%|    | 5/9 [2:56:15<2:21:30, 2122.71s/it] 22%|       | 2/9 [1:10:39<4:08:11, 2127.35s/it] 67%|   | 6/9 [3:31:49<1:46:20, 2126.71s/it] 33%|      | 3/9 [1:46:19<3:33:18, 2133.01s/it] 78%|  | 7/9 [4:07:34<1:11:05, 2132.53s/it] 44%|     | 4/9 [2:21:47<2:57:35, 2131.01s/it] 89%| | 8/9 [4:42:21<35:17, 2117.99s/it]   56%|    | 5/9 [2:57:57<2:22:59, 2144.93s/it]100%|| 9/9 [5:17:56<00:00, 2123.29s/it]100%|| 9/9 [5:17:56<00:00, 2119.59s/it]
  0%|          | 0/6 [00:00<?, ?it/s] 17%|        | 1/6 [02:05<10:26, 125.28s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 33%|      | 2/6 [02:44<04:59, 74.76s/it] /home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 50%|     | 3/6 [03:18<02:48, 56.00s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 67%|   | 4/6 [03:40<01:25, 42.79s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 83%| | 5/6 [03:49<00:30, 30.42s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
100%|| 6/6 [03:53<00:00, 21.58s/it]100%|| 6/6 [03:53<00:00, 38.97s/it]
Mean performance without DP, Perf=0.5308100231070408
  0%|          | 0/252 [00:00<?, ?it/s]{'loss': BaselineLoss(), 'optimizer_class': <class 'torch.optim.sgd.SGD'>, 'learning_rate': 0.0005, 'num_updates': 100, 'nrounds': 9, 'training_dataloaders': [<torch.utils.data.dataloader.DataLoader object at 0x7f5173b01f40>, <torch.utils.data.dataloader.DataLoader object at 0x7f5173b01cd0>, <torch.utils.data.dataloader.DataLoader object at 0x7f5173b01ac0>, <torch.utils.data.dataloader.DataLoader object at 0x7f5173b015b0>, <torch.utils.data.dataloader.DataLoader object at 0x7f5173b01e50>, <torch.utils.data.dataloader.DataLoader object at 0x7f5173b01b20>], 'dp_dynamic_noise_multiplier': 461.8661642074585, 'model': Baseline(
  (model): VisionTransformer(
    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (encoder): Encoder(
      (dropout): Dropout(p=0.0, inplace=False)
      (layers): Sequential(
        (encoder_layer_0): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_1): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_2): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_3): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_4): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_5): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_6): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_7): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_8): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_9): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_10): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_11): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (heads): Sequential(
      (head): Linear(in_features=768, out_features=8, bias=True)
    )
  )
), 'dp_target_epsilon': 50.0, 'dp_target_delta': 0.1, 'dp_max_grad_norm': 1.1}/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(

1...
1...
1...
1...
1...
1...

  0%|          | 0/9 [00:00<?, ?it/s][A 67%|   | 6/9 [3:33:35<1:47:08, 2142.81s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "

 11%|         | 1/9 [42:24<5:39:18, 2544.76s/it][A 78%|  | 7/9 [4:20:05<1:18:28, 2354.44s/it]
 22%|       | 2/9 [1:31:54<5:26:01, 2794.52s/it][A 89%| | 8/9 [5:10:48<42:53, 2573.51s/it]  
 33%|      | 3/9 [2:17:50<4:37:43, 2777.23s/it][A100%|| 9/9 [5:52:24<00:00, 2549.42s/it]100%|| 9/9 [5:52:24<00:00, 2349.43s/it]
  0%|          | 0/6 [00:00<?, ?it/s] 17%|        | 1/6 [02:29<12:26, 149.33s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 33%|      | 2/6 [03:17<05:58, 89.70s/it] /home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 50%|     | 3/6 [04:04<03:31, 70.48s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 67%|   | 4/6 [04:29<01:44, 52.21s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
 83%| | 5/6 [04:42<00:38, 38.21s/it]/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
100%|| 6/6 [04:47<00:00, 26.94s/it]100%|| 6/6 [04:47<00:00, 47.92s/it]
Mean performance without DP, Perf=0.5307503696368445
  0%|          | 0/252 [00:00<?, ?it/s]{'loss': BaselineLoss(), 'optimizer_class': <class 'torch.optim.sgd.SGD'>, 'learning_rate': 0.0005, 'num_updates': 100, 'nrounds': 9, 'training_dataloaders': [<torch.utils.data.dataloader.DataLoader object at 0x7f3c10258b20>, <torch.utils.data.dataloader.DataLoader object at 0x7f3c10258a60>, <torch.utils.data.dataloader.DataLoader object at 0x7f3c10258eb0>, <torch.utils.data.dataloader.DataLoader object at 0x7f3c10258d00>, <torch.utils.data.dataloader.DataLoader object at 0x7f3c10258dc0>, <torch.utils.data.dataloader.DataLoader object at 0x7f3c10258c10>], 'dp_dynamic_noise_multiplier': 459.2552000284195, 'model': Baseline(
  (model): VisionTransformer(
    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (encoder): Encoder(
      (dropout): Dropout(p=0.0, inplace=False)
      (layers): Sequential(
        (encoder_layer_0): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_1): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_2): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_3): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_4): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_5): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_6): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_7): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_8): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_9): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_10): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_11): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (heads): Sequential(
      (head): Linear(in_features=768, out_features=8, bias=True)
    )
  )
), 'dp_target_epsilon': 50.0, 'dp_target_delta': 0.1, 'dp_max_grad_norm': 1.1}/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(

1...
1...
1...
1...
1...
1...

  0%|          | 0/9 [00:00<?, ?it/s][A
 44%|     | 4/9 [3:04:34<3:52:19, 2787.86s/it][A/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "

 11%|         | 1/9 [49:01<6:32:10, 2941.36s/it][A
 56%|    | 5/9 [3:54:47<3:11:15, 2868.90s/it][A
 22%|       | 2/9 [1:36:45<5:37:50, 2895.76s/it][A
 67%|   | 6/9 [4:42:25<2:23:15, 2865.25s/it][A
 33%|      | 3/9 [2:20:05<4:36:03, 2760.63s/it][A
 78%|  | 7/9 [5:22:18<1:30:21, 2710.97s/it][A
 44%|     | 4/9 [3:02:01<3:42:01, 2664.27s/it][A
 89%| | 8/9 [6:08:20<45:27, 2727.07s/it]  [A
 56%|    | 5/9 [3:46:49<2:58:11, 2672.81s/it][A
100%|| 9/9 [6:46:25<00:00, 2588.71s/it][A100%|| 9/9 [6:46:25<00:00, 2709.45s/it]

  0%|          | 0/6 [00:00<?, ?it/s][A
 17%|        | 1/6 [02:15<11:17, 135.55s/it][A/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")

 33%|      | 2/6 [03:06<05:42, 85.54s/it] [A/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")

 50%|     | 3/6 [03:42<03:09, 63.03s/it][A/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")

 67%|   | 4/6 [04:09<01:37, 48.96s/it][A/home/dgxuser16/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")

 83%| | 5/6 [04:18<00:34, 34.49s/it][A
100%|| 6/6 [04:23<00:00, 24.46s/it][A100%|| 6/6 [04:23<00:00, 43.92s/it]
  0%|          | 1/252 [6:50:49<1718:36:55, 24649.47s/it]Mean performance eps=50.0, delta=0.1, Perf=0.1850934303736479
{'loss': BaselineLoss(), 'optimizer_class': <class 'torch.optim.sgd.SGD'>, 'learning_rate': 0.0005, 'num_updates': 100, 'nrounds': 9, 'training_dataloaders': [<torch.utils.data.dataloader.DataLoader object at 0x7f5184083c70>, <torch.utils.data.dataloader.DataLoader object at 0x7f5184083f40>, <torch.utils.data.dataloader.DataLoader object at 0x7f5184083e20>, <torch.utils.data.dataloader.DataLoader object at 0x7f5184083a30>, <torch.utils.data.dataloader.DataLoader object at 0x7f5184083730>, <torch.utils.data.dataloader.DataLoader object at 0x7f5184521e50>], 'dp_dynamic_noise_multiplier': 461.8661642074585, 'model': Baseline(
  (model): VisionTransformer(
    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (encoder): Encoder(
      (dropout): Dropout(p=0.0, inplace=False)
      (layers): Sequential(
        (encoder_layer_0): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_1): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_2): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_3): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_4): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_5): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_6): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_7): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_8): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_9): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_10): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (encoder_layer_11): EncoderBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): MLPBlock(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (heads): Sequential(
      (head): Linear(in_features=768, out_features=8, bias=True)
    )
  )
), 'dp_target_epsilon': 50.0, 'dp_target_delta': 0.1, 'dp_max_grad_norm': 1.1}/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(

1...
1...
1...
1...
1...
1...

  0%|          | 0/9 [00:00<?, ?it/s][A
 67%|   | 6/9 [4:23:29<2:05:36, 2512.20s/it][A/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "

 11%|         | 1/9 [35:57<4:47:41, 2157.64s/it][A
 78%|  | 7/9 [4:59:50<1:20:07, 2403.70s/it][A