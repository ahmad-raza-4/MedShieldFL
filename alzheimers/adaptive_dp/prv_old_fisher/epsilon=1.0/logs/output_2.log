nohup: ignoring input
01/27/2025 00:41:14:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 00:41:14:DEBUG:ChannelConnectivity.IDLE
01/27/2025 00:41:14:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 00:41:14:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 00:41:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:41:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bd4c591d-2493-4676-a0eb-d4bee97ed2a3
01/27/2025 00:41:47:INFO:Received: train message bd4c591d-2493-4676-a0eb-d4bee97ed2a3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:42:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:43:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:43:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80d340ab-3c33-4693-af78-744bf634541e
01/27/2025 00:43:32:INFO:Received: evaluate message 80d340ab-3c33-4693-af78-744bf634541e
[92mINFO [0m:      Sent reply
01/27/2025 00:43:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:44:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:44:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e0c07446-bf78-4a94-b345-96509154f6fd
01/27/2025 00:44:22:INFO:Received: train message e0c07446-bf78-4a94-b345-96509154f6fd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:44:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:45:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:45:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bf870f30-00e4-4957-b4ff-3b8b5cdda40e
01/27/2025 00:45:45:INFO:Received: evaluate message bf870f30-00e4-4957-b4ff-3b8b5cdda40e
[92mINFO [0m:      Sent reply
01/27/2025 00:45:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:46:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:46:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 63db820f-57c6-4686-8b50-ba64d582363f
01/27/2025 00:46:23:INFO:Received: train message 63db820f-57c6-4686-8b50-ba64d582363f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:46:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13aeeda1-5a04-46be-b381-b98fe4c7541b
01/27/2025 00:47:22:INFO:Received: evaluate message 13aeeda1-5a04-46be-b381-b98fe4c7541b
[92mINFO [0m:      Sent reply
01/27/2025 00:47:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb8bc9b6-c5c4-4a6f-aa46-a773410034c0
01/27/2025 00:47:55:INFO:Received: train message fb8bc9b6-c5c4-4a6f-aa46-a773410034c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:48:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3768c3b3-2d50-497c-829f-a075a7d00e57
01/27/2025 00:49:01:INFO:Received: evaluate message 3768c3b3-2d50-497c-829f-a075a7d00e57
[92mINFO [0m:      Sent reply
01/27/2025 00:49:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f73d0f31-a309-427a-9e5b-415fc47572a1
01/27/2025 00:49:39:INFO:Received: train message f73d0f31-a309-427a-9e5b-415fc47572a1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:49:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:50:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:50:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1059bb97-73bf-42d9-89fe-d6fefa5f5118
01/27/2025 00:50:39:INFO:Received: evaluate message 1059bb97-73bf-42d9-89fe-d6fefa5f5118
[92mINFO [0m:      Sent reply
01/27/2025 00:50:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:51:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:51:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3bae218c-94d5-403f-9f8b-6b41fdfc42f0
01/27/2025 00:51:19:INFO:Received: train message 3bae218c-94d5-403f-9f8b-6b41fdfc42f0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:51:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 562d9a32-6a2f-4764-9c8e-cfc5bd905263
01/27/2025 00:52:19:INFO:Received: evaluate message 562d9a32-6a2f-4764-9c8e-cfc5bd905263
[92mINFO [0m:      Sent reply
01/27/2025 00:52:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 71c2037e-dd2d-4f52-94bd-c28fb01f65a7
01/27/2025 00:52:57:INFO:Received: train message 71c2037e-dd2d-4f52-94bd-c28fb01f65a7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:53:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:53:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:53:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d3e9712a-24f7-43a4-9179-98c07430b69e
01/27/2025 00:53:48:INFO:Received: evaluate message d3e9712a-24f7-43a4-9179-98c07430b69e
[92mINFO [0m:      Sent reply
01/27/2025 00:53:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:54:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:54:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ab82a42a-4745-4d4c-91f3-9d38ea98f579
01/27/2025 00:54:20:INFO:Received: train message ab82a42a-4745-4d4c-91f3-9d38ea98f579
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406], 'accuracy': [0.5160281469898358], 'auc': [0.7097259125281268]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157], 'accuracy': [0.5160281469898358, 0.506645817044566], 'auc': [0.7097259125281268, 0.7279838714938965]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:54:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:55:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:55:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a92be3b-3dcf-4952-b6b5-7d26c6faaac7
01/27/2025 00:55:40:INFO:Received: evaluate message 2a92be3b-3dcf-4952-b6b5-7d26c6faaac7
[92mINFO [0m:      Sent reply
01/27/2025 00:55:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:56:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:56:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed11a3b9-81d7-439c-9d1f-aebc7605a245
01/27/2025 00:56:05:INFO:Received: train message ed11a3b9-81d7-439c-9d1f-aebc7605a245
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:56:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a19a38b0-7e02-47ab-89ba-43b1a8b3fe47
01/27/2025 00:57:06:INFO:Received: evaluate message a19a38b0-7e02-47ab-89ba-43b1a8b3fe47
[92mINFO [0m:      Sent reply
01/27/2025 00:57:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0afd31e8-517b-42dd-b9c1-e8413798cad8
01/27/2025 00:57:51:INFO:Received: train message 0afd31e8-517b-42dd-b9c1-e8413798cad8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:58:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:58:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:58:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45444920-81c3-4786-8f73-3604be68b142
01/27/2025 00:58:47:INFO:Received: evaluate message 45444920-81c3-4786-8f73-3604be68b142
[92mINFO [0m:      Sent reply
01/27/2025 00:58:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:59:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:59:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2c9ffc42-3f72-42c9-9050-064ef8b06448
01/27/2025 00:59:19:INFO:Received: train message 2c9ffc42-3f72-42c9-9050-064ef8b06448
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:59:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fa4647e4-68de-4208-b2ad-0ba8b7e0046c
01/27/2025 01:00:24:INFO:Received: evaluate message fa4647e4-68de-4208-b2ad-0ba8b7e0046c
[92mINFO [0m:      Sent reply
01/27/2025 01:00:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bcf1c9f2-9757-4f61-92df-9400a8aab230
01/27/2025 01:00:58:INFO:Received: train message bcf1c9f2-9757-4f61-92df-9400a8aab230
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:01:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c5279367-193d-4fc7-ad78-1570001ff122
01/27/2025 01:01:49:INFO:Received: evaluate message c5279367-193d-4fc7-ad78-1570001ff122
[92mINFO [0m:      Sent reply
01/27/2025 01:01:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:02:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:02:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 45586d10-3bcb-4482-94a9-a7a7eae52a5a
01/27/2025 01:02:18:INFO:Received: train message 45586d10-3bcb-4482-94a9-a7a7eae52a5a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:02:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ef156f7-3f85-4da8-9778-d2a1f420317c
01/27/2025 01:03:13:INFO:Received: evaluate message 0ef156f7-3f85-4da8-9778-d2a1f420317c
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:03:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e315d51-7263-4a04-bb2a-fb5cf18aecce
01/27/2025 01:03:31:INFO:Received: train message 0e315d51-7263-4a04-bb2a-fb5cf18aecce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:03:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8f46348c-8c9b-4a5f-82bc-e69369471a9b
01/27/2025 01:04:31:INFO:Received: evaluate message 8f46348c-8c9b-4a5f-82bc-e69369471a9b
[92mINFO [0m:      Sent reply
01/27/2025 01:04:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b15157c0-8fdd-4fc0-bb6d-0bed8f55de70
01/27/2025 01:05:05:INFO:Received: train message b15157c0-8fdd-4fc0-bb6d-0bed8f55de70
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:05:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message befb4223-d259-4e24-90c1-47bef0b11e7f
01/27/2025 01:05:55:INFO:Received: evaluate message befb4223-d259-4e24-90c1-47bef0b11e7f
[92mINFO [0m:      Sent reply
01/27/2025 01:05:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:06:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:06:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d38b2bc-f1e2-4221-a50d-c7794682cf83
01/27/2025 01:06:30:INFO:Received: train message 3d38b2bc-f1e2-4221-a50d-c7794682cf83
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:06:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 51d76136-ea2b-4117-bc1e-59b291cbf0c7
01/27/2025 01:07:17:INFO:Received: evaluate message 51d76136-ea2b-4117-bc1e-59b291cbf0c7
[92mINFO [0m:      Sent reply
01/27/2025 01:07:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7ca5d5d4-09f4-45c6-b686-c22afc505dd2
01/27/2025 01:07:50:INFO:Received: train message 7ca5d5d4-09f4-45c6-b686-c22afc505dd2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:08:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b51acf7-4d50-48c0-b1d5-e203eac17151
01/27/2025 01:08:38:INFO:Received: evaluate message 4b51acf7-4d50-48c0-b1d5-e203eac17151
[92mINFO [0m:      Sent reply
01/27/2025 01:08:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:09:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:09:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be482678-8d58-4c77-b820-13d9ece03e28
01/27/2025 01:09:17:INFO:Received: train message be482678-8d58-4c77-b820-13d9ece03e28

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:09:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 32019b79-e446-427c-abbd-3bac18389616
01/27/2025 01:10:03:INFO:Received: evaluate message 32019b79-e446-427c-abbd-3bac18389616
[92mINFO [0m:      Sent reply
01/27/2025 01:10:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3c11ca96-afbf-4bac-8b6d-31b6548afa47
01/27/2025 01:10:48:INFO:Received: train message 3c11ca96-afbf-4bac-8b6d-31b6548afa47
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:10:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:11:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:11:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c74ef902-0b3c-433b-9066-7836831327c4
01/27/2025 01:11:30:INFO:Received: evaluate message c74ef902-0b3c-433b-9066-7836831327c4
[92mINFO [0m:      Sent reply
01/27/2025 01:11:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:12:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:12:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 85940774-e1bc-4b76-b8a0-deabdc1d27c5
01/27/2025 01:12:09:INFO:Received: train message 85940774-e1bc-4b76-b8a0-deabdc1d27c5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:12:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:13:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:13:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9d3e41f5-42c9-4e38-a2b3-6f0591ce5b97
01/27/2025 01:13:22:INFO:Received: evaluate message 9d3e41f5-42c9-4e38-a2b3-6f0591ce5b97
[92mINFO [0m:      Sent reply
01/27/2025 01:13:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:14:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:14:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8711353f-ac79-4bf5-ac4b-8eeae00b9385
01/27/2025 01:14:06:INFO:Received: train message 8711353f-ac79-4bf5-ac4b-8eeae00b9385
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:14:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 14e4091c-e421-4a7f-b8c0-bc2319d247a9
01/27/2025 01:15:25:INFO:Received: evaluate message 14e4091c-e421-4a7f-b8c0-bc2319d247a9
[92mINFO [0m:      Sent reply
01/27/2025 01:15:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:16:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:16:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95cbcf0b-57a2-4bae-8388-bccf1e5e5312
01/27/2025 01:16:00:INFO:Received: train message 95cbcf0b-57a2-4bae-8388-bccf1e5e5312
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:16:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:16:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:16:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bcc85d1a-c062-41d8-b0d3-cb8441657869
01/27/2025 01:16:37:INFO:Received: evaluate message bcc85d1a-c062-41d8-b0d3-cb8441657869
[92mINFO [0m:      Sent reply
01/27/2025 01:16:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9cefeb87-9862-4bea-9217-1e7a3cca8c0a
01/27/2025 01:17:19:INFO:Received: train message 9cefeb87-9862-4bea-9217-1e7a3cca8c0a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:17:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e7a1f074-6095-4892-8770-a9f717c106e2
01/27/2025 01:17:51:INFO:Received: evaluate message e7a1f074-6095-4892-8770-a9f717c106e2
[92mINFO [0m:      Sent reply
01/27/2025 01:17:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16456cfb-a842-49b4-bfd9-39604c84a4be
01/27/2025 01:18:23:INFO:Received: train message 16456cfb-a842-49b4-bfd9-39604c84a4be
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:18:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec0baa8d-d46a-4603-9d54-211da1b7e184
01/27/2025 01:19:24:INFO:Received: evaluate message ec0baa8d-d46a-4603-9d54-211da1b7e184
[92mINFO [0m:      Sent reply
01/27/2025 01:19:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message beed90e5-9729-4294-822b-18bc300281e8
01/27/2025 01:19:46:INFO:Received: train message beed90e5-9729-4294-822b-18bc300281e8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:19:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:20:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:20:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5320b114-779a-4efb-9085-c2f01e034e03
01/27/2025 01:20:40:INFO:Received: evaluate message 5320b114-779a-4efb-9085-c2f01e034e03
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:20:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 692048ff-e325-4b8d-b52f-e04ae7b41661
01/27/2025 01:21:01:INFO:Received: train message 692048ff-e325-4b8d-b52f-e04ae7b41661
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:21:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e3598cd4-be6b-4516-af3b-2f27d3154bea
01/27/2025 01:22:09:INFO:Received: evaluate message e3598cd4-be6b-4516-af3b-2f27d3154bea
[92mINFO [0m:      Sent reply
01/27/2025 01:22:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5356329a-6dd2-4255-85c1-43795e0fbef2
01/27/2025 01:22:40:INFO:Received: train message 5356329a-6dd2-4255-85c1-43795e0fbef2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:22:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f083be3-3a42-4f53-8f69-f4ebbc9c4c93
01/27/2025 01:23:19:INFO:Received: evaluate message 9f083be3-3a42-4f53-8f69-f4ebbc9c4c93
[92mINFO [0m:      Sent reply
01/27/2025 01:23:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a0acf124-837a-4f8f-8c25-37225552a8da
01/27/2025 01:24:08:INFO:Received: train message a0acf124-837a-4f8f-8c25-37225552a8da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:24:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e376d10f-0755-4575-ae37-6feaf64ad280
01/27/2025 01:24:53:INFO:Received: evaluate message e376d10f-0755-4575-ae37-6feaf64ad280

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:24:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4fb02296-1bd6-4559-ae85-d2b4dfe94dc7
01/27/2025 01:25:22:INFO:Received: train message 4fb02296-1bd6-4559-ae85-d2b4dfe94dc7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:25:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a1548288-c6ef-4aee-9ab0-194f8d73bfce
01/27/2025 01:26:25:INFO:Received: evaluate message a1548288-c6ef-4aee-9ab0-194f8d73bfce
[92mINFO [0m:      Sent reply
01/27/2025 01:26:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3856b100-fcd5-4858-81ed-4f26fa1a81af
01/27/2025 01:26:50:INFO:Received: train message 3856b100-fcd5-4858-81ed-4f26fa1a81af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:27:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c3519c15-741a-465c-a039-03af3d9be4c5
01/27/2025 01:27:32:INFO:Received: evaluate message c3519c15-741a-465c-a039-03af3d9be4c5
[92mINFO [0m:      Sent reply
01/27/2025 01:27:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message c10ef43e-935b-497d-8926-ebe02e1b1cb1
01/27/2025 01:27:53:INFO:Received: reconnect message c10ef43e-935b-497d-8926-ebe02e1b1cb1
01/27/2025 01:27:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 01:27:53:INFO:Disconnect and shut down

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.63671875
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.09608633071184158, 0.01162977609783411, 0.024636859074234962, 0.07235877960920334]
Noise Multiplier after list and tensor:  0.0511779363732785
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}



Final client history:
{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}

