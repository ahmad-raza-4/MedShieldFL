nohup: ignoring input
02/05/2025 10:03:21:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:03:21:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:03:21:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778601.088085 1795592 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:04:04:INFO:
[92mINFO [0m:      Received: train message bfe45e0b-9206-42bc-826a-5fa6f1c19c48
02/05/2025 10:04:04:INFO:Received: train message bfe45e0b-9206-42bc-826a-5fa6f1c19c48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:04:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:05:52:INFO:
[92mINFO [0m:      Received: evaluate message d80bc81c-d00c-4eff-86d7-f6be3d5d7332
02/05/2025 10:05:52:INFO:Received: evaluate message d80bc81c-d00c-4eff-86d7-f6be3d5d7332
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:05:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:46:INFO:
[92mINFO [0m:      Received: train message 5c4465b2-3234-4351-bddd-57e1aaf487ac
02/05/2025 10:06:46:INFO:Received: train message 5c4465b2-3234-4351-bddd-57e1aaf487ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:22:INFO:
[92mINFO [0m:      Received: evaluate message 922ac1f0-e176-4be9-a889-864056db9955
02/05/2025 10:08:22:INFO:Received: evaluate message 922ac1f0-e176-4be9-a889-864056db9955
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:08:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:09:INFO:
[92mINFO [0m:      Received: train message 62f3f6db-86d4-46f6-8e66-c82c0d2b03ca
02/05/2025 10:09:09:INFO:Received: train message 62f3f6db-86d4-46f6-8e66-c82c0d2b03ca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:09:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:10:44:INFO:
[92mINFO [0m:      Received: evaluate message 59c5c10d-5a50-4028-9f65-3eac883e6ddb
02/05/2025 10:10:44:INFO:Received: evaluate message 59c5c10d-5a50-4028-9f65-3eac883e6ddb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:10:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:48:INFO:
[92mINFO [0m:      Received: train message 07726b3d-ade7-4f5e-a965-9b2eb62b9063
02/05/2025 10:11:48:INFO:Received: train message 07726b3d-ade7-4f5e-a965-9b2eb62b9063
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:09:INFO:
[92mINFO [0m:      Received: evaluate message 492c23f5-79d1-4afd-87db-479c7cbee7b8
02/05/2025 10:13:09:INFO:Received: evaluate message 492c23f5-79d1-4afd-87db-479c7cbee7b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:18:INFO:
[92mINFO [0m:      Received: train message 2d40e642-b749-4591-a8c8-befedf64f32f
02/05/2025 10:14:18:INFO:Received: train message 2d40e642-b749-4591-a8c8-befedf64f32f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:15:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:37:INFO:
[92mINFO [0m:      Received: evaluate message 97d9a893-863b-4a8d-8ed4-523086eefb0d
02/05/2025 10:15:37:INFO:Received: evaluate message 97d9a893-863b-4a8d-8ed4-523086eefb0d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:15:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:41:INFO:
[92mINFO [0m:      Received: train message e0ea86da-e95f-4b02-892b-95d3551fc9a4
02/05/2025 10:16:41:INFO:Received: train message e0ea86da-e95f-4b02-892b-95d3551fc9a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:27:INFO:
[92mINFO [0m:      Received: evaluate message 279a3f94-8bee-4bc2-a9f8-bad87ee657c6
02/05/2025 10:18:27:INFO:Received: evaluate message 279a3f94-8bee-4bc2-a9f8-bad87ee657c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:59:INFO:
[92mINFO [0m:      Received: train message 4c09a8f9-46ef-4def-9241-c704f5031120
02/05/2025 10:18:59:INFO:Received: train message 4c09a8f9-46ef-4def-9241-c704f5031120
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:45:INFO:
[92mINFO [0m:      Received: evaluate message 55d355d5-0c3a-4cb2-9913-556ae2f7401e
02/05/2025 10:21:45:INFO:Received: evaluate message 55d355d5-0c3a-4cb2-9913-556ae2f7401e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986], 'accuracy': [0.5160281469898358], 'auc': [0.7174808607527126], 'precision': [0.4096278307389395], 'recall': [0.5160281469898358], 'f1': [0.42340290242033957]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763], 'accuracy': [0.5160281469898358, 0.5371383893666928], 'auc': [0.7174808607527126, 0.743196921722251], 'precision': [0.4096278307389395, 0.4446497303781511], 'recall': [0.5160281469898358, 0.5371383893666928], 'f1': [0.42340290242033957, 0.48416854046252017]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:21:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:44:INFO:
[92mINFO [0m:      Received: train message bfd39c04-f8ea-41d9-9dcd-dc4b9b1c1942
02/05/2025 10:22:44:INFO:Received: train message bfd39c04-f8ea-41d9-9dcd-dc4b9b1c1942
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:23:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:07:INFO:
[92mINFO [0m:      Received: evaluate message ca4f0e91-e350-4a00-8b84-055ec012b2e8
02/05/2025 10:24:07:INFO:Received: evaluate message ca4f0e91-e350-4a00-8b84-055ec012b2e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:24:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:02:INFO:
[92mINFO [0m:      Received: train message db9791eb-1621-4714-8a6d-29487df4fce7
02/05/2025 10:25:02:INFO:Received: train message db9791eb-1621-4714-8a6d-29487df4fce7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:25:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:52:INFO:
[92mINFO [0m:      Received: evaluate message 33bfb644-9205-4250-8bed-98ed79e62f60
02/05/2025 10:26:52:INFO:Received: evaluate message 33bfb644-9205-4250-8bed-98ed79e62f60
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:26:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:53:INFO:
[92mINFO [0m:      Received: train message d9e6a438-3478-4b5e-a892-5f7a1f20e535
02/05/2025 10:27:53:INFO:Received: train message d9e6a438-3478-4b5e-a892-5f7a1f20e535
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:20:INFO:
[92mINFO [0m:      Received: evaluate message 6d8576dd-6e83-46c6-9a2f-69aa375c3dd1
02/05/2025 10:29:20:INFO:Received: evaluate message 6d8576dd-6e83-46c6-9a2f-69aa375c3dd1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:50:INFO:
[92mINFO [0m:      Received: train message 0cb7baf6-58d4-437d-89e3-97ba7635df72
02/05/2025 10:29:50:INFO:Received: train message 0cb7baf6-58d4-437d-89e3-97ba7635df72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:40:INFO:
[92mINFO [0m:      Received: evaluate message 42d2c39d-2fdf-45d7-bac9-e0de65136201
02/05/2025 10:31:40:INFO:Received: evaluate message 42d2c39d-2fdf-45d7-bac9-e0de65136201
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:31:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:44:INFO:
[92mINFO [0m:      Received: train message d9ef660c-74fa-4515-8f24-08cfa8111a11
02/05/2025 10:32:44:INFO:Received: train message d9ef660c-74fa-4515-8f24-08cfa8111a11

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:40:INFO:
[92mINFO [0m:      Received: evaluate message cf6213e9-e0e1-4837-a749-bf301f27fcb9
02/05/2025 10:34:40:INFO:Received: evaluate message cf6213e9-e0e1-4837-a749-bf301f27fcb9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:18:INFO:
[92mINFO [0m:      Received: train message c1ad964c-4897-4e4a-887b-ee8e6d11e07c
02/05/2025 10:35:18:INFO:Received: train message c1ad964c-4897-4e4a-887b-ee8e6d11e07c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:35:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:02:INFO:
[92mINFO [0m:      Received: evaluate message ca55a42d-1fbb-42fb-9c42-cd19db1bc0ca
02/05/2025 10:37:02:INFO:Received: evaluate message ca55a42d-1fbb-42fb-9c42-cd19db1bc0ca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:02:INFO:
[92mINFO [0m:      Received: train message df562a59-a48a-469b-9fd5-a2ac37752c99
02/05/2025 10:38:02:INFO:Received: train message df562a59-a48a-469b-9fd5-a2ac37752c99
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:45:INFO:
[92mINFO [0m:      Received: evaluate message 898b7734-04df-4efd-95e5-cf4e0f50b965
02/05/2025 10:39:45:INFO:Received: evaluate message 898b7734-04df-4efd-95e5-cf4e0f50b965
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:37:INFO:
[92mINFO [0m:      Received: train message 5e3d7016-daef-4e1f-9b0b-2f90cab14c8f
02/05/2025 10:40:37:INFO:Received: train message 5e3d7016-daef-4e1f-9b0b-2f90cab14c8f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:00:INFO:
[92mINFO [0m:      Received: evaluate message a5c6c6a8-9169-4e18-9e23-a1ed73edb577
02/05/2025 10:42:00:INFO:Received: evaluate message a5c6c6a8-9169-4e18-9e23-a1ed73edb577
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:36:INFO:
[92mINFO [0m:      Received: train message e937948b-b422-446c-afa6-92f25cfce671
02/05/2025 10:42:36:INFO:Received: train message e937948b-b422-446c-afa6-92f25cfce671
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:43:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:37:INFO:
[92mINFO [0m:      Received: evaluate message f5a2d062-d592-4c24-934a-7aaef293b393
02/05/2025 10:44:37:INFO:Received: evaluate message f5a2d062-d592-4c24-934a-7aaef293b393
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:44:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:26:INFO:
[92mINFO [0m:      Received: train message c00b42d2-79e4-41e6-837f-da5fc21de374
02/05/2025 10:45:26:INFO:Received: train message c00b42d2-79e4-41e6-837f-da5fc21de374
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:30:INFO:
[92mINFO [0m:      Received: evaluate message 2b812e27-4964-400b-ac58-50fcd509c7ab
02/05/2025 10:47:30:INFO:Received: evaluate message 2b812e27-4964-400b-ac58-50fcd509c7ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:39:INFO:
[92mINFO [0m:      Received: train message a3f71901-3fe2-41e2-8ac2-2e41cf455aad
02/05/2025 10:48:39:INFO:Received: train message a3f71901-3fe2-41e2-8ac2-2e41cf455aad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:23:INFO:
[92mINFO [0m:      Received: evaluate message 34f8221b-5e08-456a-8b7f-90dc3d8bd6f7
02/05/2025 10:50:23:INFO:Received: evaluate message 34f8221b-5e08-456a-8b7f-90dc3d8bd6f7

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:14:INFO:
[92mINFO [0m:      Received: train message f3ccc467-0bbf-4498-8e2a-48afe1900657
02/05/2025 10:51:14:INFO:Received: train message f3ccc467-0bbf-4498-8e2a-48afe1900657
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:29:INFO:
[92mINFO [0m:      Received: evaluate message a3b10ce9-3a5e-486d-b145-9f8027a7e494
02/05/2025 10:53:29:INFO:Received: evaluate message a3b10ce9-3a5e-486d-b145-9f8027a7e494
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:52:INFO:
[92mINFO [0m:      Received: train message a4ba65f5-df11-4910-af42-7da977e89ef5
02/05/2025 10:54:52:INFO:Received: train message a4ba65f5-df11-4910-af42-7da977e89ef5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:51:INFO:
[92mINFO [0m:      Received: evaluate message febbd08b-fa4a-40b6-aea3-6efafba49d7f
02/05/2025 10:56:51:INFO:Received: evaluate message febbd08b-fa4a-40b6-aea3-6efafba49d7f

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:56:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:48:INFO:
[92mINFO [0m:      Received: train message 159b6493-7f91-47f7-9f54-c0fd15c79514
02/05/2025 10:58:48:INFO:Received: train message 159b6493-7f91-47f7-9f54-c0fd15c79514
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:59:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:16:INFO:
[92mINFO [0m:      Received: evaluate message baf733f6-e72a-4770-a0ef-e334b478ef67
02/05/2025 11:01:16:INFO:Received: evaluate message baf733f6-e72a-4770-a0ef-e334b478ef67
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:01:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:02:33:INFO:
[92mINFO [0m:      Received: train message aee066c3-d2d6-46e3-98b5-e234c4e7d074
02/05/2025 11:02:33:INFO:Received: train message aee066c3-d2d6-46e3-98b5-e234c4e7d074
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:03:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:42:INFO:
[92mINFO [0m:      Received: evaluate message 2fac8153-18d1-4301-80f3-6b04e9656fc8
02/05/2025 11:04:42:INFO:Received: evaluate message 2fac8153-18d1-4301-80f3-6b04e9656fc8

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:04:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:05:INFO:
[92mINFO [0m:      Received: train message bbb38ee0-0bec-47da-8c15-66b8b316f825
02/05/2025 11:06:05:INFO:Received: train message bbb38ee0-0bec-47da-8c15-66b8b316f825
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:07:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:46:INFO:
[92mINFO [0m:      Received: evaluate message 71e0c5fb-096e-4c23-aaeb-1e703eddc901
02/05/2025 11:08:46:INFO:Received: evaluate message 71e0c5fb-096e-4c23-aaeb-1e703eddc901
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:09:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:09:58:INFO:
[92mINFO [0m:      Received: train message 1c94ca1f-8dc3-466b-9677-2927f3405ad4
02/05/2025 11:09:58:INFO:Received: train message 1c94ca1f-8dc3-466b-9677-2927f3405ad4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:10:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:51:INFO:
[92mINFO [0m:      Received: evaluate message 3b8565a7-3e14-43c0-92d8-f1dd49a815f6
02/05/2025 11:12:51:INFO:Received: evaluate message 3b8565a7-3e14-43c0-92d8-f1dd49a815f6

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:18:INFO:
[92mINFO [0m:      Received: train message d4926229-1193-416f-8315-a3262ca99399
02/05/2025 11:14:18:INFO:Received: train message d4926229-1193-416f-8315-a3262ca99399
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:33:INFO:
[92mINFO [0m:      Received: evaluate message 26c22663-6908-40d0-aa7f-cacc3310d695
02/05/2025 11:17:33:INFO:Received: evaluate message 26c22663-6908-40d0-aa7f-cacc3310d695
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:20:INFO:
[92mINFO [0m:      Received: train message e0af2354-83c9-40d2-9cf8-fbaa5707981e
02/05/2025 11:18:20:INFO:Received: train message e0af2354-83c9-40d2-9cf8-fbaa5707981e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:46:INFO:
[92mINFO [0m:      Received: evaluate message 8f63f132-19c4-4524-a7d6-6f96cd078afb
02/05/2025 11:20:46:INFO:Received: evaluate message 8f63f132-19c4-4524-a7d6-6f96cd078afb

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:28:INFO:
[92mINFO [0m:      Received: train message 6300738f-3b2b-4905-8d89-4e394a71793d
02/05/2025 11:21:28:INFO:Received: train message 6300738f-3b2b-4905-8d89-4e394a71793d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:44:INFO:
[92mINFO [0m:      Received: evaluate message 95877899-51c9-41bf-a51c-7af99acc3ed7
02/05/2025 11:23:44:INFO:Received: evaluate message 95877899-51c9-41bf-a51c-7af99acc3ed7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:22:INFO:
[92mINFO [0m:      Received: train message d2717ece-7925-4d22-9140-eb756e3cea5f
02/05/2025 11:24:22:INFO:Received: train message d2717ece-7925-4d22-9140-eb756e3cea5f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:28:INFO:
[92mINFO [0m:      Received: evaluate message 97c02b06-91cc-46c5-87cc-3a93453d21ea
02/05/2025 11:26:28:INFO:Received: evaluate message 97c02b06-91cc-46c5-87cc-3a93453d21ea

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:02:INFO:
[92mINFO [0m:      Received: train message fa5c13d2-1897-4615-9869-c706c1917ee7
02/05/2025 11:27:02:INFO:Received: train message fa5c13d2-1897-4615-9869-c706c1917ee7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:27:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:21:INFO:
[92mINFO [0m:      Received: evaluate message 7a87ea55-8da7-4ffd-a61a-47c57f6a9981
02/05/2025 11:29:21:INFO:Received: evaluate message 7a87ea55-8da7-4ffd-a61a-47c57f6a9981
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:30:05:INFO:
[92mINFO [0m:      Received: train message dd5dc547-9dcc-4e7a-a972-e41d8a9e9afd
02/05/2025 11:30:05:INFO:Received: train message dd5dc547-9dcc-4e7a-a972-e41d8a9e9afd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:30:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:47:INFO:
[92mINFO [0m:      Received: evaluate message e59e7df7-33ed-41b9-81bc-7628a4a8115c
02/05/2025 11:31:47:INFO:Received: evaluate message e59e7df7-33ed-41b9-81bc-7628a4a8115c

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:31:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:06:INFO:
[92mINFO [0m:      Received: reconnect message 509c8bbd-bf0c-4140-8103-95cb722e7fa6
02/05/2025 11:32:06:INFO:Received: reconnect message 509c8bbd-bf0c-4140-8103-95cb722e7fa6
02/05/2025 11:32:07:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:32:07:INFO:Disconnect and shut down

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585, 1.0229751927205788], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641, 0.8030471862751019], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177, 0.5915168783026465], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078, 0.5527371062811647]}



Final client history:
{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585, 1.0229751927205788], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641, 0.8030471862751019], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177, 0.5915168783026465], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078, 0.5527371062811647]}

