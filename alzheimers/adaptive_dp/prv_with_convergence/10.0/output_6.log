nohup: ignoring input
02/05/2025 10:01:50:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:01:50:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:01:50:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778510.171437 1718915 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:02:25:INFO:
[92mINFO [0m:      Received: train message ba8394fb-ab7a-41c8-91c9-96b94666f5fb
02/05/2025 10:02:25:INFO:Received: train message ba8394fb-ab7a-41c8-91c9-96b94666f5fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:03:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:01:INFO:
[92mINFO [0m:      Received: evaluate message 039f0f9d-36b6-496c-b9a2-ec457e995b94
02/05/2025 10:04:01:INFO:Received: evaluate message 039f0f9d-36b6-496c-b9a2-ec457e995b94
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:04:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:57:INFO:
[92mINFO [0m:      Received: train message 21d65200-83d0-4519-8c62-425352ee1d65
02/05/2025 10:04:57:INFO:Received: train message 21d65200-83d0-4519-8c62-425352ee1d65
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:31:INFO:
[92mINFO [0m:      Received: evaluate message a608a5f7-df40-4a79-a4ed-efc2b1349055
02/05/2025 10:06:31:INFO:Received: evaluate message a608a5f7-df40-4a79-a4ed-efc2b1349055
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:20:INFO:
[92mINFO [0m:      Received: train message 97405a43-0950-4e5a-99d7-15a460e84d77
02/05/2025 10:07:20:INFO:Received: train message 97405a43-0950-4e5a-99d7-15a460e84d77
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:08:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:57:INFO:
[92mINFO [0m:      Received: evaluate message be0d4709-fc59-42ef-a55f-802ba1583859
02/05/2025 10:08:57:INFO:Received: evaluate message be0d4709-fc59-42ef-a55f-802ba1583859
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:49:INFO:
[92mINFO [0m:      Received: train message a33752bb-0f77-4998-940f-9937d41f23f8
02/05/2025 10:09:49:INFO:Received: train message a33752bb-0f77-4998-940f-9937d41f23f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:29:INFO:
[92mINFO [0m:      Received: evaluate message cb2cb39e-2c16-4741-8c53-f8f461d4d97b
02/05/2025 10:11:29:INFO:Received: evaluate message cb2cb39e-2c16-4741-8c53-f8f461d4d97b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:50:INFO:
[92mINFO [0m:      Received: train message ab146af1-6054-4fd0-a7c0-c285af013b94
02/05/2025 10:11:50:INFO:Received: train message ab146af1-6054-4fd0-a7c0-c285af013b94
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:03:INFO:
[92mINFO [0m:      Received: evaluate message 403f7df7-c6c9-4731-a5c3-67d622d966f2
02/05/2025 10:14:03:INFO:Received: evaluate message 403f7df7-c6c9-4731-a5c3-67d622d966f2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:14:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:38:INFO:
[92mINFO [0m:      Received: train message 9b3658b3-1fcc-487c-8235-10e9e846cd43
02/05/2025 10:14:38:INFO:Received: train message 9b3658b3-1fcc-487c-8235-10e9e846cd43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:15:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:24:INFO:
[92mINFO [0m:      Received: evaluate message 751e31d7-0db8-4617-86b5-84770162e5a3
02/05/2025 10:16:24:INFO:Received: evaluate message 751e31d7-0db8-4617-86b5-84770162e5a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:50:INFO:
[92mINFO [0m:      Received: train message 35d4f938-f565-4c48-b0a3-e62941f942ce
02/05/2025 10:16:50:INFO:Received: train message 35d4f938-f565-4c48-b0a3-e62941f942ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:03:INFO:
[92mINFO [0m:      Received: evaluate message 0b6d9b14-4afc-4d80-9e15-67fcb7379335
02/05/2025 10:19:03:INFO:Received: evaluate message 0b6d9b14-4afc-4d80-9e15-67fcb7379335
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945], 'accuracy': [0.5113369820172009], 'auc': [0.7085531489885228], 'precision': [0.40347364107321304], 'recall': [0.5113369820172009], 'f1': [0.39881455974124325]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902], 'accuracy': [0.5113369820172009, 0.5293197810789679], 'auc': [0.7085531489885228, 0.7350934873405456], 'precision': [0.40347364107321304, 0.43385134376460194], 'recall': [0.5113369820172009, 0.5293197810789679], 'f1': [0.39881455974124325, 0.4722552092863052]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:19:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:42:INFO:
[92mINFO [0m:      Received: train message ce683db6-2ebd-4b11-b7f3-105dcd7e80bc
02/05/2025 10:19:42:INFO:Received: train message ce683db6-2ebd-4b11-b7f3-105dcd7e80bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:21:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:27:INFO:
[92mINFO [0m:      Received: evaluate message 3a9fb0bb-b7c2-4323-b5a5-10e0ec1b3e30
02/05/2025 10:22:27:INFO:Received: evaluate message 3a9fb0bb-b7c2-4323-b5a5-10e0ec1b3e30
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:22:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:19:INFO:
[92mINFO [0m:      Received: train message 2cc1cb85-ca63-49d0-9ea5-952aa1638501
02/05/2025 10:23:19:INFO:Received: train message 2cc1cb85-ca63-49d0-9ea5-952aa1638501
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:23:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:49:INFO:
[92mINFO [0m:      Received: evaluate message b6886e80-97f3-41be-8e6b-ab7dea1967d5
02/05/2025 10:24:49:INFO:Received: evaluate message b6886e80-97f3-41be-8e6b-ab7dea1967d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:24:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:35:INFO:
[92mINFO [0m:      Received: train message 2a6dfef9-b2e0-4a8d-b933-0d0c241f1ad8
02/05/2025 10:25:35:INFO:Received: train message 2a6dfef9-b2e0-4a8d-b933-0d0c241f1ad8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:27:INFO:
[92mINFO [0m:      Received: evaluate message a3f8ef74-032a-4f4a-8730-fee6422a6073
02/05/2025 10:27:27:INFO:Received: evaluate message a3f8ef74-032a-4f4a-8730-fee6422a6073
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:01:INFO:
[92mINFO [0m:      Received: train message bc579363-307d-4f15-bf63-0a301699204a
02/05/2025 10:28:01:INFO:Received: train message bc579363-307d-4f15-bf63-0a301699204a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:53:INFO:
[92mINFO [0m:      Received: evaluate message e831ff5a-dffc-4277-b640-3266a9b6c15c
02/05/2025 10:29:53:INFO:Received: evaluate message e831ff5a-dffc-4277-b640-3266a9b6c15c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:25:INFO:
[92mINFO [0m:      Received: train message ca6ecb5d-c653-4643-8136-44a598d6f114
02/05/2025 10:30:25:INFO:Received: train message ca6ecb5d-c653-4643-8136-44a598d6f114

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:31:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:33:INFO:
[92mINFO [0m:      Received: evaluate message 6b8ac343-67d8-4daa-8912-ce7da6c14041
02/05/2025 10:32:33:INFO:Received: evaluate message 6b8ac343-67d8-4daa-8912-ce7da6c14041
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:01:INFO:
[92mINFO [0m:      Received: train message c261a001-8f8d-4a69-8498-075fbe76354f
02/05/2025 10:33:01:INFO:Received: train message c261a001-8f8d-4a69-8498-075fbe76354f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:03:INFO:
[92mINFO [0m:      Received: evaluate message 64e62090-541f-4c1b-ba5b-e365421d3593
02/05/2025 10:35:03:INFO:Received: evaluate message 64e62090-541f-4c1b-ba5b-e365421d3593
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:35:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:46:INFO:
[92mINFO [0m:      Received: train message 7fcd145e-73ac-4731-ae19-630f17622468
02/05/2025 10:35:46:INFO:Received: train message 7fcd145e-73ac-4731-ae19-630f17622468
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:36:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:38:INFO:
[92mINFO [0m:      Received: evaluate message d195ea16-f2b5-49bd-93b6-5b69f64b2bdd
02/05/2025 10:37:38:INFO:Received: evaluate message d195ea16-f2b5-49bd-93b6-5b69f64b2bdd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:15:INFO:
[92mINFO [0m:      Received: train message f645f6ac-5699-4823-9761-d7525c29ac52
02/05/2025 10:38:15:INFO:Received: train message f645f6ac-5699-4823-9761-d7525c29ac52
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:50:INFO:
[92mINFO [0m:      Received: evaluate message 55a220ee-7bd0-4e01-ae6e-78616aaf2e17
02/05/2025 10:39:50:INFO:Received: evaluate message 55a220ee-7bd0-4e01-ae6e-78616aaf2e17
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:43:INFO:
[92mINFO [0m:      Received: train message 77978b53-3dc9-4989-ac0a-37e472363f04
02/05/2025 10:40:43:INFO:Received: train message 77978b53-3dc9-4989-ac0a-37e472363f04
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:25:INFO:
[92mINFO [0m:      Received: evaluate message 99a03af9-4ab2-4313-af2c-cfd4433f4c4c
02/05/2025 10:42:25:INFO:Received: evaluate message 99a03af9-4ab2-4313-af2c-cfd4433f4c4c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:22:INFO:
[92mINFO [0m:      Received: train message c63ab8a5-400f-447f-92ee-4bff6d11832e
02/05/2025 10:43:22:INFO:Received: train message c63ab8a5-400f-447f-92ee-4bff6d11832e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:43:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:58:INFO:
[92mINFO [0m:      Received: evaluate message 7aea8014-caf9-4a05-b909-a2aab4dca2f2
02/05/2025 10:44:58:INFO:Received: evaluate message 7aea8014-caf9-4a05-b909-a2aab4dca2f2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:45:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:56:INFO:
[92mINFO [0m:      Received: train message f18e7d22-76c8-4a29-bbf8-af21ec3b5db5
02/05/2025 10:45:56:INFO:Received: train message f18e7d22-76c8-4a29-bbf8-af21ec3b5db5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:38:INFO:
[92mINFO [0m:      Received: evaluate message b0ef162e-c446-4a37-9d1c-11d0010e2f0e
02/05/2025 10:47:38:INFO:Received: evaluate message b0ef162e-c446-4a37-9d1c-11d0010e2f0e

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:27:INFO:
[92mINFO [0m:      Received: train message f960eec0-ef09-45d1-9d02-a510f368c764
02/05/2025 10:48:27:INFO:Received: train message f960eec0-ef09-45d1-9d02-a510f368c764
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:11:INFO:
[92mINFO [0m:      Received: evaluate message 2ec4ee4f-4564-4a3a-a763-df77caac9e72
02/05/2025 10:50:11:INFO:Received: evaluate message 2ec4ee4f-4564-4a3a-a763-df77caac9e72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:02:INFO:
[92mINFO [0m:      Received: train message 2dc741d5-d16d-4df8-9cce-ef3eac9b2494
02/05/2025 10:51:02:INFO:Received: train message 2dc741d5-d16d-4df8-9cce-ef3eac9b2494
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:51:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:13:INFO:
[92mINFO [0m:      Received: evaluate message 74bf6bce-8b5a-4011-a3d3-376b1778d56b
02/05/2025 10:53:13:INFO:Received: evaluate message 74bf6bce-8b5a-4011-a3d3-376b1778d56b

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:12:INFO:
[92mINFO [0m:      Received: train message 18d30eaa-2694-4025-b2ca-94d04f0b8b09
02/05/2025 10:54:12:INFO:Received: train message 18d30eaa-2694-4025-b2ca-94d04f0b8b09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:54:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:22:INFO:
[92mINFO [0m:      Received: evaluate message cd3eaac4-759e-4325-8133-79991f09cab3
02/05/2025 10:56:22:INFO:Received: evaluate message cd3eaac4-759e-4325-8133-79991f09cab3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:56:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:23:INFO:
[92mINFO [0m:      Received: train message c982c830-b092-4277-a393-973540d628cf
02/05/2025 10:57:23:INFO:Received: train message c982c830-b092-4277-a393-973540d628cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:58:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:52:INFO:
[92mINFO [0m:      Received: evaluate message 1e8af38f-03b5-4211-a5c0-bbc2a0d6fa9f
02/05/2025 10:59:52:INFO:Received: evaluate message 1e8af38f-03b5-4211-a5c0-bbc2a0d6fa9f

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:59:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:22:INFO:
[92mINFO [0m:      Received: train message 077ef766-c406-4be4-87de-d8ae4e16cb8f
02/05/2025 11:00:22:INFO:Received: train message 077ef766-c406-4be4-87de-d8ae4e16cb8f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:01:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:25:INFO:
[92mINFO [0m:      Received: evaluate message 28a02ab2-0458-4917-b2e2-f282f31808c0
02/05/2025 11:03:25:INFO:Received: evaluate message 28a02ab2-0458-4917-b2e2-f282f31808c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:38:INFO:
[92mINFO [0m:      Received: train message 4c202d8a-8291-4065-bf75-e437b5526f7a
02/05/2025 11:04:38:INFO:Received: train message 4c202d8a-8291-4065-bf75-e437b5526f7a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:30:INFO:
[92mINFO [0m:      Received: evaluate message e5c4d904-1dea-4a26-8819-bcb039526554
02/05/2025 11:06:30:INFO:Received: evaluate message e5c4d904-1dea-4a26-8819-bcb039526554

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:06:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:31:INFO:
[92mINFO [0m:      Received: train message 0e957325-b819-415d-b96c-d1ae4fef89ec
02/05/2025 11:08:31:INFO:Received: train message 0e957325-b819-415d-b96c-d1ae4fef89ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:09:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:11:05:INFO:
[92mINFO [0m:      Received: evaluate message 1b47bfb6-ddec-4be2-b015-b51ccd327289
02/05/2025 11:11:05:INFO:Received: evaluate message 1b47bfb6-ddec-4be2-b015-b51ccd327289
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:11:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:15:INFO:
[92mINFO [0m:      Received: train message e95f8d31-c4a4-451f-bd31-ef177121375e
02/05/2025 11:12:15:INFO:Received: train message e95f8d31-c4a4-451f-bd31-ef177121375e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:12:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:45:INFO:
[92mINFO [0m:      Received: evaluate message 3d9cb7a0-78ac-4a0d-b6f2-ee14ab73be89
02/05/2025 11:14:45:INFO:Received: evaluate message 3d9cb7a0-78ac-4a0d-b6f2-ee14ab73be89

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:14:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:16:04:INFO:
[92mINFO [0m:      Received: train message faaf800b-cb3d-4e38-8d85-993af9899a13
02/05/2025 11:16:04:INFO:Received: train message faaf800b-cb3d-4e38-8d85-993af9899a13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:16:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:50:INFO:
[92mINFO [0m:      Received: evaluate message 115749b5-7b58-475e-996a-109b7a1f9de5
02/05/2025 11:17:50:INFO:Received: evaluate message 115749b5-7b58-475e-996a-109b7a1f9de5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:18:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:19:23:INFO:
[92mINFO [0m:      Received: train message 5a2fcfab-5377-44cf-8845-d0ff93b58ce3
02/05/2025 11:19:23:INFO:Received: train message 5a2fcfab-5377-44cf-8845-d0ff93b58ce3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:20:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:06:INFO:
[92mINFO [0m:      Received: evaluate message 278c61c3-043c-49bf-bdc5-354d7aaf0bea
02/05/2025 11:21:06:INFO:Received: evaluate message 278c61c3-043c-49bf-bdc5-354d7aaf0bea

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:21:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:22:06:INFO:
[92mINFO [0m:      Received: train message 44e3e072-0b4c-4a46-bf38-aff4082a0a8c
02/05/2025 11:22:06:INFO:Received: train message 44e3e072-0b4c-4a46-bf38-aff4082a0a8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:40:INFO:
[92mINFO [0m:      Received: evaluate message e69827f0-ef60-41ea-9b4f-461d1f40da6d
02/05/2025 11:23:40:INFO:Received: evaluate message e69827f0-ef60-41ea-9b4f-461d1f40da6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:28:INFO:
[92mINFO [0m:      Received: train message f1fba493-b3f9-4bb9-97d3-d9bde52b0104
02/05/2025 11:24:28:INFO:Received: train message f1fba493-b3f9-4bb9-97d3-d9bde52b0104
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:19:INFO:
[92mINFO [0m:      Received: evaluate message 5dd532a5-7b7a-46d3-9282-6ac3eb917f76
02/05/2025 11:26:19:INFO:Received: evaluate message 5dd532a5-7b7a-46d3-9282-6ac3eb917f76

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:32:INFO:
[92mINFO [0m:      Received: reconnect message d46b0ee3-1d02-46e1-98f5-56b645965c07
02/05/2025 11:26:32:INFO:Received: reconnect message d46b0ee3-1d02-46e1-98f5-56b645965c07
02/05/2025 11:26:33:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:26:33:INFO:Disconnect and shut down

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576, 1.0347204484261043], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504, 0.7977737627253754], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071, 0.595632848171531], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902, 0.5462812197574198]}



Final client history:
{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576, 1.0347204484261043], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504, 0.7977737627253754], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071, 0.595632848171531], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902, 0.5462812197574198]}

