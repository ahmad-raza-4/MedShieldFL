nohup: ignoring input
02/05/2025 10:03:25:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:03:25:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:03:25:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778605.940175 1800988 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:04:03:INFO:
[92mINFO [0m:      Received: train message e7d79abf-508c-4926-ba40-4b8aeff6f2f1
02/05/2025 10:04:03:INFO:Received: train message e7d79abf-508c-4926-ba40-4b8aeff6f2f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:04:INFO:
[92mINFO [0m:      Received: evaluate message 3546d362-173e-4e13-8096-f36555f1af4d
02/05/2025 10:06:04:INFO:Received: evaluate message 3546d362-173e-4e13-8096-f36555f1af4d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:39:INFO:
[92mINFO [0m:      Received: train message 2c13bc79-0e35-4137-8bf5-b8b7edc04b98
02/05/2025 10:06:39:INFO:Received: train message 2c13bc79-0e35-4137-8bf5-b8b7edc04b98
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:34:INFO:
[92mINFO [0m:      Received: evaluate message 9dd4e737-7827-4498-9a1f-3cd6df67b527
02/05/2025 10:08:34:INFO:Received: evaluate message 9dd4e737-7827-4498-9a1f-3cd6df67b527
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:08:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:16:INFO:
[92mINFO [0m:      Received: train message b0933f4e-412e-40d4-91da-f878118c5396
02/05/2025 10:09:16:INFO:Received: train message b0933f4e-412e-40d4-91da-f878118c5396
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:06:INFO:
[92mINFO [0m:      Received: evaluate message f01f8190-762b-409a-84af-ff79538f6e5a
02/05/2025 10:11:06:INFO:Received: evaluate message f01f8190-762b-409a-84af-ff79538f6e5a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:40:INFO:
[92mINFO [0m:      Received: train message 2f9563a3-e5ba-4ded-a082-2da9fc36c3a7
02/05/2025 10:11:40:INFO:Received: train message 2f9563a3-e5ba-4ded-a082-2da9fc36c3a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:24:INFO:
[92mINFO [0m:      Received: evaluate message 9d0c1f89-2f19-4ebf-97d9-a7fd1a3fefeb
02/05/2025 10:13:24:INFO:Received: evaluate message 9d0c1f89-2f19-4ebf-97d9-a7fd1a3fefeb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:57:INFO:
[92mINFO [0m:      Received: train message c6debf9c-8a4c-4fc3-aada-ff55309cda16
02/05/2025 10:13:57:INFO:Received: train message c6debf9c-8a4c-4fc3-aada-ff55309cda16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:49:INFO:
[92mINFO [0m:      Received: evaluate message a0b158b2-7f7d-4862-b777-433efc29862d
02/05/2025 10:15:49:INFO:Received: evaluate message a0b158b2-7f7d-4862-b777-433efc29862d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:15:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:40:INFO:
[92mINFO [0m:      Received: train message 9f9a5d6f-8c39-4577-b0d0-c71ff2f34853
02/05/2025 10:16:40:INFO:Received: train message 9f9a5d6f-8c39-4577-b0d0-c71ff2f34853
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:27:INFO:
[92mINFO [0m:      Received: evaluate message af0cda4b-05e6-43d4-87d7-61c4f910679b
02/05/2025 10:18:27:INFO:Received: evaluate message af0cda4b-05e6-43d4-87d7-61c4f910679b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:57:INFO:
[92mINFO [0m:      Received: train message 3a2dec2d-0b5b-42a5-9f6c-59f56db90e13
02/05/2025 10:18:57:INFO:Received: train message 3a2dec2d-0b5b-42a5-9f6c-59f56db90e13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:20:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:56:INFO:
[92mINFO [0m:      Received: evaluate message 201d2ad7-140c-4415-a6fb-1cfee95acbea
02/05/2025 10:21:56:INFO:Received: evaluate message 201d2ad7-140c-4415-a6fb-1cfee95acbea
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986], 'accuracy': [0.5160281469898358], 'auc': [0.7174808607527126], 'precision': [0.4096278307389395], 'recall': [0.5160281469898358], 'f1': [0.42340290242033957]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763], 'accuracy': [0.5160281469898358, 0.5371383893666928], 'auc': [0.7174808607527126, 0.743196921722251], 'precision': [0.4096278307389395, 0.4446497303781511], 'recall': [0.5160281469898358, 0.5371383893666928], 'f1': [0.42340290242033957, 0.48416854046252017]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:22:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:41:INFO:
[92mINFO [0m:      Received: train message 35d5a690-736e-4666-95b5-437e732585f3
02/05/2025 10:22:41:INFO:Received: train message 35d5a690-736e-4666-95b5-437e732585f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:23:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:29:INFO:
[92mINFO [0m:      Received: evaluate message b570e168-de5e-4c8e-a894-b212c86aa715
02/05/2025 10:24:29:INFO:Received: evaluate message b570e168-de5e-4c8e-a894-b212c86aa715
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:24:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:15:INFO:
[92mINFO [0m:      Received: train message b6db378a-045e-44ed-b333-cfc085c774cd
02/05/2025 10:25:15:INFO:Received: train message b6db378a-045e-44ed-b333-cfc085c774cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:09:INFO:
[92mINFO [0m:      Received: evaluate message 30eb35d3-c646-4cfb-add1-2b4be6a7a6bc
02/05/2025 10:27:09:INFO:Received: evaluate message 30eb35d3-c646-4cfb-add1-2b4be6a7a6bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:37:INFO:
[92mINFO [0m:      Received: train message 7d0696f3-6365-4c73-98c2-0bbf7b89f344
02/05/2025 10:27:37:INFO:Received: train message 7d0696f3-6365-4c73-98c2-0bbf7b89f344
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:15:INFO:
[92mINFO [0m:      Received: evaluate message b186fc20-ddfe-4610-afa3-0640f22f88a2
02/05/2025 10:29:15:INFO:Received: evaluate message b186fc20-ddfe-4610-afa3-0640f22f88a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:15:INFO:
[92mINFO [0m:      Received: train message f4d8d5e0-7b6f-4497-be3d-fa23eb0780e4
02/05/2025 10:30:15:INFO:Received: train message f4d8d5e0-7b6f-4497-be3d-fa23eb0780e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:31:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:01:INFO:
[92mINFO [0m:      Received: evaluate message f2c80b80-4fa5-48d8-9b6f-d944c3bd23eb
02/05/2025 10:32:01:INFO:Received: evaluate message f2c80b80-4fa5-48d8-9b6f-d944c3bd23eb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:52:INFO:
[92mINFO [0m:      Received: train message 7b04f707-121a-4b96-b442-97198401e4ad
02/05/2025 10:32:52:INFO:Received: train message 7b04f707-121a-4b96-b442-97198401e4ad

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:10:INFO:
[92mINFO [0m:      Received: evaluate message dd95dae0-0c6b-4c87-bf43-d1f6141dad96
02/05/2025 10:34:10:INFO:Received: evaluate message dd95dae0-0c6b-4c87-bf43-d1f6141dad96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:21:INFO:
[92mINFO [0m:      Received: train message 98cb5e51-47f2-477b-a7a2-72fdef669189
02/05/2025 10:35:21:INFO:Received: train message 98cb5e51-47f2-477b-a7a2-72fdef669189
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:36:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:14:INFO:
[92mINFO [0m:      Received: evaluate message 84268122-563f-4ccc-9cf9-a86d83f7c593
02/05/2025 10:37:14:INFO:Received: evaluate message 84268122-563f-4ccc-9cf9-a86d83f7c593
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:58:INFO:
[92mINFO [0m:      Received: train message d44684a5-4ce0-469d-bd06-67132fc01503
02/05/2025 10:37:58:INFO:Received: train message d44684a5-4ce0-469d-bd06-67132fc01503
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:35:INFO:
[92mINFO [0m:      Received: evaluate message d533fcff-dff8-414a-be98-5a99eebabfab
02/05/2025 10:39:35:INFO:Received: evaluate message d533fcff-dff8-414a-be98-5a99eebabfab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:10:INFO:
[92mINFO [0m:      Received: train message a42cfb41-9dd6-42f4-b069-ed6f1baaa43e
02/05/2025 10:40:10:INFO:Received: train message a42cfb41-9dd6-42f4-b069-ed6f1baaa43e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:08:INFO:
[92mINFO [0m:      Received: evaluate message 6aaf09e6-c826-42f7-accb-49fe7c8679ac
02/05/2025 10:42:08:INFO:Received: evaluate message 6aaf09e6-c826-42f7-accb-49fe7c8679ac
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:45:INFO:
[92mINFO [0m:      Received: train message 6a5dac43-350e-414a-b718-b5d96d68336a
02/05/2025 10:42:45:INFO:Received: train message 6a5dac43-350e-414a-b718-b5d96d68336a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:43:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:52:INFO:
[92mINFO [0m:      Received: evaluate message 25deda2a-e367-4812-a3e7-bf0722a1f3fd
02/05/2025 10:44:52:INFO:Received: evaluate message 25deda2a-e367-4812-a3e7-bf0722a1f3fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:44:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:39:INFO:
[92mINFO [0m:      Received: train message 24462739-0cb6-4def-bd0d-ca674a60a69d
02/05/2025 10:45:39:INFO:Received: train message 24462739-0cb6-4def-bd0d-ca674a60a69d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:44:INFO:
[92mINFO [0m:      Received: evaluate message 0b57dbd7-8e06-4a88-b535-3a1bf80d50b0
02/05/2025 10:47:44:INFO:Received: evaluate message 0b57dbd7-8e06-4a88-b535-3a1bf80d50b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:07:INFO:
[92mINFO [0m:      Received: train message c252ffd0-9c6f-4687-a806-91ae2abf5ed2
02/05/2025 10:48:07:INFO:Received: train message c252ffd0-9c6f-4687-a806-91ae2abf5ed2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:33:INFO:
[92mINFO [0m:      Received: evaluate message 5afcba03-27f8-487b-995d-a0466dcffc1d
02/05/2025 10:50:33:INFO:Received: evaluate message 5afcba03-27f8-487b-995d-a0466dcffc1d

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:17:INFO:
[92mINFO [0m:      Received: train message fe16dd19-d7dd-40cc-b1ea-5e4a3ba07bd2
02/05/2025 10:51:17:INFO:Received: train message fe16dd19-d7dd-40cc-b1ea-5e4a3ba07bd2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:19:INFO:
[92mINFO [0m:      Received: evaluate message 18672732-c707-44b7-afb7-109299af6d7d
02/05/2025 10:53:19:INFO:Received: evaluate message 18672732-c707-44b7-afb7-109299af6d7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:19:INFO:
[92mINFO [0m:      Received: train message 08aaf2a6-b52f-48bd-b7f7-0cc71c90dfd7
02/05/2025 10:54:19:INFO:Received: train message 08aaf2a6-b52f-48bd-b7f7-0cc71c90dfd7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:06:INFO:
[92mINFO [0m:      Received: evaluate message f8404464-4114-468d-927d-b86b826123e6
02/05/2025 10:57:06:INFO:Received: evaluate message f8404464-4114-468d-927d-b86b826123e6

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:57:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:25:INFO:
[92mINFO [0m:      Received: train message 4a78f5d6-1daa-4b1b-afb8-2cc0c104c65f
02/05/2025 10:58:25:INFO:Received: train message 4a78f5d6-1daa-4b1b-afb8-2cc0c104c65f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:59:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:29:INFO:
[92mINFO [0m:      Received: evaluate message f03caaf6-eb72-463b-aed7-b8e0c7820360
02/05/2025 11:01:29:INFO:Received: evaluate message f03caaf6-eb72-463b-aed7-b8e0c7820360
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:01:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:02:16:INFO:
[92mINFO [0m:      Received: train message a2da47fc-62df-4c37-ab36-759b0fbbfe6b
02/05/2025 11:02:16:INFO:Received: train message a2da47fc-62df-4c37-ab36-759b0fbbfe6b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:03:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:55:INFO:
[92mINFO [0m:      Received: evaluate message 65e3277f-f2cf-40ff-8ee2-0f8cd2de7a1d
02/05/2025 11:04:55:INFO:Received: evaluate message 65e3277f-f2cf-40ff-8ee2-0f8cd2de7a1d

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:05:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:17:INFO:
[92mINFO [0m:      Received: train message d5ebb659-f879-4854-9cd3-851e78150e6e
02/05/2025 11:06:17:INFO:Received: train message d5ebb659-f879-4854-9cd3-851e78150e6e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:07:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:09:20:INFO:
[92mINFO [0m:      Received: evaluate message 5bef3703-c76f-4d63-ac1d-e56306e97271
02/05/2025 11:09:20:INFO:Received: evaluate message 5bef3703-c76f-4d63-ac1d-e56306e97271
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:09:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:26:INFO:
[92mINFO [0m:      Received: train message 141bfdca-24b6-44c8-be5b-524426417070
02/05/2025 11:10:26:INFO:Received: train message 141bfdca-24b6-44c8-be5b-524426417070
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:24:INFO:
[92mINFO [0m:      Received: evaluate message 414dbd6a-1035-4638-a75d-0335d395e5af
02/05/2025 11:12:24:INFO:Received: evaluate message 414dbd6a-1035-4638-a75d-0335d395e5af

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:12:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:59:INFO:
[92mINFO [0m:      Received: train message 2b314a7e-9b93-4219-b7ce-5e44183411ea
02/05/2025 11:13:59:INFO:Received: train message 2b314a7e-9b93-4219-b7ce-5e44183411ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:27:INFO:
[92mINFO [0m:      Received: evaluate message 0787a509-da35-41ee-b852-fbe242811b5e
02/05/2025 11:17:27:INFO:Received: evaluate message 0787a509-da35-41ee-b852-fbe242811b5e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:49:INFO:
[92mINFO [0m:      Received: train message 2c8bb8a7-de5f-47c9-a9cc-92a352ecc1d5
02/05/2025 11:18:49:INFO:Received: train message 2c8bb8a7-de5f-47c9-a9cc-92a352ecc1d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:34:INFO:
[92mINFO [0m:      Received: evaluate message 3d0fc001-1e1d-47f8-aff1-1db65774f911
02/05/2025 11:20:34:INFO:Received: evaluate message 3d0fc001-1e1d-47f8-aff1-1db65774f911

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:44:INFO:
[92mINFO [0m:      Received: train message b67530c4-5759-4df9-8d45-215a31a512c1
02/05/2025 11:21:44:INFO:Received: train message b67530c4-5759-4df9-8d45-215a31a512c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:46:INFO:
[92mINFO [0m:      Received: evaluate message 025a28bc-72c0-44f9-9745-1d25df788726
02/05/2025 11:23:46:INFO:Received: evaluate message 025a28bc-72c0-44f9-9745-1d25df788726
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:38:INFO:
[92mINFO [0m:      Received: train message 9ae7b863-0c1c-498c-891e-d9d48342e4c1
02/05/2025 11:24:38:INFO:Received: train message 9ae7b863-0c1c-498c-891e-d9d48342e4c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:28:INFO:
[92mINFO [0m:      Received: evaluate message 0e1cbdf6-96a2-4210-8305-bd05071dde38
02/05/2025 11:26:28:INFO:Received: evaluate message 0e1cbdf6-96a2-4210-8305-bd05071dde38

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:10:INFO:
[92mINFO [0m:      Received: train message 06e33029-16f6-431b-94b3-4adf62032876
02/05/2025 11:27:10:INFO:Received: train message 06e33029-16f6-431b-94b3-4adf62032876
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:28:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:02:INFO:
[92mINFO [0m:      Received: evaluate message 2cb5694f-6f86-4c92-b452-3f1140fb3c78
02/05/2025 11:29:02:INFO:Received: evaluate message 2cb5694f-6f86-4c92-b452-3f1140fb3c78
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:55:INFO:
[92mINFO [0m:      Received: train message fc7691e4-f8a8-4a4b-9e31-89eb50fcdf0c
02/05/2025 11:29:55:INFO:Received: train message fc7691e4-f8a8-4a4b-9e31-89eb50fcdf0c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:31:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:40:INFO:
[92mINFO [0m:      Received: evaluate message da9ec155-2d90-4c6f-b675-9bf34bd81b1e
02/05/2025 11:31:40:INFO:Received: evaluate message da9ec155-2d90-4c6f-b675-9bf34bd81b1e

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:31:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:07:INFO:
[92mINFO [0m:      Received: reconnect message 770f8b86-3713-467c-a59d-11e26cdb76f1
02/05/2025 11:32:07:INFO:Received: reconnect message 770f8b86-3713-467c-a59d-11e26cdb76f1
02/05/2025 11:32:07:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:32:07:INFO:Disconnect and shut down

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585, 1.0229751927205788], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641, 0.8030471862751019], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177, 0.5915168783026465], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078, 0.5527371062811647]}



Final client history:
{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585, 1.0229751927205788], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641, 0.8030471862751019], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177, 0.5915168783026465], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078, 0.5527371062811647]}

