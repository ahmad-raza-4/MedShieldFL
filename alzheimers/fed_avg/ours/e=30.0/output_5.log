nohup: ignoring input
02/05/2025 10:03:24:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:03:24:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:03:24:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 10:03:24:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778604.451743 1799453 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:03:52:INFO:
[92mINFO [0m:      Received: train message 656cd28b-855d-4baa-b855-8bce7fdc7ac7
02/05/2025 10:03:52:INFO:Received: train message 656cd28b-855d-4baa-b855-8bce7fdc7ac7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:04:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:05:58:INFO:
[92mINFO [0m:      Received: evaluate message 3e3984f4-d691-4d7f-ade1-9a9a4a7106ed
02/05/2025 10:05:58:INFO:Received: evaluate message 3e3984f4-d691-4d7f-ade1-9a9a4a7106ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:31:INFO:
[92mINFO [0m:      Received: train message b6d54247-2a46-43b8-925e-52cefd2fef51
02/05/2025 10:06:31:INFO:Received: train message b6d54247-2a46-43b8-925e-52cefd2fef51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:35:INFO:
[92mINFO [0m:      Received: evaluate message 6784132f-cc0c-46a8-9585-b51df128d7e4
02/05/2025 10:08:35:INFO:Received: evaluate message 6784132f-cc0c-46a8-9585-b51df128d7e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:08:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:15:INFO:
[92mINFO [0m:      Received: train message 0f2b3501-c48c-4931-8908-b009212f2fbc
02/05/2025 10:09:15:INFO:Received: train message 0f2b3501-c48c-4931-8908-b009212f2fbc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:10:58:INFO:
[92mINFO [0m:      Received: evaluate message 6c24dd75-b342-44fe-aac0-a6d78e0fa0d8
02/05/2025 10:10:58:INFO:Received: evaluate message 6c24dd75-b342-44fe-aac0-a6d78e0fa0d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:50:INFO:
[92mINFO [0m:      Received: train message bd0d24d0-d17c-4595-b498-db5471033479
02/05/2025 10:11:50:INFO:Received: train message bd0d24d0-d17c-4595-b498-db5471033479
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:34:INFO:
[92mINFO [0m:      Received: evaluate message 55302c6d-69f9-4e9f-8afb-2b2077fe99b4
02/05/2025 10:13:34:INFO:Received: evaluate message 55302c6d-69f9-4e9f-8afb-2b2077fe99b4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:16:INFO:
[92mINFO [0m:      Received: train message bc3d596d-870a-437d-a662-238ba9f0850b
02/05/2025 10:14:16:INFO:Received: train message bc3d596d-870a-437d-a662-238ba9f0850b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:15:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:58:INFO:
[92mINFO [0m:      Received: evaluate message b38ca576-0d80-40b1-bf50-5483a883d0c1
02/05/2025 10:15:58:INFO:Received: evaluate message b38ca576-0d80-40b1-bf50-5483a883d0c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:29:INFO:
[92mINFO [0m:      Received: train message 3ceadaa8-3c6a-4276-a83f-e8c23c894a58
02/05/2025 10:16:29:INFO:Received: train message 3ceadaa8-3c6a-4276-a83f-e8c23c894a58
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:27:INFO:
[92mINFO [0m:      Received: evaluate message 4e640286-edb7-4b83-87ca-62ea5c5bddac
02/05/2025 10:18:27:INFO:Received: evaluate message 4e640286-edb7-4b83-87ca-62ea5c5bddac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:15:INFO:
[92mINFO [0m:      Received: train message e7a8f201-27af-4936-be1d-8709a535584b
02/05/2025 10:19:15:INFO:Received: train message e7a8f201-27af-4936-be1d-8709a535584b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:21:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:56:INFO:
[92mINFO [0m:      Received: evaluate message 3831c7c5-ed25-41ab-a915-30c5862e8ac6
02/05/2025 10:21:56:INFO:Received: evaluate message 3831c7c5-ed25-41ab-a915-30c5862e8ac6
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986], 'accuracy': [0.5160281469898358], 'auc': [0.7174808607527126], 'precision': [0.4096278307389395], 'recall': [0.5160281469898358], 'f1': [0.42340290242033957]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763], 'accuracy': [0.5160281469898358, 0.5371383893666928], 'auc': [0.7174808607527126, 0.743196921722251], 'precision': [0.4096278307389395, 0.4446497303781511], 'recall': [0.5160281469898358, 0.5371383893666928], 'f1': [0.42340290242033957, 0.48416854046252017]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:22:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:46:INFO:
[92mINFO [0m:      Received: train message 1a05c2ba-2470-4be8-9062-6a90fa9bfc18
02/05/2025 10:22:46:INFO:Received: train message 1a05c2ba-2470-4be8-9062-6a90fa9bfc18
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:23:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:32:INFO:
[92mINFO [0m:      Received: evaluate message fa5d078c-b609-4dae-839a-c0dd17cad2ab
02/05/2025 10:24:32:INFO:Received: evaluate message fa5d078c-b609-4dae-839a-c0dd17cad2ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:24:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:15:INFO:
[92mINFO [0m:      Received: train message 2fcb040f-3b30-41bd-a0a3-f5118de22d42
02/05/2025 10:25:15:INFO:Received: train message 2fcb040f-3b30-41bd-a0a3-f5118de22d42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:42:INFO:
[92mINFO [0m:      Received: evaluate message cfaf23c3-eb1a-4898-9f70-50168b1f48d2
02/05/2025 10:26:42:INFO:Received: evaluate message cfaf23c3-eb1a-4898-9f70-50168b1f48d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:26:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:46:INFO:
[92mINFO [0m:      Received: train message 884b3ffd-cda3-4e28-9413-c04c679a2f5b
02/05/2025 10:27:46:INFO:Received: train message 884b3ffd-cda3-4e28-9413-c04c679a2f5b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:10:INFO:
[92mINFO [0m:      Received: evaluate message 848d38cb-407b-4b1f-a580-ab8ad98e14a0
02/05/2025 10:29:10:INFO:Received: evaluate message 848d38cb-407b-4b1f-a580-ab8ad98e14a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:14:INFO:
[92mINFO [0m:      Received: train message 3813aa9e-9389-4eec-b396-7b268cc7a75f
02/05/2025 10:30:14:INFO:Received: train message 3813aa9e-9389-4eec-b396-7b268cc7a75f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:31:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:01:INFO:
[92mINFO [0m:      Received: evaluate message 4202738d-61c5-4c8b-8bb3-5ffa7ac5b442
02/05/2025 10:32:01:INFO:Received: evaluate message 4202738d-61c5-4c8b-8bb3-5ffa7ac5b442
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:41:INFO:
[92mINFO [0m:      Received: train message 539afe0a-5240-4d21-ba70-0d144563a7f8
02/05/2025 10:32:41:INFO:Received: train message 539afe0a-5240-4d21-ba70-0d144563a7f8

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:38:INFO:
[92mINFO [0m:      Received: evaluate message 3c31142d-f786-41de-a14e-7ae7191153e5
02/05/2025 10:34:38:INFO:Received: evaluate message 3c31142d-f786-41de-a14e-7ae7191153e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:23:INFO:
[92mINFO [0m:      Received: train message bab7e0b0-cd88-4da6-9950-8d2083f7c792
02/05/2025 10:35:23:INFO:Received: train message bab7e0b0-cd88-4da6-9950-8d2083f7c792
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:36:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:02:INFO:
[92mINFO [0m:      Received: evaluate message f8a0f7b4-b3fd-4cb0-9e00-30c9b7576c56
02/05/2025 10:37:02:INFO:Received: evaluate message f8a0f7b4-b3fd-4cb0-9e00-30c9b7576c56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:58:INFO:
[92mINFO [0m:      Received: train message 97403980-d7d4-4e45-b73e-7130380cc972
02/05/2025 10:37:58:INFO:Received: train message 97403980-d7d4-4e45-b73e-7130380cc972
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:51:INFO:
[92mINFO [0m:      Received: evaluate message 92b97e61-333a-4d66-bfd3-98429123603d
02/05/2025 10:39:51:INFO:Received: evaluate message 92b97e61-333a-4d66-bfd3-98429123603d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:35:INFO:
[92mINFO [0m:      Received: train message eea874a1-6fe1-4669-9533-98f550801d56
02/05/2025 10:40:35:INFO:Received: train message eea874a1-6fe1-4669-9533-98f550801d56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:56:INFO:
[92mINFO [0m:      Received: evaluate message 5b40bbf9-cfcd-4ef0-b0cd-3a4612b0affd
02/05/2025 10:41:56:INFO:Received: evaluate message 5b40bbf9-cfcd-4ef0-b0cd-3a4612b0affd
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:01:INFO:
[92mINFO [0m:      Received: train message cc8ffc47-f695-4567-a0cd-f37f0e16a3fb
02/05/2025 10:43:01:INFO:Received: train message cc8ffc47-f695-4567-a0cd-f37f0e16a3fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:43:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:37:INFO:
[92mINFO [0m:      Received: evaluate message 13ae5e18-5f8f-4479-a8b6-f35ee88a68e7
02/05/2025 10:44:37:INFO:Received: evaluate message 13ae5e18-5f8f-4479-a8b6-f35ee88a68e7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:44:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:18:INFO:
[92mINFO [0m:      Received: train message 5c7f3d61-e4d3-4a16-a947-6070f3b8547e
02/05/2025 10:45:18:INFO:Received: train message 5c7f3d61-e4d3-4a16-a947-6070f3b8547e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:30:INFO:
[92mINFO [0m:      Received: evaluate message e4644999-f82f-4b44-b401-2601c6cac818
02/05/2025 10:47:30:INFO:Received: evaluate message e4644999-f82f-4b44-b401-2601c6cac818
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:43:INFO:
[92mINFO [0m:      Received: train message fc70fe43-9952-4262-958b-a0871d41c81b
02/05/2025 10:48:43:INFO:Received: train message fc70fe43-9952-4262-958b-a0871d41c81b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:37:INFO:
[92mINFO [0m:      Received: evaluate message a9c6955f-0cb8-4d1a-b220-1ccd6e0c90c2
02/05/2025 10:50:37:INFO:Received: evaluate message a9c6955f-0cb8-4d1a-b220-1ccd6e0c90c2

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:25:INFO:
[92mINFO [0m:      Received: train message 43acad9a-3b9e-458c-bfac-8e6b3345bb5e
02/05/2025 10:51:25:INFO:Received: train message 43acad9a-3b9e-458c-bfac-8e6b3345bb5e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:26:INFO:
[92mINFO [0m:      Received: evaluate message 83d313b3-9f73-446d-a412-c4dbcad39f34
02/05/2025 10:53:26:INFO:Received: evaluate message 83d313b3-9f73-446d-a412-c4dbcad39f34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:48:INFO:
[92mINFO [0m:      Received: train message bd3db75b-1313-4996-9265-afd0b81863b0
02/05/2025 10:54:48:INFO:Received: train message bd3db75b-1313-4996-9265-afd0b81863b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:06:INFO:
[92mINFO [0m:      Received: evaluate message de8f8cab-c1de-4d86-881f-afcf09d22961
02/05/2025 10:57:06:INFO:Received: evaluate message de8f8cab-c1de-4d86-881f-afcf09d22961

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:57:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:25:INFO:
[92mINFO [0m:      Received: train message 9267395c-bdbf-4bac-8410-4dcf7ff1a5d5
02/05/2025 10:58:25:INFO:Received: train message 9267395c-bdbf-4bac-8410-4dcf7ff1a5d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:59:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:07:INFO:
[92mINFO [0m:      Received: evaluate message 43d24497-0dd0-4bcc-927b-f68b8c0b9aa7
02/05/2025 11:01:07:INFO:Received: evaluate message 43d24497-0dd0-4bcc-927b-f68b8c0b9aa7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:01:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:02:15:INFO:
[92mINFO [0m:      Received: train message 54498e06-1b7d-4fb4-8d82-ddc7734f9a7d
02/05/2025 11:02:15:INFO:Received: train message 54498e06-1b7d-4fb4-8d82-ddc7734f9a7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:03:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:05:11:INFO:
[92mINFO [0m:      Received: evaluate message 466e03a4-b2e1-431c-b173-dbda4e27c327
02/05/2025 11:05:11:INFO:Received: evaluate message 466e03a4-b2e1-431c-b173-dbda4e27c327

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:05:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:05:56:INFO:
[92mINFO [0m:      Received: train message 6ad127da-0fb0-47c0-9987-0f77962aff16
02/05/2025 11:05:56:INFO:Received: train message 6ad127da-0fb0-47c0-9987-0f77962aff16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:06:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:56:INFO:
[92mINFO [0m:      Received: evaluate message a22c1010-05a8-4423-9f9c-cc06172153ab
02/05/2025 11:08:56:INFO:Received: evaluate message a22c1010-05a8-4423-9f9c-cc06172153ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:09:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:09:56:INFO:
[92mINFO [0m:      Received: train message e399d31e-37c5-4691-9859-27efd4e62ee4
02/05/2025 11:09:56:INFO:Received: train message e399d31e-37c5-4691-9859-27efd4e62ee4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:10:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:21:INFO:
[92mINFO [0m:      Received: evaluate message bee3f7f4-1e1f-4eac-a442-0b9c805b2333
02/05/2025 11:13:21:INFO:Received: evaluate message bee3f7f4-1e1f-4eac-a442-0b9c805b2333

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:57:INFO:
[92mINFO [0m:      Received: train message 8002a7f2-a263-4c42-9d90-428ad8bef6b2
02/05/2025 11:13:57:INFO:Received: train message 8002a7f2-a263-4c42-9d90-428ad8bef6b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:14:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:16:52:INFO:
[92mINFO [0m:      Received: evaluate message ce40dffe-258c-469f-be5b-817ac61e871a
02/05/2025 11:16:52:INFO:Received: evaluate message ce40dffe-258c-469f-be5b-817ac61e871a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:16:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:37:INFO:
[92mINFO [0m:      Received: train message b0953342-a629-4445-a7b8-f9b37e0416d7
02/05/2025 11:18:37:INFO:Received: train message b0953342-a629-4445-a7b8-f9b37e0416d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:54:INFO:
[92mINFO [0m:      Received: evaluate message a3a9bb30-3c82-46a8-95ef-c380c1b307ed
02/05/2025 11:20:54:INFO:Received: evaluate message a3a9bb30-3c82-46a8-95ef-c380c1b307ed

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:15:INFO:
[92mINFO [0m:      Received: train message 7bad7778-d542-4599-a21a-04a2510c74c5
02/05/2025 11:21:15:INFO:Received: train message 7bad7778-d542-4599-a21a-04a2510c74c5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:44:INFO:
[92mINFO [0m:      Received: evaluate message 1aea1370-6d9c-4517-803e-47d08344a130
02/05/2025 11:23:44:INFO:Received: evaluate message 1aea1370-6d9c-4517-803e-47d08344a130
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:21:INFO:
[92mINFO [0m:      Received: train message 13331f38-5dda-4143-83ca-b222a1048e83
02/05/2025 11:24:21:INFO:Received: train message 13331f38-5dda-4143-83ca-b222a1048e83
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:38:INFO:
[92mINFO [0m:      Received: evaluate message 41fbbcfa-c9d4-42d2-9669-259c9a3c1036
02/05/2025 11:26:38:INFO:Received: evaluate message 41fbbcfa-c9d4-42d2-9669-259c9a3c1036

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:57:INFO:
[92mINFO [0m:      Received: train message 5192c02c-7e91-4a8f-ba36-6f7c622c9569
02/05/2025 11:26:57:INFO:Received: train message 5192c02c-7e91-4a8f-ba36-6f7c622c9569
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:27:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:07:INFO:
[92mINFO [0m:      Received: evaluate message 4fbbd527-adb3-4c2d-b8eb-99fb227106a7
02/05/2025 11:29:07:INFO:Received: evaluate message 4fbbd527-adb3-4c2d-b8eb-99fb227106a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:45:INFO:
[92mINFO [0m:      Received: train message 9d343de2-74e2-465d-9f51-f59675c7d6ea
02/05/2025 11:29:45:INFO:Received: train message 9d343de2-74e2-465d-9f51-f59675c7d6ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:30:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:58:INFO:
[92mINFO [0m:      Received: evaluate message 6a8f44ea-6f3c-40de-9092-32c39f84de74
02/05/2025 11:31:58:INFO:Received: evaluate message 6a8f44ea-6f3c-40de-9092-32c39f84de74

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:32:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:07:INFO:
[92mINFO [0m:      Received: reconnect message 85ff937e-7a73-497b-991f-502dc5e9c9c6
02/05/2025 11:32:07:INFO:Received: reconnect message 85ff937e-7a73-497b-991f-502dc5e9c9c6
02/05/2025 11:32:07:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:32:07:INFO:Disconnect and shut down
