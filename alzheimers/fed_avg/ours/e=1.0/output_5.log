nohup: ignoring input
02/05/2025 10:00:15:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:00:15:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:00:15:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 10:00:15:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778415.243295 1630310 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:00:52:INFO:
[92mINFO [0m:      Received: train message 54cbfa33-5d0d-44b6-8924-223a405fe550
02/05/2025 10:00:52:INFO:Received: train message 54cbfa33-5d0d-44b6-8924-223a405fe550
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:01:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:01:59:INFO:
[92mINFO [0m:      Received: evaluate message 291dbb88-7daa-458f-9f84-25267a151ff6
02/05/2025 10:01:59:INFO:Received: evaluate message 291dbb88-7daa-458f-9f84-25267a151ff6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:02:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:02:31:INFO:
[92mINFO [0m:      Received: train message 85f5628a-a151-4d2e-b92c-569e2b128c32
02/05/2025 10:02:31:INFO:Received: train message 85f5628a-a151-4d2e-b92c-569e2b128c32
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:03:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:00:INFO:
[92mINFO [0m:      Received: evaluate message d8fcb3ec-49ca-4cc9-93ac-dd7480154ea8
02/05/2025 10:04:00:INFO:Received: evaluate message d8fcb3ec-49ca-4cc9-93ac-dd7480154ea8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:04:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:42:INFO:
[92mINFO [0m:      Received: train message 279d40b1-9c39-4175-b1da-21f73ce5fba6
02/05/2025 10:04:42:INFO:Received: train message 279d40b1-9c39-4175-b1da-21f73ce5fba6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:34:INFO:
[92mINFO [0m:      Received: evaluate message 40fd39ef-719d-4181-b02a-f5f504614ac1
02/05/2025 10:06:34:INFO:Received: evaluate message 40fd39ef-719d-4181-b02a-f5f504614ac1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:56:INFO:
[92mINFO [0m:      Received: train message 76398625-8fdc-46a3-8f7a-760f5f51a467
02/05/2025 10:06:56:INFO:Received: train message 76398625-8fdc-46a3-8f7a-760f5f51a467
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:57:INFO:
[92mINFO [0m:      Received: evaluate message 27e582cb-8537-4233-8362-ee370ee28bd1
02/05/2025 10:08:57:INFO:Received: evaluate message 27e582cb-8537-4233-8362-ee370ee28bd1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:27:INFO:
[92mINFO [0m:      Received: train message 511b8716-2129-46b6-8c30-5af1ad4b1ca6
02/05/2025 10:09:27:INFO:Received: train message 511b8716-2129-46b6-8c30-5af1ad4b1ca6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:09:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:17:INFO:
[92mINFO [0m:      Received: evaluate message d6d3f959-afd0-4d1c-b813-07a70349a4e9
02/05/2025 10:11:17:INFO:Received: evaluate message d6d3f959-afd0-4d1c-b813-07a70349a4e9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:47:INFO:
[92mINFO [0m:      Received: train message b1b08027-e156-425d-857d-f0ad32a56808
02/05/2025 10:11:47:INFO:Received: train message b1b08027-e156-425d-857d-f0ad32a56808
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:41:INFO:
[92mINFO [0m:      Received: evaluate message b0941e23-389d-4dbe-bcf8-1d2f7abfda2e
02/05/2025 10:13:41:INFO:Received: evaluate message b0941e23-389d-4dbe-bcf8-1d2f7abfda2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:03:INFO:
[92mINFO [0m:      Received: train message 84f9cf38-13da-4c80-a806-9d1dd512c90f
02/05/2025 10:14:03:INFO:Received: train message 84f9cf38-13da-4c80-a806-9d1dd512c90f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:02:INFO:
[92mINFO [0m:      Received: evaluate message 2dfd9df4-db20-46ab-bf36-b16be446ab4a
02/05/2025 10:16:02:INFO:Received: evaluate message 2dfd9df4-db20-46ab-bf36-b16be446ab4a
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725], 'accuracy': [0.5011727912431587], 'auc': [0.6538837290749867], 'precision': [0.3679360491481115], 'recall': [0.5011727912431587], 'f1': [0.3373775955579418]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843], 'accuracy': [0.5011727912431587, 0.5136825645035183], 'auc': [0.6538837290749867, 0.6940075972249533], 'precision': [0.3679360491481115, 0.4066321576069065], 'recall': [0.5011727912431587, 0.5136825645035183], 'f1': [0.3373775955579418, 0.41327312149778717]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:35:INFO:
[92mINFO [0m:      Received: train message ba23b47f-2465-4895-abed-5aab0031fd82
02/05/2025 10:16:35:INFO:Received: train message ba23b47f-2465-4895-abed-5aab0031fd82
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:53:INFO:
[92mINFO [0m:      Received: evaluate message 605aa212-16f4-4db2-adc5-06547494952d
02/05/2025 10:17:53:INFO:Received: evaluate message 605aa212-16f4-4db2-adc5-06547494952d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:17:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:54:INFO:
[92mINFO [0m:      Received: train message 7cbfd0a0-ec6d-46a2-b40c-ea37ed7999d4
02/05/2025 10:18:54:INFO:Received: train message 7cbfd0a0-ec6d-46a2-b40c-ea37ed7999d4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:20:59:INFO:
[92mINFO [0m:      Received: evaluate message 3415cb1d-83a1-4905-8bd6-f4a42fa9e4d8
02/05/2025 10:20:59:INFO:Received: evaluate message 3415cb1d-83a1-4905-8bd6-f4a42fa9e4d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:21:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:53:INFO:
[92mINFO [0m:      Received: train message 4c742edc-e655-4a56-adb0-fdc5cbb3aac0
02/05/2025 10:21:53:INFO:Received: train message 4c742edc-e655-4a56-adb0-fdc5cbb3aac0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:10:INFO:
[92mINFO [0m:      Received: evaluate message 2ecf863d-180e-49a9-8e38-6ec772da05f0
02/05/2025 10:23:10:INFO:Received: evaluate message 2ecf863d-180e-49a9-8e38-6ec772da05f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:06:INFO:
[92mINFO [0m:      Received: train message 63bbeccd-831e-4f19-8ea7-d4654f6e8f52
02/05/2025 10:24:06:INFO:Received: train message 63bbeccd-831e-4f19-8ea7-d4654f6e8f52
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:24:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:14:INFO:
[92mINFO [0m:      Received: evaluate message b6caccd9-8b8e-46d7-b754-b55ae30717f5
02/05/2025 10:25:14:INFO:Received: evaluate message b6caccd9-8b8e-46d7-b754-b55ae30717f5

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:25:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:09:INFO:
[92mINFO [0m:      Received: train message 371af280-912e-4e2f-ab87-4052984990be
02/05/2025 10:26:09:INFO:Received: train message 371af280-912e-4e2f-ab87-4052984990be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:42:INFO:
[92mINFO [0m:      Received: evaluate message 0041214b-7a07-4b00-88bf-583519d37fa3
02/05/2025 10:27:42:INFO:Received: evaluate message 0041214b-7a07-4b00-88bf-583519d37fa3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:15:INFO:
[92mINFO [0m:      Received: train message f4a7a372-2210-4806-9133-3de4fb1072e5
02/05/2025 10:28:15:INFO:Received: train message f4a7a372-2210-4806-9133-3de4fb1072e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:58:INFO:
[92mINFO [0m:      Received: evaluate message a41ab000-b1c1-432f-8947-4c5f7dc0807a
02/05/2025 10:29:58:INFO:Received: evaluate message a41ab000-b1c1-432f-8947-4c5f7dc0807a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:30:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:25:INFO:
[92mINFO [0m:      Received: train message e8f96120-837a-485c-97cb-7bc3118ef3e6
02/05/2025 10:30:25:INFO:Received: train message e8f96120-837a-485c-97cb-7bc3118ef3e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:10:INFO:
[92mINFO [0m:      Received: evaluate message 6845e09a-4440-4927-aa8e-9ee3f2fe5dfe
02/05/2025 10:32:10:INFO:Received: evaluate message 6845e09a-4440-4927-aa8e-9ee3f2fe5dfe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:10:INFO:
[92mINFO [0m:      Received: train message 9fb708ba-c931-42ab-becd-c0ed314039dd
02/05/2025 10:33:10:INFO:Received: train message 9fb708ba-c931-42ab-becd-c0ed314039dd

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275]}

Step 1b: Recomputing FIM for epoch 15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:41:INFO:
[92mINFO [0m:      Received: evaluate message b9111adb-12c0-4fea-8e22-d705d650eefe
02/05/2025 10:34:41:INFO:Received: evaluate message b9111adb-12c0-4fea-8e22-d705d650eefe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:26:INFO:
[92mINFO [0m:      Received: train message 3499ac56-8929-43d1-8bb8-93967e3b90f9
02/05/2025 10:35:26:INFO:Received: train message 3499ac56-8929-43d1-8bb8-93967e3b90f9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:36:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:51:INFO:
[92mINFO [0m:      Received: evaluate message b132a318-a9bb-43e5-b3d7-8c5ec375c478
02/05/2025 10:36:51:INFO:Received: evaluate message b132a318-a9bb-43e5-b3d7-8c5ec375c478
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:36:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:41:INFO:
[92mINFO [0m:      Received: train message a847cf20-2674-4a0a-a43e-47f7b3353d4d
02/05/2025 10:37:41:INFO:Received: train message a847cf20-2674-4a0a-a43e-47f7b3353d4d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:06:INFO:
[92mINFO [0m:      Received: evaluate message d7d90226-9622-48aa-8506-4aa7230a5941
02/05/2025 10:39:06:INFO:Received: evaluate message d7d90226-9622-48aa-8506-4aa7230a5941
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:44:INFO:
[92mINFO [0m:      Received: train message 783f7048-9a1c-4897-82d0-a86c062dd4eb
02/05/2025 10:39:44:INFO:Received: train message 783f7048-9a1c-4897-82d0-a86c062dd4eb
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:40:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:37:INFO:
[92mINFO [0m:      Received: evaluate message 931e71e5-bc58-4f8e-9550-c12cee8b7972
02/05/2025 10:41:37:INFO:Received: evaluate message 931e71e5-bc58-4f8e-9550-c12cee8b7972
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:41:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:59:INFO:
[92mINFO [0m:      Received: train message e9c96365-2f17-4413-9bb9-8378f41dad0e
02/05/2025 10:41:59:INFO:Received: train message e9c96365-2f17-4413-9bb9-8378f41dad0e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:44:INFO:
[92mINFO [0m:      Received: evaluate message 1196d876-f983-41a3-af19-841d16748505
02/05/2025 10:43:44:INFO:Received: evaluate message 1196d876-f983-41a3-af19-841d16748505
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:37:INFO:
[92mINFO [0m:      Received: train message 3f22e7d6-cc79-4409-8d38-246c7319c97f
02/05/2025 10:44:37:INFO:Received: train message 3f22e7d6-cc79-4409-8d38-246c7319c97f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:45:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:02:INFO:
[92mINFO [0m:      Received: evaluate message 76d97667-590f-4460-a811-fcce9ac93c77
02/05/2025 10:46:02:INFO:Received: evaluate message 76d97667-590f-4460-a811-fcce9ac93c77
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:46:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:07:INFO:
[92mINFO [0m:      Received: train message 62725e3f-0906-44d4-bb1b-711f584af53a
02/05/2025 10:47:07:INFO:Received: train message 62725e3f-0906-44d4-bb1b-711f584af53a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:47:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:36:INFO:
[92mINFO [0m:      Received: evaluate message 4e6948e9-6735-40d3-a2d0-c1399067c7f4
02/05/2025 10:48:36:INFO:Received: evaluate message 4e6948e9-6735-40d3-a2d0-c1399067c7f4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:48:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:21:INFO:
[92mINFO [0m:      Received: train message f8145895-1300-4e90-9a59-1d0e276f7f92
02/05/2025 10:49:21:INFO:Received: train message f8145895-1300-4e90-9a59-1d0e276f7f92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:52:INFO:
[92mINFO [0m:      Received: evaluate message 27a44731-5c2c-4ad0-b560-82309acecc8c
02/05/2025 10:50:52:INFO:Received: evaluate message 27a44731-5c2c-4ad0-b560-82309acecc8c

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:12:INFO:
[92mINFO [0m:      Received: train message 107c956f-387c-47ba-a0eb-7e197a9726f5
02/05/2025 10:52:12:INFO:Received: train message 107c956f-387c-47ba-a0eb-7e197a9726f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:15:INFO:
[92mINFO [0m:      Received: evaluate message 810e590a-ed13-428a-b706-f62a780c3ecf
02/05/2025 10:54:15:INFO:Received: evaluate message 810e590a-ed13-428a-b706-f62a780c3ecf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:54:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:55:20:INFO:
[92mINFO [0m:      Received: train message 80a65b6d-f71f-4b71-8962-043b38f8078d
02/05/2025 10:55:20:INFO:Received: train message 80a65b6d-f71f-4b71-8962-043b38f8078d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:17:INFO:
[92mINFO [0m:      Received: evaluate message 2cf643ca-2b92-4f39-b9ac-a69cd66b22e3
02/05/2025 10:57:17:INFO:Received: evaluate message 2cf643ca-2b92-4f39-b9ac-a69cd66b22e3

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:57:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:27:INFO:
[92mINFO [0m:      Received: train message 91c50b2c-5c85-43ee-ba36-9c2573761b72
02/05/2025 10:58:27:INFO:Received: train message 91c50b2c-5c85-43ee-ba36-9c2573761b72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:59:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:19:INFO:
[92mINFO [0m:      Received: evaluate message 1abbf42f-11fc-452d-8bc9-026bb02c5710
02/05/2025 11:00:19:INFO:Received: evaluate message 1abbf42f-11fc-452d-8bc9-026bb02c5710
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:00:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:30:INFO:
[92mINFO [0m:      Received: train message 29824ead-47da-42b5-bb32-8455191e803b
02/05/2025 11:01:30:INFO:Received: train message 29824ead-47da-42b5-bb32-8455191e803b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:02:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:42:INFO:
[92mINFO [0m:      Received: evaluate message b7929a76-0a25-4a7b-bb64-a28a30e55114
02/05/2025 11:03:42:INFO:Received: evaluate message b7929a76-0a25-4a7b-bb64-a28a30e55114

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:59:INFO:
[92mINFO [0m:      Received: train message ba914eaf-0e4c-4644-bc84-1788120f338f
02/05/2025 11:04:59:INFO:Received: train message ba914eaf-0e4c-4644-bc84-1788120f338f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:58:INFO:
[92mINFO [0m:      Received: evaluate message e7897bc5-8cc9-4753-8ad1-8134508ab718
02/05/2025 11:06:58:INFO:Received: evaluate message e7897bc5-8cc9-4753-8ad1-8134508ab718
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:07:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:29:INFO:
[92mINFO [0m:      Received: train message 0f964a87-21d4-4039-a642-64362eccb11d
02/05/2025 11:08:29:INFO:Received: train message 0f964a87-21d4-4039-a642-64362eccb11d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:09:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:06:INFO:
[92mINFO [0m:      Received: evaluate message cef8a131-294a-49ac-aa3a-7a011d3308a8
02/05/2025 11:10:06:INFO:Received: evaluate message cef8a131-294a-49ac-aa3a-7a011d3308a8

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:10:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:11:48:INFO:
[92mINFO [0m:      Received: train message 648a563e-080b-4bff-acb2-353b600d91fe
02/05/2025 11:11:48:INFO:Received: train message 648a563e-080b-4bff-acb2-353b600d91fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:12:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:53:INFO:
[92mINFO [0m:      Received: evaluate message 1f48085f-f32b-425c-8c63-4a0007d4a922
02/05/2025 11:13:53:INFO:Received: evaluate message 1f48085f-f32b-425c-8c63-4a0007d4a922
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:15:54:INFO:
[92mINFO [0m:      Received: train message 465b71d7-dd70-498b-9f54-d541a08c1f31
02/05/2025 11:15:54:INFO:Received: train message 465b71d7-dd70-498b-9f54-d541a08c1f31
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:16:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:11:INFO:
[92mINFO [0m:      Received: evaluate message 95a74600-2b87-427c-b5e6-3495853039b3
02/05/2025 11:18:11:INFO:Received: evaluate message 95a74600-2b87-427c-b5e6-3495853039b3

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662, 1.101836214641559], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966, 0.775166807370288], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525, 0.6013600221776082], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151, 0.5015874135851307]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:18:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:31:INFO:
[92mINFO [0m:      Received: reconnect message f439b7cb-46e7-4abe-a9d3-6a2a40e0dab6
02/05/2025 11:18:31:INFO:Received: reconnect message f439b7cb-46e7-4abe-a9d3-6a2a40e0dab6
02/05/2025 11:18:32:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:18:32:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662, 1.101836214641559, 1.082568925558542], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966, 0.775166807370288, 0.7755612594047985], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525, 0.6013600221776082, 0.6104218623283054], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151, 0.5015874135851307, 0.5128626613784149]}



Final client history:
{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662, 1.101836214641559, 1.082568925558542], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966, 0.775166807370288, 0.7755612594047985], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525, 0.6013600221776082, 0.6104218623283054], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151, 0.5015874135851307, 0.5128626613784149]}

