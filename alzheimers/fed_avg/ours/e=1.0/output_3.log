nohup: ignoring input
02/05/2025 10:00:12:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:00:12:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:00:12:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 10:00:12:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778412.589286 1627534 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:00:50:INFO:
[92mINFO [0m:      Received: train message f6944485-1b61-4ad2-a74f-5973ab80256b
02/05/2025 10:00:50:INFO:Received: train message f6944485-1b61-4ad2-a74f-5973ab80256b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:01:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:02:01:INFO:
[92mINFO [0m:      Received: evaluate message b8d4caba-b388-44de-b3e8-c399ecb4eeb9
02/05/2025 10:02:01:INFO:Received: evaluate message b8d4caba-b388-44de-b3e8-c399ecb4eeb9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:02:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:02:44:INFO:
[92mINFO [0m:      Received: train message 1283aca3-74ee-4d79-8b6f-01a215f23b94
02/05/2025 10:02:44:INFO:Received: train message 1283aca3-74ee-4d79-8b6f-01a215f23b94
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:03:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:15:INFO:
[92mINFO [0m:      Received: evaluate message 07302207-9428-40b7-a7f1-851e4b506eee
02/05/2025 10:04:15:INFO:Received: evaluate message 07302207-9428-40b7-a7f1-851e4b506eee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:04:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:05:02:INFO:
[92mINFO [0m:      Received: train message d88bb0b6-0a8f-4337-97be-bbd39ebdb52c
02/05/2025 10:05:02:INFO:Received: train message d88bb0b6-0a8f-4337-97be-bbd39ebdb52c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:29:INFO:
[92mINFO [0m:      Received: evaluate message d4bcc0e4-efa1-42c1-9c18-96c8ef0818c4
02/05/2025 10:06:29:INFO:Received: evaluate message d4bcc0e4-efa1-42c1-9c18-96c8ef0818c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:16:INFO:
[92mINFO [0m:      Received: train message a0572256-ba4f-4a54-bbaa-a7919d80d7db
02/05/2025 10:07:16:INFO:Received: train message a0572256-ba4f-4a54-bbaa-a7919d80d7db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:46:INFO:
[92mINFO [0m:      Received: evaluate message e691f16d-151e-43b1-9386-acb21efca8d5
02/05/2025 10:08:46:INFO:Received: evaluate message e691f16d-151e-43b1-9386-acb21efca8d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:08:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:19:INFO:
[92mINFO [0m:      Received: train message 473fd281-a4e6-499d-92f4-1e0c3aba4d20
02/05/2025 10:09:19:INFO:Received: train message 473fd281-a4e6-499d-92f4-1e0c3aba4d20
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:09:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:23:INFO:
[92mINFO [0m:      Received: evaluate message 46a0916d-b76c-46d7-a023-91097822b015
02/05/2025 10:11:23:INFO:Received: evaluate message 46a0916d-b76c-46d7-a023-91097822b015
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:12:11:INFO:
[92mINFO [0m:      Received: train message 37a2e1fc-0bdd-42cb-9fd2-59d3499b851b
02/05/2025 10:12:11:INFO:Received: train message 37a2e1fc-0bdd-42cb-9fd2-59d3499b851b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:41:INFO:
[92mINFO [0m:      Received: evaluate message 126224b0-637b-4907-b866-3e3d9edbc56d
02/05/2025 10:13:41:INFO:Received: evaluate message 126224b0-637b-4907-b866-3e3d9edbc56d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:29:INFO:
[92mINFO [0m:      Received: train message 63866cef-04a6-47f1-9ae5-8dd8eb6b72c0
02/05/2025 10:14:29:INFO:Received: train message 63866cef-04a6-47f1-9ae5-8dd8eb6b72c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:58:INFO:
[92mINFO [0m:      Received: evaluate message ff97ef03-2881-46fe-a51f-f46a3b5928d7
02/05/2025 10:15:58:INFO:Received: evaluate message ff97ef03-2881-46fe-a51f-f46a3b5928d7
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725], 'accuracy': [0.5011727912431587], 'auc': [0.6538837290749867], 'precision': [0.3679360491481115], 'recall': [0.5011727912431587], 'f1': [0.3373775955579418]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843], 'accuracy': [0.5011727912431587, 0.5136825645035183], 'auc': [0.6538837290749867, 0.6940075972249533], 'precision': [0.3679360491481115, 0.4066321576069065], 'recall': [0.5011727912431587, 0.5136825645035183], 'f1': [0.3373775955579418, 0.41327312149778717]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:44:INFO:
[92mINFO [0m:      Received: train message 1f436b5a-b61a-4530-8eb3-301856092618
02/05/2025 10:16:44:INFO:Received: train message 1f436b5a-b61a-4530-8eb3-301856092618
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:10:INFO:
[92mINFO [0m:      Received: evaluate message 0e848f86-39b0-4d75-9ac0-66590f2660d8
02/05/2025 10:18:10:INFO:Received: evaluate message 0e848f86-39b0-4d75-9ac0-66590f2660d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:50:INFO:
[92mINFO [0m:      Received: train message 8d2ccd36-a170-4ce1-98ed-dd135b58ab21
02/05/2025 10:18:50:INFO:Received: train message 8d2ccd36-a170-4ce1-98ed-dd135b58ab21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:13:INFO:
[92mINFO [0m:      Received: evaluate message 985cab37-b776-4d3c-8e20-acebd2bcadf0
02/05/2025 10:21:13:INFO:Received: evaluate message 985cab37-b776-4d3c-8e20-acebd2bcadf0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:21:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:54:INFO:
[92mINFO [0m:      Received: train message 89d3c3be-f9b4-4d1e-8980-4476552ce3a6
02/05/2025 10:21:54:INFO:Received: train message 89d3c3be-f9b4-4d1e-8980-4476552ce3a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:22:INFO:
[92mINFO [0m:      Received: evaluate message 40c5526a-0f9b-4698-aac9-1924055b59af
02/05/2025 10:23:22:INFO:Received: evaluate message 40c5526a-0f9b-4698-aac9-1924055b59af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:42:INFO:
[92mINFO [0m:      Received: train message 4f2d8675-300e-450b-bfa9-105a4e9b9e6e
02/05/2025 10:23:42:INFO:Received: train message 4f2d8675-300e-450b-bfa9-105a4e9b9e6e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:24:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:11:INFO:
[92mINFO [0m:      Received: evaluate message 8d4474fa-1dcc-4a0a-b8c3-716c4790312e
02/05/2025 10:25:11:INFO:Received: evaluate message 8d4474fa-1dcc-4a0a-b8c3-716c4790312e

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:25:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:59:INFO:
[92mINFO [0m:      Received: train message 8d098944-97f8-4e21-b4d3-f38693be49fe
02/05/2025 10:25:59:INFO:Received: train message 8d098944-97f8-4e21-b4d3-f38693be49fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:32:INFO:
[92mINFO [0m:      Received: evaluate message 582bd2b1-5484-48de-b703-d89ce10ab7f8
02/05/2025 10:27:32:INFO:Received: evaluate message 582bd2b1-5484-48de-b703-d89ce10ab7f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:33:INFO:
[92mINFO [0m:      Received: train message 6a03ac49-2069-4a3c-91f5-dc249538f8ef
02/05/2025 10:28:33:INFO:Received: train message 6a03ac49-2069-4a3c-91f5-dc249538f8ef
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:29:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:58:INFO:
[92mINFO [0m:      Received: evaluate message b54ac0a0-05ef-4008-b029-3db2546a0fff
02/05/2025 10:29:58:INFO:Received: evaluate message b54ac0a0-05ef-4008-b029-3db2546a0fff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:30:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:49:INFO:
[92mINFO [0m:      Received: train message d35b1d40-6304-4a28-95f8-acbcfb63d9c2
02/05/2025 10:30:49:INFO:Received: train message d35b1d40-6304-4a28-95f8-acbcfb63d9c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:31:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:21:INFO:
[92mINFO [0m:      Received: evaluate message dde696ea-693b-448c-a467-3a9ea46a84a6
02/05/2025 10:32:21:INFO:Received: evaluate message dde696ea-693b-448c-a467-3a9ea46a84a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:13:INFO:
[92mINFO [0m:      Received: train message 287062d9-4c86-43ba-8243-b49bbfb08d13
02/05/2025 10:33:13:INFO:Received: train message 287062d9-4c86-43ba-8243-b49bbfb08d13

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275]}

Step 1b: Recomputing FIM for epoch 15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:41:INFO:
[92mINFO [0m:      Received: evaluate message 5ed53b10-030d-4f7c-b3b8-90b93061a420
02/05/2025 10:34:41:INFO:Received: evaluate message 5ed53b10-030d-4f7c-b3b8-90b93061a420
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:16:INFO:
[92mINFO [0m:      Received: train message 3b7c9517-d097-45e3-8ba8-7dc264a5b138
02/05/2025 10:35:16:INFO:Received: train message 3b7c9517-d097-45e3-8ba8-7dc264a5b138
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:35:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:32:INFO:
[92mINFO [0m:      Received: evaluate message c20c55e8-6400-444c-b411-067ad6d3966c
02/05/2025 10:36:32:INFO:Received: evaluate message c20c55e8-6400-444c-b411-067ad6d3966c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:36:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:31:INFO:
[92mINFO [0m:      Received: train message d5060d05-ff9b-416f-a83e-5abc7259f788
02/05/2025 10:37:31:INFO:Received: train message d5060d05-ff9b-416f-a83e-5abc7259f788
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:37:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:20:INFO:
[92mINFO [0m:      Received: evaluate message ea5d373c-850f-4407-b978-7424b8b175ec
02/05/2025 10:39:20:INFO:Received: evaluate message ea5d373c-850f-4407-b978-7424b8b175ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:02:INFO:
[92mINFO [0m:      Received: train message f3ac0d9f-7e82-4328-9aca-186d43d5a604
02/05/2025 10:40:02:INFO:Received: train message f3ac0d9f-7e82-4328-9aca-186d43d5a604
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:40:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:28:INFO:
[92mINFO [0m:      Received: evaluate message 7573cc38-8515-4291-aaeb-f652caff1883
02/05/2025 10:41:28:INFO:Received: evaluate message 7573cc38-8515-4291-aaeb-f652caff1883
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:41:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:23:INFO:
[92mINFO [0m:      Received: train message db3b2468-677a-425c-8c09-a921df1ac36c
02/05/2025 10:42:23:INFO:Received: train message db3b2468-677a-425c-8c09-a921df1ac36c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:48:INFO:
[92mINFO [0m:      Received: evaluate message b5101a3b-b23b-4184-adf2-fc0ca1ad4035
02/05/2025 10:43:48:INFO:Received: evaluate message b5101a3b-b23b-4184-adf2-fc0ca1ad4035
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:30:INFO:
[92mINFO [0m:      Received: train message 52045b5c-a672-4dfb-a6b7-43eb768887ff
02/05/2025 10:44:30:INFO:Received: train message 52045b5c-a672-4dfb-a6b7-43eb768887ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:44:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:15:INFO:
[92mINFO [0m:      Received: evaluate message f6d7bfba-2819-4df3-bfb8-f6d57edb3f41
02/05/2025 10:46:15:INFO:Received: evaluate message f6d7bfba-2819-4df3-bfb8-f6d57edb3f41
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:46:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:04:INFO:
[92mINFO [0m:      Received: train message d7da928b-d19f-40dc-9362-9ff378113431
02/05/2025 10:47:04:INFO:Received: train message d7da928b-d19f-40dc-9362-9ff378113431
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:47:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:43:INFO:
[92mINFO [0m:      Received: evaluate message 4e2ca6fc-5230-4cf4-a730-b8fbd4e3b7c8
02/05/2025 10:48:43:INFO:Received: evaluate message 4e2ca6fc-5230-4cf4-a730-b8fbd4e3b7c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:48:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:34:INFO:
[92mINFO [0m:      Received: train message 9d9a2c99-e7dc-45de-87be-0e6a0d1ab36a
02/05/2025 10:49:34:INFO:Received: train message 9d9a2c99-e7dc-45de-87be-0e6a0d1ab36a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:09:INFO:
[92mINFO [0m:      Received: evaluate message 4c73d028-8d34-4056-85ae-ce29e9dd55ac
02/05/2025 10:51:09:INFO:Received: evaluate message 4c73d028-8d34-4056-85ae-ce29e9dd55ac

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:51:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:25:INFO:
[92mINFO [0m:      Received: train message e9c90de3-2d2c-43d8-aa59-5b488e0562c2
02/05/2025 10:52:25:INFO:Received: train message e9c90de3-2d2c-43d8-aa59-5b488e0562c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:10:INFO:
[92mINFO [0m:      Received: evaluate message dce6b8a6-1544-4c24-84ba-a75c0c002dd5
02/05/2025 10:54:10:INFO:Received: evaluate message dce6b8a6-1544-4c24-84ba-a75c0c002dd5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:54:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:58:INFO:
[92mINFO [0m:      Received: train message ce76e3f4-61e3-4128-9372-bd747d63702f
02/05/2025 10:54:58:INFO:Received: train message ce76e3f4-61e3-4128-9372-bd747d63702f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:57:INFO:
[92mINFO [0m:      Received: evaluate message 64a6b5aa-8d80-434b-a244-1365b52999e1
02/05/2025 10:56:57:INFO:Received: evaluate message 64a6b5aa-8d80-434b-a244-1365b52999e1

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:57:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:09:INFO:
[92mINFO [0m:      Received: train message 6b4afb07-cc56-4d19-9ea7-c4f2acd4043b
02/05/2025 10:58:09:INFO:Received: train message 6b4afb07-cc56-4d19-9ea7-c4f2acd4043b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:58:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:12:INFO:
[92mINFO [0m:      Received: evaluate message 19483b6f-73c0-48ef-bdbb-66864d2dec05
02/05/2025 11:00:12:INFO:Received: evaluate message 19483b6f-73c0-48ef-bdbb-66864d2dec05
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:00:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:22:INFO:
[92mINFO [0m:      Received: train message a7730414-9448-4353-aa24-6813c5f542e5
02/05/2025 11:01:22:INFO:Received: train message a7730414-9448-4353-aa24-6813c5f542e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:01:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:18:INFO:
[92mINFO [0m:      Received: evaluate message 852c9c37-3fb7-4e57-84cb-6f0645635b34
02/05/2025 11:03:18:INFO:Received: evaluate message 852c9c37-3fb7-4e57-84cb-6f0645635b34

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:50:INFO:
[92mINFO [0m:      Received: train message 9bb9285e-45a9-4070-bff6-6c0b7720a2ad
02/05/2025 11:04:50:INFO:Received: train message 9bb9285e-45a9-4070-bff6-6c0b7720a2ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:50:INFO:
[92mINFO [0m:      Received: evaluate message 183e037e-1dfe-4682-821e-91a3c66532d5
02/05/2025 11:06:50:INFO:Received: evaluate message 183e037e-1dfe-4682-821e-91a3c66532d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:06:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:07:44:INFO:
[92mINFO [0m:      Received: train message 5a6e7d58-a0e0-477c-9a6c-9ac49a3aba37
02/05/2025 11:07:44:INFO:Received: train message 5a6e7d58-a0e0-477c-9a6c-9ac49a3aba37
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:08:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:20:INFO:
[92mINFO [0m:      Received: evaluate message 1df121bb-4d9b-461c-8d4c-72bf7a96e992
02/05/2025 11:10:20:INFO:Received: evaluate message 1df121bb-4d9b-461c-8d4c-72bf7a96e992

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:10:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:11:56:INFO:
[92mINFO [0m:      Received: train message c96edde9-56cd-4ba1-ab94-2af34f7b3fcb
02/05/2025 11:11:56:INFO:Received: train message c96edde9-56cd-4ba1-ab94-2af34f7b3fcb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:12:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:04:INFO:
[92mINFO [0m:      Received: evaluate message 40769490-62a5-41aa-9003-3f8d9a7dbf00
02/05/2025 11:14:04:INFO:Received: evaluate message 40769490-62a5-41aa-9003-3f8d9a7dbf00
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:14:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:15:48:INFO:
[92mINFO [0m:      Received: train message 2a1c9656-0f17-4b0c-b51b-f70a0d10055b
02/05/2025 11:15:48:INFO:Received: train message 2a1c9656-0f17-4b0c-b51b-f70a0d10055b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:16:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:42:INFO:
[92mINFO [0m:      Received: evaluate message a514a1b7-2b30-49c7-bb8e-f681206375a6
02/05/2025 11:17:42:INFO:Received: evaluate message a514a1b7-2b30-49c7-bb8e-f681206375a6

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662, 1.101836214641559], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966, 0.775166807370288], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525, 0.6013600221776082], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151, 0.5015874135851307]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:31:INFO:
[92mINFO [0m:      Received: reconnect message 7cc245aa-7af7-4c91-a3ff-66a6effedf19
02/05/2025 11:18:31:INFO:Received: reconnect message 7cc245aa-7af7-4c91-a3ff-66a6effedf19
02/05/2025 11:18:32:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:18:32:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662, 1.101836214641559, 1.082568925558542], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966, 0.775166807370288, 0.7755612594047985], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525, 0.6013600221776082, 0.6104218623283054], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151, 0.5015874135851307, 0.5128626613784149]}



Final client history:
{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662, 1.101836214641559, 1.082568925558542], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966, 0.775166807370288, 0.7755612594047985], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525, 0.6013600221776082, 0.6104218623283054], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946, 0.565285379202502], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151, 0.5015874135851307, 0.5128626613784149]}

