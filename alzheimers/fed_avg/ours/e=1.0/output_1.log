nohup: ignoring input
02/05/2025 10:00:18:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:00:18:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:00:18:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778418.801958 1634060 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:00:30:INFO:
[92mINFO [0m:      Received: train message 9dc4c420-1c2c-4c79-8968-0df488fb9198
02/05/2025 10:00:30:INFO:Received: train message 9dc4c420-1c2c-4c79-8968-0df488fb9198
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:01:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:02:03:INFO:
[92mINFO [0m:      Received: evaluate message 1fc3e7ec-4914-4334-8272-aaa06e48902e
02/05/2025 10:02:03:INFO:Received: evaluate message 1fc3e7ec-4914-4334-8272-aaa06e48902e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:02:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:02:27:INFO:
[92mINFO [0m:      Received: train message b3e28468-8ed9-4eab-ae7a-b7ea85fca928
02/05/2025 10:02:27:INFO:Received: train message b3e28468-8ed9-4eab-ae7a-b7ea85fca928
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:03:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:08:INFO:
[92mINFO [0m:      Received: evaluate message 0eb1ef34-8afe-496a-b472-a0e605c660fe
02/05/2025 10:04:08:INFO:Received: evaluate message 0eb1ef34-8afe-496a-b472-a0e605c660fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:04:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:05:01:INFO:
[92mINFO [0m:      Received: train message efece671-3385-424b-9b2a-1e10842b0990
02/05/2025 10:05:01:INFO:Received: train message efece671-3385-424b-9b2a-1e10842b0990
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:26:INFO:
[92mINFO [0m:      Received: evaluate message b3b39771-9216-4f0c-8001-83ebd10f2ddf
02/05/2025 10:06:26:INFO:Received: evaluate message b3b39771-9216-4f0c-8001-83ebd10f2ddf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:21:INFO:
[92mINFO [0m:      Received: train message 2924ebbc-3bb8-4199-81e8-c2b6241751d0
02/05/2025 10:07:21:INFO:Received: train message 2924ebbc-3bb8-4199-81e8-c2b6241751d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:08:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:00:INFO:
[92mINFO [0m:      Received: evaluate message ef83c269-39af-437f-afe0-1354ca09db0c
02/05/2025 10:09:00:INFO:Received: evaluate message ef83c269-39af-437f-afe0-1354ca09db0c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:42:INFO:
[92mINFO [0m:      Received: train message d8b0afcd-f2e0-4076-b0ba-0322e44e6855
02/05/2025 10:09:42:INFO:Received: train message d8b0afcd-f2e0-4076-b0ba-0322e44e6855
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:22:INFO:
[92mINFO [0m:      Received: evaluate message bc9d742f-13cc-4357-a49c-646d426e208b
02/05/2025 10:11:22:INFO:Received: evaluate message bc9d742f-13cc-4357-a49c-646d426e208b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:51:INFO:
[92mINFO [0m:      Received: train message 843258ef-6738-4f3a-8c00-1d3032325474
02/05/2025 10:11:51:INFO:Received: train message 843258ef-6738-4f3a-8c00-1d3032325474
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:37:INFO:
[92mINFO [0m:      Received: evaluate message 09116520-f73e-449d-9406-49412804f9dd
02/05/2025 10:13:37:INFO:Received: evaluate message 09116520-f73e-449d-9406-49412804f9dd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:19:INFO:
[92mINFO [0m:      Received: train message 4f38393b-593b-48e4-95fc-12142b413095
02/05/2025 10:14:19:INFO:Received: train message 4f38393b-593b-48e4-95fc-12142b413095
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:15:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:58:INFO:
[92mINFO [0m:      Received: evaluate message 67cd3d95-f0da-4b4e-9ebc-41df052d22c5
02/05/2025 10:15:58:INFO:Received: evaluate message 67cd3d95-f0da-4b4e-9ebc-41df052d22c5
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725], 'accuracy': [0.5011727912431587], 'auc': [0.6538837290749867], 'precision': [0.3679360491481115], 'recall': [0.5011727912431587], 'f1': [0.3373775955579418]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843], 'accuracy': [0.5011727912431587, 0.5136825645035183], 'auc': [0.6538837290749867, 0.6940075972249533], 'precision': [0.3679360491481115, 0.4066321576069065], 'recall': [0.5011727912431587, 0.5136825645035183], 'f1': [0.3373775955579418, 0.41327312149778717]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:37:INFO:
[92mINFO [0m:      Received: train message 0e0113f3-aac1-4ae4-974d-2fde4aad023a
02/05/2025 10:16:37:INFO:Received: train message 0e0113f3-aac1-4ae4-974d-2fde4aad023a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:47:INFO:
[92mINFO [0m:      Received: evaluate message 9d683134-24eb-4e68-ac43-05477e186a07
02/05/2025 10:17:47:INFO:Received: evaluate message 9d683134-24eb-4e68-ac43-05477e186a07
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:17:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:37:INFO:
[92mINFO [0m:      Received: train message fe48e1b5-f46b-46c0-86a6-526c9c843415
02/05/2025 10:18:37:INFO:Received: train message fe48e1b5-f46b-46c0-86a6-526c9c843415
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:03:INFO:
[92mINFO [0m:      Received: evaluate message fc9a25c9-751f-4070-90f6-b52016ef1e56
02/05/2025 10:21:03:INFO:Received: evaluate message fc9a25c9-751f-4070-90f6-b52016ef1e56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:21:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:41:INFO:
[92mINFO [0m:      Received: train message b45d4f2f-30c5-4d5f-8646-959dd35cc416
02/05/2025 10:21:41:INFO:Received: train message b45d4f2f-30c5-4d5f-8646-959dd35cc416
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:58:INFO:
[92mINFO [0m:      Received: evaluate message 8154b0e6-fca7-45ff-a5ec-890244cdf4fa
02/05/2025 10:22:58:INFO:Received: evaluate message 8154b0e6-fca7-45ff-a5ec-890244cdf4fa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:01:INFO:
[92mINFO [0m:      Received: train message b1014ec6-e0a6-4e5c-94a3-064b235d58de
02/05/2025 10:24:01:INFO:Received: train message b1014ec6-e0a6-4e5c-94a3-064b235d58de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:24:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:30:INFO:
[92mINFO [0m:      Received: evaluate message 305d7db5-de3d-4080-bca2-129685117787
02/05/2025 10:25:30:INFO:Received: evaluate message 305d7db5-de3d-4080-bca2-129685117787

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:25:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:16:INFO:
[92mINFO [0m:      Received: train message 3a80d490-0d88-454c-9b40-66623449f345
02/05/2025 10:26:16:INFO:Received: train message 3a80d490-0d88-454c-9b40-66623449f345
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:44:INFO:
[92mINFO [0m:      Received: evaluate message f33d22fc-bc9d-4017-83e0-0e12e287f2d7
02/05/2025 10:27:44:INFO:Received: evaluate message f33d22fc-bc9d-4017-83e0-0e12e287f2d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:13:INFO:
[92mINFO [0m:      Received: train message 8d660311-fa01-48da-8363-8932414c48c1
02/05/2025 10:28:13:INFO:Received: train message 8d660311-fa01-48da-8363-8932414c48c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:29:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:55:INFO:
[92mINFO [0m:      Received: evaluate message 8c666fd6-4574-4abf-964d-2baab3ca8e3f
02/05/2025 10:29:55:INFO:Received: evaluate message 8c666fd6-4574-4abf-964d-2baab3ca8e3f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:51:INFO:
[92mINFO [0m:      Received: train message 2628f4d4-6796-4acc-9815-8bb25fb64030
02/05/2025 10:30:51:INFO:Received: train message 2628f4d4-6796-4acc-9815-8bb25fb64030
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:31:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:26:INFO:
[92mINFO [0m:      Received: evaluate message 346031a7-7660-4b10-bb72-690fedbc57ff
02/05/2025 10:32:26:INFO:Received: evaluate message 346031a7-7660-4b10-bb72-690fedbc57ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:00:INFO:
[92mINFO [0m:      Received: train message d3016b0e-d90b-4a50-a6e6-97167839cd8c
02/05/2025 10:33:00:INFO:Received: train message d3016b0e-d90b-4a50-a6e6-97167839cd8c

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275]}

Step 1b: Recomputing FIM for epoch 15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:34:INFO:
[92mINFO [0m:      Received: evaluate message ea0125fc-4c8a-4a6b-8af8-0912638627d0
02/05/2025 10:34:34:INFO:Received: evaluate message ea0125fc-4c8a-4a6b-8af8-0912638627d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:07:INFO:
[92mINFO [0m:      Received: train message 5e7da90d-202f-4ad0-a650-68ae54367660
02/05/2025 10:35:07:INFO:Received: train message 5e7da90d-202f-4ad0-a650-68ae54367660
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:35:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:54:INFO:
[92mINFO [0m:      Received: evaluate message bb7bf650-0e50-4688-8527-de7e8458475b
02/05/2025 10:36:54:INFO:Received: evaluate message bb7bf650-0e50-4688-8527-de7e8458475b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:44:INFO:
[92mINFO [0m:      Received: train message 9b4b3a4d-460b-4bd1-93bd-91648b9f1bd6
02/05/2025 10:37:44:INFO:Received: train message 9b4b3a4d-460b-4bd1-93bd-91648b9f1bd6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:04:INFO:
[92mINFO [0m:      Received: evaluate message 7a43a27d-3e62-4759-b5ca-dbeedd76c7f6
02/05/2025 10:39:04:INFO:Received: evaluate message 7a43a27d-3e62-4759-b5ca-dbeedd76c7f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:04:INFO:
[92mINFO [0m:      Received: train message ed65dbe4-fb66-40a9-9f57-bb7448d50850
02/05/2025 10:40:04:INFO:Received: train message ed65dbe4-fb66-40a9-9f57-bb7448d50850
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:40:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:20:INFO:
[92mINFO [0m:      Received: evaluate message 42856e59-d2ee-415d-8028-8b0c20ee02d2
02/05/2025 10:41:20:INFO:Received: evaluate message 42856e59-d2ee-415d-8028-8b0c20ee02d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:41:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:17:INFO:
[92mINFO [0m:      Received: train message c90df161-cc9d-424b-931e-2d1811316b5e
02/05/2025 10:42:17:INFO:Received: train message c90df161-cc9d-424b-931e-2d1811316b5e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:50:INFO:
[92mINFO [0m:      Received: evaluate message 3e451149-f241-43fb-a8c8-71dab3975c84
02/05/2025 10:43:50:INFO:Received: evaluate message 3e451149-f241-43fb-a8c8-71dab3975c84
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:38:INFO:
[92mINFO [0m:      Received: train message 1edbaeba-0afd-47ab-b9b5-4dc74545edf1
02/05/2025 10:44:39:INFO:Received: train message 1edbaeba-0afd-47ab-b9b5-4dc74545edf1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:45:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:58:INFO:
[92mINFO [0m:      Received: evaluate message 2ecbb0fd-58d0-4cdf-984f-8133b2ac09a2
02/05/2025 10:45:58:INFO:Received: evaluate message 2ecbb0fd-58d0-4cdf-984f-8133b2ac09a2
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:46:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:56:INFO:
[92mINFO [0m:      Received: train message 9c726ca5-4318-4667-9dc9-85fd61809383
02/05/2025 10:46:56:INFO:Received: train message 9c726ca5-4318-4667-9dc9-85fd61809383
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:47:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:49:INFO:
[92mINFO [0m:      Received: evaluate message cb05de16-f416-429e-bd39-ba72d3f5a2ed
02/05/2025 10:48:50:INFO:Received: evaluate message cb05de16-f416-429e-bd39-ba72d3f5a2ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:48:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:40:INFO:
[92mINFO [0m:      Received: train message 7011a53c-0c8f-483d-aa87-f232f73f2543
02/05/2025 10:49:40:INFO:Received: train message 7011a53c-0c8f-483d-aa87-f232f73f2543
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:50:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:08:INFO:
[92mINFO [0m:      Received: evaluate message 4e714242-951d-4142-9a4b-766d1e7b929c
02/05/2025 10:51:08:INFO:Received: evaluate message 4e714242-951d-4142-9a4b-766d1e7b929c

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:51:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:01:INFO:
[92mINFO [0m:      Received: train message 15608b63-6729-4686-a923-90d21cd03e45
02/05/2025 10:52:01:INFO:Received: train message 15608b63-6729-4686-a923-90d21cd03e45
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:37:INFO:
[92mINFO [0m:      Received: evaluate message cabb9e8c-6c62-4d59-b555-b0b65438202c
02/05/2025 10:53:37:INFO:Received: evaluate message cabb9e8c-6c62-4d59-b555-b0b65438202c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:57:INFO:
[92mINFO [0m:      Received: train message 9b22e6a7-266f-4adc-a411-93c60ff6448d
02/05/2025 10:54:57:INFO:Received: train message 9b22e6a7-266f-4adc-a411-93c60ff6448d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:03:INFO:
[92mINFO [0m:      Received: evaluate message eacf515a-e91b-4dba-8884-9472fe8a3714
02/05/2025 10:57:03:INFO:Received: evaluate message eacf515a-e91b-4dba-8884-9472fe8a3714

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:57:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:00:INFO:
[92mINFO [0m:      Received: train message 9c745c50-71af-4fc4-a9d3-b1a1f0b6c855
02/05/2025 10:58:00:INFO:Received: train message 9c745c50-71af-4fc4-a9d3-b1a1f0b6c855
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:59:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:37:INFO:
[92mINFO [0m:      Received: evaluate message 70c26b09-b6c7-4428-af79-eb42c48d8d69
02/05/2025 11:00:37:INFO:Received: evaluate message 70c26b09-b6c7-4428-af79-eb42c48d8d69
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:00:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:22:INFO:
[92mINFO [0m:      Received: train message 86bce566-eed5-4972-9408-0604c5cc7b77
02/05/2025 11:01:22:INFO:Received: train message 86bce566-eed5-4972-9408-0604c5cc7b77
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:02:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:51:INFO:
[92mINFO [0m:      Received: evaluate message e2cacfa6-96ab-47d2-a65f-8bdc852ce3e3
02/05/2025 11:03:51:INFO:Received: evaluate message e2cacfa6-96ab-47d2-a65f-8bdc852ce3e3

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:54:INFO:
[92mINFO [0m:      Received: train message 4f7abd4a-c174-4262-9ecc-0de2dc89b685
02/05/2025 11:04:54:INFO:Received: train message 4f7abd4a-c174-4262-9ecc-0de2dc89b685
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:07:13:INFO:
[92mINFO [0m:      Received: evaluate message ba2e302f-79dc-47cc-854b-6d520cb41571
02/05/2025 11:07:13:INFO:Received: evaluate message ba2e302f-79dc-47cc-854b-6d520cb41571
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:07:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:07:54:INFO:
[92mINFO [0m:      Received: train message 338eebb2-a9a6-44a7-ad50-da503a386c40
02/05/2025 11:07:54:INFO:Received: train message 338eebb2-a9a6-44a7-ad50-da503a386c40
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:08:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:09:42:INFO:
[92mINFO [0m:      Received: evaluate message 20e27556-1534-4b5c-b819-3e0126934731
02/05/2025 11:09:42:INFO:Received: evaluate message 20e27556-1534-4b5c-b819-3e0126934731

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:09:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:11:40:INFO:
[92mINFO [0m:      Received: train message 3ca98d74-9ab9-4cfc-96f2-0e1d9e897347
02/05/2025 11:11:40:INFO:Received: train message 3ca98d74-9ab9-4cfc-96f2-0e1d9e897347
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:12:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:21:INFO:
[92mINFO [0m:      Received: evaluate message 38b60009-fc60-4ede-bc1b-16da0d5ac561
02/05/2025 11:14:21:INFO:Received: evaluate message 38b60009-fc60-4ede-bc1b-16da0d5ac561
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:14:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:15:25:INFO:
[92mINFO [0m:      Received: train message adadb725-1377-4f68-84e0-a2b43c981d83
02/05/2025 11:15:25:INFO:Received: train message adadb725-1377-4f68-84e0-a2b43c981d83
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:16:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:06:INFO:
[92mINFO [0m:      Received: evaluate message 46da733d-b0d9-42e4-995c-133515e97ad6
02/05/2025 11:18:06:INFO:Received: evaluate message 46da733d-b0d9-42e4-995c-133515e97ad6

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1061162476990725, 1.058420633691843, 1.1206819923526146, 1.1309531196380984, 1.1190013658077667, 1.1027142179487643, 1.1280771465950221, 1.127055273380384, 1.1274133661559451, 1.1043551822860693, 1.164597787551343, 1.1138114814556979, 1.1614570259768298, 1.154972855890049, 1.1430931422186605, 1.1132255084725262, 1.1624930099736348, 1.107217569990583, 1.1268050638002003, 1.1163196584225819, 1.115742455198021, 1.1313776035528802, 1.146346255985436, 1.1300341315899536, 1.1276932094505376, 1.1155389874031807, 1.092132354593538, 1.1071534750981662, 1.101836214641559], 'accuracy': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946], 'auc': [0.6538837290749867, 0.6940075972249533, 0.7137412239499901, 0.7204929090142318, 0.7292796955293936, 0.7322651953589602, 0.7369030450922003, 0.7423114261113215, 0.7458027311880089, 0.7470794354326935, 0.7496427687879964, 0.7516291080380374, 0.7541420172581413, 0.7557228658206813, 0.7578693316528601, 0.7580815519934007, 0.7598835718360925, 0.7613277177421471, 0.7622995585504664, 0.7643069950274328, 0.7649499758191336, 0.7667329105186012, 0.7674839281465169, 0.7688541063035942, 0.7699560751567117, 0.7709250861537965, 0.7724202843671276, 0.7734948067855966, 0.775166807370288], 'precision': [0.3679360491481115, 0.4066321576069065, 0.40679242473096755, 0.4218767094141376, 0.41978730197622166, 0.43153868880246976, 0.43410217696697145, 0.43401160360764846, 0.43371971881621924, 0.4471768649478458, 0.43201982586275056, 0.44753246307746314, 0.44152538824715754, 0.44479091034332474, 0.4433466240567402, 0.45191873091553963, 0.43605797106417926, 0.45176801175505077, 0.44724744736395255, 0.5918289982849348, 0.45620680418182996, 0.5912722466509954, 0.584942978080936, 0.5894144092575099, 0.5868462447731848, 0.5975163988165842, 0.6081837908579799, 0.6030058873819525, 0.6013600221776082], 'recall': [0.5011727912431587, 0.5136825645035183, 0.5121188428459734, 0.5230648944487881, 0.5238467552775606, 0.5308835027365129, 0.5347928068803753, 0.5340109460516028, 0.5347928068803753, 0.5418295543393276, 0.5332290852228303, 0.544175136825645, 0.5418295543393276, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5363565285379203, 0.547302580140735, 0.54573885848319, 0.5496481626270524, 0.5512118842845973, 0.5496481626270524, 0.5449569976544175, 0.5488663017982799, 0.5465207193119624, 0.5543393275996873, 0.5629397967161845, 0.5590304925723222, 0.5598123534010946], 'f1': [0.3373775955579418, 0.41327312149778717, 0.4105067949994163, 0.4354092661320886, 0.4416870088120141, 0.4676074150973268, 0.4687563657166903, 0.46942381396688637, 0.46742499097926005, 0.486516845554829, 0.4639502225119983, 0.48616408821145196, 0.47665714649281804, 0.4801311942088275, 0.48000071321534105, 0.49109823607556635, 0.47060077304162345, 0.49133243167408375, 0.48548095242283257, 0.4923014157697828, 0.4962557078867206, 0.49113550792740446, 0.48196302402008223, 0.4879514151386787, 0.48468124406643803, 0.4988184369413146, 0.5104747847172403, 0.5049408575831151, 0.5015874135851307]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:18:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:31:INFO:
[92mINFO [0m:      Received: reconnect message 2adbd94f-f2b7-47ad-9320-5a3ffa2f198c
02/05/2025 11:18:31:INFO:Received: reconnect message 2adbd94f-f2b7-47ad-9320-5a3ffa2f198c
02/05/2025 11:18:32:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:18:32:INFO:Disconnect and shut down
