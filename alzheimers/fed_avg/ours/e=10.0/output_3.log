nohup: ignoring input
02/05/2025 10:01:50:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:01:50:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:01:50:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 10:01:50:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778510.089435 1718806 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:02:32:INFO:
[92mINFO [0m:      Received: train message c25a6f0e-3ed8-4613-8823-1f0d83ab1c42
02/05/2025 10:02:33:INFO:Received: train message c25a6f0e-3ed8-4613-8823-1f0d83ab1c42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:03:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:03:51:INFO:
[92mINFO [0m:      Received: evaluate message 601825c4-4d35-4fba-8a7d-9b6c93bc75e2
02/05/2025 10:03:51:INFO:Received: evaluate message 601825c4-4d35-4fba-8a7d-9b6c93bc75e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:03:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:38:INFO:
[92mINFO [0m:      Received: train message df6d6929-02c3-4f7a-87e4-2a89ee111acd
02/05/2025 10:04:38:INFO:Received: train message df6d6929-02c3-4f7a-87e4-2a89ee111acd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:15:INFO:
[92mINFO [0m:      Received: evaluate message 6a441634-3595-47cd-8c21-a0840d5953f1
02/05/2025 10:06:15:INFO:Received: evaluate message 6a441634-3595-47cd-8c21-a0840d5953f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:20:INFO:
[92mINFO [0m:      Received: train message 0f62b6d9-1933-4538-a80b-36c6cb18cfd7
02/05/2025 10:07:20:INFO:Received: train message 0f62b6d9-1933-4538-a80b-36c6cb18cfd7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:03:INFO:
[92mINFO [0m:      Received: evaluate message 34a5b5e1-bf26-4e86-9688-2794a6b6be32
02/05/2025 10:09:03:INFO:Received: evaluate message 34a5b5e1-bf26-4e86-9688-2794a6b6be32
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:47:INFO:
[92mINFO [0m:      Received: train message 486202d0-80d5-493b-8424-7a7552a461cb
02/05/2025 10:09:47:INFO:Received: train message 486202d0-80d5-493b-8424-7a7552a461cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:23:INFO:
[92mINFO [0m:      Received: evaluate message e4635679-406b-4815-863e-c65dc70be316
02/05/2025 10:11:23:INFO:Received: evaluate message e4635679-406b-4815-863e-c65dc70be316
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:12:12:INFO:
[92mINFO [0m:      Received: train message 8f128eda-9b76-4f7d-93ae-b7ced9bd10c8
02/05/2025 10:12:12:INFO:Received: train message 8f128eda-9b76-4f7d-93ae-b7ced9bd10c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:44:INFO:
[92mINFO [0m:      Received: evaluate message c05abfd7-4527-4589-904f-8fb9e98ae049
02/05/2025 10:13:45:INFO:Received: evaluate message c05abfd7-4527-4589-904f-8fb9e98ae049
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:19:INFO:
[92mINFO [0m:      Received: train message a9c92967-d7ff-4789-9eb4-5733a29f6e47
02/05/2025 10:14:19:INFO:Received: train message a9c92967-d7ff-4789-9eb4-5733a29f6e47
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:25:INFO:
[92mINFO [0m:      Received: evaluate message 6aab5cbe-f73c-4702-9118-c669d113ab8a
02/05/2025 10:16:25:INFO:Received: evaluate message 6aab5cbe-f73c-4702-9118-c669d113ab8a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:16:INFO:
[92mINFO [0m:      Received: train message e89964da-bffb-486d-8b1c-1e2fe4f7948f
02/05/2025 10:17:16:INFO:Received: train message e89964da-bffb-486d-8b1c-1e2fe4f7948f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:44:INFO:
[92mINFO [0m:      Received: evaluate message b21335db-c152-4e10-ad2f-92260d63c0ac
02/05/2025 10:18:44:INFO:Received: evaluate message b21335db-c152-4e10-ad2f-92260d63c0ac
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945], 'accuracy': [0.5113369820172009], 'auc': [0.7085531489885228], 'precision': [0.40347364107321304], 'recall': [0.5113369820172009], 'f1': [0.39881455974124325]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902], 'accuracy': [0.5113369820172009, 0.5293197810789679], 'auc': [0.7085531489885228, 0.7350934873405456], 'precision': [0.40347364107321304, 0.43385134376460194], 'recall': [0.5113369820172009, 0.5293197810789679], 'f1': [0.39881455974124325, 0.4722552092863052]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:42:INFO:
[92mINFO [0m:      Received: train message 208fe421-0240-4f55-ba70-5bf6b55eddc1
02/05/2025 10:19:42:INFO:Received: train message 208fe421-0240-4f55-ba70-5bf6b55eddc1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:21:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:37:INFO:
[92mINFO [0m:      Received: evaluate message 830397b4-8ff2-4008-956f-4603f482deaf
02/05/2025 10:22:37:INFO:Received: evaluate message 830397b4-8ff2-4008-956f-4603f482deaf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:22:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:10:INFO:
[92mINFO [0m:      Received: train message 86b4aa0a-40ba-4ea7-9b81-f2f71e0966a2
02/05/2025 10:23:10:INFO:Received: train message 86b4aa0a-40ba-4ea7-9b81-f2f71e0966a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:23:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:48:INFO:
[92mINFO [0m:      Received: evaluate message 2f7c2ad1-171d-41e5-89a1-5311084b906a
02/05/2025 10:24:48:INFO:Received: evaluate message 2f7c2ad1-171d-41e5-89a1-5311084b906a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:24:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:43:INFO:
[92mINFO [0m:      Received: train message a446693f-fe87-4ff3-9954-7480a2b62978
02/05/2025 10:25:43:INFO:Received: train message a446693f-fe87-4ff3-9954-7480a2b62978
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:29:INFO:
[92mINFO [0m:      Received: evaluate message 8e0b3777-5a66-4aa1-a489-449a022cbbce
02/05/2025 10:27:29:INFO:Received: evaluate message 8e0b3777-5a66-4aa1-a489-449a022cbbce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:12:INFO:
[92mINFO [0m:      Received: train message b1cb3271-b832-452d-aa16-3d5616f1a3c6
02/05/2025 10:28:12:INFO:Received: train message b1cb3271-b832-452d-aa16-3d5616f1a3c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:34:INFO:
[92mINFO [0m:      Received: evaluate message de9755e3-91f0-427d-ac02-a14f81a543fb
02/05/2025 10:29:34:INFO:Received: evaluate message de9755e3-91f0-427d-ac02-a14f81a543fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:21:INFO:
[92mINFO [0m:      Received: train message 14f1d61b-c929-4030-9945-0ddf0c5da2b8
02/05/2025 10:30:21:INFO:Received: train message 14f1d61b-c929-4030-9945-0ddf0c5da2b8

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:31:INFO:
[92mINFO [0m:      Received: evaluate message ae5014e8-e338-4cdd-b504-3bc4e340fde1
02/05/2025 10:32:31:INFO:Received: evaluate message ae5014e8-e338-4cdd-b504-3bc4e340fde1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:21:INFO:
[92mINFO [0m:      Received: train message 961a9cde-e02c-4515-8ca5-8f8a210b23ba
02/05/2025 10:33:21:INFO:Received: train message 961a9cde-e02c-4515-8ca5-8f8a210b23ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:06:INFO:
[92mINFO [0m:      Received: evaluate message 1a8b83f0-9b7e-42d9-9395-09f195e4ca7f
02/05/2025 10:35:06:INFO:Received: evaluate message 1a8b83f0-9b7e-42d9-9395-09f195e4ca7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:35:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:50:INFO:
[92mINFO [0m:      Received: train message 11a1c8d1-1601-4d66-9b71-0d65ea9ae9ed
02/05/2025 10:35:50:INFO:Received: train message 11a1c8d1-1601-4d66-9b71-0d65ea9ae9ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:36:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:22:INFO:
[92mINFO [0m:      Received: evaluate message f377e9c9-8e42-42d8-816e-a2aed739e508
02/05/2025 10:37:22:INFO:Received: evaluate message f377e9c9-8e42-42d8-816e-a2aed739e508
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:20:INFO:
[92mINFO [0m:      Received: train message 0131b43e-406a-4c6d-af81-201b2695e94b
02/05/2025 10:38:20:INFO:Received: train message 0131b43e-406a-4c6d-af81-201b2695e94b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:03:INFO:
[92mINFO [0m:      Received: evaluate message dd8ff489-acd9-44e9-a36d-848a000db15e
02/05/2025 10:40:03:INFO:Received: evaluate message dd8ff489-acd9-44e9-a36d-848a000db15e
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:40:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:50:INFO:
[92mINFO [0m:      Received: train message ea2b89fe-726d-47f0-a5a1-6e104eda05a5
02/05/2025 10:40:50:INFO:Received: train message ea2b89fe-726d-47f0-a5a1-6e104eda05a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:29:INFO:
[92mINFO [0m:      Received: evaluate message f4736bd1-44fb-48f7-99c0-56c09a31e7d3
02/05/2025 10:42:29:INFO:Received: evaluate message f4736bd1-44fb-48f7-99c0-56c09a31e7d3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:26:INFO:
[92mINFO [0m:      Received: train message 6ac41c8c-e73b-4256-8b81-9f8a343971af
02/05/2025 10:43:26:INFO:Received: train message 6ac41c8c-e73b-4256-8b81-9f8a343971af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:44:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:10:INFO:
[92mINFO [0m:      Received: evaluate message 218bd3dd-c2be-4e6e-bf95-ee5aaff3eba0
02/05/2025 10:45:10:INFO:Received: evaluate message 218bd3dd-c2be-4e6e-bf95-ee5aaff3eba0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:45:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:54:INFO:
[92mINFO [0m:      Received: train message 54e596ad-a7be-4508-9f21-72a27536f09f
02/05/2025 10:45:54:INFO:Received: train message 54e596ad-a7be-4508-9f21-72a27536f09f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:31:INFO:
[92mINFO [0m:      Received: evaluate message b3ea73a9-18ab-4dc2-b8dc-bb4160ff1894
02/05/2025 10:47:31:INFO:Received: evaluate message b3ea73a9-18ab-4dc2-b8dc-bb4160ff1894

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:37:INFO:
[92mINFO [0m:      Received: train message 106b4964-d174-4c39-bdd0-9140845ffe66
02/05/2025 10:48:37:INFO:Received: train message 106b4964-d174-4c39-bdd0-9140845ffe66
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:21:INFO:
[92mINFO [0m:      Received: evaluate message f72162e9-d5c7-4517-9d8a-6baed154c0f3
02/05/2025 10:50:21:INFO:Received: evaluate message f72162e9-d5c7-4517-9d8a-6baed154c0f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:55:INFO:
[92mINFO [0m:      Received: train message d2c5ce41-13a4-4ef5-a1da-52544e7e9342
02/05/2025 10:50:55:INFO:Received: train message d2c5ce41-13a4-4ef5-a1da-52544e7e9342
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:51:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:14:INFO:
[92mINFO [0m:      Received: evaluate message fe37837a-3dae-4d9a-8459-7c0b33365d7e
02/05/2025 10:53:14:INFO:Received: evaluate message fe37837a-3dae-4d9a-8459-7c0b33365d7e

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:08:INFO:
[92mINFO [0m:      Received: train message 5947320c-80d8-42ac-9d63-15acc207ba2e
02/05/2025 10:54:08:INFO:Received: train message 5947320c-80d8-42ac-9d63-15acc207ba2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:54:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:01:INFO:
[92mINFO [0m:      Received: evaluate message ec43f02d-1489-45bb-9be5-547c62e0772d
02/05/2025 10:56:01:INFO:Received: evaluate message ec43f02d-1489-45bb-9be5-547c62e0772d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:56:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:26:INFO:
[92mINFO [0m:      Received: train message 14e8f7c0-8224-4e8e-8425-f8315e37b714
02/05/2025 10:57:26:INFO:Received: train message 14e8f7c0-8224-4e8e-8425-f8315e37b714
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:58:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:45:INFO:
[92mINFO [0m:      Received: evaluate message 1407ff62-6fb7-415a-8dc1-06f1b62f8c28
02/05/2025 10:59:45:INFO:Received: evaluate message 1407ff62-6fb7-415a-8dc1-06f1b62f8c28

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:59:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:46:INFO:
[92mINFO [0m:      Received: train message c790c936-c15b-4a6a-822b-1fd74a4e7ef3
02/05/2025 11:00:46:INFO:Received: train message c790c936-c15b-4a6a-822b-1fd74a4e7ef3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:01:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:33:INFO:
[92mINFO [0m:      Received: evaluate message 75f9d807-e1d7-4e1e-bc12-c913385a309f
02/05/2025 11:03:33:INFO:Received: evaluate message 75f9d807-e1d7-4e1e-bc12-c913385a309f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:39:INFO:
[92mINFO [0m:      Received: train message f26f8f5e-855e-4d8f-8fbd-5f99d7f05512
02/05/2025 11:04:39:INFO:Received: train message f26f8f5e-855e-4d8f-8fbd-5f99d7f05512
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:07:00:INFO:
[92mINFO [0m:      Received: evaluate message 88e1af60-a218-4073-a27b-6cb629f3496a
02/05/2025 11:07:00:INFO:Received: evaluate message 88e1af60-a218-4073-a27b-6cb629f3496a

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:07:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:07:47:INFO:
[92mINFO [0m:      Received: train message fd6015f5-6655-4aaf-9aa4-01a904cadbb1
02/05/2025 11:07:47:INFO:Received: train message fd6015f5-6655-4aaf-9aa4-01a904cadbb1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:08:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:49:INFO:
[92mINFO [0m:      Received: evaluate message 9bd691a4-775a-44dc-84bf-4f7df10cd5be
02/05/2025 11:10:49:INFO:Received: evaluate message 9bd691a4-775a-44dc-84bf-4f7df10cd5be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:10:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:29:INFO:
[92mINFO [0m:      Received: train message 21516988-a70e-4e6c-80bd-2b6087fd9cf0
02/05/2025 11:12:29:INFO:Received: train message 21516988-a70e-4e6c-80bd-2b6087fd9cf0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:13:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:31:INFO:
[92mINFO [0m:      Received: evaluate message 4b12fb85-abcf-4408-89eb-24430bb238f3
02/05/2025 11:14:31:INFO:Received: evaluate message 4b12fb85-abcf-4408-89eb-24430bb238f3

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:14:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:15:08:INFO:
[92mINFO [0m:      Received: train message a6c83615-7944-4248-b58c-0830b7174fb5
02/05/2025 11:15:08:INFO:Received: train message a6c83615-7944-4248-b58c-0830b7174fb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:24:INFO:
[92mINFO [0m:      Received: evaluate message fb7f1857-3ccc-43af-91f2-b5a297b6ed16
02/05/2025 11:18:24:INFO:Received: evaluate message fb7f1857-3ccc-43af-91f2-b5a297b6ed16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:18:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:49:INFO:
[92mINFO [0m:      Received: train message 7e998f3a-7d6e-4420-9898-122578894fda
02/05/2025 11:18:49:INFO:Received: train message 7e998f3a-7d6e-4420-9898-122578894fda
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:51:INFO:
[92mINFO [0m:      Received: evaluate message 5e1da4a6-6db3-4543-aa5a-3c4ce6906f3e
02/05/2025 11:20:51:INFO:Received: evaluate message 5e1da4a6-6db3-4543-aa5a-3c4ce6906f3e

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:41:INFO:
[92mINFO [0m:      Received: train message 26b399c6-f2db-4322-9a0a-6876c1113901
02/05/2025 11:21:41:INFO:Received: train message 26b399c6-f2db-4322-9a0a-6876c1113901
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:50:INFO:
[92mINFO [0m:      Received: evaluate message e75589f1-4de6-4197-99e0-1bb2b42d1773
02/05/2025 11:23:50:INFO:Received: evaluate message e75589f1-4de6-4197-99e0-1bb2b42d1773
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:35:INFO:
[92mINFO [0m:      Received: train message 324f7265-d7a6-4e66-ae5b-62cabe3e6274
02/05/2025 11:24:35:INFO:Received: train message 324f7265-d7a6-4e66-ae5b-62cabe3e6274
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:11:INFO:
[92mINFO [0m:      Received: evaluate message b98a3419-5ade-48ab-b710-6f21fc3523d0
02/05/2025 11:26:11:INFO:Received: evaluate message b98a3419-5ade-48ab-b710-6f21fc3523d0

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:33:INFO:
[92mINFO [0m:      Received: reconnect message 18472cee-fcf4-4965-be11-e1e09f3313c2
02/05/2025 11:26:33:INFO:Received: reconnect message 18472cee-fcf4-4965-be11-e1e09f3313c2
02/05/2025 11:26:33:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:26:33:INFO:Disconnect and shut down

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576, 1.0347204484261043], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504, 0.7977737627253754], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071, 0.595632848171531], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902, 0.5462812197574198]}



Final client history:
{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576, 1.0347204484261043], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504, 0.7977737627253754], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071, 0.595632848171531], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902, 0.5462812197574198]}

