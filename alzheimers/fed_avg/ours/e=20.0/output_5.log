nohup: ignoring input
02/05/2025 10:04:51:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:04:51:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:04:51:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778691.235220 1880278 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:05:28:INFO:
[92mINFO [0m:      Received: train message b9558f22-2d13-412d-b51f-1c98754cfe76
02/05/2025 10:05:28:INFO:Received: train message b9558f22-2d13-412d-b51f-1c98754cfe76
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:06:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:32:INFO:
[92mINFO [0m:      Received: evaluate message 09c5bff9-d2b4-4f19-a6b1-81498533d381
02/05/2025 10:07:32:INFO:Received: evaluate message 09c5bff9-d2b4-4f19-a6b1-81498533d381
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:07:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:17:INFO:
[92mINFO [0m:      Received: train message 292c4573-1cef-4f45-83db-47939d6a3347
02/05/2025 10:08:17:INFO:Received: train message 292c4573-1cef-4f45-83db-47939d6a3347
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:09:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:53:INFO:
[92mINFO [0m:      Received: evaluate message 4af8e2c4-9824-4d46-ba47-9f9b02d747cc
02/05/2025 10:09:53:INFO:Received: evaluate message 4af8e2c4-9824-4d46-ba47-9f9b02d747cc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:10:41:INFO:
[92mINFO [0m:      Received: train message 7b717d14-f5de-485d-a5e4-bd4c220e0389
02/05/2025 10:10:41:INFO:Received: train message 7b717d14-f5de-485d-a5e4-bd4c220e0389
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:11:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:12:34:INFO:
[92mINFO [0m:      Received: evaluate message bf9cb500-b264-47c8-a3fb-5c535e6e8059
02/05/2025 10:12:34:INFO:Received: evaluate message bf9cb500-b264-47c8-a3fb-5c535e6e8059
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:12:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:15:INFO:
[92mINFO [0m:      Received: train message 1aee7599-a2dc-4f78-86ed-8b952018a575
02/05/2025 10:13:15:INFO:Received: train message 1aee7599-a2dc-4f78-86ed-8b952018a575
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:43:INFO:
[92mINFO [0m:      Received: evaluate message e59ec6bb-320f-41ae-94e0-66adbcdbbe2e
02/05/2025 10:14:43:INFO:Received: evaluate message e59ec6bb-320f-41ae-94e0-66adbcdbbe2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:14:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:36:INFO:
[92mINFO [0m:      Received: train message 088f27a4-fc11-45da-acf4-b24626cb58e4
02/05/2025 10:15:36:INFO:Received: train message 088f27a4-fc11-45da-acf4-b24626cb58e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:16:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:28:INFO:
[92mINFO [0m:      Received: evaluate message 3dc8f3ce-fefa-4a6d-a8ea-df87bd15dd0b
02/05/2025 10:17:28:INFO:Received: evaluate message 3dc8f3ce-fefa-4a6d-a8ea-df87bd15dd0b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:17:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:08:INFO:
[92mINFO [0m:      Received: train message 1721c259-491d-43b7-9ed5-2cc789a0e750
02/05/2025 10:18:08:INFO:Received: train message 1721c259-491d-43b7-9ed5-2cc789a0e750
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:18:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:56:INFO:
[92mINFO [0m:      Received: evaluate message cc11811b-93ea-4362-a40d-9eeac92f5e58
02/05/2025 10:19:56:INFO:Received: evaluate message cc11811b-93ea-4362-a40d-9eeac92f5e58
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:20:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:35:INFO:
[92mINFO [0m:      Received: train message 6d68904f-0adf-427d-ab25-901c47cf66c7
02/05/2025 10:21:35:INFO:Received: train message 6d68904f-0adf-427d-ab25-901c47cf66c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:39:INFO:
[92mINFO [0m:      Received: evaluate message 0b0090fa-9bfb-4e3e-a2c9-84b3e1e60e6c
02/05/2025 10:23:39:INFO:Received: evaluate message 0b0090fa-9bfb-4e3e-a2c9-84b3e1e60e6c
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814], 'accuracy': [0.5175918686473807], 'auc': [0.714457107011283], 'precision': [0.4117264737633955], 'recall': [0.5175918686473807], 'f1': [0.42093829387716764]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771], 'accuracy': [0.5175918686473807, 0.5363565285379203], 'auc': [0.714457107011283, 0.7407919892413165], 'precision': [0.4117264737633955, 0.4429501750516807], 'recall': [0.5175918686473807, 0.5363565285379203], 'f1': [0.42093829387716764, 0.4824075932878562]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:09:INFO:
[92mINFO [0m:      Received: train message 2daeadce-0383-4c43-9bc3-d50675ac7dfa
02/05/2025 10:24:09:INFO:Received: train message 2daeadce-0383-4c43-9bc3-d50675ac7dfa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:24:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:07:INFO:
[92mINFO [0m:      Received: evaluate message be4e32b8-b6a3-45b5-a0d5-c3e80317760f
02/05/2025 10:26:07:INFO:Received: evaluate message be4e32b8-b6a3-45b5-a0d5-c3e80317760f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:26:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:54:INFO:
[92mINFO [0m:      Received: train message 211dd535-7a15-4488-8fa8-d4cf72252bfd
02/05/2025 10:26:54:INFO:Received: train message 211dd535-7a15-4488-8fa8-d4cf72252bfd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:27:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:26:INFO:
[92mINFO [0m:      Received: evaluate message e66c3b2d-83f6-48b0-a22d-74090be26168
02/05/2025 10:28:26:INFO:Received: evaluate message e66c3b2d-83f6-48b0-a22d-74090be26168
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:28:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:19:INFO:
[92mINFO [0m:      Received: train message a44b23c9-9b39-4a74-934a-4a120f6c38d4
02/05/2025 10:29:19:INFO:Received: train message a44b23c9-9b39-4a74-934a-4a120f6c38d4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:55:INFO:
[92mINFO [0m:      Received: evaluate message 9a948828-b454-4b53-a404-4861846b2e51
02/05/2025 10:30:55:INFO:Received: evaluate message 9a948828-b454-4b53-a404-4861846b2e51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:31:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:54:INFO:
[92mINFO [0m:      Received: train message 6e561a24-99da-46c2-94c2-ef85add474e8
02/05/2025 10:31:54:INFO:Received: train message 6e561a24-99da-46c2-94c2-ef85add474e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:32:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:33:INFO:
[92mINFO [0m:      Received: evaluate message 009a5269-7ee9-443a-8555-8aa44bc548e0
02/05/2025 10:33:33:INFO:Received: evaluate message 009a5269-7ee9-443a-8555-8aa44bc548e0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:33:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:06:INFO:
[92mINFO [0m:      Received: train message 5e051cec-64e0-46cf-88e2-b02798e957aa
02/05/2025 10:34:06:INFO:Received: train message 5e051cec-64e0-46cf-88e2-b02798e957aa

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:34:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:01:INFO:
[92mINFO [0m:      Received: evaluate message 9b78dcf4-d500-4aca-a476-472013a512f1
02/05/2025 10:36:01:INFO:Received: evaluate message 9b78dcf4-d500-4aca-a476-472013a512f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:36:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:53:INFO:
[92mINFO [0m:      Received: train message 19253575-c240-4d94-bad7-5aa35c5c34d8
02/05/2025 10:36:53:INFO:Received: train message 19253575-c240-4d94-bad7-5aa35c5c34d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:37:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:20:INFO:
[92mINFO [0m:      Received: evaluate message 9e1ef48e-1de7-4240-b59c-55f51988a90f
02/05/2025 10:38:20:INFO:Received: evaluate message 9e1ef48e-1de7-4240-b59c-55f51988a90f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:38:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:17:INFO:
[92mINFO [0m:      Received: train message 5bfdf930-98fa-4e89-bfee-b678a6651c6d
02/05/2025 10:39:17:INFO:Received: train message 5bfdf930-98fa-4e89-bfee-b678a6651c6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:40:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:44:INFO:
[92mINFO [0m:      Received: evaluate message cfcf87f7-dec3-4b83-9db4-37714c353776
02/05/2025 10:40:44:INFO:Received: evaluate message cfcf87f7-dec3-4b83-9db4-37714c353776
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:40:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:43:INFO:
[92mINFO [0m:      Received: train message 4750d826-f41e-4403-8615-22170451347d
02/05/2025 10:41:43:INFO:Received: train message 4750d826-f41e-4403-8615-22170451347d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:30:INFO:
[92mINFO [0m:      Received: evaluate message 5fa2d755-65e4-4876-8ea9-48a4cbbb1091
02/05/2025 10:43:30:INFO:Received: evaluate message 5fa2d755-65e4-4876-8ea9-48a4cbbb1091
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:05:INFO:
[92mINFO [0m:      Received: train message 1227f74e-71c9-4e13-860d-b39f6fe49d9f
02/05/2025 10:44:05:INFO:Received: train message 1227f74e-71c9-4e13-860d-b39f6fe49d9f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:44:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:44:INFO:
[92mINFO [0m:      Received: evaluate message d0c5f0fa-ca27-4d6c-ba19-f88e311420a3
02/05/2025 10:45:44:INFO:Received: evaluate message d0c5f0fa-ca27-4d6c-ba19-f88e311420a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:45:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:50:INFO:
[92mINFO [0m:      Received: train message b65c4b31-9e0a-47d8-88ba-ea1a1601c949
02/05/2025 10:46:50:INFO:Received: train message b65c4b31-9e0a-47d8-88ba-ea1a1601c949
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:47:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:50:INFO:
[92mINFO [0m:      Received: evaluate message a8e911d2-f02f-496d-bec7-044cea4f8053
02/05/2025 10:48:50:INFO:Received: evaluate message a8e911d2-f02f-496d-bec7-044cea4f8053
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:48:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:52:INFO:
[92mINFO [0m:      Received: train message 24b133c2-cbc2-4e8c-a7d0-d4e3ec997baf
02/05/2025 10:49:52:INFO:Received: train message 24b133c2-cbc2-4e8c-a7d0-d4e3ec997baf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:50:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:10:INFO:
[92mINFO [0m:      Received: evaluate message e42ee21d-1f9d-4d27-9a53-e132a9dd1bec
02/05/2025 10:52:10:INFO:Received: evaluate message e42ee21d-1f9d-4d27-9a53-e132a9dd1bec

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:52:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:55:INFO:
[92mINFO [0m:      Received: train message 95706d9f-2ee8-4fe0-8e45-1da4120bf5a2
02/05/2025 10:52:55:INFO:Received: train message 95706d9f-2ee8-4fe0-8e45-1da4120bf5a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:53:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:04:INFO:
[92mINFO [0m:      Received: evaluate message 36df75dd-fa6e-47c6-975a-32fb2fdb296a
02/05/2025 10:56:04:INFO:Received: evaluate message 36df75dd-fa6e-47c6-975a-32fb2fdb296a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:56:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:59:INFO:
[92mINFO [0m:      Received: train message 4932f77d-d7e9-4ffa-bac2-d2e85a58fb65
02/05/2025 10:56:59:INFO:Received: train message 4932f77d-d7e9-4ffa-bac2-d2e85a58fb65
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:58:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:10:INFO:
[92mINFO [0m:      Received: evaluate message bacb4982-f01d-4a7a-a3a5-b11a8e8b507b
02/05/2025 10:59:10:INFO:Received: evaluate message bacb4982-f01d-4a7a-a3a5-b11a8e8b507b

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:59:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:50:INFO:
[92mINFO [0m:      Received: train message 6e8bea55-da69-4f51-988e-ddf30c6df7ff
02/05/2025 11:00:50:INFO:Received: train message 6e8bea55-da69-4f51-988e-ddf30c6df7ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:02:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:33:INFO:
[92mINFO [0m:      Received: evaluate message 236e679d-1455-471e-bb40-32467bb1ae2f
02/05/2025 11:03:33:INFO:Received: evaluate message 236e679d-1455-471e-bb40-32467bb1ae2f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:35:INFO:
[92mINFO [0m:      Received: train message f6e46c53-da31-48cf-98a2-4bb3f3f401c5
02/05/2025 11:04:35:INFO:Received: train message f6e46c53-da31-48cf-98a2-4bb3f3f401c5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:07:48:INFO:
[92mINFO [0m:      Received: evaluate message 947d163f-e7c4-4ba0-bc6b-e1e0fab5cbb3
02/05/2025 11:07:48:INFO:Received: evaluate message 947d163f-e7c4-4ba0-bc6b-e1e0fab5cbb3

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:08:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:21:INFO:
[92mINFO [0m:      Received: train message 5821a0c2-1f6c-43f8-99db-b36fce94b295
02/05/2025 11:10:21:INFO:Received: train message 5821a0c2-1f6c-43f8-99db-b36fce94b295
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:09:INFO:
[92mINFO [0m:      Received: evaluate message aa25e42c-7b8d-4071-8138-4e48fc2c068c
02/05/2025 11:13:09:INFO:Received: evaluate message aa25e42c-7b8d-4071-8138-4e48fc2c068c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:30:INFO:
[92mINFO [0m:      Received: train message 61b7c987-00f5-48a8-a20e-87a633ec4b35
02/05/2025 11:14:30:INFO:Received: train message 61b7c987-00f5-48a8-a20e-87a633ec4b35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:20:INFO:
[92mINFO [0m:      Received: evaluate message 20729455-cfc8-43cc-a131-13ee8a6b8dad
02/05/2025 11:17:20:INFO:Received: evaluate message 20729455-cfc8-43cc-a131-13ee8a6b8dad

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:22:INFO:
[92mINFO [0m:      Received: train message 3eed5891-03e9-4a73-8ccb-136f751aa10f
02/05/2025 11:18:22:INFO:Received: train message 3eed5891-03e9-4a73-8ccb-136f751aa10f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:49:INFO:
[92mINFO [0m:      Received: evaluate message bd88238e-37ab-49e9-b2c0-ef4ef016a60c
02/05/2025 11:20:49:INFO:Received: evaluate message bd88238e-37ab-49e9-b2c0-ef4ef016a60c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:35:INFO:
[92mINFO [0m:      Received: train message 7e1bd36b-8088-4889-bc53-4cc7e4184f1b
02/05/2025 11:21:35:INFO:Received: train message 7e1bd36b-8088-4889-bc53-4cc7e4184f1b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:21:INFO:
[92mINFO [0m:      Received: evaluate message e834d591-189d-44fe-9b7a-c1f71499d135
02/05/2025 11:23:21:INFO:Received: evaluate message e834d591-189d-44fe-9b7a-c1f71499d135

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:31:INFO:
[92mINFO [0m:      Received: train message c85501f1-6e26-410b-82a1-12126b965986
02/05/2025 11:24:31:INFO:Received: train message c85501f1-6e26-410b-82a1-12126b965986
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:37:INFO:
[92mINFO [0m:      Received: evaluate message e8a6d8db-a7c0-4b29-9551-cd5a170678fb
02/05/2025 11:26:37:INFO:Received: evaluate message e8a6d8db-a7c0-4b29-9551-cd5a170678fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:29:INFO:
[92mINFO [0m:      Received: train message cb83f760-7593-441e-810b-c6e484a46b11
02/05/2025 11:27:29:INFO:Received: train message cb83f760-7593-441e-810b-c6e484a46b11
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:28:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:23:INFO:
[92mINFO [0m:      Received: evaluate message daca7bef-d835-48e2-9863-c5182ff1218d
02/05/2025 11:29:23:INFO:Received: evaluate message daca7bef-d835-48e2-9863-c5182ff1218d

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:30:12:INFO:
[92mINFO [0m:      Received: train message 9e9b43c9-cebd-4e2c-b1c2-dd2bab8ffc11
02/05/2025 11:30:12:INFO:Received: train message 9e9b43c9-cebd-4e2c-b1c2-dd2bab8ffc11
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:31:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:44:INFO:
[92mINFO [0m:      Received: evaluate message d8dc66e4-e65e-497c-87fc-c330a119d1d5
02/05/2025 11:31:44:INFO:Received: evaluate message d8dc66e4-e65e-497c-87fc-c330a119d1d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:31:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:40:INFO:
[92mINFO [0m:      Received: train message c2135ab8-ecbf-4da6-b50a-3f765a304b9a
02/05/2025 11:32:40:INFO:Received: train message c2135ab8-ecbf-4da6-b50a-3f765a304b9a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:33:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:37:INFO:
[92mINFO [0m:      Received: evaluate message 00f4d5d5-cb53-4ff7-929d-38039bdb631d
02/05/2025 11:34:37:INFO:Received: evaluate message 00f4d5d5-cb53-4ff7-929d-38039bdb631d

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:34:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:55:INFO:
[92mINFO [0m:      Received: reconnect message 8206ef68-4ae2-439c-96b5-38006f49292f
02/05/2025 11:34:55:INFO:Received: reconnect message 8206ef68-4ae2-439c-96b5-38006f49292f
02/05/2025 11:34:55:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:34:55:INFO:Disconnect and shut down

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}



Final client history:
{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}

