nohup: ignoring input
02/05/2025 10:04:48:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:04:48:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:04:48:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/05/2025 10:04:48:INFO:
[92mINFO [0m:      Received: get_parameters message c2585dc3-6f43-4dc1-a8a6-d4d74e749360
02/05/2025 10:04:48:INFO:Received: get_parameters message c2585dc3-6f43-4dc1-a8a6-d4d74e749360
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778688.973145 1878152 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      Sent reply
02/05/2025 10:04:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:05:19:INFO:
[92mINFO [0m:      Received: train message 3cd95699-9a05-4ce7-afc3-b719289ad3a5
02/05/2025 10:05:19:INFO:Received: train message 3cd95699-9a05-4ce7-afc3-b719289ad3a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:26:INFO:
[92mINFO [0m:      Received: evaluate message e875a942-5ec2-4dfd-aabe-d673c1ecf058
02/05/2025 10:07:26:INFO:Received: evaluate message e875a942-5ec2-4dfd-aabe-d673c1ecf058
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:07:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:11:INFO:
[92mINFO [0m:      Received: train message 54432da9-5833-4b5a-a459-647f33fd248c
02/05/2025 10:08:11:INFO:Received: train message 54432da9-5833-4b5a-a459-647f33fd248c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:08:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:57:INFO:
[92mINFO [0m:      Received: evaluate message 0faaf534-fb6d-4504-9164-89bee0eb7f00
02/05/2025 10:09:57:INFO:Received: evaluate message 0faaf534-fb6d-4504-9164-89bee0eb7f00
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:10:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:10:45:INFO:
[92mINFO [0m:      Received: train message f6016afa-04f5-4c78-8b44-192556634dbc
02/05/2025 10:10:45:INFO:Received: train message f6016afa-04f5-4c78-8b44-192556634dbc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:11:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:12:35:INFO:
[92mINFO [0m:      Received: evaluate message eb2bc141-8f57-49fd-856f-efb8d95a8ddb
02/05/2025 10:12:35:INFO:Received: evaluate message eb2bc141-8f57-49fd-856f-efb8d95a8ddb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:12:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:07:INFO:
[92mINFO [0m:      Received: train message 87c1d31a-b851-484a-9f73-e28fac9af3df
02/05/2025 10:13:07:INFO:Received: train message 87c1d31a-b851-484a-9f73-e28fac9af3df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:13:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:03:INFO:
[92mINFO [0m:      Received: evaluate message 44f0c89d-0355-49ca-8e84-f31879f6c77e
02/05/2025 10:15:03:INFO:Received: evaluate message 44f0c89d-0355-49ca-8e84-f31879f6c77e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:15:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:43:INFO:
[92mINFO [0m:      Received: train message 61e49a52-ad86-4b56-a204-a87d01a9534d
02/05/2025 10:15:43:INFO:Received: train message 61e49a52-ad86-4b56-a204-a87d01a9534d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:16:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:36:INFO:
[92mINFO [0m:      Received: evaluate message ac886116-2da5-4fa9-94a7-76ee9413aca7
02/05/2025 10:17:36:INFO:Received: evaluate message ac886116-2da5-4fa9-94a7-76ee9413aca7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:17:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:20:INFO:
[92mINFO [0m:      Received: train message f0a32810-9053-4f29-90f6-6e12e76fd877
02/05/2025 10:18:20:INFO:Received: train message f0a32810-9053-4f29-90f6-6e12e76fd877
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:07:INFO:
[92mINFO [0m:      Received: evaluate message 0d8c88b8-536f-4e08-9bfa-517290171fe4
02/05/2025 10:21:07:INFO:Received: evaluate message 0d8c88b8-536f-4e08-9bfa-517290171fe4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:21:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:43:INFO:
[92mINFO [0m:      Received: train message 9b58521c-2f1a-48ff-8ed5-a1a10ede52bc
02/05/2025 10:21:43:INFO:Received: train message 9b58521c-2f1a-48ff-8ed5-a1a10ede52bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:29:INFO:
[92mINFO [0m:      Received: evaluate message 705030df-4a91-47d0-a2fe-64f2b1f1a13e
02/05/2025 10:23:29:INFO:Received: evaluate message 705030df-4a91-47d0-a2fe-64f2b1f1a13e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814], 'accuracy': [0.5175918686473807], 'auc': [0.714457107011283], 'precision': [0.4117264737633955], 'recall': [0.5175918686473807], 'f1': [0.42093829387716764]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771], 'accuracy': [0.5175918686473807, 0.5363565285379203], 'auc': [0.714457107011283, 0.7407919892413165], 'precision': [0.4117264737633955, 0.4429501750516807], 'recall': [0.5175918686473807, 0.5363565285379203], 'f1': [0.42093829387716764, 0.4824075932878562]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:18:INFO:
[92mINFO [0m:      Received: train message 88d6903d-8c18-4e1e-9ecc-ecd12f6940e9
02/05/2025 10:24:18:INFO:Received: train message 88d6903d-8c18-4e1e-9ecc-ecd12f6940e9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:24:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:13:INFO:
[92mINFO [0m:      Received: evaluate message ecf2b5f9-c4c7-4f99-9a44-092d1e3f6d92
02/05/2025 10:26:13:INFO:Received: evaluate message ecf2b5f9-c4c7-4f99-9a44-092d1e3f6d92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:26:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:56:INFO:
[92mINFO [0m:      Received: train message f9e88f0b-0ff6-4fe7-b2d5-3846e88481fd
02/05/2025 10:26:56:INFO:Received: train message f9e88f0b-0ff6-4fe7-b2d5-3846e88481fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:27:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:31:INFO:
[92mINFO [0m:      Received: evaluate message e9039da8-73e2-4732-af35-0420df876d7e
02/05/2025 10:28:31:INFO:Received: evaluate message e9039da8-73e2-4732-af35-0420df876d7e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:28:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:14:INFO:
[92mINFO [0m:      Received: train message 177d34a0-a92f-4f95-873c-56fbcde48ad8
02/05/2025 10:29:14:INFO:Received: train message 177d34a0-a92f-4f95-873c-56fbcde48ad8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:29:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:05:INFO:
[92mINFO [0m:      Received: evaluate message 6a689afe-c5d6-49db-89f3-81e7500a1af7
02/05/2025 10:31:06:INFO:Received: evaluate message 6a689afe-c5d6-49db-89f3-81e7500a1af7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:31:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:54:INFO:
[92mINFO [0m:      Received: train message ce279504-ecac-4c66-8029-c3b9f07d147e
02/05/2025 10:31:54:INFO:Received: train message ce279504-ecac-4c66-8029-c3b9f07d147e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:32:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:12:INFO:
[92mINFO [0m:      Received: evaluate message fca971bf-ecf5-4d5e-b725-daeca102393b
02/05/2025 10:33:12:INFO:Received: evaluate message fca971bf-ecf5-4d5e-b725-daeca102393b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:33:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:23:INFO:
[92mINFO [0m:      Received: train message ec2931c1-9e22-454a-9b78-f07d6e9862d6
02/05/2025 10:34:23:INFO:Received: train message ec2931c1-9e22-454a-9b78-f07d6e9862d6

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:35:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:05:INFO:
[92mINFO [0m:      Received: evaluate message f7799a6a-a992-4419-9cb9-ef0775ff0695
02/05/2025 10:36:05:INFO:Received: evaluate message f7799a6a-a992-4419-9cb9-ef0775ff0695
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:36:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:50:INFO:
[92mINFO [0m:      Received: train message 36830737-170c-4bae-9b74-abeed32bea56
02/05/2025 10:36:50:INFO:Received: train message 36830737-170c-4bae-9b74-abeed32bea56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:37:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:19:INFO:
[92mINFO [0m:      Received: evaluate message 8e829f3a-e509-4c08-afd6-59cd837b3390
02/05/2025 10:38:19:INFO:Received: evaluate message 8e829f3a-e509-4c08-afd6-59cd837b3390
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:38:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:17:INFO:
[92mINFO [0m:      Received: train message 2cc99ec9-7f05-46ef-94d9-7a372ad76712
02/05/2025 10:39:17:INFO:Received: train message 2cc99ec9-7f05-46ef-94d9-7a372ad76712
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:39:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:42:INFO:
[92mINFO [0m:      Received: evaluate message dcc01796-c0c2-4851-9241-c065470c8cf7
02/05/2025 10:40:42:INFO:Received: evaluate message dcc01796-c0c2-4851-9241-c065470c8cf7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:40:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:46:INFO:
[92mINFO [0m:      Received: train message 13bc198b-2504-4c0e-bd0e-c68b9a6a922f
02/05/2025 10:41:46:INFO:Received: train message 13bc198b-2504-4c0e-bd0e-c68b9a6a922f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:29:INFO:
[92mINFO [0m:      Received: evaluate message 4585119b-f4a7-4343-9541-bc7abedb7d2f
02/05/2025 10:43:29:INFO:Received: evaluate message 4585119b-f4a7-4343-9541-bc7abedb7d2f
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:17:INFO:
[92mINFO [0m:      Received: train message 60e2ba32-fb47-4838-9b9c-b5da29c2a227
02/05/2025 10:44:17:INFO:Received: train message 60e2ba32-fb47-4838-9b9c-b5da29c2a227
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:44:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:58:INFO:
[92mINFO [0m:      Received: evaluate message dac98cef-4a96-48b3-9d24-1c40ba715bfc
02/05/2025 10:45:58:INFO:Received: evaluate message dac98cef-4a96-48b3-9d24-1c40ba715bfc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:46:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:47:INFO:
[92mINFO [0m:      Received: train message b5615d91-35e8-45e3-9203-b3ec1997f3d9
02/05/2025 10:46:47:INFO:Received: train message b5615d91-35e8-45e3-9203-b3ec1997f3d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:47:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:04:INFO:
[92mINFO [0m:      Received: evaluate message 60b75ae5-7512-4059-bdda-dfe7a2f4fd0f
02/05/2025 10:49:04:INFO:Received: evaluate message 60b75ae5-7512-4059-bdda-dfe7a2f4fd0f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:49:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:27:INFO:
[92mINFO [0m:      Received: train message 423c26c1-7384-4ea8-a5e3-13e31818c975
02/05/2025 10:49:27:INFO:Received: train message 423c26c1-7384-4ea8-a5e3-13e31818c975
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:50:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:57:INFO:
[92mINFO [0m:      Received: evaluate message 42f89940-a0a1-494c-9408-2861144556e0
02/05/2025 10:51:57:INFO:Received: evaluate message 42f89940-a0a1-494c-9408-2861144556e0

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:52:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:53:INFO:
[92mINFO [0m:      Received: train message 646fa588-7ae5-4bbb-9aed-645d817bd187
02/05/2025 10:52:53:INFO:Received: train message 646fa588-7ae5-4bbb-9aed-645d817bd187
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:53:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:55:58:INFO:
[92mINFO [0m:      Received: evaluate message 6e52e5fa-d618-4a18-83cd-413664633842
02/05/2025 10:55:58:INFO:Received: evaluate message 6e52e5fa-d618-4a18-83cd-413664633842
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:56:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:46:INFO:
[92mINFO [0m:      Received: train message 1fe11c8d-3f8e-42f6-931c-1649b8573955
02/05/2025 10:56:46:INFO:Received: train message 1fe11c8d-3f8e-42f6-931c-1649b8573955
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:57:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:30:INFO:
[92mINFO [0m:      Received: evaluate message c787b16d-d558-4364-9345-102253a258d1
02/05/2025 10:59:30:INFO:Received: evaluate message c787b16d-d558-4364-9345-102253a258d1

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:00:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:40:INFO:
[92mINFO [0m:      Received: train message eaef96fc-cd5b-4447-9c4a-478a1d588011
02/05/2025 11:00:40:INFO:Received: train message eaef96fc-cd5b-4447-9c4a-478a1d588011
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:01:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:44:INFO:
[92mINFO [0m:      Received: evaluate message 06ff1fb9-042a-48a3-bc99-daa81973d6b6
02/05/2025 11:03:44:INFO:Received: evaluate message 06ff1fb9-042a-48a3-bc99-daa81973d6b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:33:INFO:
[92mINFO [0m:      Received: train message 1d42675a-67eb-44e2-bf12-4229b9f7c503
02/05/2025 11:04:33:INFO:Received: train message 1d42675a-67eb-44e2-bf12-4229b9f7c503
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:12:INFO:
[92mINFO [0m:      Received: evaluate message 52f8f979-d175-406d-aa4a-5e517f3cfe64
02/05/2025 11:08:12:INFO:Received: evaluate message 52f8f979-d175-406d-aa4a-5e517f3cfe64

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:08:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:22:INFO:
[92mINFO [0m:      Received: train message 5fd2f44c-01a3-4532-bbde-0dbb11241677
02/05/2025 11:10:22:INFO:Received: train message 5fd2f44c-01a3-4532-bbde-0dbb11241677
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:28:INFO:
[92mINFO [0m:      Received: evaluate message 60ee1f77-bdfa-40a4-b53c-ef62ff35c027
02/05/2025 11:12:28:INFO:Received: evaluate message 60ee1f77-bdfa-40a4-b53c-ef62ff35c027
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:12:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:35:INFO:
[92mINFO [0m:      Received: train message 9663bebd-8b1d-4419-9a0b-2c96b1b39c38
02/05/2025 11:14:35:INFO:Received: train message 9663bebd-8b1d-4419-9a0b-2c96b1b39c38
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:16:28:INFO:
[92mINFO [0m:      Received: evaluate message ca9acb53-dc64-4d46-bd82-54ee98ef4ba7
02/05/2025 11:16:28:INFO:Received: evaluate message ca9acb53-dc64-4d46-bd82-54ee98ef4ba7

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:16:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:31:INFO:
[92mINFO [0m:      Received: train message 434add97-fa8d-4f7c-ae33-cf7c74414e5f
02/05/2025 11:18:31:INFO:Received: train message 434add97-fa8d-4f7c-ae33-cf7c74414e5f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:50:INFO:
[92mINFO [0m:      Received: evaluate message a450f1b0-ba3a-4bd8-96ed-a0b2395082df
02/05/2025 11:20:50:INFO:Received: evaluate message a450f1b0-ba3a-4bd8-96ed-a0b2395082df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:41:INFO:
[92mINFO [0m:      Received: train message 939280d0-746f-449a-a224-49efb422e68d
02/05/2025 11:21:41:INFO:Received: train message 939280d0-746f-449a-a224-49efb422e68d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:38:INFO:
[92mINFO [0m:      Received: evaluate message ec684b28-b364-4238-8ca9-e21da5d47fb3
02/05/2025 11:23:38:INFO:Received: evaluate message ec684b28-b364-4238-8ca9-e21da5d47fb3

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:13:INFO:
[92mINFO [0m:      Received: train message 89c244ff-2674-4a1a-858c-00d2d6dce522
02/05/2025 11:24:13:INFO:Received: train message 89c244ff-2674-4a1a-858c-00d2d6dce522
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:13:INFO:
[92mINFO [0m:      Received: evaluate message e5225f69-458c-4f85-ac37-44d6b0f8442f
02/05/2025 11:26:13:INFO:Received: evaluate message e5225f69-458c-4f85-ac37-44d6b0f8442f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:13:INFO:
[92mINFO [0m:      Received: train message 09bac99d-3633-4cd0-a830-feae0f19910d
02/05/2025 11:27:13:INFO:Received: train message 09bac99d-3633-4cd0-a830-feae0f19910d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:28:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:23:INFO:
[92mINFO [0m:      Received: evaluate message 90312743-bd4a-41e1-8f7e-158af6889d47
02/05/2025 11:29:23:INFO:Received: evaluate message 90312743-bd4a-41e1-8f7e-158af6889d47

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:52:INFO:
[92mINFO [0m:      Received: train message cd9badd3-b884-41d9-88d8-acac58447b43
02/05/2025 11:29:52:INFO:Received: train message cd9badd3-b884-41d9-88d8-acac58447b43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:30:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:55:INFO:
[92mINFO [0m:      Received: evaluate message e82c1a09-fd33-460e-9634-62e9a8e3a9ad
02/05/2025 11:31:55:INFO:Received: evaluate message e82c1a09-fd33-460e-9634-62e9a8e3a9ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:32:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:43:INFO:
[92mINFO [0m:      Received: train message 596ea7fb-a642-41e4-88cf-9687cd034b5e
02/05/2025 11:32:43:INFO:Received: train message 596ea7fb-a642-41e4-88cf-9687cd034b5e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:33:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:45:INFO:
[92mINFO [0m:      Received: evaluate message b7add9ec-d629-4971-8aed-3787c47d1ea5
02/05/2025 11:34:45:INFO:Received: evaluate message b7add9ec-d629-4971-8aed-3787c47d1ea5

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:34:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:55:INFO:
[92mINFO [0m:      Received: reconnect message b78db4d6-a267-4c81-9165-8693d1aa5529
02/05/2025 11:34:55:INFO:Received: reconnect message b78db4d6-a267-4c81-9165-8693d1aa5529
02/05/2025 11:34:55:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:34:55:INFO:Disconnect and shut down
