nohup: ignoring input
02/15/2025 01:17:49:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:17:49:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:17:49:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739611069.515388 1666971 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:18:26:INFO:
[92mINFO [0m:      Received: train message e5588855-088d-4b2f-b513-e998b9d03c1f
02/15/2025 01:18:26:INFO:Received: train message e5588855-088d-4b2f-b513-e998b9d03c1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:45:INFO:
[92mINFO [0m:      Received: evaluate message ee16cf7e-a961-4d97-9c22-172143d46b18
02/15/2025 01:20:45:INFO:Received: evaluate message ee16cf7e-a961-4d97-9c22-172143d46b18
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:39:INFO:
[92mINFO [0m:      Received: train message a4f59b1a-e866-42f2-b416-9efa5a0f3141
02/15/2025 01:21:39:INFO:Received: train message a4f59b1a-e866-42f2-b416-9efa5a0f3141
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:23:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:25:INFO:
[92mINFO [0m:      Received: evaluate message 229cad37-56b6-4332-9054-7b6ebbdc11ab
02/15/2025 01:24:25:INFO:Received: evaluate message 229cad37-56b6-4332-9054-7b6ebbdc11ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:24:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:16:INFO:
[92mINFO [0m:      Received: train message e5f292f2-8c37-4b34-89fc-ccfe99e179e8
02/15/2025 01:25:16:INFO:Received: train message e5f292f2-8c37-4b34-89fc-ccfe99e179e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:35:INFO:
[92mINFO [0m:      Received: evaluate message 82fae38a-c073-4ea8-b82d-d42d1599af20
02/15/2025 01:27:35:INFO:Received: evaluate message 82fae38a-c073-4ea8-b82d-d42d1599af20
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:35:INFO:
[92mINFO [0m:      Received: train message 59594074-06d0-430a-99bf-9b3e84186fc2
02/15/2025 01:28:35:INFO:Received: train message 59594074-06d0-430a-99bf-9b3e84186fc2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:29:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:47:INFO:
[92mINFO [0m:      Received: evaluate message 3b0c4c44-04d5-4dd9-93ef-170de9697808
02/15/2025 01:30:47:INFO:Received: evaluate message 3b0c4c44-04d5-4dd9-93ef-170de9697808
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:30:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:34:INFO:
[92mINFO [0m:      Received: train message 3e6f6603-7ae2-4f51-a3f1-58294f3910b6
02/15/2025 01:31:34:INFO:Received: train message 3e6f6603-7ae2-4f51-a3f1-58294f3910b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:32:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:39:INFO:
[92mINFO [0m:      Received: evaluate message 58e5f49e-10ad-4301-b91f-5c5555f9b079
02/15/2025 01:33:39:INFO:Received: evaluate message 58e5f49e-10ad-4301-b91f-5c5555f9b079
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:28:INFO:
[92mINFO [0m:      Received: train message a209163a-d7a6-44fe-9509-43b39834fa4b
02/15/2025 01:34:28:INFO:Received: train message a209163a-d7a6-44fe-9509-43b39834fa4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:26:INFO:
[92mINFO [0m:      Received: evaluate message 432d1ebe-3a62-4ff3-b653-846db6d0b993
02/15/2025 01:36:26:INFO:Received: evaluate message 432d1ebe-3a62-4ff3-b653-846db6d0b993
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:13:INFO:
[92mINFO [0m:      Received: train message 3f331401-bde4-4c42-9faf-5e9219601357
02/15/2025 01:37:13:INFO:Received: train message 3f331401-bde4-4c42-9faf-5e9219601357
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:38:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:16:INFO:
[92mINFO [0m:      Received: evaluate message b4413557-5a03-443d-8a34-bb327fe32f3f
02/15/2025 01:39:16:INFO:Received: evaluate message b4413557-5a03-443d-8a34-bb327fe32f3f
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906], 'accuracy': [0.5027365129007036], 'auc': [0.6897600584676676], 'precision': [0.3991915055150511], 'recall': [0.5027365129007036], 'f1': [0.3614852344286372]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294], 'accuracy': [0.5027365129007036, 0.524628616106333], 'auc': [0.6897600584676676, 0.7198632289872761], 'precision': [0.3991915055150511, 0.4206049829674183], 'recall': [0.5027365129007036, 0.524628616106333], 'f1': [0.3614852344286372, 0.4422280621916187]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:45:INFO:
[92mINFO [0m:      Received: train message 556144b8-e81f-4d47-a287-2c81fc29cc87
02/15/2025 01:39:45:INFO:Received: train message 556144b8-e81f-4d47-a287-2c81fc29cc87
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:40:INFO:
[92mINFO [0m:      Received: evaluate message a0646ed8-5625-4c52-912c-6f86d4d721b1
02/15/2025 01:41:40:INFO:Received: evaluate message a0646ed8-5625-4c52-912c-6f86d4d721b1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:22:INFO:
[92mINFO [0m:      Received: train message 686a91fe-5852-4140-b0fe-22a74144b4ba
02/15/2025 01:42:22:INFO:Received: train message 686a91fe-5852-4140-b0fe-22a74144b4ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:43:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:01:INFO:
[92mINFO [0m:      Received: evaluate message 14abe0d8-78da-4f9c-8477-00a40e30f398
02/15/2025 01:44:01:INFO:Received: evaluate message 14abe0d8-78da-4f9c-8477-00a40e30f398
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:44:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:26:INFO:
[92mINFO [0m:      Received: train message d3187aaa-b973-4d93-b635-c05340f70f23
02/15/2025 01:44:26:INFO:Received: train message d3187aaa-b973-4d93-b635-c05340f70f23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:04:INFO:
[92mINFO [0m:      Received: evaluate message 2f0ff776-53bb-4f73-935a-aeaa2fb81845
02/15/2025 01:46:04:INFO:Received: evaluate message 2f0ff776-53bb-4f73-935a-aeaa2fb81845
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:57:INFO:
[92mINFO [0m:      Received: train message 9b71097a-204e-4539-9627-0eeb2991eeb3
02/15/2025 01:46:57:INFO:Received: train message 9b71097a-204e-4539-9627-0eeb2991eeb3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:26:INFO:
[92mINFO [0m:      Received: evaluate message 339b083d-28c7-45f2-8dfc-7d95ea3e4775
02/15/2025 01:48:26:INFO:Received: evaluate message 339b083d-28c7-45f2-8dfc-7d95ea3e4775
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:22:INFO:
[92mINFO [0m:      Received: train message 73695e0f-4319-45c2-aba7-a568c0b0ed29
02/15/2025 01:49:22:INFO:Received: train message 73695e0f-4319-45c2-aba7-a568c0b0ed29

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:57:INFO:
[92mINFO [0m:      Received: evaluate message 3e32c3f5-0b14-4ba6-b888-d4f6589be091
02/15/2025 01:50:57:INFO:Received: evaluate message 3e32c3f5-0b14-4ba6-b888-d4f6589be091
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:51:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:43:INFO:
[92mINFO [0m:      Received: train message be66b543-c573-4d5b-b200-f855fda9b1bf
02/15/2025 01:51:43:INFO:Received: train message be66b543-c573-4d5b-b200-f855fda9b1bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:52:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:52:50:INFO:
[92mINFO [0m:      Received: evaluate message 25b3191f-18d7-4a1b-b3a0-3364b2f1bf87
02/15/2025 01:52:50:INFO:Received: evaluate message 25b3191f-18d7-4a1b-b3a0-3364b2f1bf87
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:52:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:42:INFO:
[92mINFO [0m:      Received: train message 6753d37b-9940-430b-874f-1abae7702401
02/15/2025 01:53:42:INFO:Received: train message 6753d37b-9940-430b-874f-1abae7702401
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:55:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:17:INFO:
[92mINFO [0m:      Received: evaluate message 7df0019a-eb1a-4694-ada5-084657e2576f
02/15/2025 01:56:17:INFO:Received: evaluate message 7df0019a-eb1a-4694-ada5-084657e2576f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:57:05:INFO:
[92mINFO [0m:      Received: train message 0339052f-3c8a-4fed-a268-5528a751ff9f
02/15/2025 01:57:05:INFO:Received: train message 0339052f-3c8a-4fed-a268-5528a751ff9f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:58:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:58:INFO:
[92mINFO [0m:      Received: evaluate message ff33d09b-4dda-49ae-867d-bd75c82bd671
02/15/2025 01:58:58:INFO:Received: evaluate message ff33d09b-4dda-49ae-867d-bd75c82bd671
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:59:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:43:INFO:
[92mINFO [0m:      Received: train message 9338fcd8-ce16-48e4-abc9-f4f31ce00138
02/15/2025 01:59:43:INFO:Received: train message 9338fcd8-ce16-48e4-abc9-f4f31ce00138
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:01:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:59:INFO:
[92mINFO [0m:      Received: evaluate message fa2c0f97-1926-4bc7-ae0c-c0bedba547cf
02/15/2025 02:01:59:INFO:Received: evaluate message fa2c0f97-1926-4bc7-ae0c-c0bedba547cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:02:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:02:39:INFO:
[92mINFO [0m:      Received: train message 35480a2d-f801-4f8a-878e-37f1b6b20a3d
02/15/2025 02:02:39:INFO:Received: train message 35480a2d-f801-4f8a-878e-37f1b6b20a3d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:03:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:49:INFO:
[92mINFO [0m:      Received: evaluate message c31d6644-6078-4711-96e5-d66c526d96f4
02/15/2025 02:04:49:INFO:Received: evaluate message c31d6644-6078-4711-96e5-d66c526d96f4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:04:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:15:INFO:
[92mINFO [0m:      Received: train message a0fc4489-79e4-47bb-b6fe-cf94eac61234
02/15/2025 02:05:15:INFO:Received: train message a0fc4489-79e4-47bb-b6fe-cf94eac61234
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:37:INFO:
[92mINFO [0m:      Received: evaluate message 3dfae79c-0792-4c08-b437-7d390be696a0
02/15/2025 02:07:37:INFO:Received: evaluate message 3dfae79c-0792-4c08-b437-7d390be696a0

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:19:INFO:
[92mINFO [0m:      Received: train message b0a32e73-69c8-466e-902e-b247727a890b
02/15/2025 02:08:19:INFO:Received: train message b0a32e73-69c8-466e-902e-b247727a890b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:09:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:32:INFO:
[92mINFO [0m:      Received: evaluate message 45048ce2-9473-43c1-8ce0-8eeef51369ea
02/15/2025 02:10:32:INFO:Received: evaluate message 45048ce2-9473-43c1-8ce0-8eeef51369ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:10:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:11:12:INFO:
[92mINFO [0m:      Received: train message 3d024b2a-8114-45c6-8b35-ff52e14dea3c
02/15/2025 02:11:12:INFO:Received: train message 3d024b2a-8114-45c6-8b35-ff52e14dea3c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:12:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:43:INFO:
[92mINFO [0m:      Received: evaluate message d8534011-e0b4-4aa5-9165-817f50dd3f76
02/15/2025 02:12:43:INFO:Received: evaluate message d8534011-e0b4-4aa5-9165-817f50dd3f76

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:12:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:13:38:INFO:
[92mINFO [0m:      Received: train message 61a135bc-3354-4354-b43d-09ad23e77830
02/15/2025 02:13:38:INFO:Received: train message 61a135bc-3354-4354-b43d-09ad23e77830
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:14:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:22:INFO:
[92mINFO [0m:      Received: evaluate message 1e444b2f-9103-488d-bd1c-af7f0c2fc652
02/15/2025 02:15:22:INFO:Received: evaluate message 1e444b2f-9103-488d-bd1c-af7f0c2fc652
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:15:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:06:INFO:
[92mINFO [0m:      Received: train message 282af029-6bd5-419d-9071-db68e2b1a79b
02/15/2025 02:16:06:INFO:Received: train message 282af029-6bd5-419d-9071-db68e2b1a79b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:16:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:54:INFO:
[92mINFO [0m:      Received: evaluate message 5bbd1531-22e0-4f99-b303-4d1b9ddb09bf
02/15/2025 02:17:54:INFO:Received: evaluate message 5bbd1531-22e0-4f99-b303-4d1b9ddb09bf

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:17:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:19:INFO:
[92mINFO [0m:      Received: train message 450f9063-6e54-4365-8266-eaeb0e9923ed
02/15/2025 02:18:19:INFO:Received: train message 450f9063-6e54-4365-8266-eaeb0e9923ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:14:INFO:
[92mINFO [0m:      Received: evaluate message 8232e59a-d0de-4489-b1b8-750b9f7f30e8
02/15/2025 02:20:14:INFO:Received: evaluate message 8232e59a-d0de-4489-b1b8-750b9f7f30e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:39:INFO:
[92mINFO [0m:      Received: train message 41c3bbdf-e1ad-4c73-aaf4-744d1209a41c
02/15/2025 02:20:39:INFO:Received: train message 41c3bbdf-e1ad-4c73-aaf4-744d1209a41c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:21:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:17:INFO:
[92mINFO [0m:      Received: evaluate message 9d8a7f68-8000-4776-a914-6c79809e2984
02/15/2025 02:22:17:INFO:Received: evaluate message 9d8a7f68-8000-4776-a914-6c79809e2984

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:22:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:56:INFO:
[92mINFO [0m:      Received: train message 56b61929-7634-467c-a30f-f8e9f6da7db7
02/15/2025 02:22:56:INFO:Received: train message 56b61929-7634-467c-a30f-f8e9f6da7db7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:23:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:24:26:INFO:
[92mINFO [0m:      Received: evaluate message 63ec985a-80e0-45a2-88f9-9cd2b6c5f23f
02/15/2025 02:24:26:INFO:Received: evaluate message 63ec985a-80e0-45a2-88f9-9cd2b6c5f23f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:24:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:24:52:INFO:
[92mINFO [0m:      Received: train message 30599805-e4b1-4299-9832-623c6456dd51
02/15/2025 02:24:52:INFO:Received: train message 30599805-e4b1-4299-9832-623c6456dd51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:25:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:26:34:INFO:
[92mINFO [0m:      Received: evaluate message f1119743-d89e-4473-93f4-ef67481c2d93
02/15/2025 02:26:34:INFO:Received: evaluate message f1119743-d89e-4473-93f4-ef67481c2d93

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:26:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:27:16:INFO:
[92mINFO [0m:      Received: train message 79418db5-ba28-46d3-9063-9937723fd8b7
02/15/2025 02:27:16:INFO:Received: train message 79418db5-ba28-46d3-9063-9937723fd8b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:28:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:29:32:INFO:
[92mINFO [0m:      Received: evaluate message 38334569-8de1-4485-ab4c-2c11eacb2cb5
02/15/2025 02:29:32:INFO:Received: evaluate message 38334569-8de1-4485-ab4c-2c11eacb2cb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:29:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:30:32:INFO:
[92mINFO [0m:      Received: train message b95e8f93-c539-438f-a89e-5215e4e8ea8c
02/15/2025 02:30:32:INFO:Received: train message b95e8f93-c539-438f-a89e-5215e4e8ea8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:31:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:31:49:INFO:
[92mINFO [0m:      Received: evaluate message c9afaf18-2071-4be3-84d9-278f7a290548
02/15/2025 02:31:49:INFO:Received: evaluate message c9afaf18-2071-4be3-84d9-278f7a290548

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:32:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:32:34:INFO:
[92mINFO [0m:      Received: train message 4c0daa0e-9c4b-4569-8ab2-6ea4d2de5449
02/15/2025 02:32:34:INFO:Received: train message 4c0daa0e-9c4b-4569-8ab2-6ea4d2de5449
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:34:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:35:05:INFO:
[92mINFO [0m:      Received: evaluate message 8d1adb1c-5251-4b85-a28d-facebdfc498f
02/15/2025 02:35:05:INFO:Received: evaluate message 8d1adb1c-5251-4b85-a28d-facebdfc498f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:35:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:35:56:INFO:
[92mINFO [0m:      Received: train message 9cb4d14d-f4e0-4050-b6a1-6bf6763a3c35
02/15/2025 02:35:56:INFO:Received: train message 9cb4d14d-f4e0-4050-b6a1-6bf6763a3c35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:37:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:37:53:INFO:
[92mINFO [0m:      Received: evaluate message 157955ee-98ec-4343-89ef-2873779c56ae
02/15/2025 02:37:53:INFO:Received: evaluate message 157955ee-98ec-4343-89ef-2873779c56ae

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:38:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:38:02:INFO:
[92mINFO [0m:      Received: reconnect message 60cf87b0-66c7-440e-b92b-d6f30285c7b1
02/15/2025 02:38:02:INFO:Received: reconnect message 60cf87b0-66c7-440e-b92b-d6f30285c7b1
02/15/2025 02:38:02:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:38:02:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}



Final client history:
{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}

