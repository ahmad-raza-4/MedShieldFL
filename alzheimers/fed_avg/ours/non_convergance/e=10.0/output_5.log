nohup: ignoring input
02/15/2025 01:17:46:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:17:46:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:17:46:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739611066.128174 1666844 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:18:12:INFO:
[92mINFO [0m:      Received: train message 67d2ef70-7881-4da1-957d-85e6a0c9a466
02/15/2025 01:18:12:INFO:Received: train message 67d2ef70-7881-4da1-957d-85e6a0c9a466
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:04:INFO:
[92mINFO [0m:      Received: evaluate message e66bb8f2-ae7f-4f5d-958f-1c03d121d1f9
02/15/2025 01:21:04:INFO:Received: evaluate message e66bb8f2-ae7f-4f5d-958f-1c03d121d1f9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:21:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:02:INFO:
[92mINFO [0m:      Received: train message af105e6b-1886-4a22-81b5-d936dc3560e8
02/15/2025 01:22:02:INFO:Received: train message af105e6b-1886-4a22-81b5-d936dc3560e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:23:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:15:INFO:
[92mINFO [0m:      Received: evaluate message a3ea165d-8a4c-456d-a00a-b763beef4421
02/15/2025 01:24:15:INFO:Received: evaluate message a3ea165d-8a4c-456d-a00a-b763beef4421
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:24:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:14:INFO:
[92mINFO [0m:      Received: train message 6314a2f3-02ea-438f-a63f-0cbd41fb3450
02/15/2025 01:25:14:INFO:Received: train message 6314a2f3-02ea-438f-a63f-0cbd41fb3450
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:46:INFO:
[92mINFO [0m:      Received: evaluate message 427b833a-be84-420b-8143-9df7e9a8b61c
02/15/2025 01:27:46:INFO:Received: evaluate message 427b833a-be84-420b-8143-9df7e9a8b61c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:25:INFO:
[92mINFO [0m:      Received: train message 53965d53-70b6-4566-8fc6-2435067b7c2b
02/15/2025 01:28:25:INFO:Received: train message 53965d53-70b6-4566-8fc6-2435067b7c2b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:29:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:41:INFO:
[92mINFO [0m:      Received: evaluate message 8ae0fa02-2ee8-4231-a385-28360840efc9
02/15/2025 01:30:41:INFO:Received: evaluate message 8ae0fa02-2ee8-4231-a385-28360840efc9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:30:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:34:INFO:
[92mINFO [0m:      Received: train message 3ef15989-1aba-4825-98e6-9ee672305d7c
02/15/2025 01:31:34:INFO:Received: train message 3ef15989-1aba-4825-98e6-9ee672305d7c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:32:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:45:INFO:
[92mINFO [0m:      Received: evaluate message 28aca1be-72f4-4433-9745-f8830cb7130b
02/15/2025 01:33:45:INFO:Received: evaluate message 28aca1be-72f4-4433-9745-f8830cb7130b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:13:INFO:
[92mINFO [0m:      Received: train message 379474c9-505d-4368-893f-0f2b534d5f24
02/15/2025 01:34:13:INFO:Received: train message 379474c9-505d-4368-893f-0f2b534d5f24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:26:INFO:
[92mINFO [0m:      Received: evaluate message 7ef878e3-ea75-4ec2-83d9-acad7c6d888b
02/15/2025 01:36:26:INFO:Received: evaluate message 7ef878e3-ea75-4ec2-83d9-acad7c6d888b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:54:INFO:
[92mINFO [0m:      Received: train message afff9443-8b05-4b10-a787-f061406126fb
02/15/2025 01:36:54:INFO:Received: train message afff9443-8b05-4b10-a787-f061406126fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:01:INFO:
[92mINFO [0m:      Received: evaluate message 3371347f-d79c-4645-ace7-2bbc1b36bd62
02/15/2025 01:39:01:INFO:Received: evaluate message 3371347f-d79c-4645-ace7-2bbc1b36bd62
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906], 'accuracy': [0.5027365129007036], 'auc': [0.6897600584676676], 'precision': [0.3991915055150511], 'recall': [0.5027365129007036], 'f1': [0.3614852344286372]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294], 'accuracy': [0.5027365129007036, 0.524628616106333], 'auc': [0.6897600584676676, 0.7198632289872761], 'precision': [0.3991915055150511, 0.4206049829674183], 'recall': [0.5027365129007036, 0.524628616106333], 'f1': [0.3614852344286372, 0.4422280621916187]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:55:INFO:
[92mINFO [0m:      Received: train message 4a68f602-4001-43f2-bdb7-6f64501759c6
02/15/2025 01:39:55:INFO:Received: train message 4a68f602-4001-43f2-bdb7-6f64501759c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:30:INFO:
[92mINFO [0m:      Received: evaluate message 9fe992bc-459f-47d4-84eb-d106f0cb3ab8
02/15/2025 01:41:30:INFO:Received: evaluate message 9fe992bc-459f-47d4-84eb-d106f0cb3ab8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:20:INFO:
[92mINFO [0m:      Received: train message ef1706ca-779c-4918-8f13-ad74b9785910
02/15/2025 01:42:20:INFO:Received: train message ef1706ca-779c-4918-8f13-ad74b9785910
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:43:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:55:INFO:
[92mINFO [0m:      Received: evaluate message 1bc8214d-5229-4890-819c-131997ce82d2
02/15/2025 01:43:55:INFO:Received: evaluate message 1bc8214d-5229-4890-819c-131997ce82d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:18:INFO:
[92mINFO [0m:      Received: train message 144eff88-ac05-43bb-966c-fa7cec850506
02/15/2025 01:44:18:INFO:Received: train message 144eff88-ac05-43bb-966c-fa7cec850506
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:44:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:14:INFO:
[92mINFO [0m:      Received: evaluate message b43c86e7-883a-4348-ae62-ba71e6d69bea
02/15/2025 01:46:14:INFO:Received: evaluate message b43c86e7-883a-4348-ae62-ba71e6d69bea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:36:INFO:
[92mINFO [0m:      Received: train message e2064385-1a01-4d9d-a819-300f1571d32f
02/15/2025 01:46:36:INFO:Received: train message e2064385-1a01-4d9d-a819-300f1571d32f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:40:INFO:
[92mINFO [0m:      Received: evaluate message 610ed5c9-62e3-46d9-ae90-06a1416f679c
02/15/2025 01:48:40:INFO:Received: evaluate message 610ed5c9-62e3-46d9-ae90-06a1416f679c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:22:INFO:
[92mINFO [0m:      Received: train message ec329fec-a51b-4680-8e77-530688b967da
02/15/2025 01:49:22:INFO:Received: train message ec329fec-a51b-4680-8e77-530688b967da

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:59:INFO:
[92mINFO [0m:      Received: evaluate message 6d651510-71dc-48d7-bacc-0f9a5841a1a8
02/15/2025 01:50:59:INFO:Received: evaluate message 6d651510-71dc-48d7-bacc-0f9a5841a1a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:51:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:25:INFO:
[92mINFO [0m:      Received: train message 715988f3-7aac-476b-9b99-6d28b55666d6
02/15/2025 01:51:25:INFO:Received: train message 715988f3-7aac-476b-9b99-6d28b55666d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:52:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:04:INFO:
[92mINFO [0m:      Received: evaluate message 86a659cf-1ea7-47c5-bb56-99f913f67977
02/15/2025 01:53:04:INFO:Received: evaluate message 86a659cf-1ea7-47c5-bb56-99f913f67977
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:53:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:50:INFO:
[92mINFO [0m:      Received: train message d00bb3c3-9ab3-4c75-bc7b-7446ce9789d6
02/15/2025 01:53:50:INFO:Received: train message d00bb3c3-9ab3-4c75-bc7b-7446ce9789d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:55:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:12:INFO:
[92mINFO [0m:      Received: evaluate message afb2786f-3b16-41d4-9971-004718a4b6a9
02/15/2025 01:56:12:INFO:Received: evaluate message afb2786f-3b16-41d4-9971-004718a4b6a9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:57:01:INFO:
[92mINFO [0m:      Received: train message 659ef101-7fdf-4760-8532-a0ee75476072
02/15/2025 01:57:01:INFO:Received: train message 659ef101-7fdf-4760-8532-a0ee75476072
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:58:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:41:INFO:
[92mINFO [0m:      Received: evaluate message f4eb504b-6994-4fc1-80a0-687781534977
02/15/2025 01:58:41:INFO:Received: evaluate message f4eb504b-6994-4fc1-80a0-687781534977
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:58:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:21:INFO:
[92mINFO [0m:      Received: train message a001fbaa-9c74-4531-aeb8-c2ca91c35651
02/15/2025 01:59:21:INFO:Received: train message a001fbaa-9c74-4531-aeb8-c2ca91c35651
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:00:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:02:03:INFO:
[92mINFO [0m:      Received: evaluate message be4905bd-ee52-4e8d-919c-caa6ab7793f8
02/15/2025 02:02:03:INFO:Received: evaluate message be4905bd-ee52-4e8d-919c-caa6ab7793f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:02:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:02:49:INFO:
[92mINFO [0m:      Received: train message 11a1885c-d7a8-4af1-9d8e-c32f5ff614e3
02/15/2025 02:02:49:INFO:Received: train message 11a1885c-d7a8-4af1-9d8e-c32f5ff614e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:03:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:51:INFO:
[92mINFO [0m:      Received: evaluate message 467cea14-abba-45ca-89d1-a0942694d09b
02/15/2025 02:04:51:INFO:Received: evaluate message 467cea14-abba-45ca-89d1-a0942694d09b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:05:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:27:INFO:
[92mINFO [0m:      Received: train message d0f3a0f0-d789-47d7-8273-0e7576611674
02/15/2025 02:05:27:INFO:Received: train message d0f3a0f0-d789-47d7-8273-0e7576611674
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:51:INFO:
[92mINFO [0m:      Received: evaluate message 0bfe1749-5a1a-429f-9104-44131088b7e6
02/15/2025 02:07:51:INFO:Received: evaluate message 0bfe1749-5a1a-429f-9104-44131088b7e6

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:08:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:25:INFO:
[92mINFO [0m:      Received: train message 56c00f0d-56f0-459a-8361-0f77ba1f2328
02/15/2025 02:08:25:INFO:Received: train message 56c00f0d-56f0-459a-8361-0f77ba1f2328
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:09:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:16:INFO:
[92mINFO [0m:      Received: evaluate message 173a262c-4996-49eb-9cb7-0cd28fc132ea
02/15/2025 02:10:16:INFO:Received: evaluate message 173a262c-4996-49eb-9cb7-0cd28fc132ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:10:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:11:03:INFO:
[92mINFO [0m:      Received: train message 3f78ef97-57d8-4d2b-a9bc-8d782ba7a833
02/15/2025 02:11:03:INFO:Received: train message 3f78ef97-57d8-4d2b-a9bc-8d782ba7a833
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:11:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:13:07:INFO:
[92mINFO [0m:      Received: evaluate message e9936a63-e6b2-4607-be64-53b008b61c2d
02/15/2025 02:13:07:INFO:Received: evaluate message e9936a63-e6b2-4607-be64-53b008b61c2d

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:13:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:13:44:INFO:
[92mINFO [0m:      Received: train message a2be02f3-9b60-424f-bee2-1d320efa0207
02/15/2025 02:13:44:INFO:Received: train message a2be02f3-9b60-424f-bee2-1d320efa0207
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:14:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:08:INFO:
[92mINFO [0m:      Received: evaluate message b48409d0-4e2c-4da0-875a-a378db294076
02/15/2025 02:15:08:INFO:Received: evaluate message b48409d0-4e2c-4da0-875a-a378db294076
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:15:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:12:INFO:
[92mINFO [0m:      Received: train message 738b7124-3703-464a-95f5-b533924010f3
02/15/2025 02:16:12:INFO:Received: train message 738b7124-3703-464a-95f5-b533924010f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:16:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:58:INFO:
[92mINFO [0m:      Received: evaluate message 9c60dbf0-f4d8-47d9-8eff-0c6de3bd32dd
02/15/2025 02:17:58:INFO:Received: evaluate message 9c60dbf0-f4d8-47d9-8eff-0c6de3bd32dd

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:18:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:37:INFO:
[92mINFO [0m:      Received: train message 40ed6a85-59ca-43b9-a496-01a9f0dec0cf
02/15/2025 02:18:37:INFO:Received: train message 40ed6a85-59ca-43b9-a496-01a9f0dec0cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:08:INFO:
[92mINFO [0m:      Received: evaluate message a43d5f7c-090c-4e1b-bdf6-b6bb1d89318c
02/15/2025 02:20:08:INFO:Received: evaluate message a43d5f7c-090c-4e1b-bdf6-b6bb1d89318c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:56:INFO:
[92mINFO [0m:      Received: train message 7f6d0a96-ffb6-406d-9741-3490bb1b688d
02/15/2025 02:20:56:INFO:Received: train message 7f6d0a96-ffb6-406d-9741-3490bb1b688d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:21:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:01:INFO:
[92mINFO [0m:      Received: evaluate message a08cfc4e-140b-4128-92d1-1036efeb26b4
02/15/2025 02:22:01:INFO:Received: evaluate message a08cfc4e-140b-4128-92d1-1036efeb26b4

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:22:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:56:INFO:
[92mINFO [0m:      Received: train message 53b229d9-bd1d-49ae-9ddb-1757314e4352
02/15/2025 02:22:56:INFO:Received: train message 53b229d9-bd1d-49ae-9ddb-1757314e4352
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:23:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:24:24:INFO:
[92mINFO [0m:      Received: evaluate message 385e881d-326c-47f2-bdc6-4a224ebdf0c2
02/15/2025 02:24:24:INFO:Received: evaluate message 385e881d-326c-47f2-bdc6-4a224ebdf0c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:24:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:24:48:INFO:
[92mINFO [0m:      Received: train message 01f0f713-1bc9-442d-b38d-a00a9ceaa956
02/15/2025 02:24:48:INFO:Received: train message 01f0f713-1bc9-442d-b38d-a00a9ceaa956
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:25:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:26:31:INFO:
[92mINFO [0m:      Received: evaluate message d64a8ae3-dc42-4710-97ce-3e539ef990aa
02/15/2025 02:26:31:INFO:Received: evaluate message d64a8ae3-dc42-4710-97ce-3e539ef990aa

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:26:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:27:09:INFO:
[92mINFO [0m:      Received: train message b343b865-dfb1-4a76-9917-610a1367faf2
02/15/2025 02:27:09:INFO:Received: train message b343b865-dfb1-4a76-9917-610a1367faf2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:28:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:29:39:INFO:
[92mINFO [0m:      Received: evaluate message 3bc5fe31-9b0b-45fa-bb40-71dd76e6a980
02/15/2025 02:29:39:INFO:Received: evaluate message 3bc5fe31-9b0b-45fa-bb40-71dd76e6a980
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:29:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:30:29:INFO:
[92mINFO [0m:      Received: train message f2d79c61-1138-4ee2-84ec-4b5c2f3d8dfd
02/15/2025 02:30:29:INFO:Received: train message f2d79c61-1138-4ee2-84ec-4b5c2f3d8dfd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:31:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:31:52:INFO:
[92mINFO [0m:      Received: evaluate message 05592b87-6489-49ad-a4af-f087939e8167
02/15/2025 02:31:52:INFO:Received: evaluate message 05592b87-6489-49ad-a4af-f087939e8167

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:32:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:32:36:INFO:
[92mINFO [0m:      Received: train message 4ee77c1c-c038-4fd8-80e0-c487ad2bacfb
02/15/2025 02:32:36:INFO:Received: train message 4ee77c1c-c038-4fd8-80e0-c487ad2bacfb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:33:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:35:18:INFO:
[92mINFO [0m:      Received: evaluate message 205a3095-580a-4acc-bd9e-fad9c08fb145
02/15/2025 02:35:18:INFO:Received: evaluate message 205a3095-580a-4acc-bd9e-fad9c08fb145
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:35:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:36:01:INFO:
[92mINFO [0m:      Received: train message 10556221-1996-41d4-aea0-e4565c6da166
02/15/2025 02:36:01:INFO:Received: train message 10556221-1996-41d4-aea0-e4565c6da166
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:37:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:37:49:INFO:
[92mINFO [0m:      Received: evaluate message 86427295-3e93-4ae7-a688-6a745df1e95f
02/15/2025 02:37:49:INFO:Received: evaluate message 86427295-3e93-4ae7-a688-6a745df1e95f

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:37:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:38:02:INFO:
[92mINFO [0m:      Received: reconnect message f08754e7-8ddb-432d-90a8-380e858454aa
02/15/2025 02:38:02:INFO:Received: reconnect message f08754e7-8ddb-432d-90a8-380e858454aa
02/15/2025 02:38:02:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:38:02:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}



Final client history:
{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}

