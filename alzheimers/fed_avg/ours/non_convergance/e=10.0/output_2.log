nohup: ignoring input
02/15/2025 01:17:45:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:17:45:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:17:45:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 01:17:45:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739611065.611103 1666763 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:18:17:INFO:
[92mINFO [0m:      Received: train message 6532c998-ab8e-42c0-9c49-96b78c7d4c35
02/15/2025 01:18:17:INFO:Received: train message 6532c998-ab8e-42c0-9c49-96b78c7d4c35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:04:INFO:
[92mINFO [0m:      Received: evaluate message 613d7410-0208-42d0-967c-eff4b4054100
02/15/2025 01:21:04:INFO:Received: evaluate message 613d7410-0208-42d0-967c-eff4b4054100
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:21:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:02:INFO:
[92mINFO [0m:      Received: train message abfa068d-a590-4aad-a70f-928aca423b48
02/15/2025 01:22:02:INFO:Received: train message abfa068d-a590-4aad-a70f-928aca423b48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:23:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:29:INFO:
[92mINFO [0m:      Received: evaluate message 7e5517e6-a9cc-40e6-bf98-8e60e339ca24
02/15/2025 01:24:29:INFO:Received: evaluate message 7e5517e6-a9cc-40e6-bf98-8e60e339ca24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:24:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:52:INFO:
[92mINFO [0m:      Received: train message a86a042f-9aff-4e5a-8102-ffe0d948bfe8
02/15/2025 01:24:52:INFO:Received: train message a86a042f-9aff-4e5a-8102-ffe0d948bfe8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:25:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:42:INFO:
[92mINFO [0m:      Received: evaluate message b063b64e-2605-4c2e-9a86-3b436803d063
02/15/2025 01:27:42:INFO:Received: evaluate message b063b64e-2605-4c2e-9a86-3b436803d063
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:22:INFO:
[92mINFO [0m:      Received: train message 55d9d9bf-9b71-4786-a43c-0c81e75905a0
02/15/2025 01:28:22:INFO:Received: train message 55d9d9bf-9b71-4786-a43c-0c81e75905a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:29:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:52:INFO:
[92mINFO [0m:      Received: evaluate message 800affd9-45c1-4643-abba-57c97fd80c08
02/15/2025 01:30:52:INFO:Received: evaluate message 800affd9-45c1-4643-abba-57c97fd80c08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:31:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:38:INFO:
[92mINFO [0m:      Received: train message 0db251dd-fb5b-4bb4-8632-9e6a4e84dc2a
02/15/2025 01:31:38:INFO:Received: train message 0db251dd-fb5b-4bb4-8632-9e6a4e84dc2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:32:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:33:INFO:
[92mINFO [0m:      Received: evaluate message 1608a376-f75b-4091-abc2-007020fb740d
02/15/2025 01:33:33:INFO:Received: evaluate message 1608a376-f75b-4091-abc2-007020fb740d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:21:INFO:
[92mINFO [0m:      Received: train message 1d307c2f-7039-4cb0-b3dd-4c53e0da76fd
02/15/2025 01:34:21:INFO:Received: train message 1d307c2f-7039-4cb0-b3dd-4c53e0da76fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:26:INFO:
[92mINFO [0m:      Received: evaluate message 9a597a43-b7ea-4fb1-b341-3ad1571060df
02/15/2025 01:36:26:INFO:Received: evaluate message 9a597a43-b7ea-4fb1-b341-3ad1571060df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:17:INFO:
[92mINFO [0m:      Received: train message 45b625d7-a672-4afb-9ea7-e99ed61dd54e
02/15/2025 01:37:17:INFO:Received: train message 45b625d7-a672-4afb-9ea7-e99ed61dd54e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:38:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:12:INFO:
[92mINFO [0m:      Received: evaluate message 1b7eccd5-11a0-4035-a5c1-77132a080aef
02/15/2025 01:39:12:INFO:Received: evaluate message 1b7eccd5-11a0-4035-a5c1-77132a080aef
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906], 'accuracy': [0.5027365129007036], 'auc': [0.6897600584676676], 'precision': [0.3991915055150511], 'recall': [0.5027365129007036], 'f1': [0.3614852344286372]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294], 'accuracy': [0.5027365129007036, 0.524628616106333], 'auc': [0.6897600584676676, 0.7198632289872761], 'precision': [0.3991915055150511, 0.4206049829674183], 'recall': [0.5027365129007036, 0.524628616106333], 'f1': [0.3614852344286372, 0.4422280621916187]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:59:INFO:
[92mINFO [0m:      Received: train message 1d05b06d-82f5-4d47-bf52-7ce137e4eef9
02/15/2025 01:39:59:INFO:Received: train message 1d05b06d-82f5-4d47-bf52-7ce137e4eef9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:26:INFO:
[92mINFO [0m:      Received: evaluate message c94f2a1a-46f7-4243-9633-28a6f8c5bd82
02/15/2025 01:41:26:INFO:Received: evaluate message c94f2a1a-46f7-4243-9633-28a6f8c5bd82
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:20:INFO:
[92mINFO [0m:      Received: train message 00d8acb1-6ef9-456c-8f5d-fa179bf59597
02/15/2025 01:42:20:INFO:Received: train message 00d8acb1-6ef9-456c-8f5d-fa179bf59597
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:42:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:51:INFO:
[92mINFO [0m:      Received: evaluate message 6dc72e9c-834b-4852-9d71-6183b975c445
02/15/2025 01:43:51:INFO:Received: evaluate message 6dc72e9c-834b-4852-9d71-6183b975c445
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:35:INFO:
[92mINFO [0m:      Received: train message c64888bd-e3a4-4e66-bfc8-99e614ac2319
02/15/2025 01:44:35:INFO:Received: train message c64888bd-e3a4-4e66-bfc8-99e614ac2319
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:00:INFO:
[92mINFO [0m:      Received: evaluate message b2ebade6-9377-4e3c-8acb-76761dc49fec
02/15/2025 01:46:00:INFO:Received: evaluate message b2ebade6-9377-4e3c-8acb-76761dc49fec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:57:INFO:
[92mINFO [0m:      Received: train message 41109337-b537-4119-ae58-41e2e6fe5cd5
02/15/2025 01:46:57:INFO:Received: train message 41109337-b537-4119-ae58-41e2e6fe5cd5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:36:INFO:
[92mINFO [0m:      Received: evaluate message 7e23b9a6-4017-468f-8876-df6a62799457
02/15/2025 01:48:36:INFO:Received: evaluate message 7e23b9a6-4017-468f-8876-df6a62799457
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:16:INFO:
[92mINFO [0m:      Received: train message a2557388-1872-4f0f-b883-713131aa1c98
02/15/2025 01:49:16:INFO:Received: train message a2557388-1872-4f0f-b883-713131aa1c98

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:49:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:01:INFO:
[92mINFO [0m:      Received: evaluate message fe8fb78c-a22a-4436-86fa-1483166d3598
02/15/2025 01:51:01:INFO:Received: evaluate message fe8fb78c-a22a-4436-86fa-1483166d3598
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:51:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:21:INFO:
[92mINFO [0m:      Received: train message 632f3cc5-859a-44f2-b6dd-700d27653938
02/15/2025 01:51:21:INFO:Received: train message 632f3cc5-859a-44f2-b6dd-700d27653938
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:51:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:08:INFO:
[92mINFO [0m:      Received: evaluate message 575b5e83-ca4d-4011-9149-5b349c867c4e
02/15/2025 01:53:08:INFO:Received: evaluate message 575b5e83-ca4d-4011-9149-5b349c867c4e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:53:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:46:INFO:
[92mINFO [0m:      Received: train message 701385a5-2d60-4fd8-a4c7-7c14f729a460
02/15/2025 01:53:46:INFO:Received: train message 701385a5-2d60-4fd8-a4c7-7c14f729a460
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:54:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:55:56:INFO:
[92mINFO [0m:      Received: evaluate message f3fcf968-4834-4e96-8bb0-d49b432a7881
02/15/2025 01:55:56:INFO:Received: evaluate message f3fcf968-4834-4e96-8bb0-d49b432a7881
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:54:INFO:
[92mINFO [0m:      Received: train message f92e9a32-af4f-4f1f-a668-bfdd79c6bcbc
02/15/2025 01:56:54:INFO:Received: train message f92e9a32-af4f-4f1f-a668-bfdd79c6bcbc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:57:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:41:INFO:
[92mINFO [0m:      Received: evaluate message 48fb2275-c7cc-4da3-81f1-6f16abdb947b
02/15/2025 01:58:41:INFO:Received: evaluate message 48fb2275-c7cc-4da3-81f1-6f16abdb947b
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:58:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:40:INFO:
[92mINFO [0m:      Received: train message b32b18c2-f6f7-4247-9f81-c23428f016da
02/15/2025 01:59:40:INFO:Received: train message b32b18c2-f6f7-4247-9f81-c23428f016da
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:00:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:02:01:INFO:
[92mINFO [0m:      Received: evaluate message f973a98c-c513-41df-81ca-5450deb63583
02/15/2025 02:02:01:INFO:Received: evaluate message f973a98c-c513-41df-81ca-5450deb63583
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:02:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:02:32:INFO:
[92mINFO [0m:      Received: train message 07bae656-0cee-490c-b2c6-328a54ba74e2
02/15/2025 02:02:32:INFO:Received: train message 07bae656-0cee-490c-b2c6-328a54ba74e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:03:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:25:INFO:
[92mINFO [0m:      Received: evaluate message db84415e-4cdd-4ded-902e-ce03f2180f91
02/15/2025 02:04:25:INFO:Received: evaluate message db84415e-4cdd-4ded-902e-ce03f2180f91
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:04:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:31:INFO:
[92mINFO [0m:      Received: train message 55fb9be1-e3ab-4c90-909a-d270b4be40a6
02/15/2025 02:05:31:INFO:Received: train message 55fb9be1-e3ab-4c90-909a-d270b4be40a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:44:INFO:
[92mINFO [0m:      Received: evaluate message 3a01f087-9961-4adf-90f2-4e7e8d8c2969
02/15/2025 02:07:44:INFO:Received: evaluate message 3a01f087-9961-4adf-90f2-4e7e8d8c2969

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:35:INFO:
[92mINFO [0m:      Received: train message a4143e65-ee50-47e7-8693-2b7d64665cd7
02/15/2025 02:08:35:INFO:Received: train message a4143e65-ee50-47e7-8693-2b7d64665cd7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:09:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:27:INFO:
[92mINFO [0m:      Received: evaluate message 674ed86b-be58-48b3-9610-c45b9c05cdd0
02/15/2025 02:10:27:INFO:Received: evaluate message 674ed86b-be58-48b3-9610-c45b9c05cdd0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:10:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:56:INFO:
[92mINFO [0m:      Received: train message 04eecd3c-60fe-4d69-9dcd-b132d2427f13
02/15/2025 02:10:56:INFO:Received: train message 04eecd3c-60fe-4d69-9dcd-b132d2427f13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:11:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:53:INFO:
[92mINFO [0m:      Received: evaluate message 3d2ffafa-c098-4ca3-a858-75453b6ef1ef
02/15/2025 02:12:53:INFO:Received: evaluate message 3d2ffafa-c098-4ca3-a858-75453b6ef1ef

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:12:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:13:34:INFO:
[92mINFO [0m:      Received: train message ba56341d-5123-4b3e-b172-48c93ead6a37
02/15/2025 02:13:34:INFO:Received: train message ba56341d-5123-4b3e-b172-48c93ead6a37
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:14:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:33:INFO:
[92mINFO [0m:      Received: evaluate message b54325b6-6d55-430c-aad8-d2a08221c02f
02/15/2025 02:15:33:INFO:Received: evaluate message b54325b6-6d55-430c-aad8-d2a08221c02f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:15:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:51:INFO:
[92mINFO [0m:      Received: train message f81dec6c-d0c7-40a9-9c69-f695b9e57884
02/15/2025 02:15:51:INFO:Received: train message f81dec6c-d0c7-40a9-9c69-f695b9e57884
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:16:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:48:INFO:
[92mINFO [0m:      Received: evaluate message bbe1ab6a-ba46-428f-93ae-60f453d02ebd
02/15/2025 02:17:48:INFO:Received: evaluate message bbe1ab6a-ba46-428f-93ae-60f453d02ebd

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:17:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:35:INFO:
[92mINFO [0m:      Received: train message 3c0e3e5d-074e-4562-a465-5a3a286c9f30
02/15/2025 02:18:35:INFO:Received: train message 3c0e3e5d-074e-4562-a465-5a3a286c9f30
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:19:59:INFO:
[92mINFO [0m:      Received: evaluate message 9d6c12e6-725d-4eda-bdb8-aa15987c30e5
02/15/2025 02:19:59:INFO:Received: evaluate message 9d6c12e6-725d-4eda-bdb8-aa15987c30e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:54:INFO:
[92mINFO [0m:      Received: train message 62bceec9-7039-4f0c-9fa2-1b3971496083
02/15/2025 02:20:54:INFO:Received: train message 62bceec9-7039-4f0c-9fa2-1b3971496083
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:21:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:05:INFO:
[92mINFO [0m:      Received: evaluate message 7a4d2711-c574-4a0a-a1a4-fd8b6acff2b9
02/15/2025 02:22:05:INFO:Received: evaluate message 7a4d2711-c574-4a0a-a1a4-fd8b6acff2b9

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:22:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:42:INFO:
[92mINFO [0m:      Received: train message b78a050b-8b72-4b27-8a9c-b4c5905897bd
02/15/2025 02:22:42:INFO:Received: train message b78a050b-8b72-4b27-8a9c-b4c5905897bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:23:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:24:05:INFO:
[92mINFO [0m:      Received: evaluate message e371365f-38ac-418c-9d8e-6ffd1ef18f4b
02/15/2025 02:24:05:INFO:Received: evaluate message e371365f-38ac-418c-9d8e-6ffd1ef18f4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:24:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:25:04:INFO:
[92mINFO [0m:      Received: train message e7d2ef63-f93b-4e70-8056-355a52ac3949
02/15/2025 02:25:04:INFO:Received: train message e7d2ef63-f93b-4e70-8056-355a52ac3949
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:25:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:26:18:INFO:
[92mINFO [0m:      Received: evaluate message a600ae44-71fc-4df1-8eb7-4c73b5252563
02/15/2025 02:26:18:INFO:Received: evaluate message a600ae44-71fc-4df1-8eb7-4c73b5252563

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:26:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:27:18:INFO:
[92mINFO [0m:      Received: train message f1d2c237-9413-4c7a-9ca5-b63e88455d60
02/15/2025 02:27:18:INFO:Received: train message f1d2c237-9413-4c7a-9ca5-b63e88455d60
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:28:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:29:39:INFO:
[92mINFO [0m:      Received: evaluate message 303efb22-3780-4058-8f9d-98ab4fbe6307
02/15/2025 02:29:39:INFO:Received: evaluate message 303efb22-3780-4058-8f9d-98ab4fbe6307
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:29:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:30:28:INFO:
[92mINFO [0m:      Received: train message f7dd3c64-eb8a-43cb-9ed3-3d28016f84b6
02/15/2025 02:30:28:INFO:Received: train message f7dd3c64-eb8a-43cb-9ed3-3d28016f84b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:31:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:32:00:INFO:
[92mINFO [0m:      Received: evaluate message 287eb6a4-9c96-4ebc-9524-f0e2096e520e
02/15/2025 02:32:00:INFO:Received: evaluate message 287eb6a4-9c96-4ebc-9524-f0e2096e520e

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:32:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:32:49:INFO:
[92mINFO [0m:      Received: train message b1dd5969-1db3-4991-b77c-7696bdafed37
02/15/2025 02:32:49:INFO:Received: train message b1dd5969-1db3-4991-b77c-7696bdafed37
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:33:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:35:09:INFO:
[92mINFO [0m:      Received: evaluate message 8f7db41c-7187-4158-b0c3-56528bdf566d
02/15/2025 02:35:09:INFO:Received: evaluate message 8f7db41c-7187-4158-b0c3-56528bdf566d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:35:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:36:03:INFO:
[92mINFO [0m:      Received: train message 844c1a24-5e76-4150-92b0-b9bb8ea14d2a
02/15/2025 02:36:03:INFO:Received: train message 844c1a24-5e76-4150-92b0-b9bb8ea14d2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:36:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:37:41:INFO:
[92mINFO [0m:      Received: evaluate message d8f7bad6-9da8-4857-9bfc-ae3a03b169b8
02/15/2025 02:37:41:INFO:Received: evaluate message d8f7bad6-9da8-4857-9bfc-ae3a03b169b8

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:37:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:38:02:INFO:
[92mINFO [0m:      Received: reconnect message e5db7a60-9cf3-42ca-958c-0638b318c300
02/15/2025 02:38:02:INFO:Received: reconnect message e5db7a60-9cf3-42ca-958c-0638b318c300
02/15/2025 02:38:02:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:38:02:INFO:Disconnect and shut down

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}



Final client history:
{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}

