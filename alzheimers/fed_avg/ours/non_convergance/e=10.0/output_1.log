nohup: ignoring input
02/15/2025 01:17:50:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:17:50:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:17:50:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 01:17:50:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739611070.250821 1667058 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:18:27:INFO:
[92mINFO [0m:      Received: train message 99e845b9-0a38-4f95-bfff-2c07434d3bba
02/15/2025 01:18:27:INFO:Received: train message 99e845b9-0a38-4f95-bfff-2c07434d3bba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:20:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:03:INFO:
[92mINFO [0m:      Received: evaluate message c0e42016-2b58-4829-8cdf-02e0f00176ee
02/15/2025 01:21:03:INFO:Received: evaluate message c0e42016-2b58-4829-8cdf-02e0f00176ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:21:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:03:INFO:
[92mINFO [0m:      Received: train message abd6cd55-1fe5-493c-a26b-2b0ac4daccff
02/15/2025 01:22:03:INFO:Received: train message abd6cd55-1fe5-493c-a26b-2b0ac4daccff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:23:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:24:INFO:
[92mINFO [0m:      Received: evaluate message 635fbc7f-9664-49b7-a27d-a685f8665a9e
02/15/2025 01:24:24:INFO:Received: evaluate message 635fbc7f-9664-49b7-a27d-a685f8665a9e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:24:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:18:INFO:
[92mINFO [0m:      Received: train message b0a5c366-9c26-4318-8960-a3ac196acaf8
02/15/2025 01:25:18:INFO:Received: train message b0a5c366-9c26-4318-8960-a3ac196acaf8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:27:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:44:INFO:
[92mINFO [0m:      Received: evaluate message 79a93e97-51d3-4f0a-ba7d-b94cf11dfdcd
02/15/2025 01:27:44:INFO:Received: evaluate message 79a93e97-51d3-4f0a-ba7d-b94cf11dfdcd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:37:INFO:
[92mINFO [0m:      Received: train message 1d49f81d-95f2-4ee4-bb51-d495e62c96fd
02/15/2025 01:28:37:INFO:Received: train message 1d49f81d-95f2-4ee4-bb51-d495e62c96fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:30:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:28:INFO:
[92mINFO [0m:      Received: evaluate message 9f578452-08af-4727-96ef-bbf3dc54b84f
02/15/2025 01:30:28:INFO:Received: evaluate message 9f578452-08af-4727-96ef-bbf3dc54b84f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:30:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:30:INFO:
[92mINFO [0m:      Received: train message ff107e30-ca93-4ce0-ae82-70adc50ccac1
02/15/2025 01:31:30:INFO:Received: train message ff107e30-ca93-4ce0-ae82-70adc50ccac1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:23:INFO:
[92mINFO [0m:      Received: evaluate message 5f0821f4-aa53-45b3-95fd-f9460000380b
02/15/2025 01:33:23:INFO:Received: evaluate message 5f0821f4-aa53-45b3-95fd-f9460000380b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:29:INFO:
[92mINFO [0m:      Received: train message 90bce161-cbe7-4aa6-89a8-82dab9e534a4
02/15/2025 01:34:29:INFO:Received: train message 90bce161-cbe7-4aa6-89a8-82dab9e534a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:31:INFO:
[92mINFO [0m:      Received: evaluate message c0692d43-5d23-46c3-bda3-e3f1c0219f55
02/15/2025 01:36:31:INFO:Received: evaluate message c0692d43-5d23-46c3-bda3-e3f1c0219f55
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:10:INFO:
[92mINFO [0m:      Received: train message 3724ed91-65f7-4b12-af1a-5b4dd5a43015
02/15/2025 01:37:10:INFO:Received: train message 3724ed91-65f7-4b12-af1a-5b4dd5a43015
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:38:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:56:INFO:
[92mINFO [0m:      Received: evaluate message 8aab99d3-1213-40d8-8c61-5740c50214c9
02/15/2025 01:38:56:INFO:Received: evaluate message 8aab99d3-1213-40d8-8c61-5740c50214c9
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906], 'accuracy': [0.5027365129007036], 'auc': [0.6897600584676676], 'precision': [0.3991915055150511], 'recall': [0.5027365129007036], 'f1': [0.3614852344286372]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294], 'accuracy': [0.5027365129007036, 0.524628616106333], 'auc': [0.6897600584676676, 0.7198632289872761], 'precision': [0.3991915055150511, 0.4206049829674183], 'recall': [0.5027365129007036, 0.524628616106333], 'f1': [0.3614852344286372, 0.4422280621916187]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:59:INFO:
[92mINFO [0m:      Received: train message ac395e2e-4df8-470b-8cf4-fa7ae645e082
02/15/2025 01:39:59:INFO:Received: train message ac395e2e-4df8-470b-8cf4-fa7ae645e082
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:40:INFO:
[92mINFO [0m:      Received: evaluate message b497ec54-68ce-4fb2-8685-258880e474e4
02/15/2025 01:41:40:INFO:Received: evaluate message b497ec54-68ce-4fb2-8685-258880e474e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:15:INFO:
[92mINFO [0m:      Received: train message b1f67dc3-52fb-430e-bfdd-4f098b5913fe
02/15/2025 01:42:15:INFO:Received: train message b1f67dc3-52fb-430e-bfdd-4f098b5913fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:43:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:44:INFO:
[92mINFO [0m:      Received: evaluate message ff680407-2c12-4073-bfad-0fa800f951b5
02/15/2025 01:43:44:INFO:Received: evaluate message ff680407-2c12-4073-bfad-0fa800f951b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:28:INFO:
[92mINFO [0m:      Received: train message 5dc7d334-65cf-4719-8b1a-1066b679a56d
02/15/2025 01:44:28:INFO:Received: train message 5dc7d334-65cf-4719-8b1a-1066b679a56d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:12:INFO:
[92mINFO [0m:      Received: evaluate message 5f89fcc4-82d5-4796-ae53-c2e8d24dd959
02/15/2025 01:46:12:INFO:Received: evaluate message 5f89fcc4-82d5-4796-ae53-c2e8d24dd959
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:54:INFO:
[92mINFO [0m:      Received: train message 958c9647-8384-4a28-a9bb-e36f0082cdd3
02/15/2025 01:46:54:INFO:Received: train message 958c9647-8384-4a28-a9bb-e36f0082cdd3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:25:INFO:
[92mINFO [0m:      Received: evaluate message 9e491448-0789-434b-b43b-1063845a55a0
02/15/2025 01:48:25:INFO:Received: evaluate message 9e491448-0789-434b-b43b-1063845a55a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:09:INFO:
[92mINFO [0m:      Received: train message cbe2b665-aa87-4b68-9a08-faed924d927f
02/15/2025 01:49:09:INFO:Received: train message cbe2b665-aa87-4b68-9a08-faed924d927f

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:48:INFO:
[92mINFO [0m:      Received: evaluate message cb00ddaf-102f-488d-8680-94eeb55fd7e7
02/15/2025 01:50:48:INFO:Received: evaluate message cb00ddaf-102f-488d-8680-94eeb55fd7e7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:25:INFO:
[92mINFO [0m:      Received: train message 01830961-d972-46d1-99a2-8507b5838f1f
02/15/2025 01:51:25:INFO:Received: train message 01830961-d972-46d1-99a2-8507b5838f1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:52:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:10:INFO:
[92mINFO [0m:      Received: evaluate message 3706ccde-045e-4a75-8d12-55d56c581cd0
02/15/2025 01:53:10:INFO:Received: evaluate message 3706ccde-045e-4a75-8d12-55d56c581cd0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:53:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:48:INFO:
[92mINFO [0m:      Received: train message 95862fb5-9974-40b6-a46b-6e3acfb749fe
02/15/2025 01:53:48:INFO:Received: train message 95862fb5-9974-40b6-a46b-6e3acfb749fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:55:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:09:INFO:
[92mINFO [0m:      Received: evaluate message 469928fc-a675-49b6-a2ef-943f167b9d71
02/15/2025 01:56:09:INFO:Received: evaluate message 469928fc-a675-49b6-a2ef-943f167b9d71
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:50:INFO:
[92mINFO [0m:      Received: train message 768c3c1b-03f0-4717-9228-5b817ec379ba
02/15/2025 01:56:50:INFO:Received: train message 768c3c1b-03f0-4717-9228-5b817ec379ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:58:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:56:INFO:
[92mINFO [0m:      Received: evaluate message da64f081-a8e4-4d38-8a06-61a036b6d826
02/15/2025 01:58:56:INFO:Received: evaluate message da64f081-a8e4-4d38-8a06-61a036b6d826
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:59:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:32:INFO:
[92mINFO [0m:      Received: train message a332c365-f1e8-44f9-b4ae-8db43b268750
02/15/2025 01:59:32:INFO:Received: train message a332c365-f1e8-44f9-b4ae-8db43b268750
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:01:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:55:INFO:
[92mINFO [0m:      Received: evaluate message 95985402-9190-45f6-a825-adb369c297c7
02/15/2025 02:01:55:INFO:Received: evaluate message 95985402-9190-45f6-a825-adb369c297c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:02:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:02:41:INFO:
[92mINFO [0m:      Received: train message 41f881d7-12d1-4e65-a413-129b63c60ce3
02/15/2025 02:02:41:INFO:Received: train message 41f881d7-12d1-4e65-a413-129b63c60ce3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:04:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:43:INFO:
[92mINFO [0m:      Received: evaluate message e4592f08-d9a9-4571-994a-7500d03c6b16
02/15/2025 02:04:43:INFO:Received: evaluate message e4592f08-d9a9-4571-994a-7500d03c6b16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:04:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:35:INFO:
[92mINFO [0m:      Received: train message 26ee0480-26e7-435e-ac21-78235fe7da59
02/15/2025 02:05:35:INFO:Received: train message 26ee0480-26e7-435e-ac21-78235fe7da59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:07:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:49:INFO:
[92mINFO [0m:      Received: evaluate message 86879926-cbb7-4c93-9aa6-764154ec3337
02/15/2025 02:07:49:INFO:Received: evaluate message 86879926-cbb7-4c93-9aa6-764154ec3337

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:28:INFO:
[92mINFO [0m:      Received: train message 94897beb-d3ea-4640-bbf5-2fbee7f1e90b
02/15/2025 02:08:28:INFO:Received: train message 94897beb-d3ea-4640-bbf5-2fbee7f1e90b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:09:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:15:INFO:
[92mINFO [0m:      Received: evaluate message 928db5df-18e7-475c-bf6c-beff45a82061
02/15/2025 02:10:15:INFO:Received: evaluate message 928db5df-18e7-475c-bf6c-beff45a82061
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:10:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:56:INFO:
[92mINFO [0m:      Received: train message a930a981-5656-4be0-88c1-c9e8a3ed4e6c
02/15/2025 02:10:56:INFO:Received: train message a930a981-5656-4be0-88c1-c9e8a3ed4e6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:12:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:48:INFO:
[92mINFO [0m:      Received: evaluate message a512a146-ed25-46ce-a3ef-5e843a36e5f9
02/15/2025 02:12:48:INFO:Received: evaluate message a512a146-ed25-46ce-a3ef-5e843a36e5f9

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:12:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:13:51:INFO:
[92mINFO [0m:      Received: train message 1c30d3dc-2d4b-4e0b-8cc1-dd586b6258b2
02/15/2025 02:13:51:INFO:Received: train message 1c30d3dc-2d4b-4e0b-8cc1-dd586b6258b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:14:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:16:INFO:
[92mINFO [0m:      Received: evaluate message 3d83388a-8ee2-42ff-a6e6-3a9a63c78b72
02/15/2025 02:15:16:INFO:Received: evaluate message 3d83388a-8ee2-42ff-a6e6-3a9a63c78b72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:15:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:14:INFO:
[92mINFO [0m:      Received: train message 0fdf3487-d0b9-4d77-913e-a123f73431d7
02/15/2025 02:16:14:INFO:Received: train message 0fdf3487-d0b9-4d77-913e-a123f73431d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:17:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:40:INFO:
[92mINFO [0m:      Received: evaluate message 7fea4c41-f00c-4d9c-ae29-1837928c45a9
02/15/2025 02:17:40:INFO:Received: evaluate message 7fea4c41-f00c-4d9c-ae29-1837928c45a9

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:17:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:31:INFO:
[92mINFO [0m:      Received: train message 76422e70-f5aa-49cb-bdd1-6185065bda79
02/15/2025 02:18:31:INFO:Received: train message 76422e70-f5aa-49cb-bdd1-6185065bda79
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:19:58:INFO:
[92mINFO [0m:      Received: evaluate message a4df8a7d-3302-4fbc-a1e2-3cd3636eef09
02/15/2025 02:19:58:INFO:Received: evaluate message a4df8a7d-3302-4fbc-a1e2-3cd3636eef09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:44:INFO:
[92mINFO [0m:      Received: train message 39187cdd-015a-4a2b-b89c-063d0e42f6c7
02/15/2025 02:20:44:INFO:Received: train message 39187cdd-015a-4a2b-b89c-063d0e42f6c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:21:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:20:INFO:
[92mINFO [0m:      Received: evaluate message eab2cb2f-eb4f-4de7-8c15-5bd7eee0e959
02/15/2025 02:22:20:INFO:Received: evaluate message eab2cb2f-eb4f-4de7-8c15-5bd7eee0e959

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:22:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:34:INFO:
[92mINFO [0m:      Received: train message 93562ea7-bdb0-43b6-a62f-51bf913f4b6a
02/15/2025 02:22:34:INFO:Received: train message 93562ea7-bdb0-43b6-a62f-51bf913f4b6a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:23:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:24:18:INFO:
[92mINFO [0m:      Received: evaluate message 47ff7f3a-9e8e-4bd8-bd8a-a34864baf1c6
02/15/2025 02:24:18:INFO:Received: evaluate message 47ff7f3a-9e8e-4bd8-bd8a-a34864baf1c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:24:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:25:02:INFO:
[92mINFO [0m:      Received: train message 1263a06d-000a-4193-99b8-344d429bb6da
02/15/2025 02:25:02:INFO:Received: train message 1263a06d-000a-4193-99b8-344d429bb6da
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:25:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:26:29:INFO:
[92mINFO [0m:      Received: evaluate message f714653f-79a7-4aa2-b14f-c8691509d02b
02/15/2025 02:26:29:INFO:Received: evaluate message f714653f-79a7-4aa2-b14f-c8691509d02b

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:26:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:27:23:INFO:
[92mINFO [0m:      Received: train message 331c539d-c5b5-4594-834f-43dc99724e23
02/15/2025 02:27:23:INFO:Received: train message 331c539d-c5b5-4594-834f-43dc99724e23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:29:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:29:41:INFO:
[92mINFO [0m:      Received: evaluate message a1433068-c96c-454a-b3fd-e6bd10349ee4
02/15/2025 02:29:41:INFO:Received: evaluate message a1433068-c96c-454a-b3fd-e6bd10349ee4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:29:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:30:31:INFO:
[92mINFO [0m:      Received: train message bd9bd74a-d439-4d0b-97a4-94a9a469d372
02/15/2025 02:30:31:INFO:Received: train message bd9bd74a-d439-4d0b-97a4-94a9a469d372
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:31:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:31:55:INFO:
[92mINFO [0m:      Received: evaluate message 6144f5ec-ffcd-4f25-b433-56ef0d795e28
02/15/2025 02:31:55:INFO:Received: evaluate message 6144f5ec-ffcd-4f25-b433-56ef0d795e28

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:32:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:32:51:INFO:
[92mINFO [0m:      Received: train message 31e7e241-0aec-40cc-ba3b-54a4147d0e37
02/15/2025 02:32:51:INFO:Received: train message 31e7e241-0aec-40cc-ba3b-54a4147d0e37
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:34:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:35:16:INFO:
[92mINFO [0m:      Received: evaluate message 4e21e9ca-01ff-414d-bae3-c0a4ab74f9cb
02/15/2025 02:35:16:INFO:Received: evaluate message 4e21e9ca-01ff-414d-bae3-c0a4ab74f9cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:35:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:35:38:INFO:
[92mINFO [0m:      Received: train message 3a463414-4e4a-4edf-8814-873c590193f7
02/15/2025 02:35:38:INFO:Received: train message 3a463414-4e4a-4edf-8814-873c590193f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:37:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:37:50:INFO:
[92mINFO [0m:      Received: evaluate message 1130f8d0-0230-4fa4-8cf3-f193a6351b71
02/15/2025 02:37:50:INFO:Received: evaluate message 1130f8d0-0230-4fa4-8cf3-f193a6351b71

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:38:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:38:02:INFO:
[92mINFO [0m:      Received: reconnect message 91f3ea92-3275-42c1-8a82-3b03e6ad412d
02/15/2025 02:38:02:INFO:Received: reconnect message 91f3ea92-3275-42c1-8a82-3b03e6ad412d
02/15/2025 02:38:02:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:38:02:INFO:Disconnect and shut down
