nohup: ignoring input
02/15/2025 01:17:44:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:17:44:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:17:44:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 01:17:44:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/15/2025 01:17:44:INFO:
[92mINFO [0m:      Received: get_parameters message 7abc506d-46fa-4c3f-89c2-e0259dce37c8
02/15/2025 01:17:44:INFO:Received: get_parameters message 7abc506d-46fa-4c3f-89c2-e0259dce37c8
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739611064.534175 1666580 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      Sent reply
02/15/2025 01:17:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:26:INFO:
[92mINFO [0m:      Received: train message f04a9178-4818-41c2-8e8a-6d4f7f04395b
02/15/2025 01:18:26:INFO:Received: train message f04a9178-4818-41c2-8e8a-6d4f7f04395b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:46:INFO:
[92mINFO [0m:      Received: evaluate message 746b9e84-7668-42a6-a7bb-ed278073cc21
02/15/2025 01:20:46:INFO:Received: evaluate message 746b9e84-7668-42a6-a7bb-ed278073cc21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:36:INFO:
[92mINFO [0m:      Received: train message ebd3d1d0-25c1-47bd-9339-81f1ac1dc297
02/15/2025 01:21:36:INFO:Received: train message ebd3d1d0-25c1-47bd-9339-81f1ac1dc297
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:07:INFO:
[92mINFO [0m:      Received: evaluate message 25fa8d7b-d168-4c30-8b03-3635c7fbd3ff
02/15/2025 01:24:07:INFO:Received: evaluate message 25fa8d7b-d168-4c30-8b03-3635c7fbd3ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:24:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:15:INFO:
[92mINFO [0m:      Received: train message 67c944a6-0c6d-4242-931e-e12480fa5c21
02/15/2025 01:25:15:INFO:Received: train message 67c944a6-0c6d-4242-931e-e12480fa5c21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:27:INFO:
[92mINFO [0m:      Received: evaluate message 097cfefe-2743-4648-9acc-4387aa0cfa7d
02/15/2025 01:27:27:INFO:Received: evaluate message 097cfefe-2743-4648-9acc-4387aa0cfa7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:38:INFO:
[92mINFO [0m:      Received: train message 56e8b700-410d-4841-a1d2-807dd8a0a0f0
02/15/2025 01:28:38:INFO:Received: train message 56e8b700-410d-4841-a1d2-807dd8a0a0f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:29:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:49:INFO:
[92mINFO [0m:      Received: evaluate message 30b153b0-c0e5-44e6-b850-86f839d7c631
02/15/2025 01:30:49:INFO:Received: evaluate message 30b153b0-c0e5-44e6-b850-86f839d7c631
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:30:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:18:INFO:
[92mINFO [0m:      Received: train message eaae717f-029f-4c2c-914b-801384607135
02/15/2025 01:31:18:INFO:Received: train message eaae717f-029f-4c2c-914b-801384607135
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:31:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:45:INFO:
[92mINFO [0m:      Received: evaluate message 27f60a3d-9e1f-4269-9480-dd6ad51f29b6
02/15/2025 01:33:45:INFO:Received: evaluate message 27f60a3d-9e1f-4269-9480-dd6ad51f29b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:09:INFO:
[92mINFO [0m:      Received: train message 7f64d0f6-3fef-4157-b8cd-17df6a62cc9d
02/15/2025 01:34:09:INFO:Received: train message 7f64d0f6-3fef-4157-b8cd-17df6a62cc9d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:34:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:14:INFO:
[92mINFO [0m:      Received: evaluate message 91d3aad2-99ed-4df8-8fb9-40e78e8a6b2e
02/15/2025 01:36:14:INFO:Received: evaluate message 91d3aad2-99ed-4df8-8fb9-40e78e8a6b2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:13:INFO:
[92mINFO [0m:      Received: train message 0db41dc0-c2eb-4bd0-bb2f-34c772e38d7e
02/15/2025 01:37:13:INFO:Received: train message 0db41dc0-c2eb-4bd0-bb2f-34c772e38d7e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:15:INFO:
[92mINFO [0m:      Received: evaluate message 1b96a43d-6e2f-4d22-8c0a-3052864b343b
02/15/2025 01:39:15:INFO:Received: evaluate message 1b96a43d-6e2f-4d22-8c0a-3052864b343b
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906], 'accuracy': [0.5027365129007036], 'auc': [0.6897600584676676], 'precision': [0.3991915055150511], 'recall': [0.5027365129007036], 'f1': [0.3614852344286372]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294], 'accuracy': [0.5027365129007036, 0.524628616106333], 'auc': [0.6897600584676676, 0.7198632289872761], 'precision': [0.3991915055150511, 0.4206049829674183], 'recall': [0.5027365129007036, 0.524628616106333], 'f1': [0.3614852344286372, 0.4422280621916187]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:57:INFO:
[92mINFO [0m:      Received: train message e3a48e43-4291-4b8f-9b59-bac69f02e72d
02/15/2025 01:39:57:INFO:Received: train message e3a48e43-4291-4b8f-9b59-bac69f02e72d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:27:INFO:
[92mINFO [0m:      Received: evaluate message 140457b5-1620-4a7d-956f-80ae8fe557e2
02/15/2025 01:41:27:INFO:Received: evaluate message 140457b5-1620-4a7d-956f-80ae8fe557e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:07:INFO:
[92mINFO [0m:      Received: train message 7ed79f88-14dc-470e-8e07-4a8b77fd01d5
02/15/2025 01:42:07:INFO:Received: train message 7ed79f88-14dc-470e-8e07-4a8b77fd01d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:42:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:57:INFO:
[92mINFO [0m:      Received: evaluate message bec1a01b-12f4-487c-a381-f50a784c7ae6
02/15/2025 01:43:57:INFO:Received: evaluate message bec1a01b-12f4-487c-a381-f50a784c7ae6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:44:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:41:INFO:
[92mINFO [0m:      Received: train message 64b0468e-1d1b-45b8-8691-0349286b2261
02/15/2025 01:44:41:INFO:Received: train message 64b0468e-1d1b-45b8-8691-0349286b2261
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:55:INFO:
[92mINFO [0m:      Received: evaluate message 91620a6a-2264-4011-9c85-e43432142ec4
02/15/2025 01:45:55:INFO:Received: evaluate message 91620a6a-2264-4011-9c85-e43432142ec4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:45:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:36:INFO:
[92mINFO [0m:      Received: train message 9abe1390-a437-4b53-ab5d-ded5623928db
02/15/2025 01:46:36:INFO:Received: train message 9abe1390-a437-4b53-ab5d-ded5623928db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:36:INFO:
[92mINFO [0m:      Received: evaluate message 8b6d7e10-0ade-4665-b62e-c93fcbf38860
02/15/2025 01:48:36:INFO:Received: evaluate message 8b6d7e10-0ade-4665-b62e-c93fcbf38860
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:00:INFO:
[92mINFO [0m:      Received: train message 6dd10548-746c-4aa3-8e19-b7dc696c6e33
02/15/2025 01:49:00:INFO:Received: train message 6dd10548-746c-4aa3-8e19-b7dc696c6e33

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:49:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:57:INFO:
[92mINFO [0m:      Received: evaluate message 66d6559b-9f5d-499b-ad6f-b13ce183ef6d
02/15/2025 01:50:57:INFO:Received: evaluate message 66d6559b-9f5d-499b-ad6f-b13ce183ef6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:51:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:35:INFO:
[92mINFO [0m:      Received: train message 52d44d48-7ff4-44b4-8f8d-9e81ec0f5cb8
02/15/2025 01:51:35:INFO:Received: train message 52d44d48-7ff4-44b4-8f8d-9e81ec0f5cb8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:52:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:04:INFO:
[92mINFO [0m:      Received: evaluate message 9b10d289-798a-4a3e-8616-9130551d461d
02/15/2025 01:53:04:INFO:Received: evaluate message 9b10d289-798a-4a3e-8616-9130551d461d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:53:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:41:INFO:
[92mINFO [0m:      Received: train message 604317f7-3795-4f49-b5c0-0ae1c07964e8
02/15/2025 01:53:41:INFO:Received: train message 604317f7-3795-4f49-b5c0-0ae1c07964e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:54:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:17:INFO:
[92mINFO [0m:      Received: evaluate message 760c64d7-e9ca-4337-b4b7-60d453b004b0
02/15/2025 01:56:17:INFO:Received: evaluate message 760c64d7-e9ca-4337-b4b7-60d453b004b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:53:INFO:
[92mINFO [0m:      Received: train message 9b8ff283-16dc-4e57-9911-6f394a20e4ea
02/15/2025 01:56:53:INFO:Received: train message 9b8ff283-16dc-4e57-9911-6f394a20e4ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:57:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:38:INFO:
[92mINFO [0m:      Received: evaluate message 34f977b3-da6f-4482-8ac7-5a74e191dbf6
02/15/2025 01:58:38:INFO:Received: evaluate message 34f977b3-da6f-4482-8ac7-5a74e191dbf6
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:58:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:41:INFO:
[92mINFO [0m:      Received: train message b1b8ca56-5841-46a6-bed3-0db545a8b4f5
02/15/2025 01:59:41:INFO:Received: train message b1b8ca56-5841-46a6-bed3-0db545a8b4f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:00:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:49:INFO:
[92mINFO [0m:      Received: evaluate message 8b84ccb5-73a2-48fd-b9ee-63a35bc036d9
02/15/2025 02:01:49:INFO:Received: evaluate message 8b84ccb5-73a2-48fd-b9ee-63a35bc036d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:01:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:02:38:INFO:
[92mINFO [0m:      Received: train message 73e7c62b-2a78-4deb-999d-022933d4470e
02/15/2025 02:02:38:INFO:Received: train message 73e7c62b-2a78-4deb-999d-022933d4470e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:03:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:47:INFO:
[92mINFO [0m:      Received: evaluate message d812803e-46eb-457f-bdf3-eb43f0282c05
02/15/2025 02:04:47:INFO:Received: evaluate message d812803e-46eb-457f-bdf3-eb43f0282c05
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:04:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:35:INFO:
[92mINFO [0m:      Received: train message 667c6988-c54f-4702-91ff-a8e98f713770
02/15/2025 02:05:35:INFO:Received: train message 667c6988-c54f-4702-91ff-a8e98f713770
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:49:INFO:
[92mINFO [0m:      Received: evaluate message 41a85367-18bf-4c3f-bbbb-56a0e47bca5a
02/15/2025 02:07:49:INFO:Received: evaluate message 41a85367-18bf-4c3f-bbbb-56a0e47bca5a

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:36:INFO:
[92mINFO [0m:      Received: train message 52df0c99-e6e0-46ce-a3e5-b3a5bcd59117
02/15/2025 02:08:36:INFO:Received: train message 52df0c99-e6e0-46ce-a3e5-b3a5bcd59117
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:09:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:27:INFO:
[92mINFO [0m:      Received: evaluate message 155b1385-d159-4704-b611-6bcff6606210
02/15/2025 02:10:27:INFO:Received: evaluate message 155b1385-d159-4704-b611-6bcff6606210
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:10:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:11:16:INFO:
[92mINFO [0m:      Received: train message 8b489811-c93e-4f84-83d5-4f480d5c6e43
02/15/2025 02:11:16:INFO:Received: train message 8b489811-c93e-4f84-83d5-4f480d5c6e43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:12:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:13:04:INFO:
[92mINFO [0m:      Received: evaluate message bd4c0324-2730-4c80-a3bd-f1ce137aafd9
02/15/2025 02:13:04:INFO:Received: evaluate message bd4c0324-2730-4c80-a3bd-f1ce137aafd9

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:13:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:13:49:INFO:
[92mINFO [0m:      Received: train message 30b04546-48ea-49aa-8f57-cc7397f4e692
02/15/2025 02:13:49:INFO:Received: train message 30b04546-48ea-49aa-8f57-cc7397f4e692
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:14:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:31:INFO:
[92mINFO [0m:      Received: evaluate message b164829a-05d9-4e88-bc32-e123377d8526
02/15/2025 02:15:31:INFO:Received: evaluate message b164829a-05d9-4e88-bc32-e123377d8526
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:15:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:02:INFO:
[92mINFO [0m:      Received: train message 3c427fb9-fc1e-445e-981d-9ddea9fd5e9e
02/15/2025 02:16:02:INFO:Received: train message 3c427fb9-fc1e-445e-981d-9ddea9fd5e9e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:16:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:56:INFO:
[92mINFO [0m:      Received: evaluate message c4d974f1-dac7-4f7e-b881-1ad3e7a00992
02/15/2025 02:17:56:INFO:Received: evaluate message c4d974f1-dac7-4f7e-b881-1ad3e7a00992

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:18:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:25:INFO:
[92mINFO [0m:      Received: train message 0b8760b3-a290-4195-af07-b5dd7030848a
02/15/2025 02:18:25:INFO:Received: train message 0b8760b3-a290-4195-af07-b5dd7030848a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:18:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:08:INFO:
[92mINFO [0m:      Received: evaluate message fcc09bb8-ccd3-440b-a320-a4091cee508c
02/15/2025 02:20:08:INFO:Received: evaluate message fcc09bb8-ccd3-440b-a320-a4091cee508c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:37:INFO:
[92mINFO [0m:      Received: train message e8e91a69-c330-4d85-a6ac-ed8d0bc4cc31
02/15/2025 02:20:37:INFO:Received: train message e8e91a69-c330-4d85-a6ac-ed8d0bc4cc31
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:21:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:10:INFO:
[92mINFO [0m:      Received: evaluate message 43a48a86-5c04-4f6e-a072-bf471cbd154e
02/15/2025 02:22:10:INFO:Received: evaluate message 43a48a86-5c04-4f6e-a072-bf471cbd154e

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:22:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:22:51:INFO:
[92mINFO [0m:      Received: train message 96943dbc-7d2a-4569-bf56-f32b1fc12224
02/15/2025 02:22:51:INFO:Received: train message 96943dbc-7d2a-4569-bf56-f32b1fc12224
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:23:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:24:23:INFO:
[92mINFO [0m:      Received: evaluate message 5ea2e23d-5ce2-467d-8776-4f47891f001a
02/15/2025 02:24:23:INFO:Received: evaluate message 5ea2e23d-5ce2-467d-8776-4f47891f001a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:24:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:25:03:INFO:
[92mINFO [0m:      Received: train message ceebb446-836f-4635-95d7-2e132f02acd9
02/15/2025 02:25:03:INFO:Received: train message ceebb446-836f-4635-95d7-2e132f02acd9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:25:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:26:23:INFO:
[92mINFO [0m:      Received: evaluate message 3c523880-b6b9-4414-be32-91d334d01f39
02/15/2025 02:26:23:INFO:Received: evaluate message 3c523880-b6b9-4414-be32-91d334d01f39

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:26:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:27:23:INFO:
[92mINFO [0m:      Received: train message 5abf99e7-3529-4d05-bbce-994cefb0eb56
02/15/2025 02:27:23:INFO:Received: train message 5abf99e7-3529-4d05-bbce-994cefb0eb56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:28:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:29:50:INFO:
[92mINFO [0m:      Received: evaluate message 50032dd5-967e-459b-a321-1bad7bbb5813
02/15/2025 02:29:50:INFO:Received: evaluate message 50032dd5-967e-459b-a321-1bad7bbb5813
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:30:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:30:19:INFO:
[92mINFO [0m:      Received: train message 419356a3-f193-41bc-80f5-5084fea755db
02/15/2025 02:30:19:INFO:Received: train message 419356a3-f193-41bc-80f5-5084fea755db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:30:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:32:04:INFO:
[92mINFO [0m:      Received: evaluate message ef753431-05c5-45c5-89bc-f79168e9c1bc
02/15/2025 02:32:04:INFO:Received: evaluate message ef753431-05c5-45c5-89bc-f79168e9c1bc

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:32:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:32:38:INFO:
[92mINFO [0m:      Received: train message ce8c5f7c-6b3d-4b52-8a1c-ebc8484aba21
02/15/2025 02:32:38:INFO:Received: train message ce8c5f7c-6b3d-4b52-8a1c-ebc8484aba21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:33:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:35:16:INFO:
[92mINFO [0m:      Received: evaluate message 7266ecff-5747-495f-bbc4-d70004b2672e
02/15/2025 02:35:16:INFO:Received: evaluate message 7266ecff-5747-495f-bbc4-d70004b2672e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:35:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:36:02:INFO:
[92mINFO [0m:      Received: train message 3e7b4806-233d-4ddc-a7ed-5dc41c2d204f
02/15/2025 02:36:02:INFO:Received: train message 3e7b4806-233d-4ddc-a7ed-5dc41c2d204f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:36:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:37:47:INFO:
[92mINFO [0m:      Received: evaluate message 1c6aa3b2-84f5-4b00-b088-08b5d3d0dd48
02/15/2025 02:37:47:INFO:Received: evaluate message 1c6aa3b2-84f5-4b00-b088-08b5d3d0dd48

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:37:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:38:02:INFO:
[92mINFO [0m:      Received: reconnect message 1070e7c8-b96e-4394-bfac-2d9ae13ba92c
02/15/2025 02:38:02:INFO:Received: reconnect message 1070e7c8-b96e-4394-bfac-2d9ae13ba92c
02/15/2025 02:38:02:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:38:02:INFO:Disconnect and shut down

{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}



Final client history:
{'loss': [1.0949175675647906, 1.0632013046154294, 1.1532356695237804, 1.1425245524011989, 1.1555175704821838, 1.1143011861335868, 1.1394870911751807, 1.1409403698998004, 1.1589795861792247, 1.1166670140202293, 1.182579463305559, 1.1301859669428118, 1.1587272593059794, 1.1569018953940755, 1.1400999068934252, 1.1120290847566559, 1.1617386053091292, 1.1047222686522262, 1.1105509780644438, 1.1146262049022553, 1.1032028870716797, 1.1294277252584253, 1.1403489464237133, 1.1235911769732727, 1.1220131700536624, 1.105747198806879, 1.0894286406515536, 1.1018045852201073, 1.0906746826477587, 1.077993558616355], 'accuracy': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'auc': [0.6897600584676676, 0.7198632289872761, 0.733217035990762, 0.7374474220514065, 0.7439784936802434, 0.7467417616514873, 0.749408367017386, 0.7536057183334169, 0.7555017272825144, 0.757517591864626, 0.7582056546184912, 0.7607219153128851, 0.7623777935557414, 0.7630702299804933, 0.7649024089118099, 0.7650981096265362, 0.7664366812120501, 0.7678894764502417, 0.7689210869051456, 0.7705158938174465, 0.7715898335938051, 0.7725320474382507, 0.7726568672918854, 0.7736076035456734, 0.7746751777454906, 0.7749973225673763, 0.776782097486979, 0.7763626516131502, 0.7771267378246414, 0.7781628455475862], 'precision': [0.3991915055150511, 0.4206049829674183, 0.42067252198137933, 0.43106529548200667, 0.42638999487092105, 0.4462950698327711, 0.44724636840791643, 0.4457698708400854, 0.4340998432926132, 0.4549403249919418, 0.4318874351467878, 0.45150372178546067, 0.45176212711430974, 0.4519332730473767, 0.4552366786704644, 0.5946108259396494, 0.5839177945264115, 0.5976932835128588, 0.5961339183428809, 0.5966716791541004, 0.6097094709548566, 0.6003043891505301, 0.5894470230877041, 0.5985926505082095, 0.5981076513348306, 0.6120308621373731, 0.5936469213358142, 0.6128105465169784, 0.5959460485059858, 0.597680360072439], 'recall': [0.5027365129007036, 0.524628616106333, 0.5215011727912432, 0.5324472243940579, 0.5285379202501954, 0.5402658326817826, 0.544175136825645, 0.5433932759968726, 0.5347928068803753, 0.5488663017982799, 0.5332290852228303, 0.5488663017982799, 0.5496481626270524, 0.5488663017982799, 0.5527756059421423, 0.5504300234558248, 0.5433932759968726, 0.5535574667709148, 0.5527756059421423, 0.5551211884284597, 0.5613760750586395, 0.5574667709147771, 0.5496481626270524, 0.5574667709147771, 0.5590304925723222, 0.5676309616888194, 0.5691946833463644, 0.5684128225175918, 0.5723221266614542, 0.5746677091477717], 'f1': [0.3614852344286372, 0.4422280621916187, 0.4336561053760245, 0.46447353571926275, 0.4544267115222353, 0.485736863354021, 0.48592536339053055, 0.4839073999375843, 0.46825734851972267, 0.49470789844511176, 0.46498403172457087, 0.4888859952419995, 0.4892797855003859, 0.4905658798808342, 0.4933077285683267, 0.49555768662839744, 0.4797661359349566, 0.5004864097844436, 0.5001564922046515, 0.5011080235256697, 0.5130252323600942, 0.5004852574768975, 0.4904573878248007, 0.5004779076064387, 0.5025269235918701, 0.5169612601749576, 0.5223679096251428, 0.5176858834060535, 0.5225239074806309, 0.5264342757892415]}

