nohup: ignoring input
02/15/2025 01:14:04:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:14:04:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:14:04:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739610844.586251 1656065 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:14:43:INFO:
[92mINFO [0m:      Received: train message 0423a632-f882-47ae-a075-6bb6d0a716aa
02/15/2025 01:14:43:INFO:Received: train message 0423a632-f882-47ae-a075-6bb6d0a716aa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:15:INFO:
[92mINFO [0m:      Received: evaluate message a0133ad2-24bb-4c15-b780-70224dc1cb9f
02/15/2025 01:16:15:INFO:Received: evaluate message a0133ad2-24bb-4c15-b780-70224dc1cb9f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:16:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:55:INFO:
[92mINFO [0m:      Received: train message 057d9dbc-300f-42ee-9ada-f79cf1e92f2f
02/15/2025 01:16:55:INFO:Received: train message 057d9dbc-300f-42ee-9ada-f79cf1e92f2f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:19:INFO:
[92mINFO [0m:      Received: evaluate message 228d1c23-8f28-4da6-be5c-c6aa406c5cf1
02/15/2025 01:18:19:INFO:Received: evaluate message 228d1c23-8f28-4da6-be5c-c6aa406c5cf1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:19:03:INFO:
[92mINFO [0m:      Received: train message 6b5c8da0-dfed-4166-b722-37e7e9e93905
02/15/2025 01:19:03:INFO:Received: train message 6b5c8da0-dfed-4166-b722-37e7e9e93905
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:47:INFO:
[92mINFO [0m:      Received: evaluate message c9029fca-0ad3-4010-99af-9ff347b1eb92
02/15/2025 01:20:47:INFO:Received: evaluate message c9029fca-0ad3-4010-99af-9ff347b1eb92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:42:INFO:
[92mINFO [0m:      Received: train message 71645f58-5cbd-4973-a816-3f00d14360c9
02/15/2025 01:21:42:INFO:Received: train message 71645f58-5cbd-4973-a816-3f00d14360c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:22:INFO:
[92mINFO [0m:      Received: evaluate message a673224a-04da-425d-b54c-6821387df22b
02/15/2025 01:23:22:INFO:Received: evaluate message a673224a-04da-425d-b54c-6821387df22b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:23:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:51:INFO:
[92mINFO [0m:      Received: train message d2d312ed-a291-43b9-99fe-d92c97f49657
02/15/2025 01:23:51:INFO:Received: train message d2d312ed-a291-43b9-99fe-d92c97f49657
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:37:INFO:
[92mINFO [0m:      Received: evaluate message f9dec03a-07cb-43cc-a585-b0b19015a64c
02/15/2025 01:25:37:INFO:Received: evaluate message f9dec03a-07cb-43cc-a585-b0b19015a64c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:26:08:INFO:
[92mINFO [0m:      Received: train message 51293bb4-5428-4ef0-a24b-5d035bd2d8a2
02/15/2025 01:26:08:INFO:Received: train message 51293bb4-5428-4ef0-a24b-5d035bd2d8a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:27:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:39:INFO:
[92mINFO [0m:      Received: evaluate message b5b00e73-1468-40ae-863a-cf0b07069aad
02/15/2025 01:27:39:INFO:Received: evaluate message b5b00e73-1468-40ae-863a-cf0b07069aad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:34:INFO:
[92mINFO [0m:      Received: train message 766575d6-0230-4d41-ad28-8440ec353e08
02/15/2025 01:28:34:INFO:Received: train message 766575d6-0230-4d41-ad28-8440ec353e08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:29:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:57:INFO:
[92mINFO [0m:      Received: evaluate message 36be1dd1-7128-4d3d-a357-8f73f600dbe2
02/15/2025 01:29:57:INFO:Received: evaluate message 36be1dd1-7128-4d3d-a357-8f73f600dbe2
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563], 'accuracy': [0.4988272087568413], 'auc': [0.6039493601988546], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003], 'accuracy': [0.4988272087568413, 0.5058639562157936], 'auc': [0.6039493601988546, 0.6592898585275673], 'precision': [0.24999954080217573, 0.40000145617924], 'recall': [0.4988272087568413, 0.5058639562157936], 'f1': [0.3330718973441611, 0.38101002254991995]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:30:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:49:INFO:
[92mINFO [0m:      Received: train message 97b1e93b-af2f-4397-b5e6-7db7ee434303
02/15/2025 01:30:49:INFO:Received: train message 97b1e93b-af2f-4397-b5e6-7db7ee434303
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:31:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:23:INFO:
[92mINFO [0m:      Received: evaluate message 4faf4883-b0c3-4cd7-8bf9-667abcbf62ea
02/15/2025 01:32:23:INFO:Received: evaluate message 4faf4883-b0c3-4cd7-8bf9-667abcbf62ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:32:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:59:INFO:
[92mINFO [0m:      Received: train message 6e90d534-bf39-455f-bf10-d0bea7f24830
02/15/2025 01:32:59:INFO:Received: train message 6e90d534-bf39-455f-bf10-d0bea7f24830
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:34:INFO:
[92mINFO [0m:      Received: evaluate message e71e142a-59fb-4360-9066-ec175078bf54
02/15/2025 01:34:34:INFO:Received: evaluate message e71e142a-59fb-4360-9066-ec175078bf54
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:34:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:53:INFO:
[92mINFO [0m:      Received: train message 25d75378-f80f-4af6-9e95-5b5297eea870
02/15/2025 01:34:53:INFO:Received: train message 25d75378-f80f-4af6-9e95-5b5297eea870
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:30:INFO:
[92mINFO [0m:      Received: evaluate message d219bb0c-d731-4081-a84f-5bce5a8c32b3
02/15/2025 01:36:30:INFO:Received: evaluate message d219bb0c-d731-4081-a84f-5bce5a8c32b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:27:INFO:
[92mINFO [0m:      Received: train message 8c193cf9-1fa7-4d05-9009-d2f4c57af2e2
02/15/2025 01:37:27:INFO:Received: train message 8c193cf9-1fa7-4d05-9009-d2f4c57af2e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:38:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:04:INFO:
[92mINFO [0m:      Received: evaluate message 5e9605f8-fd0c-436e-9066-465d99b126a1
02/15/2025 01:39:04:INFO:Received: evaluate message 5e9605f8-fd0c-436e-9066-465d99b126a1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:45:INFO:
[92mINFO [0m:      Received: train message bbf8b80a-193c-4b1d-b441-06b043d457ba
02/15/2025 01:39:45:INFO:Received: train message bbf8b80a-193c-4b1d-b441-06b043d457ba

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:14:INFO:
[92mINFO [0m:      Received: evaluate message b32df8ce-c456-4048-b0db-5c06bef6679c
02/15/2025 01:41:14:INFO:Received: evaluate message b32df8ce-c456-4048-b0db-5c06bef6679c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:04:INFO:
[92mINFO [0m:      Received: train message e2b497ec-bc45-49e1-8645-eb34d851abab
02/15/2025 01:42:04:INFO:Received: train message e2b497ec-bc45-49e1-8645-eb34d851abab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:42:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:40:INFO:
[92mINFO [0m:      Received: evaluate message 90ac9099-660b-4112-aeda-32480984858c
02/15/2025 01:43:40:INFO:Received: evaluate message 90ac9099-660b-4112-aeda-32480984858c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:20:INFO:
[92mINFO [0m:      Received: train message eb0a1854-420a-409f-bc97-d08914f6da0b
02/15/2025 01:44:20:INFO:Received: train message eb0a1854-420a-409f-bc97-d08914f6da0b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:48:INFO:
[92mINFO [0m:      Received: evaluate message 1a7797d6-c9ba-4414-9f2b-f72c6ce523b8
02/15/2025 01:45:48:INFO:Received: evaluate message 1a7797d6-c9ba-4414-9f2b-f72c6ce523b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:45:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:35:INFO:
[92mINFO [0m:      Received: train message d828f568-ea40-4171-81cc-96a0d37c6ce4
02/15/2025 01:46:35:INFO:Received: train message d828f568-ea40-4171-81cc-96a0d37c6ce4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:01:INFO:
[92mINFO [0m:      Received: evaluate message b43478ca-f373-44a6-a49e-43c135ce0144
02/15/2025 01:48:01:INFO:Received: evaluate message b43478ca-f373-44a6-a49e-43c135ce0144
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:49:INFO:
[92mINFO [0m:      Received: train message e94400b4-56cc-4a52-8622-0983a033d435
02/15/2025 01:48:49:INFO:Received: train message e94400b4-56cc-4a52-8622-0983a033d435
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:49:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:17:INFO:
[92mINFO [0m:      Received: evaluate message e5d4d57e-522f-4030-adb3-36fb57a089b3
02/15/2025 01:50:17:INFO:Received: evaluate message e5d4d57e-522f-4030-adb3-36fb57a089b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:10:INFO:
[92mINFO [0m:      Received: train message 7cba03f2-e4cd-4a9c-ace0-9bbd6e33ddc5
02/15/2025 01:51:10:INFO:Received: train message 7cba03f2-e4cd-4a9c-ace0-9bbd6e33ddc5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:51:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:52:40:INFO:
[92mINFO [0m:      Received: evaluate message 83449850-abc7-4e4c-ac60-f1d1a5dd09c1
02/15/2025 01:52:40:INFO:Received: evaluate message 83449850-abc7-4e4c-ac60-f1d1a5dd09c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:52:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:14:INFO:
[92mINFO [0m:      Received: train message 371aa86f-0629-4b04-9818-ab58334e3721
02/15/2025 01:53:14:INFO:Received: train message 371aa86f-0629-4b04-9818-ab58334e3721
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:54:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:54:43:INFO:
[92mINFO [0m:      Received: evaluate message 875fe6a2-6aac-44dc-99db-f24aca29ddcc
02/15/2025 01:54:43:INFO:Received: evaluate message 875fe6a2-6aac-44dc-99db-f24aca29ddcc

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:54:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:55:08:INFO:
[92mINFO [0m:      Received: train message 5c3f7fc5-7477-412c-92fe-5c94589fb257
02/15/2025 01:55:08:INFO:Received: train message 5c3f7fc5-7477-412c-92fe-5c94589fb257
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:55:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:51:INFO:
[92mINFO [0m:      Received: evaluate message 49aaa534-eeda-494d-803e-880868691d5d
02/15/2025 01:56:51:INFO:Received: evaluate message 49aaa534-eeda-494d-803e-880868691d5d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:57:36:INFO:
[92mINFO [0m:      Received: train message 7a00f8ff-69b6-4e95-80fc-0ce2b421abb1
02/15/2025 01:57:36:INFO:Received: train message 7a00f8ff-69b6-4e95-80fc-0ce2b421abb1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:58:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:52:INFO:
[92mINFO [0m:      Received: evaluate message a5c5e5d1-00bd-4e05-a499-ec5c272df4c6
02/15/2025 01:58:52:INFO:Received: evaluate message a5c5e5d1-00bd-4e05-a499-ec5c272df4c6

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:58:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:30:INFO:
[92mINFO [0m:      Received: train message da3bdd4b-890b-4658-a385-49ffbd6ebb4b
02/15/2025 01:59:30:INFO:Received: train message da3bdd4b-890b-4658-a385-49ffbd6ebb4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:00:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:03:INFO:
[92mINFO [0m:      Received: evaluate message 7524ec03-eacb-4495-9376-926929dd5aa4
02/15/2025 02:01:03:INFO:Received: evaluate message 7524ec03-eacb-4495-9376-926929dd5aa4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:01:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:50:INFO:
[92mINFO [0m:      Received: train message bf7ccbac-f738-40a6-9895-e4acc0c6943f
02/15/2025 02:01:50:INFO:Received: train message bf7ccbac-f738-40a6-9895-e4acc0c6943f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:02:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:03:13:INFO:
[92mINFO [0m:      Received: evaluate message 8590f847-b8d7-4c15-b237-7d1beb35c604
02/15/2025 02:03:13:INFO:Received: evaluate message 8590f847-b8d7-4c15-b237-7d1beb35c604

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:03:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:03:58:INFO:
[92mINFO [0m:      Received: train message befa1cee-f64c-493b-aaa9-b78fc3f4f57d
02/15/2025 02:03:58:INFO:Received: train message befa1cee-f64c-493b-aaa9-b78fc3f4f57d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:04:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:17:INFO:
[92mINFO [0m:      Received: evaluate message ea44ff41-3ee1-49c1-9aa3-bbe2e4422c85
02/15/2025 02:05:17:INFO:Received: evaluate message ea44ff41-3ee1-49c1-9aa3-bbe2e4422c85
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:05:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:06:10:INFO:
[92mINFO [0m:      Received: train message 9c2fecc1-5b06-4675-abf3-493e69f04cfb
02/15/2025 02:06:10:INFO:Received: train message 9c2fecc1-5b06-4675-abf3-493e69f04cfb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:07:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:43:INFO:
[92mINFO [0m:      Received: evaluate message f14bccb9-3d4d-4eb6-81fe-8f3fd8e033d4
02/15/2025 02:07:43:INFO:Received: evaluate message f14bccb9-3d4d-4eb6-81fe-8f3fd8e033d4

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:01:INFO:
[92mINFO [0m:      Received: train message 0d87b852-b5a3-4560-9ceb-d0a5edfae3c0
02/15/2025 02:08:01:INFO:Received: train message 0d87b852-b5a3-4560-9ceb-d0a5edfae3c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:08:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:09:47:INFO:
[92mINFO [0m:      Received: evaluate message 0f57ebc2-e7d3-405a-a559-7f812a867119
02/15/2025 02:09:47:INFO:Received: evaluate message 0f57ebc2-e7d3-405a-a559-7f812a867119
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:09:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:14:INFO:
[92mINFO [0m:      Received: train message a556f6bb-9087-4598-a61f-cb9da89193e6
02/15/2025 02:10:14:INFO:Received: train message a556f6bb-9087-4598-a61f-cb9da89193e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:10:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:05:INFO:
[92mINFO [0m:      Received: evaluate message 5388fe64-008a-4d85-8ac5-b0e835fa622a
02/15/2025 02:12:05:INFO:Received: evaluate message 5388fe64-008a-4d85-8ac5-b0e835fa622a

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:12:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:45:INFO:
[92mINFO [0m:      Received: train message 717dee5c-bf30-493c-8f28-b347dd8614cf
02/15/2025 02:12:45:INFO:Received: train message 717dee5c-bf30-493c-8f28-b347dd8614cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:13:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:11:INFO:
[92mINFO [0m:      Received: evaluate message 575ef5f4-0333-48c2-a857-218aa98967ff
02/15/2025 02:14:11:INFO:Received: evaluate message 575ef5f4-0333-48c2-a857-218aa98967ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:14:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:03:INFO:
[92mINFO [0m:      Received: train message 67075851-4cc5-4690-a099-e8e940c0c649
02/15/2025 02:15:03:INFO:Received: train message 67075851-4cc5-4690-a099-e8e940c0c649
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:15:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:33:INFO:
[92mINFO [0m:      Received: evaluate message 59597f29-70eb-4ce9-be39-120498db8143
02/15/2025 02:16:33:INFO:Received: evaluate message 59597f29-70eb-4ce9-be39-120498db8143

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:16:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:01:INFO:
[92mINFO [0m:      Received: train message 01f82bae-6c42-49e4-b0f9-abd262ac4a6a
02/15/2025 02:17:01:INFO:Received: train message 01f82bae-6c42-49e4-b0f9-abd262ac4a6a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:17:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:22:INFO:
[92mINFO [0m:      Received: evaluate message f8760662-658a-4977-a219-e3eeea360c59
02/15/2025 02:18:22:INFO:Received: evaluate message f8760662-658a-4977-a219-e3eeea360c59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:18:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:19:16:INFO:
[92mINFO [0m:      Received: train message 0621d8e1-3bd5-4b24-a9eb-b0db78384ac4
02/15/2025 02:19:16:INFO:Received: train message 0621d8e1-3bd5-4b24-a9eb-b0db78384ac4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:20:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:38:INFO:
[92mINFO [0m:      Received: evaluate message 1223a3f7-0bef-4196-9537-60b37154e5a4
02/15/2025 02:20:38:INFO:Received: evaluate message 1223a3f7-0bef-4196-9537-60b37154e5a4

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:54:INFO:
[92mINFO [0m:      Received: reconnect message 037388c5-aaba-455e-9202-ab65f077f9e8
02/15/2025 02:20:54:INFO:Received: reconnect message 037388c5-aaba-455e-9202-ab65f077f9e8
02/15/2025 02:20:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:20:54:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}



Final client history:
{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}

