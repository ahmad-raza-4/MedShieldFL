nohup: ignoring input
02/15/2025 10:51:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 10:51:31:DEBUG:ChannelConnectivity.IDLE
02/15/2025 10:51:31:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739645491.825291 2282694 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 10:52:02:INFO:
[92mINFO [0m:      Received: train message e7822d81-9faf-46f1-a5de-ad5f10ac7bd6
02/15/2025 10:52:02:INFO:Received: train message e7822d81-9faf-46f1-a5de-ad5f10ac7bd6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:52:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:53:25:INFO:
[92mINFO [0m:      Received: evaluate message 7bb5e179-01a7-458c-9e0d-d35c7ece3219
02/15/2025 10:53:25:INFO:Received: evaluate message 7bb5e179-01a7-458c-9e0d-d35c7ece3219
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:53:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:53:53:INFO:
[92mINFO [0m:      Received: train message 4e2210f1-2752-490b-81a4-a984a5d86013
02/15/2025 10:53:53:INFO:Received: train message 4e2210f1-2752-490b-81a4-a984a5d86013
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:54:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:55:43:INFO:
[92mINFO [0m:      Received: evaluate message 385cbce6-a4a7-478d-ab5b-a44cdaf634e4
02/15/2025 10:55:43:INFO:Received: evaluate message 385cbce6-a4a7-478d-ab5b-a44cdaf634e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:55:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:55:58:INFO:
[92mINFO [0m:      Received: train message ead207c0-1791-4cb8-97ce-e8c4b2663e28
02/15/2025 10:55:58:INFO:Received: train message ead207c0-1791-4cb8-97ce-e8c4b2663e28
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:56:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:57:49:INFO:
[92mINFO [0m:      Received: evaluate message 630c6aa7-d525-491f-a78c-318684b90641
02/15/2025 10:57:49:INFO:Received: evaluate message 630c6aa7-d525-491f-a78c-318684b90641
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:57:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:58:12:INFO:
[92mINFO [0m:      Received: train message eb4bda18-c919-418b-ade2-febeb21a1b55
02/15/2025 10:58:12:INFO:Received: train message eb4bda18-c919-418b-ade2-febeb21a1b55
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:58:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:59:41:INFO:
[92mINFO [0m:      Received: evaluate message d7336c74-7fdc-4d46-980f-a226c95d0040
02/15/2025 10:59:41:INFO:Received: evaluate message d7336c74-7fdc-4d46-980f-a226c95d0040
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:59:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:00:20:INFO:
[92mINFO [0m:      Received: train message 8b4372c9-83c5-4047-9b96-6f8981cb9761
02/15/2025 11:00:20:INFO:Received: train message 8b4372c9-83c5-4047-9b96-6f8981cb9761
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:00:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:01:45:INFO:
[92mINFO [0m:      Received: evaluate message 769165be-d263-47fc-a6cf-a76638548514
02/15/2025 11:01:45:INFO:Received: evaluate message 769165be-d263-47fc-a6cf-a76638548514
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:01:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:02:04:INFO:
[92mINFO [0m:      Received: train message 16d0edb9-3f7b-49f7-9d2a-700a87dcac4a
02/15/2025 11:02:04:INFO:Received: train message 16d0edb9-3f7b-49f7-9d2a-700a87dcac4a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:02:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:03:35:INFO:
[92mINFO [0m:      Received: evaluate message 09a286e6-531a-4fe7-b5a2-64b836e3456c
02/15/2025 11:03:35:INFO:Received: evaluate message 09a286e6-531a-4fe7-b5a2-64b836e3456c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:03:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:04:18:INFO:
[92mINFO [0m:      Received: train message f8183cbd-a4a3-4d77-a87a-ecd1c7c2af08
02/15/2025 11:04:18:INFO:Received: train message f8183cbd-a4a3-4d77-a87a-ecd1c7c2af08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:04:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:05:39:INFO:
[92mINFO [0m:      Received: evaluate message 5a091524-b981-4649-98e2-844426810208
02/15/2025 11:05:39:INFO:Received: evaluate message 5a091524-b981-4649-98e2-844426810208
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144], 'accuracy': [0.506645817044566], 'auc': [0.7056891333322408], 'precision': [0.3993532547885127], 'recall': [0.506645817044566], 'f1': [0.3855554490777106]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269], 'accuracy': [0.506645817044566, 0.5285379202501954], 'auc': [0.7056891333322408, 0.732304691606418], 'precision': [0.3993532547885127, 0.4299345130435604], 'recall': [0.506645817044566, 0.5285379202501954], 'f1': [0.3855554490777106, 0.4662334545136853]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:05:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:06:17:INFO:
[92mINFO [0m:      Received: train message 98d251fe-b4b2-4ba0-b242-8576f6c8f9b9
02/15/2025 11:06:17:INFO:Received: train message 98d251fe-b4b2-4ba0-b242-8576f6c8f9b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:06:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:07:32:INFO:
[92mINFO [0m:      Received: evaluate message 967bc058-cf9e-4a46-be56-6f0f97fa84ee
02/15/2025 11:07:32:INFO:Received: evaluate message 967bc058-cf9e-4a46-be56-6f0f97fa84ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:07:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:08:10:INFO:
[92mINFO [0m:      Received: train message 1e5fa904-d219-4799-995d-a3d9bf0a7718
02/15/2025 11:08:10:INFO:Received: train message 1e5fa904-d219-4799-995d-a3d9bf0a7718
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:08:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:09:38:INFO:
[92mINFO [0m:      Received: evaluate message e06da50f-77c9-4751-87aa-784d379b2656
02/15/2025 11:09:38:INFO:Received: evaluate message e06da50f-77c9-4751-87aa-784d379b2656
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:09:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:10:01:INFO:
[92mINFO [0m:      Received: train message d7c1a6cc-080a-4993-9337-a41118962742
02/15/2025 11:10:01:INFO:Received: train message d7c1a6cc-080a-4993-9337-a41118962742
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:10:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:11:28:INFO:
[92mINFO [0m:      Received: evaluate message 4ef7a1c6-f905-42a9-9628-11927ed18b71
02/15/2025 11:11:28:INFO:Received: evaluate message 4ef7a1c6-f905-42a9-9628-11927ed18b71
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:11:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:12:08:INFO:
[92mINFO [0m:      Received: train message 14809fec-a2a7-4e62-ab1c-1168e503f50c
02/15/2025 11:12:08:INFO:Received: train message 14809fec-a2a7-4e62-ab1c-1168e503f50c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:12:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:13:19:INFO:
[92mINFO [0m:      Received: evaluate message 3ed72728-b9a4-44c4-9ce8-9938df24f275
02/15/2025 11:13:19:INFO:Received: evaluate message 3ed72728-b9a4-44c4-9ce8-9938df24f275
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:13:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:14:04:INFO:
[92mINFO [0m:      Received: train message 99e0c7da-d3ba-42ff-b0f5-202b54a67121
02/15/2025 11:14:04:INFO:Received: train message 99e0c7da-d3ba-42ff-b0f5-202b54a67121

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:14:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:15:04:INFO:
[92mINFO [0m:      Received: evaluate message 456344c0-52dc-480f-96a5-5d764baf0715
02/15/2025 11:15:04:INFO:Received: evaluate message 456344c0-52dc-480f-96a5-5d764baf0715
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:15:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:15:50:INFO:
[92mINFO [0m:      Received: train message 4eef0c81-2f2c-4a88-8116-a3ac356e808f
02/15/2025 11:15:50:INFO:Received: train message 4eef0c81-2f2c-4a88-8116-a3ac356e808f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:16:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:17:20:INFO:
[92mINFO [0m:      Received: evaluate message c2b11358-936b-4e3f-b136-7fb1db1637e5
02/15/2025 11:17:20:INFO:Received: evaluate message c2b11358-936b-4e3f-b136-7fb1db1637e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:17:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:17:54:INFO:
[92mINFO [0m:      Received: train message 8947bc55-c10a-4884-94c5-f4fa080074e2
02/15/2025 11:17:54:INFO:Received: train message 8947bc55-c10a-4884-94c5-f4fa080074e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:18:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:19:08:INFO:
[92mINFO [0m:      Received: evaluate message e6d84190-090b-40cf-847d-d5f0d832b68f
02/15/2025 11:19:08:INFO:Received: evaluate message e6d84190-090b-40cf-847d-d5f0d832b68f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:19:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:19:56:INFO:
[92mINFO [0m:      Received: train message ba3002a9-d67c-47bc-adb2-69cbe0d986b0
02/15/2025 11:19:56:INFO:Received: train message ba3002a9-d67c-47bc-adb2-69cbe0d986b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:20:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:21:05:INFO:
[92mINFO [0m:      Received: evaluate message 5f1d7d19-cee8-4610-bdcb-f183efec2ea4
02/15/2025 11:21:05:INFO:Received: evaluate message 5f1d7d19-cee8-4610-bdcb-f183efec2ea4
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:21:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:21:52:INFO:
[92mINFO [0m:      Received: train message eedcf970-7041-4c11-af22-a4e74e9ad6be
02/15/2025 11:21:52:INFO:Received: train message eedcf970-7041-4c11-af22-a4e74e9ad6be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:22:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:23:21:INFO:
[92mINFO [0m:      Received: evaluate message 1bbabdf2-d131-4d0b-aed7-eb92cf3baa7c
02/15/2025 11:23:21:INFO:Received: evaluate message 1bbabdf2-d131-4d0b-aed7-eb92cf3baa7c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:23:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:23:52:INFO:
[92mINFO [0m:      Received: train message 86402ccf-ffe1-4077-901c-155ed6393d34
02/15/2025 11:23:52:INFO:Received: train message 86402ccf-ffe1-4077-901c-155ed6393d34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:24:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:25:14:INFO:
[92mINFO [0m:      Received: evaluate message d6bccfcf-ada7-401a-b634-8ccde0256750
02/15/2025 11:25:14:INFO:Received: evaluate message d6bccfcf-ada7-401a-b634-8ccde0256750
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:25:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:25:54:INFO:
[92mINFO [0m:      Received: train message 965b30ba-55f5-4137-91a7-c374ddb6abe5
02/15/2025 11:25:54:INFO:Received: train message 965b30ba-55f5-4137-91a7-c374ddb6abe5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:26:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:27:09:INFO:
[92mINFO [0m:      Received: evaluate message 5c9e3f36-7388-418e-9151-abf893475f93
02/15/2025 11:27:09:INFO:Received: evaluate message 5c9e3f36-7388-418e-9151-abf893475f93

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:27:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:27:53:INFO:
[92mINFO [0m:      Received: train message 741c7922-be09-4999-bb64-86f0f0ea0a5a
02/15/2025 11:27:53:INFO:Received: train message 741c7922-be09-4999-bb64-86f0f0ea0a5a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:28:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:29:17:INFO:
[92mINFO [0m:      Received: evaluate message 2338d0c9-63b5-4400-8ea4-8c6152582433
02/15/2025 11:29:17:INFO:Received: evaluate message 2338d0c9-63b5-4400-8ea4-8c6152582433
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:29:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:29:39:INFO:
[92mINFO [0m:      Received: train message c5f06c00-de5b-4666-a9d1-c6eabb03d405
02/15/2025 11:29:39:INFO:Received: train message c5f06c00-de5b-4666-a9d1-c6eabb03d405
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:30:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:31:17:INFO:
[92mINFO [0m:      Received: evaluate message 46324a62-b72f-42ed-96da-d28a2885f0bd
02/15/2025 11:31:17:INFO:Received: evaluate message 46324a62-b72f-42ed-96da-d28a2885f0bd

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:31:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:31:41:INFO:
[92mINFO [0m:      Received: train message fa202e20-e358-4a3d-b8ce-58b139636256
02/15/2025 11:31:41:INFO:Received: train message fa202e20-e358-4a3d-b8ce-58b139636256
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:32:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:33:16:INFO:
[92mINFO [0m:      Received: evaluate message 0ae676f8-ce40-4e6e-8d0e-29b1a2d351db
02/15/2025 11:33:16:INFO:Received: evaluate message 0ae676f8-ce40-4e6e-8d0e-29b1a2d351db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:33:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:33:39:INFO:
[92mINFO [0m:      Received: train message 5200fdf3-6a74-4300-a188-b4c1813f4cdb
02/15/2025 11:33:39:INFO:Received: train message 5200fdf3-6a74-4300-a188-b4c1813f4cdb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:34:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:35:10:INFO:
[92mINFO [0m:      Received: evaluate message 0e678a79-1b32-4b1c-aa3c-a84cdc2d9e28
02/15/2025 11:35:10:INFO:Received: evaluate message 0e678a79-1b32-4b1c-aa3c-a84cdc2d9e28

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:35:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:35:36:INFO:
[92mINFO [0m:      Received: train message 097ecb70-92bd-4bd0-b7f7-30fa7474b57c
02/15/2025 11:35:36:INFO:Received: train message 097ecb70-92bd-4bd0-b7f7-30fa7474b57c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:36:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:37:03:INFO:
[92mINFO [0m:      Received: evaluate message c2042486-a642-4bd2-a41e-ce5918986acd
02/15/2025 11:37:03:INFO:Received: evaluate message c2042486-a642-4bd2-a41e-ce5918986acd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:37:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:37:39:INFO:
[92mINFO [0m:      Received: train message 71806567-e16d-4e29-bdcf-568c2ec3c9ed
02/15/2025 11:37:39:INFO:Received: train message 71806567-e16d-4e29-bdcf-568c2ec3c9ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:38:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:39:02:INFO:
[92mINFO [0m:      Received: evaluate message 0bd587d3-c5bd-4419-b966-3ad2dacf5e2c
02/15/2025 11:39:02:INFO:Received: evaluate message 0bd587d3-c5bd-4419-b966-3ad2dacf5e2c

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:39:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:39:39:INFO:
[92mINFO [0m:      Received: train message 8a8cfca1-090b-4686-82fb-ee537ddfc5db
02/15/2025 11:39:39:INFO:Received: train message 8a8cfca1-090b-4686-82fb-ee537ddfc5db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:40:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:40:47:INFO:
[92mINFO [0m:      Received: evaluate message f3fc6bbf-ddf7-4f36-ae48-461f448701cf
02/15/2025 11:40:47:INFO:Received: evaluate message f3fc6bbf-ddf7-4f36-ae48-461f448701cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:40:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:41:33:INFO:
[92mINFO [0m:      Received: train message c5a6049c-cf6b-420c-bae9-77d98dea0df6
02/15/2025 11:41:33:INFO:Received: train message c5a6049c-cf6b-420c-bae9-77d98dea0df6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:42:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:42:43:INFO:
[92mINFO [0m:      Received: evaluate message b83866db-14ad-4ab6-abf1-71da42aa644b
02/15/2025 11:42:43:INFO:Received: evaluate message b83866db-14ad-4ab6-abf1-71da42aa644b

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:42:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:43:18:INFO:
[92mINFO [0m:      Received: train message 2f9b478b-d42e-47e5-8226-4aa68bf10be2
02/15/2025 11:43:18:INFO:Received: train message 2f9b478b-d42e-47e5-8226-4aa68bf10be2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:43:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:44:43:INFO:
[92mINFO [0m:      Received: evaluate message d5aa4f1f-e6c8-408c-996b-9b91cdcac49e
02/15/2025 11:44:43:INFO:Received: evaluate message d5aa4f1f-e6c8-408c-996b-9b91cdcac49e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:44:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:45:07:INFO:
[92mINFO [0m:      Received: train message 86d92701-fed8-4641-bed2-7f69bfa37504
02/15/2025 11:45:07:INFO:Received: train message 86d92701-fed8-4641-bed2-7f69bfa37504
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:45:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:46:50:INFO:
[92mINFO [0m:      Received: evaluate message a1a032a1-738e-4275-b0ad-dd67f45783dc
02/15/2025 11:46:50:INFO:Received: evaluate message a1a032a1-738e-4275-b0ad-dd67f45783dc

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:46:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:47:25:INFO:
[92mINFO [0m:      Received: train message 076475bb-4984-4292-98b5-0859f5b4cf02
02/15/2025 11:47:25:INFO:Received: train message 076475bb-4984-4292-98b5-0859f5b4cf02
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:48:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:48:44:INFO:
[92mINFO [0m:      Received: evaluate message f80750b3-36d9-4628-8bfe-4a84011872d7
02/15/2025 11:48:44:INFO:Received: evaluate message f80750b3-36d9-4628-8bfe-4a84011872d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:48:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:49:27:INFO:
[92mINFO [0m:      Received: train message a18f7233-f017-4f6a-9067-116585b5b4ff
02/15/2025 11:49:27:INFO:Received: train message a18f7233-f017-4f6a-9067-116585b5b4ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:50:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:50:36:INFO:
[92mINFO [0m:      Received: evaluate message 2a659d5f-45a0-4432-948c-e5c624c9963a
02/15/2025 11:50:36:INFO:Received: evaluate message 2a659d5f-45a0-4432-948c-e5c624c9963a

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:50:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:50:55:INFO:
[92mINFO [0m:      Received: reconnect message 35d768b5-0bdc-43c5-9d85-19fffb9beac4
02/15/2025 11:50:55:INFO:Received: reconnect message 35d768b5-0bdc-43c5-9d85-19fffb9beac4
02/15/2025 11:50:55:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 11:50:55:INFO:Disconnect and shut down

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966, 1.0564935112297769], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003, 0.7894125548593478], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599, 0.5876472686112911], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808, 0.5344292164861765]}



Final client history:
{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966, 1.0564935112297769], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003, 0.7894125548593478], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599, 0.5876472686112911], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808, 0.5344292164861765]}

