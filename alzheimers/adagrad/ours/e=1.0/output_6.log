nohup: ignoring input
02/17/2025 15:45:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:45:31:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:45:31:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739835931.463858  742116 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:46:13:INFO:
[92mINFO [0m:      Received: train message 983b3e03-d9b3-4aec-8dd8-74402b914850
02/17/2025 15:46:13:INFO:Received: train message 983b3e03-d9b3-4aec-8dd8-74402b914850
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:46:34:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:47:21:INFO:
[92mINFO [0m:      Received: evaluate message f3322c92-0126-4f51-9e8e-5e1e1be84b46
02/17/2025 15:47:21:INFO:Received: evaluate message f3322c92-0126-4f51-9e8e-5e1e1be84b46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:47:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:48:23:INFO:
[92mINFO [0m:      Received: train message 8bc54672-cc14-4534-99c4-180a33a2d3e2
02/17/2025 15:48:23:INFO:Received: train message 8bc54672-cc14-4534-99c4-180a33a2d3e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:48:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:49:41:INFO:
[92mINFO [0m:      Received: evaluate message 6efa6bef-40bc-4bff-8a27-84ce2558ba65
02/17/2025 15:49:41:INFO:Received: evaluate message 6efa6bef-40bc-4bff-8a27-84ce2558ba65
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:49:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:50:30:INFO:
[92mINFO [0m:      Received: train message a35b27e8-608f-42ed-afe2-419fb620c62b
02/17/2025 15:50:30:INFO:Received: train message a35b27e8-608f-42ed-afe2-419fb620c62b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:50:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:20:INFO:
[92mINFO [0m:      Received: evaluate message d27b4875-ac37-46d5-880d-0d6deea17f48
02/17/2025 15:52:20:INFO:Received: evaluate message d27b4875-ac37-46d5-880d-0d6deea17f48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:53:08:INFO:
[92mINFO [0m:      Received: train message 2bdef49c-6aa6-422c-b999-36e52842d0d2
02/17/2025 15:53:08:INFO:Received: train message 2bdef49c-6aa6-422c-b999-36e52842d0d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:53:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:39:INFO:
[92mINFO [0m:      Received: evaluate message 038e4776-b625-4fe7-8386-47ee1e240fd9
02/17/2025 15:54:39:INFO:Received: evaluate message 038e4776-b625-4fe7-8386-47ee1e240fd9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:20:INFO:
[92mINFO [0m:      Received: train message 528e857c-a126-4bd9-8bb3-b8fa808b92c5
02/17/2025 15:55:20:INFO:Received: train message 528e857c-a126-4bd9-8bb3-b8fa808b92c5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:55:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:56:51:INFO:
[92mINFO [0m:      Received: evaluate message 7d0c1143-84aa-43d3-aed2-5e087660fde5
02/17/2025 15:56:51:INFO:Received: evaluate message 7d0c1143-84aa-43d3-aed2-5e087660fde5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:56:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:23:INFO:
[92mINFO [0m:      Received: train message e363f97a-6485-4c57-a42b-f27b940ff04c
02/17/2025 15:57:23:INFO:Received: train message e363f97a-6485-4c57-a42b-f27b940ff04c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:57:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:09:INFO:
[92mINFO [0m:      Received: evaluate message f6bc1d31-f867-44b4-a724-f60138cdd23b
02/17/2025 15:59:09:INFO:Received: evaluate message f6bc1d31-f867-44b4-a724-f60138cdd23b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:32:INFO:
[92mINFO [0m:      Received: train message cd78c3f3-2031-4777-8467-34a8f7b63b9c
02/17/2025 15:59:32:INFO:Received: train message cd78c3f3-2031-4777-8467-34a8f7b63b9c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:59:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:09:INFO:
[92mINFO [0m:      Received: evaluate message 71d3faa7-3ad7-4ec9-b334-747bea537b0c
02/17/2025 16:01:09:INFO:Received: evaluate message 71d3faa7-3ad7-4ec9-b334-747bea537b0c
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866], 'accuracy': [0.49960906958561374], 'auc': [0.6223554252585197], 'precision': [0.3853113538182961], 'recall': [0.49960906958561374], 'f1': [0.34049198829300026]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282], 'accuracy': [0.49960906958561374, 0.5222830336200156], 'auc': [0.6223554252585197, 0.6407563691851618], 'precision': [0.3853113538182961, 0.4289287570845902], 'recall': [0.49960906958561374, 0.5222830336200156], 'f1': [0.34049198829300026, 0.4252045406151577]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:59:INFO:
[92mINFO [0m:      Received: train message 7b65184d-de49-4cde-b8e6-808e359ceddc
02/17/2025 16:01:59:INFO:Received: train message 7b65184d-de49-4cde-b8e6-808e359ceddc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:02:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:29:INFO:
[92mINFO [0m:      Received: evaluate message f23e3d8b-72d9-4377-87ec-5ebf52e3c26e
02/17/2025 16:03:29:INFO:Received: evaluate message f23e3d8b-72d9-4377-87ec-5ebf52e3c26e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:03:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:57:INFO:
[92mINFO [0m:      Received: train message d7d610eb-eadd-491c-a0d6-1a153e3b99e3
02/17/2025 16:03:57:INFO:Received: train message d7d610eb-eadd-491c-a0d6-1a153e3b99e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:04:17:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:05:43:INFO:
[92mINFO [0m:      Received: evaluate message fee3f5b5-4d6e-4a35-8ce6-9cdc92a26dd9
02/17/2025 16:05:43:INFO:Received: evaluate message fee3f5b5-4d6e-4a35-8ce6-9cdc92a26dd9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:05:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:21:INFO:
[92mINFO [0m:      Received: train message 7042aa0b-a8cf-4965-8561-c941d74232b0
02/17/2025 16:06:21:INFO:Received: train message 7042aa0b-a8cf-4965-8561-c941d74232b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:06:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:07:49:INFO:
[92mINFO [0m:      Received: evaluate message 64448a5b-4358-4328-b4e8-c8b3fb38d1ec
02/17/2025 16:07:49:INFO:Received: evaluate message 64448a5b-4358-4328-b4e8-c8b3fb38d1ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:07:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:58:INFO:
[92mINFO [0m:      Received: train message 320efdb5-dbfd-4fe3-ac48-0d3b06cf7bc5
02/17/2025 16:08:58:INFO:Received: train message 320efdb5-dbfd-4fe3-ac48-0d3b06cf7bc5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:09:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:16:INFO:
[92mINFO [0m:      Received: evaluate message 5880614d-8a43-40ff-9a04-77823c766025
02/17/2025 16:10:16:INFO:Received: evaluate message 5880614d-8a43-40ff-9a04-77823c766025
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:10:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:14:INFO:
[92mINFO [0m:      Received: train message 04c9dd9c-32aa-42f4-84f5-2351821cf79b
02/17/2025 16:11:14:INFO:Received: train message 04c9dd9c-32aa-42f4-84f5-2351821cf79b

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:11:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:12:41:INFO:
[92mINFO [0m:      Received: evaluate message adaa8807-2c40-475b-9cac-dfc1a699ec92
02/17/2025 16:12:41:INFO:Received: evaluate message adaa8807-2c40-475b-9cac-dfc1a699ec92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:12:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:10:INFO:
[92mINFO [0m:      Received: train message fc0f16c3-7b0e-450d-ada2-3e2918a36a6f
02/17/2025 16:13:10:INFO:Received: train message fc0f16c3-7b0e-450d-ada2-3e2918a36a6f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:13:30:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:41:INFO:
[92mINFO [0m:      Received: evaluate message 391438bc-6f54-4faa-8669-57691c258e0c
02/17/2025 16:14:41:INFO:Received: evaluate message 391438bc-6f54-4faa-8669-57691c258e0c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:14:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:39:INFO:
[92mINFO [0m:      Received: train message f7d1ed50-7358-4774-86d2-11a9cc195c54
02/17/2025 16:15:39:INFO:Received: train message f7d1ed50-7358-4774-86d2-11a9cc195c54
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:16:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:38:INFO:
[92mINFO [0m:      Received: evaluate message d73659e3-43b8-4b81-91bd-9a599a12cdd6
02/17/2025 16:16:38:INFO:Received: evaluate message d73659e3-43b8-4b81-91bd-9a599a12cdd6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:16:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:17:49:INFO:
[92mINFO [0m:      Received: train message 187e16b6-41ad-47ab-bae1-d47fc9dd9302
02/17/2025 16:17:49:INFO:Received: train message 187e16b6-41ad-47ab-bae1-d47fc9dd9302
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:18:09:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:45:INFO:
[92mINFO [0m:      Received: evaluate message 4f5e1d74-1e13-494a-8d4b-aa29eb66d960
02/17/2025 16:18:45:INFO:Received: evaluate message 4f5e1d74-1e13-494a-8d4b-aa29eb66d960
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:18:51:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:19:54:INFO:
[92mINFO [0m:      Received: train message 9ae514ac-5495-4daf-a80d-ac3b5cf6af8c
02/17/2025 16:19:54:INFO:Received: train message 9ae514ac-5495-4daf-a80d-ac3b5cf6af8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:20:20:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:20:54:INFO:
[92mINFO [0m:      Received: evaluate message 02d0b5aa-b84b-46b4-835e-6fe8101f8792
02/17/2025 16:20:54:INFO:Received: evaluate message 02d0b5aa-b84b-46b4-835e-6fe8101f8792
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:20:58:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:22:05:INFO:
[92mINFO [0m:      Received: train message cf6fdd83-fa00-4506-b1d4-f43316abadcb
02/17/2025 16:22:05:INFO:Received: train message cf6fdd83-fa00-4506-b1d4-f43316abadcb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:31:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:14:INFO:
[92mINFO [0m:      Received: evaluate message 5787c041-d26f-41ce-a093-c9dc87050941
02/17/2025 16:23:14:INFO:Received: evaluate message 5787c041-d26f-41ce-a093-c9dc87050941
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:17:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:24:06:INFO:
[92mINFO [0m:      Received: train message 06bc69a0-7b48-4ae5-b555-6621af5177f4
02/17/2025 16:24:06:INFO:Received: train message 06bc69a0-7b48-4ae5-b555-6621af5177f4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:44:INFO:
[92mINFO [0m:      Received: evaluate message 3f6d0ebb-b147-439b-96c7-c120832f6539
02/17/2025 16:25:44:INFO:Received: evaluate message 3f6d0ebb-b147-439b-96c7-c120832f6539

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:23:INFO:
[92mINFO [0m:      Received: train message d6cf792f-57d0-40f5-befb-3fed19850f7f
02/17/2025 16:26:23:INFO:Received: train message d6cf792f-57d0-40f5-befb-3fed19850f7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:26:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:55:INFO:
[92mINFO [0m:      Received: evaluate message d3bf5d26-35f1-4eaa-8b37-2a0d1740a8df
02/17/2025 16:27:55:INFO:Received: evaluate message d3bf5d26-35f1-4eaa-8b37-2a0d1740a8df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:27:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:39:INFO:
[92mINFO [0m:      Received: train message 3fda0d50-3ae5-4191-a96f-bf2da26e0b46
02/17/2025 16:28:39:INFO:Received: train message 3fda0d50-3ae5-4191-a96f-bf2da26e0b46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:07:INFO:
[92mINFO [0m:      Received: evaluate message 64765aa5-e088-41a8-a59b-54a2e8a1ed4b
02/17/2025 16:30:07:INFO:Received: evaluate message 64765aa5-e088-41a8-a59b-54a2e8a1ed4b

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:09:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:31:03:INFO:
[92mINFO [0m:      Received: train message a6b4bedd-30db-4687-9015-ee64c6d96b8b
02/17/2025 16:31:03:INFO:Received: train message a6b4bedd-30db-4687-9015-ee64c6d96b8b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:32:INFO:
[92mINFO [0m:      Received: evaluate message 2180829e-d1bd-4698-9495-15e0074eba2b
02/17/2025 16:32:32:INFO:Received: evaluate message 2180829e-d1bd-4698-9495-15e0074eba2b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:32:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:24:INFO:
[92mINFO [0m:      Received: train message 44faa4a8-1b30-4c37-90da-c2d55de15b0e
02/17/2025 16:33:24:INFO:Received: train message 44faa4a8-1b30-4c37-90da-c2d55de15b0e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:33:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:34:43:INFO:
[92mINFO [0m:      Received: evaluate message 7c4b5802-3035-4ef6-a6ec-efe5138fd39c
02/17/2025 16:34:43:INFO:Received: evaluate message 7c4b5802-3035-4ef6-a6ec-efe5138fd39c

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:34:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:29:INFO:
[92mINFO [0m:      Received: train message c6543228-483a-4651-ace4-9ffff1c8d92e
02/17/2025 16:35:29:INFO:Received: train message c6543228-483a-4651-ace4-9ffff1c8d92e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:35:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:26:INFO:
[92mINFO [0m:      Received: evaluate message 4ae61ac3-0f07-4cb8-959b-c95ee10140ad
02/17/2025 16:36:26:INFO:Received: evaluate message 4ae61ac3-0f07-4cb8-959b-c95ee10140ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:36:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:24:INFO:
[92mINFO [0m:      Received: train message 15c3f215-f18e-4795-ac55-996b8fedd42e
02/17/2025 16:37:24:INFO:Received: train message 15c3f215-f18e-4795-ac55-996b8fedd42e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:37:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:44:INFO:
[92mINFO [0m:      Received: evaluate message 62233b53-c777-41a9-8eb6-98db777de398
02/17/2025 16:38:44:INFO:Received: evaluate message 62233b53-c777-41a9-8eb6-98db777de398

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:38:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:10:INFO:
[92mINFO [0m:      Received: train message 4d919cb2-82bc-4237-9a84-2e4a854cd4c3
02/17/2025 16:39:10:INFO:Received: train message 4d919cb2-82bc-4237-9a84-2e4a854cd4c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:39:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:24:INFO:
[92mINFO [0m:      Received: evaluate message 0fed236f-0056-416c-9335-4d5f547d0f2d
02/17/2025 16:40:24:INFO:Received: evaluate message 0fed236f-0056-416c-9335-4d5f547d0f2d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:40:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:16:INFO:
[92mINFO [0m:      Received: train message c7b781ee-9ff3-46c8-8868-ee63157f2a55
02/17/2025 16:41:16:INFO:Received: train message c7b781ee-9ff3-46c8-8868-ee63157f2a55
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:41:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:25:INFO:
[92mINFO [0m:      Received: evaluate message 4815c56b-b0fc-4252-98a1-f479f96e6e42
02/17/2025 16:42:25:INFO:Received: evaluate message 4815c56b-b0fc-4252-98a1-f479f96e6e42

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:42:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:50:INFO:
[92mINFO [0m:      Received: train message a3d7f897-5fc5-49cb-ad4a-e63f414137cb
02/17/2025 16:42:50:INFO:Received: train message a3d7f897-5fc5-49cb-ad4a-e63f414137cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:43:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:06:INFO:
[92mINFO [0m:      Received: evaluate message 9b2336aa-3dcb-487c-8e95-b444ea1b7717
02/17/2025 16:44:06:INFO:Received: evaluate message 9b2336aa-3dcb-487c-8e95-b444ea1b7717
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:44:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:57:INFO:
[92mINFO [0m:      Received: train message 22cd3319-bd7a-4da7-b4b2-336a8f3e505c
02/17/2025 16:44:57:INFO:Received: train message 22cd3319-bd7a-4da7-b4b2-336a8f3e505c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:45:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:12:INFO:
[92mINFO [0m:      Received: evaluate message bd146aea-ee7f-49e8-975d-52c1dee14291
02/17/2025 16:46:12:INFO:Received: evaluate message bd146aea-ee7f-49e8-975d-52c1dee14291

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:46:16:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:52:INFO:
[92mINFO [0m:      Received: train message 702cbd18-1570-4efe-8e61-41947eb67a3b
02/17/2025 16:46:52:INFO:Received: train message 702cbd18-1570-4efe-8e61-41947eb67a3b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:47:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:12:INFO:
[92mINFO [0m:      Received: evaluate message f0964d3b-5ad3-4baa-ad2a-9c0b5eadbe0d
02/17/2025 16:48:12:INFO:Received: evaluate message f0964d3b-5ad3-4baa-ad2a-9c0b5eadbe0d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:48:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:30:INFO:
[92mINFO [0m:      Received: train message 379f3f32-f2a4-42d9-aaf0-a053b6650516
02/17/2025 16:48:30:INFO:Received: train message 379f3f32-f2a4-42d9-aaf0-a053b6650516
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:48:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:09:INFO:
[92mINFO [0m:      Received: evaluate message 7193df60-da43-4c65-a3c7-fadce52f507e
02/17/2025 16:50:09:INFO:Received: evaluate message 7193df60-da43-4c65-a3c7-fadce52f507e

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:50:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:14:INFO:
[92mINFO [0m:      Received: reconnect message 24b70488-8736-418f-8455-9d21af66f386
02/17/2025 16:50:14:INFO:Received: reconnect message 24b70488-8736-418f-8455-9d21af66f386
02/17/2025 16:50:14:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:50:14:INFO:Disconnect and shut down

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}



Final client history:
{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}

