nohup: ignoring input
02/14/2025 23:50:56:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:50:56:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:50:57:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739605857.034588 1504531 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:51:34:INFO:
[92mINFO [0m:      Received: train message 56b9bf09-23ce-4408-adb9-456c0409caa6
02/14/2025 23:51:34:INFO:Received: train message 56b9bf09-23ce-4408-adb9-456c0409caa6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:51:53:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:52:40:INFO:
[92mINFO [0m:      Received: evaluate message b2615965-8908-491f-8e58-73216fb4e6e1
02/14/2025 23:52:40:INFO:Received: evaluate message b2615965-8908-491f-8e58-73216fb4e6e1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:52:42:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:53:16:INFO:
[92mINFO [0m:      Received: train message 9a539af7-768a-4f06-bef1-3df51a0127b6
02/14/2025 23:53:16:INFO:Received: train message 9a539af7-768a-4f06-bef1-3df51a0127b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:53:35:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:54:37:INFO:
[92mINFO [0m:      Received: evaluate message 61e13fdf-4f74-45bb-a92f-4fbf099417ae
02/14/2025 23:54:37:INFO:Received: evaluate message 61e13fdf-4f74-45bb-a92f-4fbf099417ae
[92mINFO [0m:      Sent reply
02/14/2025 23:54:40:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:55:16:INFO:
[92mINFO [0m:      Received: train message 9e30d078-6b71-4cbd-9bfc-e77b035aa17d
02/14/2025 23:55:16:INFO:Received: train message 9e30d078-6b71-4cbd-9bfc-e77b035aa17d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:55:35:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:16:INFO:
[92mINFO [0m:      Received: evaluate message 104bdef0-9e59-4b84-9dd5-21f93234523a
02/14/2025 23:56:16:INFO:Received: evaluate message 104bdef0-9e59-4b84-9dd5-21f93234523a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:56:19:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:57:00:INFO:
[92mINFO [0m:      Received: train message ee43bad5-1ab3-4471-b9a2-88a9a7b842c9
02/14/2025 23:57:00:INFO:Received: train message ee43bad5-1ab3-4471-b9a2-88a9a7b842c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:57:20:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:57:56:INFO:
[92mINFO [0m:      Received: evaluate message 629c6f10-9a83-4956-b4c6-f81c9f6ae6f6
02/14/2025 23:57:56:INFO:Received: evaluate message 629c6f10-9a83-4956-b4c6-f81c9f6ae6f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:58:00:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:41:INFO:
[92mINFO [0m:      Received: train message 54311cc2-21ba-4f12-ba98-35a4bcf5d1db
02/14/2025 23:58:41:INFO:Received: train message 54311cc2-21ba-4f12-ba98-35a4bcf5d1db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:59:00:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:59:53:INFO:
[92mINFO [0m:      Received: evaluate message ed5e7e31-8fb5-4c09-9a94-8c7b8f84f1b8
02/14/2025 23:59:53:INFO:Received: evaluate message ed5e7e31-8fb5-4c09-9a94-8c7b8f84f1b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:59:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:50:INFO:
[92mINFO [0m:      Received: train message 944de148-5dd6-4b7b-80a7-89c372a6d238
02/15/2025 00:00:50:INFO:Received: train message 944de148-5dd6-4b7b-80a7-89c372a6d238
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:01:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:01:57:INFO:
[92mINFO [0m:      Received: evaluate message 848f451c-5c6a-451f-97ae-5a3cbbbce38d
02/15/2025 00:01:57:INFO:Received: evaluate message 848f451c-5c6a-451f-97ae-5a3cbbbce38d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:02:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:44:INFO:
[92mINFO [0m:      Received: train message 7e1ac95a-2a7a-4613-80fa-64c84233fbf6
02/15/2025 00:02:44:INFO:Received: train message 7e1ac95a-2a7a-4613-80fa-64c84233fbf6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:03:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:53:INFO:
[92mINFO [0m:      Received: evaluate message 0aece43a-b4e6-43db-93ab-e99726c1999d
02/15/2025 00:03:53:INFO:Received: evaluate message 0aece43a-b4e6-43db-93ab-e99726c1999d
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392], 'accuracy': [0.17279124315871774], 'auc': [0.36910406569460863], 'precision': [0.19215364459241882], 'recall': [0.17279124315871774], 'f1': [0.15428779652186775]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477], 'accuracy': [0.17279124315871774, 0.23534010946051603], 'auc': [0.36910406569460863, 0.37509675462346337], 'precision': [0.19215364459241882, 0.20470434628072817], 'recall': [0.17279124315871774, 0.23534010946051603], 'f1': [0.15428779652186775, 0.2139057332831779]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:03:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:04:39:INFO:
[92mINFO [0m:      Received: train message 8a8341dd-50b4-4c4b-89df-a116efe8e3d8
02/15/2025 00:04:39:INFO:Received: train message 8a8341dd-50b4-4c4b-89df-a116efe8e3d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:04:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:07:INFO:
[92mINFO [0m:      Received: evaluate message 01fa2d86-bc7b-422b-9f50-1cfcbb41003c
02/15/2025 00:06:07:INFO:Received: evaluate message 01fa2d86-bc7b-422b-9f50-1cfcbb41003c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:06:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:31:INFO:
[92mINFO [0m:      Received: train message ab8cff5f-2f8a-46a6-bc47-1b997619add3
02/15/2025 00:06:31:INFO:Received: train message ab8cff5f-2f8a-46a6-bc47-1b997619add3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:06:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:04:INFO:
[92mINFO [0m:      Received: evaluate message 56b28712-cece-4eb9-a11b-a892da52d882
02/15/2025 00:08:04:INFO:Received: evaluate message 56b28712-cece-4eb9-a11b-a892da52d882
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:08:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:48:INFO:
[92mINFO [0m:      Received: train message a8ba1802-7723-447f-a213-b0167d6749f1
02/15/2025 00:08:48:INFO:Received: train message a8ba1802-7723-447f-a213-b0167d6749f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:09:45:INFO:
[92mINFO [0m:      Received: evaluate message 74b133b4-e822-4aae-8af3-921190f3eb7c
02/15/2025 00:09:45:INFO:Received: evaluate message 74b133b4-e822-4aae-8af3-921190f3eb7c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:09:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:43:INFO:
[92mINFO [0m:      Received: train message 1ff404f7-3250-49cd-80f7-31c9c425eb2a
02/15/2025 00:10:43:INFO:Received: train message 1ff404f7-3250-49cd-80f7-31c9c425eb2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:11:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:11:52:INFO:
[92mINFO [0m:      Received: evaluate message 7cdfe8a8-d264-43f6-aef4-bc2071f75abc
02/15/2025 00:11:52:INFO:Received: evaluate message 7cdfe8a8-d264-43f6-aef4-bc2071f75abc

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:11:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:42:INFO:
[92mINFO [0m:      Received: train message 33f03146-a068-489c-aee0-3476936fdedc
02/15/2025 00:12:42:INFO:Received: train message 33f03146-a068-489c-aee0-3476936fdedc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:13:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:59:INFO:
[92mINFO [0m:      Received: evaluate message cc2804d8-c2f1-4a79-bd8d-91e8a844b2ab
02/15/2025 00:13:59:INFO:Received: evaluate message cc2804d8-c2f1-4a79-bd8d-91e8a844b2ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:14:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:38:INFO:
[92mINFO [0m:      Received: train message fd351fa7-f893-4d84-8436-4808633325a8
02/15/2025 00:14:38:INFO:Received: train message fd351fa7-f893-4d84-8436-4808633325a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:14:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:47:INFO:
[92mINFO [0m:      Received: evaluate message 9ac0da9d-bd0c-4592-820d-6b272adc9e4f
02/15/2025 00:15:47:INFO:Received: evaluate message 9ac0da9d-bd0c-4592-820d-6b272adc9e4f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:15:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:16:24:INFO:
[92mINFO [0m:      Received: train message 53582b43-25e5-4492-99a9-c7c4597a8dea
02/15/2025 00:16:24:INFO:Received: train message 53582b43-25e5-4492-99a9-c7c4597a8dea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:42:INFO:
[92mINFO [0m:      Received: evaluate message bbede573-e8bc-4cf4-8979-334d6e397d2d
02/15/2025 00:17:42:INFO:Received: evaluate message bbede573-e8bc-4cf4-8979-334d6e397d2d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:17:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:18:07:INFO:
[92mINFO [0m:      Received: train message 96498a9f-058a-47bf-ae03-b91e9e0d01df
02/15/2025 00:18:07:INFO:Received: train message 96498a9f-058a-47bf-ae03-b91e9e0d01df

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:25:INFO:
[92mINFO [0m:      Received: evaluate message bf9c82d5-2522-457f-8806-3ae3bde1f3d8
02/15/2025 00:19:25:INFO:Received: evaluate message bf9c82d5-2522-457f-8806-3ae3bde1f3d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:59:INFO:
[92mINFO [0m:      Received: train message 35173556-5253-49b0-a819-cc4dda14e5fb
02/15/2025 00:19:59:INFO:Received: train message 35173556-5253-49b0-a819-cc4dda14e5fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:20:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:31:INFO:
[92mINFO [0m:      Received: evaluate message 0323ef27-b026-475d-933f-78b965e4ee35
02/15/2025 00:21:31:INFO:Received: evaluate message 0323ef27-b026-475d-933f-78b965e4ee35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:16:INFO:
[92mINFO [0m:      Received: train message f86e6789-6fc9-4b27-b9a6-34cd4e8075c3
02/15/2025 00:22:16:INFO:Received: train message f86e6789-6fc9-4b27-b9a6-34cd4e8075c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:22:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:23:25:INFO:
[92mINFO [0m:      Received: evaluate message 4587cec0-5fff-4825-bb09-6ca0fe053f15
02/15/2025 00:23:25:INFO:Received: evaluate message 4587cec0-5fff-4825-bb09-6ca0fe053f15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:23:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:23:49:INFO:
[92mINFO [0m:      Received: train message 149d1369-a041-4e37-b7a9-7d583528efa5
02/15/2025 00:23:49:INFO:Received: train message 149d1369-a041-4e37-b7a9-7d583528efa5
Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:24:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:25:34:INFO:
[92mINFO [0m:      Received: evaluate message 131b42b1-91df-4190-884b-bfff71d8fb99
02/15/2025 00:25:34:INFO:Received: evaluate message 131b42b1-91df-4190-884b-bfff71d8fb99
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:25:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:05:INFO:
[92mINFO [0m:      Received: train message 971f2e76-c573-47e8-89bf-16c7f5d2e6a2
02/15/2025 00:26:05:INFO:Received: train message 971f2e76-c573-47e8-89bf-16c7f5d2e6a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:26:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:27:INFO:
[92mINFO [0m:      Received: evaluate message 9d0f4d32-ea79-4cf4-a5f0-c2c730a1f70a
02/15/2025 00:27:27:INFO:Received: evaluate message 9d0f4d32-ea79-4cf4-a5f0-c2c730a1f70a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:27:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:28:04:INFO:
[92mINFO [0m:      Received: train message 9a090315-a986-46bc-9d74-aba150957eb5
02/15/2025 00:28:04:INFO:Received: train message 9a090315-a986-46bc-9d74-aba150957eb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:28:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:27:INFO:
[92mINFO [0m:      Received: evaluate message a059e824-f364-4c3e-8bde-75d24bb28e2b
02/15/2025 00:29:27:INFO:Received: evaluate message a059e824-f364-4c3e-8bde-75d24bb28e2b
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:45:INFO:
[92mINFO [0m:      Received: train message d3c3defe-c338-4ef4-aced-f89e4154ff80
02/15/2025 00:29:45:INFO:Received: train message d3c3defe-c338-4ef4-aced-f89e4154ff80
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:24:INFO:
[92mINFO [0m:      Received: evaluate message f26a644a-7bf8-4fc9-9433-46807cbd18b3
02/15/2025 00:31:24:INFO:Received: evaluate message f26a644a-7bf8-4fc9-9433-46807cbd18b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:32:09:INFO:
[92mINFO [0m:      Received: train message af56b803-aa8f-474d-bc00-334ad00fe8ad
02/15/2025 00:32:09:INFO:Received: train message af56b803-aa8f-474d-bc00-334ad00fe8ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:24:INFO:
[92mINFO [0m:      Received: evaluate message 7b343269-3240-4ec4-a61f-af40f48dfe6f
02/15/2025 00:33:24:INFO:Received: evaluate message 7b343269-3240-4ec4-a61f-af40f48dfe6f

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:53:INFO:
[92mINFO [0m:      Received: train message 984f1b5a-fa9d-4c0a-b36c-985608e70b79
02/15/2025 00:33:53:INFO:Received: train message 984f1b5a-fa9d-4c0a-b36c-985608e70b79
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:34:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:13:INFO:
[92mINFO [0m:      Received: evaluate message ea70849f-60d8-498f-9ff1-c80ce10df71d
02/15/2025 00:35:13:INFO:Received: evaluate message ea70849f-60d8-498f-9ff1-c80ce10df71d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:35:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:42:INFO:
[92mINFO [0m:      Received: train message c0a69e60-1ea3-457d-adf8-b22b384ba824
02/15/2025 00:35:42:INFO:Received: train message c0a69e60-1ea3-457d-adf8-b22b384ba824
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:35:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:10:INFO:
[92mINFO [0m:      Received: evaluate message 82b4f65c-11df-44f0-b4ec-db3bef1ed40c
02/15/2025 00:37:10:INFO:Received: evaluate message 82b4f65c-11df-44f0-b4ec-db3bef1ed40c

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:37:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:48:INFO:
[92mINFO [0m:      Received: train message 49eb52dd-a6e1-474a-ab01-714f6741a576
02/15/2025 00:37:48:INFO:Received: train message 49eb52dd-a6e1-474a-ab01-714f6741a576
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:38:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:39:03:INFO:
[92mINFO [0m:      Received: evaluate message 0e239b01-01b0-48fa-b029-2a4614c66661
02/15/2025 00:39:03:INFO:Received: evaluate message 0e239b01-01b0-48fa-b029-2a4614c66661
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:39:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:39:25:INFO:
[92mINFO [0m:      Received: train message a0dc7a95-5e47-462b-8b0c-1606f264a039
02/15/2025 00:39:25:INFO:Received: train message a0dc7a95-5e47-462b-8b0c-1606f264a039
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:39:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:54:INFO:
[92mINFO [0m:      Received: evaluate message e5b1499f-4439-4e30-8959-98587a0d6264
02/15/2025 00:40:54:INFO:Received: evaluate message e5b1499f-4439-4e30-8959-98587a0d6264

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:41:26:INFO:
[92mINFO [0m:      Received: train message 2f2416ec-b74a-43ce-8128-27efb55d183d
02/15/2025 00:41:26:INFO:Received: train message 2f2416ec-b74a-43ce-8128-27efb55d183d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:43:INFO:
[92mINFO [0m:      Received: evaluate message 2a04a24a-75c2-4052-8e75-5f439f875998
02/15/2025 00:42:43:INFO:Received: evaluate message 2a04a24a-75c2-4052-8e75-5f439f875998
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:42:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:43:19:INFO:
[92mINFO [0m:      Received: train message 7b9b33b0-0864-4e94-ba1a-38a5546fa034
02/15/2025 00:43:19:INFO:Received: train message 7b9b33b0-0864-4e94-ba1a-38a5546fa034
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:31:INFO:
[92mINFO [0m:      Received: evaluate message 5c17e99d-d497-4ac3-b9da-4674ecd2f590
02/15/2025 00:44:31:INFO:Received: evaluate message 5c17e99d-d497-4ac3-b9da-4674ecd2f590

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:45:16:INFO:
[92mINFO [0m:      Received: train message 61907c9d-9b37-43f2-beb4-7e198e817898
02/15/2025 00:45:16:INFO:Received: train message 61907c9d-9b37-43f2-beb4-7e198e817898
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:24:INFO:
[92mINFO [0m:      Received: evaluate message 86412bb0-54bc-4129-a90c-5933b5f5117e
02/15/2025 00:46:24:INFO:Received: evaluate message 86412bb0-54bc-4129-a90c-5933b5f5117e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:57:INFO:
[92mINFO [0m:      Received: train message ecb67b25-74ae-47fc-80b2-fc5c602c8db6
02/15/2025 00:46:57:INFO:Received: train message ecb67b25-74ae-47fc-80b2-fc5c602c8db6

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:47:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:08:INFO:
[92mINFO [0m:      Received: evaluate message f37f0690-3021-49bb-bc16-05455ca9c392
02/15/2025 00:48:08:INFO:Received: evaluate message f37f0690-3021-49bb-bc16-05455ca9c392
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:17:INFO:
[92mINFO [0m:      Received: reconnect message 06c058f2-4f7b-4974-83c9-fe291fbe321a
02/15/2025 00:48:17:INFO:Received: reconnect message 06c058f2-4f7b-4974-83c9-fe291fbe321a
02/15/2025 00:48:17:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 00:48:17:INFO:Disconnect and shut down
Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}



Final client history:
{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}

