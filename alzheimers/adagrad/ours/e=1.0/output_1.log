nohup: ignoring input
02/17/2025 15:45:39:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:45:39:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:45:39:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739835940.041725  742549 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:46:07:INFO:
[92mINFO [0m:      Received: train message 833d2e82-c255-43d1-92d0-4e12964a9b16
02/17/2025 15:46:07:INFO:Received: train message 833d2e82-c255-43d1-92d0-4e12964a9b16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:46:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:47:21:INFO:
[92mINFO [0m:      Received: evaluate message c59b72dd-3fba-48f7-b90e-1aa1a6393e3c
02/17/2025 15:47:21:INFO:Received: evaluate message c59b72dd-3fba-48f7-b90e-1aa1a6393e3c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:47:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:48:13:INFO:
[92mINFO [0m:      Received: train message edc40ec7-059d-4d26-a83f-c0005d0149cd
02/17/2025 15:48:13:INFO:Received: train message edc40ec7-059d-4d26-a83f-c0005d0149cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:48:56:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:49:55:INFO:
[92mINFO [0m:      Received: evaluate message 3a44cd2d-ed01-4665-b998-e4d6e54a2964
02/17/2025 15:49:55:INFO:Received: evaluate message 3a44cd2d-ed01-4665-b998-e4d6e54a2964
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:49:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:50:44:INFO:
[92mINFO [0m:      Received: train message d00bd818-37e8-4323-b84e-ba4dee8ed806
02/17/2025 15:50:44:INFO:Received: train message d00bd818-37e8-4323-b84e-ba4dee8ed806
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:51:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:22:INFO:
[92mINFO [0m:      Received: evaluate message 9e4dafb4-53fa-4b3b-ac89-e35b62fbce0f
02/17/2025 15:52:22:INFO:Received: evaluate message 9e4dafb4-53fa-4b3b-ac89-e35b62fbce0f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:53:06:INFO:
[92mINFO [0m:      Received: train message a83e652f-0608-43a5-afac-a662a8ee5efc
02/17/2025 15:53:06:INFO:Received: train message a83e652f-0608-43a5-afac-a662a8ee5efc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:53:51:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:40:INFO:
[92mINFO [0m:      Received: evaluate message b0f9e476-76af-46ca-9de2-166ac84865e8
02/17/2025 15:54:40:INFO:Received: evaluate message b0f9e476-76af-46ca-9de2-166ac84865e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:19:INFO:
[92mINFO [0m:      Received: train message 97abeca0-d528-4bc6-95da-6392e6f5a792
02/17/2025 15:55:19:INFO:Received: train message 97abeca0-d528-4bc6-95da-6392e6f5a792
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:56:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:56:58:INFO:
[92mINFO [0m:      Received: evaluate message 3c32fda6-626d-477b-bce6-824d50724e9b
02/17/2025 15:56:58:INFO:Received: evaluate message 3c32fda6-626d-477b-bce6-824d50724e9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:57:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:21:INFO:
[92mINFO [0m:      Received: train message d6171c2d-da4b-4901-88a9-98e81d6e5a1c
02/17/2025 15:57:21:INFO:Received: train message d6171c2d-da4b-4901-88a9-98e81d6e5a1c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:58:51:INFO:
[92mINFO [0m:      Received: evaluate message 95ea00c3-5834-4ab8-a004-de1bad2b9ba6
02/17/2025 15:58:51:INFO:Received: evaluate message 95ea00c3-5834-4ab8-a004-de1bad2b9ba6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:58:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:47:INFO:
[92mINFO [0m:      Received: train message 07edf8ee-ecfa-40aa-9d75-10c77841164b
02/17/2025 15:59:47:INFO:Received: train message 07edf8ee-ecfa-40aa-9d75-10c77841164b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:00:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:04:INFO:
[92mINFO [0m:      Received: evaluate message ebad6a64-8b1b-4033-a41a-886f7c8a04d5
02/17/2025 16:01:04:INFO:Received: evaluate message ebad6a64-8b1b-4033-a41a-886f7c8a04d5
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866], 'accuracy': [0.49960906958561374], 'auc': [0.6223554252585197], 'precision': [0.3853113538182961], 'recall': [0.49960906958561374], 'f1': [0.34049198829300026]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282], 'accuracy': [0.49960906958561374, 0.5222830336200156], 'auc': [0.6223554252585197, 0.6407563691851618], 'precision': [0.3853113538182961, 0.4289287570845902], 'recall': [0.49960906958561374, 0.5222830336200156], 'f1': [0.34049198829300026, 0.4252045406151577]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:59:INFO:
[92mINFO [0m:      Received: train message 465381c0-8e65-4fd8-92ce-27b4d59a5509
02/17/2025 16:01:59:INFO:Received: train message 465381c0-8e65-4fd8-92ce-27b4d59a5509
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:02:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:14:INFO:
[92mINFO [0m:      Received: evaluate message 6bb0277e-cac9-4f82-b437-f396fa97322d
02/17/2025 16:03:14:INFO:Received: evaluate message 6bb0277e-cac9-4f82-b437-f396fa97322d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:03:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:18:INFO:
[92mINFO [0m:      Received: train message fd7b0f51-043a-4b90-a798-aa4256027633
02/17/2025 16:04:18:INFO:Received: train message fd7b0f51-043a-4b90-a798-aa4256027633
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:04:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:05:40:INFO:
[92mINFO [0m:      Received: evaluate message b4baa128-82e6-44d7-9c72-eba6a4a56750
02/17/2025 16:05:40:INFO:Received: evaluate message b4baa128-82e6-44d7-9c72-eba6a4a56750
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:05:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:35:INFO:
[92mINFO [0m:      Received: train message 18482832-abc9-420b-b630-40add9bf7c71
02/17/2025 16:06:35:INFO:Received: train message 18482832-abc9-420b-b630-40add9bf7c71
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:07:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:13:INFO:
[92mINFO [0m:      Received: evaluate message acbd172b-fa87-4888-8dfe-1c80e5a2c25c
02/17/2025 16:08:13:INFO:Received: evaluate message acbd172b-fa87-4888-8dfe-1c80e5a2c25c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:16:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:54:INFO:
[92mINFO [0m:      Received: train message c80742af-cba1-4231-8d76-195e358a5bfa
02/17/2025 16:08:54:INFO:Received: train message c80742af-cba1-4231-8d76-195e358a5bfa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:09:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:09:INFO:
[92mINFO [0m:      Received: evaluate message f7c735d2-0ec3-4896-abed-ba16a0b70123
02/17/2025 16:10:09:INFO:Received: evaluate message f7c735d2-0ec3-4896-abed-ba16a0b70123
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:10:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:16:INFO:
[92mINFO [0m:      Received: train message 07478e5a-6430-464d-8cf4-1c175c6e011e
02/17/2025 16:11:17:INFO:Received: train message 07478e5a-6430-464d-8cf4-1c175c6e011e

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:11:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:12:45:INFO:
[92mINFO [0m:      Received: evaluate message 8e9b87ae-af32-4ffe-9a31-8275ac891766
02/17/2025 16:12:45:INFO:Received: evaluate message 8e9b87ae-af32-4ffe-9a31-8275ac891766
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:12:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:26:INFO:
[92mINFO [0m:      Received: train message a53ebf54-b687-4ccf-b52c-952dabed09ec
02/17/2025 16:13:26:INFO:Received: train message a53ebf54-b687-4ccf-b52c-952dabed09ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:14:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:53:INFO:
[92mINFO [0m:      Received: evaluate message ac56bbd7-c668-4c06-8777-818524d84e1d
02/17/2025 16:14:53:INFO:Received: evaluate message ac56bbd7-c668-4c06-8777-818524d84e1d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:14:58:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:31:INFO:
[92mINFO [0m:      Received: train message deab049a-51b6-469a-b67b-3f8555166150
02/17/2025 16:15:31:INFO:Received: train message deab049a-51b6-469a-b67b-3f8555166150
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:16:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:59:INFO:
[92mINFO [0m:      Received: evaluate message ff994f0a-a527-4ecd-8e2f-231f384fc027
02/17/2025 16:16:59:INFO:Received: evaluate message ff994f0a-a527-4ecd-8e2f-231f384fc027
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:17:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:17:46:INFO:
[92mINFO [0m:      Received: train message be7ecda4-1161-44ba-8aab-3e933484fdd6
02/17/2025 16:17:46:INFO:Received: train message be7ecda4-1161-44ba-8aab-3e933484fdd6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:18:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:45:INFO:
[92mINFO [0m:      Received: evaluate message cf3d830c-8fef-4c3d-9579-8edf56d4006b
02/17/2025 16:18:45:INFO:Received: evaluate message cf3d830c-8fef-4c3d-9579-8edf56d4006b
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:18:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:19:49:INFO:
[92mINFO [0m:      Received: train message 49934e9f-0c01-474d-9203-6c20c5c0d8cc
02/17/2025 16:19:49:INFO:Received: train message 49934e9f-0c01-474d-9203-6c20c5c0d8cc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:20:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:23:INFO:
[92mINFO [0m:      Received: evaluate message 45e37b72-d630-4ad4-8d15-265b670c4c48
02/17/2025 16:21:23:INFO:Received: evaluate message 45e37b72-d630-4ad4-8d15-265b670c4c48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:21:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:58:INFO:
[92mINFO [0m:      Received: train message 3869e428-d5fb-40a5-8499-efccb9a1e2b1
02/17/2025 16:21:58:INFO:Received: train message 3869e428-d5fb-40a5-8499-efccb9a1e2b1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:31:INFO:
[92mINFO [0m:      Received: evaluate message 8cc31d78-6c0d-434c-935e-7611ead8d7c3
02/17/2025 16:23:31:INFO:Received: evaluate message 8cc31d78-6c0d-434c-935e-7611ead8d7c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:24:13:INFO:
[92mINFO [0m:      Received: train message a4aabcb7-bc8f-4c55-bdeb-8b5e65392d6a
02/17/2025 16:24:13:INFO:Received: train message a4aabcb7-bc8f-4c55-bdeb-8b5e65392d6a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:25:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:38:INFO:
[92mINFO [0m:      Received: evaluate message 36102e19-3079-4aec-946d-8352ef9a1f95
02/17/2025 16:25:38:INFO:Received: evaluate message 36102e19-3079-4aec-946d-8352ef9a1f95

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:31:INFO:
[92mINFO [0m:      Received: train message 9bc0ac8f-bc8e-4183-bdbc-bd7830d1fdf8
02/17/2025 16:26:31:INFO:Received: train message 9bc0ac8f-bc8e-4183-bdbc-bd7830d1fdf8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:27:19:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:50:INFO:
[92mINFO [0m:      Received: evaluate message 646314d3-1a8f-4a8e-8118-32518f38c6d2
02/17/2025 16:27:50:INFO:Received: evaluate message 646314d3-1a8f-4a8e-8118-32518f38c6d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:27:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:46:INFO:
[92mINFO [0m:      Received: train message b47edbb4-bca8-44f3-8b03-8c2d53c08c1a
02/17/2025 16:28:46:INFO:Received: train message b47edbb4-bca8-44f3-8b03-8c2d53c08c1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:31:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:12:INFO:
[92mINFO [0m:      Received: evaluate message 3a152b5d-5716-42ec-9f85-d2a7dc8ce868
02/17/2025 16:30:12:INFO:Received: evaluate message 3a152b5d-5716-42ec-9f85-d2a7dc8ce868

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:31:03:INFO:
[92mINFO [0m:      Received: train message 077459de-59bc-46f8-9c8a-443bfdde65af
02/17/2025 16:31:03:INFO:Received: train message 077459de-59bc-46f8-9c8a-443bfdde65af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:22:INFO:
[92mINFO [0m:      Received: evaluate message 7df0e9a0-70c4-4564-b14a-f9bef8f150e4
02/17/2025 16:32:22:INFO:Received: evaluate message 7df0e9a0-70c4-4564-b14a-f9bef8f150e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:32:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:06:INFO:
[92mINFO [0m:      Received: train message ab795594-2dbc-4ebb-b255-718deb77121f
02/17/2025 16:33:06:INFO:Received: train message ab795594-2dbc-4ebb-b255-718deb77121f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:33:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:34:50:INFO:
[92mINFO [0m:      Received: evaluate message 115d89c8-b4d0-4d6d-a5d7-45ae2745b97e
02/17/2025 16:34:50:INFO:Received: evaluate message 115d89c8-b4d0-4d6d-a5d7-45ae2745b97e

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:34:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:27:INFO:
[92mINFO [0m:      Received: train message 9c82ad0a-1547-4f5b-9b94-17fb9aa35e6a
02/17/2025 16:35:27:INFO:Received: train message 9c82ad0a-1547-4f5b-9b94-17fb9aa35e6a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:36:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:46:INFO:
[92mINFO [0m:      Received: evaluate message 3ce8f71c-ec7d-4ea9-9d4a-f1edc08f6022
02/17/2025 16:36:46:INFO:Received: evaluate message 3ce8f71c-ec7d-4ea9-9d4a-f1edc08f6022
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:36:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:20:INFO:
[92mINFO [0m:      Received: train message 8b81ad96-95d1-48b9-8522-a83c92e47d85
02/17/2025 16:37:20:INFO:Received: train message 8b81ad96-95d1-48b9-8522-a83c92e47d85
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:38:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:40:INFO:
[92mINFO [0m:      Received: evaluate message 086fe33c-49db-4084-ad69-419243972c6a
02/17/2025 16:38:40:INFO:Received: evaluate message 086fe33c-49db-4084-ad69-419243972c6a

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:38:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:27:INFO:
[92mINFO [0m:      Received: train message 0c9f4525-fe79-4d2f-94a6-ba7c7c2f2012
02/17/2025 16:39:27:INFO:Received: train message 0c9f4525-fe79-4d2f-94a6-ba7c7c2f2012
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:40:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:42:INFO:
[92mINFO [0m:      Received: evaluate message 393663a1-a404-4985-9117-3a007d1c137b
02/17/2025 16:40:42:INFO:Received: evaluate message 393663a1-a404-4985-9117-3a007d1c137b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:40:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:01:INFO:
[92mINFO [0m:      Received: train message 984f0955-6761-4ab3-874f-f1f3a4f73a04
02/17/2025 16:41:01:INFO:Received: train message 984f0955-6761-4ab3-874f-f1f3a4f73a04
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:41:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:25:INFO:
[92mINFO [0m:      Received: evaluate message 45ed16a6-da65-4fab-9f5d-b434faff5c29
02/17/2025 16:42:25:INFO:Received: evaluate message 45ed16a6-da65-4fab-9f5d-b434faff5c29

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:42:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:08:INFO:
[92mINFO [0m:      Received: train message f6673a95-1d10-41df-97d8-6a522205f2b3
02/17/2025 16:43:08:INFO:Received: train message f6673a95-1d10-41df-97d8-6a522205f2b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:43:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:08:INFO:
[92mINFO [0m:      Received: evaluate message db662beb-1c27-4a2a-be07-6dd9b614159a
02/17/2025 16:44:08:INFO:Received: evaluate message db662beb-1c27-4a2a-be07-6dd9b614159a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:44:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:58:INFO:
[92mINFO [0m:      Received: train message 8b55636d-a4a6-41cc-b876-4e7d070ac151
02/17/2025 16:44:58:INFO:Received: train message 8b55636d-a4a6-41cc-b876-4e7d070ac151
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:45:31:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:12:INFO:
[92mINFO [0m:      Received: evaluate message 237e89e7-7612-499e-8535-9261e4f2337a
02/17/2025 16:46:12:INFO:Received: evaluate message 237e89e7-7612-499e-8535-9261e4f2337a

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:46:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:56:INFO:
[92mINFO [0m:      Received: train message e2c0205f-a6eb-4951-8a15-d6efc1138a2a
02/17/2025 16:46:56:INFO:Received: train message e2c0205f-a6eb-4951-8a15-d6efc1138a2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:47:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:47:53:INFO:
[92mINFO [0m:      Received: evaluate message e941fc24-48d2-4d14-a67a-dd073b05f4fd
02/17/2025 16:47:53:INFO:Received: evaluate message e941fc24-48d2-4d14-a67a-dd073b05f4fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:47:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:54:INFO:
[92mINFO [0m:      Received: train message 383e9b85-90c4-4bcb-8505-5cf98580f846
02/17/2025 16:48:54:INFO:Received: train message 383e9b85-90c4-4bcb-8505-5cf98580f846
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:49:51:INFO:
[92mINFO [0m:      Received: evaluate message 285f2656-e722-4aa6-aed6-f38b619c822b
02/17/2025 16:49:51:INFO:Received: evaluate message 285f2656-e722-4aa6-aed6-f38b619c822b

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:49:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:14:INFO:
[92mINFO [0m:      Received: reconnect message 26217457-5ce6-4a50-80e6-e786ee3119c0
02/17/2025 16:50:14:INFO:Received: reconnect message 26217457-5ce6-4a50-80e6-e786ee3119c0
02/17/2025 16:50:14:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:50:14:INFO:Disconnect and shut down

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}



Final client history:
{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}

