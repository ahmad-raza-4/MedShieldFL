nohup: ignoring input
02/17/2025 15:45:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:45:31:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:45:31:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739835932.022530  742200 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:46:17:INFO:
[92mINFO [0m:      Received: train message 910c8a28-6359-487a-ae55-375bb78eabfe
02/17/2025 15:46:17:INFO:Received: train message 910c8a28-6359-487a-ae55-375bb78eabfe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:46:40:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:47:32:INFO:
[92mINFO [0m:      Received: evaluate message b9abd315-2a64-41e4-aced-62f5a47f6e1f
02/17/2025 15:47:32:INFO:Received: evaluate message b9abd315-2a64-41e4-aced-62f5a47f6e1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:47:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:48:12:INFO:
[92mINFO [0m:      Received: train message 0301fa92-ab2d-4651-8b6e-0710a82cb607
02/17/2025 15:48:12:INFO:Received: train message 0301fa92-ab2d-4651-8b6e-0710a82cb607
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:48:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:49:43:INFO:
[92mINFO [0m:      Received: evaluate message 9291d783-b480-46a7-9d71-3ff630f47ceb
02/17/2025 15:49:43:INFO:Received: evaluate message 9291d783-b480-46a7-9d71-3ff630f47ceb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:49:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:50:28:INFO:
[92mINFO [0m:      Received: train message 504391b3-911c-4091-82ec-fc39ca4b84bf
02/17/2025 15:50:28:INFO:Received: train message 504391b3-911c-4091-82ec-fc39ca4b84bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:50:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:21:INFO:
[92mINFO [0m:      Received: evaluate message a180320a-31e7-412d-bb20-186263301ee9
02/17/2025 15:52:21:INFO:Received: evaluate message a180320a-31e7-412d-bb20-186263301ee9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:53:06:INFO:
[92mINFO [0m:      Received: train message cf7bc76c-6f27-4edb-b416-2d5f3ba238af
02/17/2025 15:53:06:INFO:Received: train message cf7bc76c-6f27-4edb-b416-2d5f3ba238af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:53:34:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:38:INFO:
[92mINFO [0m:      Received: evaluate message e4ef1152-ed9a-4eec-9739-5192ee186531
02/17/2025 15:54:38:INFO:Received: evaluate message e4ef1152-ed9a-4eec-9739-5192ee186531
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:11:INFO:
[92mINFO [0m:      Received: train message 731713e7-b542-4413-b853-f9b61cc8ba8f
02/17/2025 15:55:11:INFO:Received: train message 731713e7-b542-4413-b853-f9b61cc8ba8f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:55:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:56:56:INFO:
[92mINFO [0m:      Received: evaluate message c0c46a31-3161-49b5-b7b8-ecd89ba7c80f
02/17/2025 15:56:56:INFO:Received: evaluate message c0c46a31-3161-49b5-b7b8-ecd89ba7c80f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:57:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:34:INFO:
[92mINFO [0m:      Received: train message 570936b9-d4d5-43fe-8c17-7c72170540f7
02/17/2025 15:57:34:INFO:Received: train message 570936b9-d4d5-43fe-8c17-7c72170540f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:58:40:INFO:
[92mINFO [0m:      Received: evaluate message aa6559ac-1bcd-47de-81b8-96bf2a8a872f
02/17/2025 15:58:40:INFO:Received: evaluate message aa6559ac-1bcd-47de-81b8-96bf2a8a872f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:58:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:28:INFO:
[92mINFO [0m:      Received: train message 8e5f0518-6787-402d-a5e6-d27135a83889
02/17/2025 15:59:28:INFO:Received: train message 8e5f0518-6787-402d-a5e6-d27135a83889
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:59:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:04:INFO:
[92mINFO [0m:      Received: evaluate message f627b7a2-9d3b-4087-b2a6-7c89c87a6602
02/17/2025 16:01:04:INFO:Received: evaluate message f627b7a2-9d3b-4087-b2a6-7c89c87a6602
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866], 'accuracy': [0.49960906958561374], 'auc': [0.6223554252585197], 'precision': [0.3853113538182961], 'recall': [0.49960906958561374], 'f1': [0.34049198829300026]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282], 'accuracy': [0.49960906958561374, 0.5222830336200156], 'auc': [0.6223554252585197, 0.6407563691851618], 'precision': [0.3853113538182961, 0.4289287570845902], 'recall': [0.49960906958561374, 0.5222830336200156], 'f1': [0.34049198829300026, 0.4252045406151577]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:44:INFO:
[92mINFO [0m:      Received: train message 719ff5fa-5828-486f-9b1a-3d656143a608
02/17/2025 16:01:44:INFO:Received: train message 719ff5fa-5828-486f-9b1a-3d656143a608
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:02:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:31:INFO:
[92mINFO [0m:      Received: evaluate message f7e107c3-23a2-4ba3-ac83-005d315ea829
02/17/2025 16:03:31:INFO:Received: evaluate message f7e107c3-23a2-4ba3-ac83-005d315ea829
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:03:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:59:INFO:
[92mINFO [0m:      Received: train message 3965e80e-aad7-421a-a4f4-b8a33db3e311
02/17/2025 16:03:59:INFO:Received: train message 3965e80e-aad7-421a-a4f4-b8a33db3e311
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:04:19:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:05:43:INFO:
[92mINFO [0m:      Received: evaluate message a4b7be51-7355-4a84-8809-584ec120d92c
02/17/2025 16:05:43:INFO:Received: evaluate message a4b7be51-7355-4a84-8809-584ec120d92c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:05:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:37:INFO:
[92mINFO [0m:      Received: train message 0a911597-d23c-4abb-a24c-7b2e378aa6bf
02/17/2025 16:06:37:INFO:Received: train message 0a911597-d23c-4abb-a24c-7b2e378aa6bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:06:58:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:16:INFO:
[92mINFO [0m:      Received: evaluate message c6dbcb2a-8f94-4a28-9520-6527fab434d9
02/17/2025 16:08:16:INFO:Received: evaluate message c6dbcb2a-8f94-4a28-9520-6527fab434d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:49:INFO:
[92mINFO [0m:      Received: train message 35ba9f13-c016-458b-b3d5-90ff0cd006bc
02/17/2025 16:08:49:INFO:Received: train message 35ba9f13-c016-458b-b3d5-90ff0cd006bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:09:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:35:INFO:
[92mINFO [0m:      Received: evaluate message 809caf30-865d-43d1-91b2-09f6d1d776e2
02/17/2025 16:10:35:INFO:Received: evaluate message 809caf30-865d-43d1-91b2-09f6d1d776e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:10:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:58:INFO:
[92mINFO [0m:      Received: train message d285dd02-fae9-4fd2-831f-71c06c0b256e
02/17/2025 16:10:58:INFO:Received: train message d285dd02-fae9-4fd2-831f-71c06c0b256e

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:11:16:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:12:29:INFO:
[92mINFO [0m:      Received: evaluate message 196d70f4-cbf0-4258-a43c-11623b7c3aee
02/17/2025 16:12:29:INFO:Received: evaluate message 196d70f4-cbf0-4258-a43c-11623b7c3aee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:12:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:19:INFO:
[92mINFO [0m:      Received: train message f10b07d3-4be4-461c-8747-293e4c46cca7
02/17/2025 16:13:19:INFO:Received: train message f10b07d3-4be4-461c-8747-293e4c46cca7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:13:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:53:INFO:
[92mINFO [0m:      Received: evaluate message 5f5ea03c-4b91-4d3e-bb2d-210cbd2c42de
02/17/2025 16:14:53:INFO:Received: evaluate message 5f5ea03c-4b91-4d3e-bb2d-210cbd2c42de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:14:58:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:43:INFO:
[92mINFO [0m:      Received: train message 4ca35223-700d-43d6-b5d7-cd8af0051780
02/17/2025 16:15:43:INFO:Received: train message 4ca35223-700d-43d6-b5d7-cd8af0051780
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:16:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:32:INFO:
[92mINFO [0m:      Received: evaluate message bcc8114e-44bb-459f-ac13-0d0db8850205
02/17/2025 16:16:32:INFO:Received: evaluate message bcc8114e-44bb-459f-ac13-0d0db8850205
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:16:34:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:17:32:INFO:
[92mINFO [0m:      Received: train message 70989f48-1abb-45f6-b801-cd8fa87cc31b
02/17/2025 16:17:32:INFO:Received: train message 70989f48-1abb-45f6-b801-cd8fa87cc31b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:17:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:19:08:INFO:
[92mINFO [0m:      Received: evaluate message 5e6d55f6-4cd2-4262-bb0f-ed16b02eab53
02/17/2025 16:19:08:INFO:Received: evaluate message 5e6d55f6-4cd2-4262-bb0f-ed16b02eab53
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:19:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:19:58:INFO:
[92mINFO [0m:      Received: train message 4f00c207-880a-482c-8674-21a88daac309
02/17/2025 16:19:58:INFO:Received: train message 4f00c207-880a-482c-8674-21a88daac309
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:20:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:19:INFO:
[92mINFO [0m:      Received: evaluate message 5eecff7a-e155-4984-b782-850629eef26c
02/17/2025 16:21:19:INFO:Received: evaluate message 5eecff7a-e155-4984-b782-850629eef26c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:21:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:54:INFO:
[92mINFO [0m:      Received: train message 388ebb4b-c238-4834-aeff-59376f6fbbd3
02/17/2025 16:21:54:INFO:Received: train message 388ebb4b-c238-4834-aeff-59376f6fbbd3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:23:INFO:
[92mINFO [0m:      Received: evaluate message 7a7ca186-9f1c-430a-b2cd-ae313fe4c720
02/17/2025 16:23:23:INFO:Received: evaluate message 7a7ca186-9f1c-430a-b2cd-ae313fe4c720
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:24:17:INFO:
[92mINFO [0m:      Received: train message 93147d71-1e42-47a4-acf0-60906f06adb7
02/17/2025 16:24:17:INFO:Received: train message 93147d71-1e42-47a4-acf0-60906f06adb7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:45:INFO:
[92mINFO [0m:      Received: evaluate message 7485a44d-0fee-49e6-8430-5aed3f49f97e
02/17/2025 16:25:45:INFO:Received: evaluate message 7485a44d-0fee-49e6-8430-5aed3f49f97e

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:51:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:36:INFO:
[92mINFO [0m:      Received: train message dbd437db-ddff-40e8-ac07-88a38e560ff1
02/17/2025 16:26:36:INFO:Received: train message dbd437db-ddff-40e8-ac07-88a38e560ff1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:27:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:40:INFO:
[92mINFO [0m:      Received: evaluate message db17d207-4ee9-466e-b1f4-69cd6f7007bb
02/17/2025 16:27:40:INFO:Received: evaluate message db17d207-4ee9-466e-b1f4-69cd6f7007bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:27:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:19:INFO:
[92mINFO [0m:      Received: train message 956e4920-a00f-424a-8abf-8b062b151e27
02/17/2025 16:28:19:INFO:Received: train message 956e4920-a00f-424a-8abf-8b062b151e27
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:28:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:19:INFO:
[92mINFO [0m:      Received: evaluate message 999488cd-f623-4850-b8ea-0f76f8528b3c
02/17/2025 16:30:19:INFO:Received: evaluate message 999488cd-f623-4850-b8ea-0f76f8528b3c

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:57:INFO:
[92mINFO [0m:      Received: train message 1c394b6b-107a-465a-945b-572c49fbad55
02/17/2025 16:30:57:INFO:Received: train message 1c394b6b-107a-465a-945b-572c49fbad55
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:22:INFO:
[92mINFO [0m:      Received: evaluate message e745e12f-150f-400e-bc3f-2d653a588065
02/17/2025 16:32:22:INFO:Received: evaluate message e745e12f-150f-400e-bc3f-2d653a588065
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:32:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:10:INFO:
[92mINFO [0m:      Received: train message 5e937c63-f127-452a-a1f2-2d799827dc72
02/17/2025 16:33:10:INFO:Received: train message 5e937c63-f127-452a-a1f2-2d799827dc72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:33:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:34:27:INFO:
[92mINFO [0m:      Received: evaluate message e7c393c1-b16f-4fec-bf2f-6aa37a542fac
02/17/2025 16:34:27:INFO:Received: evaluate message e7c393c1-b16f-4fec-bf2f-6aa37a542fac

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:34:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:21:INFO:
[92mINFO [0m:      Received: train message c0ba5223-fb06-4730-85d9-9b957e3fce96
02/17/2025 16:35:21:INFO:Received: train message c0ba5223-fb06-4730-85d9-9b957e3fce96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:35:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:34:INFO:
[92mINFO [0m:      Received: evaluate message c59cc747-c91a-4b0f-969d-d5179047ac1e
02/17/2025 16:36:34:INFO:Received: evaluate message c59cc747-c91a-4b0f-969d-d5179047ac1e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:36:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:27:INFO:
[92mINFO [0m:      Received: train message 6c8304ba-4e82-4d33-8396-049b5b5414ab
02/17/2025 16:37:27:INFO:Received: train message 6c8304ba-4e82-4d33-8396-049b5b5414ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:37:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:51:INFO:
[92mINFO [0m:      Received: evaluate message 54f9bd52-2fd7-4ada-abd5-1a564fcbfe7e
02/17/2025 16:38:51:INFO:Received: evaluate message 54f9bd52-2fd7-4ada-abd5-1a564fcbfe7e

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:38:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:14:INFO:
[92mINFO [0m:      Received: train message 2eb5d1cf-4f83-4d5c-ad33-f448ae0205d6
02/17/2025 16:39:14:INFO:Received: train message 2eb5d1cf-4f83-4d5c-ad33-f448ae0205d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:39:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:43:INFO:
[92mINFO [0m:      Received: evaluate message ae7c00c5-6a67-44a9-bc4d-94a73f1ec8d3
02/17/2025 16:40:43:INFO:Received: evaluate message ae7c00c5-6a67-44a9-bc4d-94a73f1ec8d3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:40:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:22:INFO:
[92mINFO [0m:      Received: train message 8be4faa0-32ef-44eb-9de6-0a4faa7fc579
02/17/2025 16:41:22:INFO:Received: train message 8be4faa0-32ef-44eb-9de6-0a4faa7fc579
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:41:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:27:INFO:
[92mINFO [0m:      Received: evaluate message 788c7bf2-3917-4c19-ab87-3479ab937406
02/17/2025 16:42:27:INFO:Received: evaluate message 788c7bf2-3917-4c19-ab87-3479ab937406

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:42:31:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:46:INFO:
[92mINFO [0m:      Received: train message cd34338a-4e8a-4d7d-9fa6-cf8fa0fbacb5
02/17/2025 16:42:46:INFO:Received: train message cd34338a-4e8a-4d7d-9fa6-cf8fa0fbacb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:43:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:22:INFO:
[92mINFO [0m:      Received: evaluate message 562f4f3d-9bbd-473b-99f0-285614446081
02/17/2025 16:44:22:INFO:Received: evaluate message 562f4f3d-9bbd-473b-99f0-285614446081
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:44:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:43:INFO:
[92mINFO [0m:      Received: train message 5ce75459-3681-4264-8201-0f31f4d8e9c2
02/17/2025 16:44:43:INFO:Received: train message 5ce75459-3681-4264-8201-0f31f4d8e9c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:45:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:59:INFO:
[92mINFO [0m:      Received: evaluate message d1e6030d-be51-46a3-88c5-295ffd83fa56
02/17/2025 16:45:59:INFO:Received: evaluate message d1e6030d-be51-46a3-88c5-295ffd83fa56

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:46:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:34:INFO:
[92mINFO [0m:      Received: train message b25b9e98-4c82-450f-87f6-df9a1b83e762
02/17/2025 16:46:34:INFO:Received: train message b25b9e98-4c82-450f-87f6-df9a1b83e762
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:46:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:11:INFO:
[92mINFO [0m:      Received: evaluate message e6259902-daca-4b47-ba28-2ff9a2a385ed
02/17/2025 16:48:11:INFO:Received: evaluate message e6259902-daca-4b47-ba28-2ff9a2a385ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:48:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:50:INFO:
[92mINFO [0m:      Received: train message e59b7a9f-8fe6-49b1-9a41-2a7419251d77
02/17/2025 16:48:50:INFO:Received: train message e59b7a9f-8fe6-49b1-9a41-2a7419251d77
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:07:INFO:
[92mINFO [0m:      Received: evaluate message 729da2b1-c6af-460d-935a-b2d5d8f533e3
02/17/2025 16:50:07:INFO:Received: evaluate message 729da2b1-c6af-460d-935a-b2d5d8f533e3

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:50:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:14:INFO:
[92mINFO [0m:      Received: reconnect message c9962c2d-e079-4a04-8b16-599e510f00d7
02/17/2025 16:50:14:INFO:Received: reconnect message c9962c2d-e079-4a04-8b16-599e510f00d7
02/17/2025 16:50:14:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:50:14:INFO:Disconnect and shut down

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}



Final client history:
{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}

