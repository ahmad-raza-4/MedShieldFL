nohup: ignoring input
02/14/2025 23:50:56:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:50:56:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:50:56:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739605856.412461 1504447 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:51:28:INFO:
[92mINFO [0m:      Received: train message a18aa27c-3e56-4b42-b894-b253512f338b
02/14/2025 23:51:28:INFO:Received: train message a18aa27c-3e56-4b42-b894-b253512f338b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:51:46:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:52:34:INFO:
[92mINFO [0m:      Received: evaluate message 17ef3ff5-666b-47db-99c9-a564b5ec287e
02/14/2025 23:52:34:INFO:Received: evaluate message 17ef3ff5-666b-47db-99c9-a564b5ec287e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:52:36:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:53:23:INFO:
[92mINFO [0m:      Received: train message a3c6430c-e4f0-4c42-8611-c754882012e2
02/14/2025 23:53:23:INFO:Received: train message a3c6430c-e4f0-4c42-8611-c754882012e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:53:46:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:54:41:INFO:
[92mINFO [0m:      Received: evaluate message 040f07d2-df29-4dbe-873f-5d6d61f7f83e
02/14/2025 23:54:41:INFO:Received: evaluate message 040f07d2-df29-4dbe-873f-5d6d61f7f83e
[92mINFO [0m:      Sent reply
02/14/2025 23:54:43:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:55:15:INFO:
[92mINFO [0m:      Received: train message 8ab7e053-8372-4200-a94f-1283c93deb67
02/14/2025 23:55:15:INFO:Received: train message 8ab7e053-8372-4200-a94f-1283c93deb67
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:55:36:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:23:INFO:
[92mINFO [0m:      Received: evaluate message 49eb9844-92a3-4b38-a7a7-bfce8868a166
02/14/2025 23:56:23:INFO:Received: evaluate message 49eb9844-92a3-4b38-a7a7-bfce8868a166
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:56:26:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:50:INFO:
[92mINFO [0m:      Received: train message aae5281e-35e3-4522-adee-82401cb71f43
02/14/2025 23:56:50:INFO:Received: train message aae5281e-35e3-4522-adee-82401cb71f43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:57:11:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:12:INFO:
[92mINFO [0m:      Received: evaluate message 7876b4ca-02c6-40e7-a426-da39e53faf72
02/14/2025 23:58:12:INFO:Received: evaluate message 7876b4ca-02c6-40e7-a426-da39e53faf72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:58:16:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:47:INFO:
[92mINFO [0m:      Received: train message 547ba66d-d541-43a1-a477-1422194bfb7f
02/14/2025 23:58:47:INFO:Received: train message 547ba66d-d541-43a1-a477-1422194bfb7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:59:10:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:59:56:INFO:
[92mINFO [0m:      Received: evaluate message 16ee9a01-463d-4ddf-8384-7139f5a28770
02/14/2025 23:59:56:INFO:Received: evaluate message 16ee9a01-463d-4ddf-8384-7139f5a28770
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:00:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:53:INFO:
[92mINFO [0m:      Received: train message dfbd893f-30d0-43ff-a835-4c831d9b50e2
02/15/2025 00:00:53:INFO:Received: train message dfbd893f-30d0-43ff-a835-4c831d9b50e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:01:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:10:INFO:
[92mINFO [0m:      Received: evaluate message a1331125-dcac-44d5-81e6-f44ee194084d
02/15/2025 00:02:10:INFO:Received: evaluate message a1331125-dcac-44d5-81e6-f44ee194084d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:02:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:43:INFO:
[92mINFO [0m:      Received: train message a04842d0-8006-4aba-b9c4-a97e044d8c43
02/15/2025 00:02:43:INFO:Received: train message a04842d0-8006-4aba-b9c4-a97e044d8c43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:03:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:52:INFO:
[92mINFO [0m:      Received: evaluate message 0e76b590-4f83-4938-8cc2-5df3d187d29f
02/15/2025 00:03:52:INFO:Received: evaluate message 0e76b590-4f83-4938-8cc2-5df3d187d29f
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392], 'accuracy': [0.17279124315871774], 'auc': [0.36910406569460863], 'precision': [0.19215364459241882], 'recall': [0.17279124315871774], 'f1': [0.15428779652186775]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477], 'accuracy': [0.17279124315871774, 0.23534010946051603], 'auc': [0.36910406569460863, 0.37509675462346337], 'precision': [0.19215364459241882, 0.20470434628072817], 'recall': [0.17279124315871774, 0.23534010946051603], 'f1': [0.15428779652186775, 0.2139057332831779]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:03:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:04:48:INFO:
[92mINFO [0m:      Received: train message ff45d317-b0f1-4cd7-98dd-100620cd0688
02/15/2025 00:04:48:INFO:Received: train message ff45d317-b0f1-4cd7-98dd-100620cd0688
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:05:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:08:INFO:
[92mINFO [0m:      Received: evaluate message ab4d7498-35c0-4c1d-8c00-2dd0cd0fc30a
02/15/2025 00:06:08:INFO:Received: evaluate message ab4d7498-35c0-4c1d-8c00-2dd0cd0fc30a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:06:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:51:INFO:
[92mINFO [0m:      Received: train message ecf6d9a3-1142-47d7-a1b1-7e5eb57bf9a6
02/15/2025 00:06:51:INFO:Received: train message ecf6d9a3-1142-47d7-a1b1-7e5eb57bf9a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:07:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:00:INFO:
[92mINFO [0m:      Received: evaluate message c4ec3e03-3d07-4751-8329-c8434f4f9711
02/15/2025 00:08:00:INFO:Received: evaluate message c4ec3e03-3d07-4751-8329-c8434f4f9711
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:08:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:41:INFO:
[92mINFO [0m:      Received: train message 3ea9f30a-a791-4dab-b4ff-d4bd8865e7ba
02/15/2025 00:08:41:INFO:Received: train message 3ea9f30a-a791-4dab-b4ff-d4bd8865e7ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:02:INFO:
[92mINFO [0m:      Received: evaluate message aca92c2f-3c32-4b09-88ea-90cfd836531b
02/15/2025 00:10:02:INFO:Received: evaluate message aca92c2f-3c32-4b09-88ea-90cfd836531b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:10:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:24:INFO:
[92mINFO [0m:      Received: train message eabded44-7b16-47a0-91a4-134ca08b98f6
02/15/2025 00:10:24:INFO:Received: train message eabded44-7b16-47a0-91a4-134ca08b98f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:10:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:05:INFO:
[92mINFO [0m:      Received: evaluate message 14b21412-ebca-4121-9935-35dad1b891e3
02/15/2025 00:12:05:INFO:Received: evaluate message 14b21412-ebca-4121-9935-35dad1b891e3

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:12:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:46:INFO:
[92mINFO [0m:      Received: train message 64e2ec19-13e4-4063-bc30-dfa3b4d50e8f
02/15/2025 00:12:46:INFO:Received: train message 64e2ec19-13e4-4063-bc30-dfa3b4d50e8f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:13:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:50:INFO:
[92mINFO [0m:      Received: evaluate message a5c555ad-6326-4737-b817-b0c83d5bb4d2
02/15/2025 00:13:50:INFO:Received: evaluate message a5c555ad-6326-4737-b817-b0c83d5bb4d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:13:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:21:INFO:
[92mINFO [0m:      Received: train message 4a6b5524-32ba-4bbc-b1e8-2358ff29afa8
02/15/2025 00:14:21:INFO:Received: train message 4a6b5524-32ba-4bbc-b1e8-2358ff29afa8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:14:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:50:INFO:
[92mINFO [0m:      Received: evaluate message 16fd5eb8-4d6d-44e8-98af-5deb0fc523e0
02/15/2025 00:15:50:INFO:Received: evaluate message 16fd5eb8-4d6d-44e8-98af-5deb0fc523e0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:15:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:16:14:INFO:
[92mINFO [0m:      Received: train message 4fa85c99-f4de-44e2-9952-2d31fbfb84ea
02/15/2025 00:16:14:INFO:Received: train message 4fa85c99-f4de-44e2-9952-2d31fbfb84ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:41:INFO:
[92mINFO [0m:      Received: evaluate message 5b1b1708-37f7-4123-a90e-0f0df841f647
02/15/2025 00:17:41:INFO:Received: evaluate message 5b1b1708-37f7-4123-a90e-0f0df841f647
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:17:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:18:05:INFO:
[92mINFO [0m:      Received: train message b237adf9-0089-42bf-b8ec-d16241b1d72e
02/15/2025 00:18:05:INFO:Received: train message b237adf9-0089-42bf-b8ec-d16241b1d72e

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:39:INFO:
[92mINFO [0m:      Received: evaluate message 930e6c40-f690-4a6e-97ae-71776f603342
02/15/2025 00:19:39:INFO:Received: evaluate message 930e6c40-f690-4a6e-97ae-71776f603342
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:20:15:INFO:
[92mINFO [0m:      Received: train message dac1af48-42f7-4cbf-9194-63c0af0a3884
02/15/2025 00:20:15:INFO:Received: train message dac1af48-42f7-4cbf-9194-63c0af0a3884
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:20:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:36:INFO:
[92mINFO [0m:      Received: evaluate message db993ef0-be36-4bf2-b18d-d3bc40fda15b
02/15/2025 00:21:36:INFO:Received: evaluate message db993ef0-be36-4bf2-b18d-d3bc40fda15b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:10:INFO:
[92mINFO [0m:      Received: train message 595e855b-721d-442c-9f41-b697a4e83ece
02/15/2025 00:22:10:INFO:Received: train message 595e855b-721d-442c-9f41-b697a4e83ece
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:22:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:23:34:INFO:
[92mINFO [0m:      Received: evaluate message 04afdc03-08b3-4356-9809-5281c3935e70
02/15/2025 00:23:34:INFO:Received: evaluate message 04afdc03-08b3-4356-9809-5281c3935e70
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:23:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:14:INFO:
[92mINFO [0m:      Received: train message f08ea0c9-e202-4e3a-bea5-ef8ab21b664f
02/15/2025 00:24:14:INFO:Received: train message f08ea0c9-e202-4e3a-bea5-ef8ab21b664f
Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:24:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:25:33:INFO:
[92mINFO [0m:      Received: evaluate message a3825c9d-8f33-4413-97ae-c76239677921
02/15/2025 00:25:33:INFO:Received: evaluate message a3825c9d-8f33-4413-97ae-c76239677921
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:25:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:03:INFO:
[92mINFO [0m:      Received: train message 0268955c-9ebd-496f-b659-a60d6d5f352d
02/15/2025 00:26:03:INFO:Received: train message 0268955c-9ebd-496f-b659-a60d6d5f352d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:26:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:33:INFO:
[92mINFO [0m:      Received: evaluate message 110fe79c-6beb-44ae-a750-b1cc82f9566a
02/15/2025 00:27:33:INFO:Received: evaluate message 110fe79c-6beb-44ae-a750-b1cc82f9566a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:27:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:57:INFO:
[92mINFO [0m:      Received: train message 00807caf-8950-450e-9904-3f012855eaa5
02/15/2025 00:27:57:INFO:Received: train message 00807caf-8950-450e-9904-3f012855eaa5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:28:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:15:INFO:
[92mINFO [0m:      Received: evaluate message 0743e1f1-e274-4abe-a88a-7e8b8f21566a
02/15/2025 00:29:15:INFO:Received: evaluate message 0743e1f1-e274-4abe-a88a-7e8b8f21566a
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:30:07:INFO:
[92mINFO [0m:      Received: train message 618e507d-05f5-4728-bf30-9d16c724041c
02/15/2025 00:30:07:INFO:Received: train message 618e507d-05f5-4728-bf30-9d16c724041c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:28:INFO:
[92mINFO [0m:      Received: evaluate message aec55b1d-0553-448c-b465-23657f2c076f
02/15/2025 00:31:28:INFO:Received: evaluate message aec55b1d-0553-448c-b465-23657f2c076f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:52:INFO:
[92mINFO [0m:      Received: train message cb029ae4-3c4b-4c6d-87d7-9be0e39d5764
02/15/2025 00:31:52:INFO:Received: train message cb029ae4-3c4b-4c6d-87d7-9be0e39d5764
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:24:INFO:
[92mINFO [0m:      Received: evaluate message ca940856-5472-4f11-9782-92fc2d803216
02/15/2025 00:33:24:INFO:Received: evaluate message ca940856-5472-4f11-9782-92fc2d803216

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:59:INFO:
[92mINFO [0m:      Received: train message 56c1ca76-7b28-4ad4-9841-ff475a3e9202
02/15/2025 00:33:59:INFO:Received: train message 56c1ca76-7b28-4ad4-9841-ff475a3e9202
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:34:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:17:INFO:
[92mINFO [0m:      Received: evaluate message f5c81a10-ca3b-48bf-9a8a-1cbad4d57b8d
02/15/2025 00:35:17:INFO:Received: evaluate message f5c81a10-ca3b-48bf-9a8a-1cbad4d57b8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:35:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:47:INFO:
[92mINFO [0m:      Received: train message 5f27d48e-5607-409b-892b-9ea195cf25b0
02/15/2025 00:35:47:INFO:Received: train message 5f27d48e-5607-409b-892b-9ea195cf25b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:36:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:10:INFO:
[92mINFO [0m:      Received: evaluate message 025b139e-2e60-4313-91e9-aae332c35891
02/15/2025 00:37:10:INFO:Received: evaluate message 025b139e-2e60-4313-91e9-aae332c35891

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:37:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:48:INFO:
[92mINFO [0m:      Received: train message 4897e4b9-b6ac-4c6f-a18b-593281a97825
02/15/2025 00:37:48:INFO:Received: train message 4897e4b9-b6ac-4c6f-a18b-593281a97825
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:38:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:38:47:INFO:
[92mINFO [0m:      Received: evaluate message 5b3fd18e-fe89-40d7-80d5-41818cbd01ef
02/15/2025 00:38:47:INFO:Received: evaluate message 5b3fd18e-fe89-40d7-80d5-41818cbd01ef
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:38:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:39:35:INFO:
[92mINFO [0m:      Received: train message a78ecec0-3d4f-469d-91eb-cb91e79cfc10
02/15/2025 00:39:35:INFO:Received: train message a78ecec0-3d4f-469d-91eb-cb91e79cfc10
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:39:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:45:INFO:
[92mINFO [0m:      Received: evaluate message 1a1e34ee-27f5-43f0-9fa0-3fa385f28daa
02/15/2025 00:40:45:INFO:Received: evaluate message 1a1e34ee-27f5-43f0-9fa0-3fa385f28daa

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:41:26:INFO:
[92mINFO [0m:      Received: train message f394d3d5-6717-437f-9997-bb836a21c6f3
02/15/2025 00:41:26:INFO:Received: train message f394d3d5-6717-437f-9997-bb836a21c6f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:43:INFO:
[92mINFO [0m:      Received: evaluate message 6972bee3-c9e5-4896-b832-a222f141049e
02/15/2025 00:42:43:INFO:Received: evaluate message 6972bee3-c9e5-4896-b832-a222f141049e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:42:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:43:21:INFO:
[92mINFO [0m:      Received: train message e3982b93-b9a1-4511-965e-45afbe0070ed
02/15/2025 00:43:21:INFO:Received: train message e3982b93-b9a1-4511-965e-45afbe0070ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:37:INFO:
[92mINFO [0m:      Received: evaluate message 9df6aead-54e2-4a34-92df-da717cb1bdad
02/15/2025 00:44:37:INFO:Received: evaluate message 9df6aead-54e2-4a34-92df-da717cb1bdad

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:45:12:INFO:
[92mINFO [0m:      Received: train message b7f54607-df26-4bce-ab9b-6e1887bdd4c0
02/15/2025 00:45:12:INFO:Received: train message b7f54607-df26-4bce-ab9b-6e1887bdd4c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:21:INFO:
[92mINFO [0m:      Received: evaluate message 703fb8b6-4140-4429-8e24-dab2c508a019
02/15/2025 00:46:21:INFO:Received: evaluate message 703fb8b6-4140-4429-8e24-dab2c508a019
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:40:INFO:
[92mINFO [0m:      Received: train message c8f12a18-749e-4d0a-8855-7eb4b7279aef
02/15/2025 00:46:40:INFO:Received: train message c8f12a18-749e-4d0a-8855-7eb4b7279aef

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:46:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:06:INFO:
[92mINFO [0m:      Received: evaluate message 3174714e-e395-4312-9148-9d0a6acb0711
02/15/2025 00:48:06:INFO:Received: evaluate message 3174714e-e395-4312-9148-9d0a6acb0711
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:17:INFO:
[92mINFO [0m:      Received: reconnect message d3f440b8-06f4-455a-bccb-de305dbc8bce
02/15/2025 00:48:17:INFO:Received: reconnect message d3f440b8-06f4-455a-bccb-de305dbc8bce
02/15/2025 00:48:17:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 00:48:17:INFO:Disconnect and shut down
Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}



Final client history:
{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}

