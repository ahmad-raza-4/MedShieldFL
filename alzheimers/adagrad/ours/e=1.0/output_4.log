nohup: ignoring input
02/17/2025 15:45:38:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:45:38:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:45:38:DEBUG:ChannelConnectivity.CONNECTING
02/17/2025 15:45:38:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739835938.470494  742485 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:46:17:INFO:
[92mINFO [0m:      Received: train message 0ad9fa98-1f3e-4a4c-8bbd-803630a0d3b5
02/17/2025 15:46:17:INFO:Received: train message 0ad9fa98-1f3e-4a4c-8bbd-803630a0d3b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:46:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:47:31:INFO:
[92mINFO [0m:      Received: evaluate message e2f21b79-33e7-41a9-a817-a5bea1858bbd
02/17/2025 15:47:31:INFO:Received: evaluate message e2f21b79-33e7-41a9-a817-a5bea1858bbd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:47:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:48:02:INFO:
[92mINFO [0m:      Received: train message 170f3ff3-484e-4849-8f25-f09fc6baeb4e
02/17/2025 15:48:02:INFO:Received: train message 170f3ff3-484e-4849-8f25-f09fc6baeb4e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:48:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:49:55:INFO:
[92mINFO [0m:      Received: evaluate message 88af8ccc-8e22-4dfa-9c17-8a4846903d4a
02/17/2025 15:49:55:INFO:Received: evaluate message 88af8ccc-8e22-4dfa-9c17-8a4846903d4a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:49:58:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:50:28:INFO:
[92mINFO [0m:      Received: train message 9eb762af-8939-4e4f-bb32-546b6ee686a8
02/17/2025 15:50:28:INFO:Received: train message 9eb762af-8939-4e4f-bb32-546b6ee686a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:51:09:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:06:INFO:
[92mINFO [0m:      Received: evaluate message 46a3c149-4447-49f8-b9ff-7f54474680d2
02/17/2025 15:52:06:INFO:Received: evaluate message 46a3c149-4447-49f8-b9ff-7f54474680d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:58:INFO:
[92mINFO [0m:      Received: train message 28f60cb7-b66b-4c18-80f6-08c706fd30ec
02/17/2025 15:52:58:INFO:Received: train message 28f60cb7-b66b-4c18-80f6-08c706fd30ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:53:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:32:INFO:
[92mINFO [0m:      Received: evaluate message 64e81ffc-10dc-4b7d-a186-0f6155887171
02/17/2025 15:54:32:INFO:Received: evaluate message 64e81ffc-10dc-4b7d-a186-0f6155887171
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:26:INFO:
[92mINFO [0m:      Received: train message 53910553-571c-49e5-bd4c-c5bccbbc0350
02/17/2025 15:55:26:INFO:Received: train message 53910553-571c-49e5-bd4c-c5bccbbc0350
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:56:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:56:54:INFO:
[92mINFO [0m:      Received: evaluate message 0520e54b-c1cb-4641-9288-10d56af27e42
02/17/2025 15:56:54:INFO:Received: evaluate message 0520e54b-c1cb-4641-9288-10d56af27e42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:56:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:37:INFO:
[92mINFO [0m:      Received: train message b83ca81f-d122-4610-93c5-a2baee7d7b74
02/17/2025 15:57:37:INFO:Received: train message b83ca81f-d122-4610-93c5-a2baee7d7b74
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:08:INFO:
[92mINFO [0m:      Received: evaluate message c3c2cb57-5ef4-4d20-8d2a-b5735c23e06c
02/17/2025 15:59:08:INFO:Received: evaluate message c3c2cb57-5ef4-4d20-8d2a-b5735c23e06c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:51:INFO:
[92mINFO [0m:      Received: train message 06c272a6-1e10-46c9-a1e4-425648fbd841
02/17/2025 15:59:51:INFO:Received: train message 06c272a6-1e10-46c9-a1e4-425648fbd841
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:00:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:09:INFO:
[92mINFO [0m:      Received: evaluate message 2ce25ae9-1c35-4a58-b388-6ec5f1b8d1d9
02/17/2025 16:01:09:INFO:Received: evaluate message 2ce25ae9-1c35-4a58-b388-6ec5f1b8d1d9
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866], 'accuracy': [0.49960906958561374], 'auc': [0.6223554252585197], 'precision': [0.3853113538182961], 'recall': [0.49960906958561374], 'f1': [0.34049198829300026]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282], 'accuracy': [0.49960906958561374, 0.5222830336200156], 'auc': [0.6223554252585197, 0.6407563691851618], 'precision': [0.3853113538182961, 0.4289287570845902], 'recall': [0.49960906958561374, 0.5222830336200156], 'f1': [0.34049198829300026, 0.4252045406151577]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:02:02:INFO:
[92mINFO [0m:      Received: train message 1b42d3ff-851e-44ca-88b0-12a9c8c9ba90
02/17/2025 16:02:02:INFO:Received: train message 1b42d3ff-851e-44ca-88b0-12a9c8c9ba90
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:02:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:36:INFO:
[92mINFO [0m:      Received: evaluate message dd6df65f-f8d7-4f9a-84b2-dcea380fb365
02/17/2025 16:03:36:INFO:Received: evaluate message dd6df65f-f8d7-4f9a-84b2-dcea380fb365
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:03:40:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:04:INFO:
[92mINFO [0m:      Received: train message 9ba4d311-9921-43c3-9162-daeef2d5ea8c
02/17/2025 16:04:04:INFO:Received: train message 9ba4d311-9921-43c3-9162-daeef2d5ea8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:04:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:05:18:INFO:
[92mINFO [0m:      Received: evaluate message a853763a-46ee-4d5a-89cf-852aee6d6043
02/17/2025 16:05:18:INFO:Received: evaluate message a853763a-46ee-4d5a-89cf-852aee6d6043
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:05:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:28:INFO:
[92mINFO [0m:      Received: train message 4a4b6b49-0a7f-4a33-846d-1753152bb9f6
02/17/2025 16:06:28:INFO:Received: train message 4a4b6b49-0a7f-4a33-846d-1753152bb9f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:07:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:07:INFO:
[92mINFO [0m:      Received: evaluate message 5a84d0d4-f55e-45b2-a1c2-4152becc1f5a
02/17/2025 16:08:07:INFO:Received: evaluate message 5a84d0d4-f55e-45b2-a1c2-4152becc1f5a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:42:INFO:
[92mINFO [0m:      Received: train message 520b88c0-0667-47be-8607-34e479b6cfc9
02/17/2025 16:08:42:INFO:Received: train message 520b88c0-0667-47be-8607-34e479b6cfc9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:09:19:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:06:INFO:
[92mINFO [0m:      Received: evaluate message 1741fea2-3d2e-4eac-98f5-861084b80159
02/17/2025 16:10:06:INFO:Received: evaluate message 1741fea2-3d2e-4eac-98f5-861084b80159
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:10:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:09:INFO:
[92mINFO [0m:      Received: train message 4f0fe4b1-dc5f-4bdb-aa2e-d82db81b243f
02/17/2025 16:11:09:INFO:Received: train message 4f0fe4b1-dc5f-4bdb-aa2e-d82db81b243f

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:11:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:12:30:INFO:
[92mINFO [0m:      Received: evaluate message 537cb75e-bb66-4994-974a-8d58459000d9
02/17/2025 16:12:30:INFO:Received: evaluate message 537cb75e-bb66-4994-974a-8d58459000d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:12:34:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:22:INFO:
[92mINFO [0m:      Received: train message 5aaa43cd-d931-4aaf-831c-d91aadaef834
02/17/2025 16:13:22:INFO:Received: train message 5aaa43cd-d931-4aaf-831c-d91aadaef834
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:14:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:46:INFO:
[92mINFO [0m:      Received: evaluate message 7595d3f5-d774-48bd-9b00-460b2e569002
02/17/2025 16:14:46:INFO:Received: evaluate message 7595d3f5-d774-48bd-9b00-460b2e569002
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:14:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:15:INFO:
[92mINFO [0m:      Received: train message 15b15ae8-31d0-4ef1-bb37-8446a50686fc
02/17/2025 16:15:15:INFO:Received: train message 15b15ae8-31d0-4ef1-bb37-8446a50686fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:15:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:49:INFO:
[92mINFO [0m:      Received: evaluate message a7730304-526c-4626-a20a-67c12210ddeb
02/17/2025 16:16:49:INFO:Received: evaluate message a7730304-526c-4626-a20a-67c12210ddeb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:16:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:17:47:INFO:
[92mINFO [0m:      Received: train message 91f64a01-f10c-4c09-9831-6b461ba4272b
02/17/2025 16:17:47:INFO:Received: train message 91f64a01-f10c-4c09-9831-6b461ba4272b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:18:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:51:INFO:
[92mINFO [0m:      Received: evaluate message 92bf947a-e9e9-41a7-b618-06223e53e829
02/17/2025 16:18:51:INFO:Received: evaluate message 92bf947a-e9e9-41a7-b618-06223e53e829
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:18:56:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:19:41:INFO:
[92mINFO [0m:      Received: train message 986a1d6f-cb44-47a0-ad89-ce7fe43c645f
02/17/2025 16:19:41:INFO:Received: train message 986a1d6f-cb44-47a0-ad89-ce7fe43c645f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:20:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:11:INFO:
[92mINFO [0m:      Received: evaluate message 1f0f7d2c-629e-4f11-9297-783efbf1f301
02/17/2025 16:21:11:INFO:Received: evaluate message 1f0f7d2c-629e-4f11-9297-783efbf1f301
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:21:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:22:05:INFO:
[92mINFO [0m:      Received: train message 510013ed-ebfc-4e43-8f86-605ad8cbbb4d
02/17/2025 16:22:05:INFO:Received: train message 510013ed-ebfc-4e43-8f86-605ad8cbbb4d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:26:INFO:
[92mINFO [0m:      Received: evaluate message 914793ce-edc1-40e9-81c9-3df2b8bd7eb9
02/17/2025 16:23:26:INFO:Received: evaluate message 914793ce-edc1-40e9-81c9-3df2b8bd7eb9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:54:INFO:
[92mINFO [0m:      Received: train message 4ccca6dd-c9f0-4630-88db-0719da719d88
02/17/2025 16:23:54:INFO:Received: train message 4ccca6dd-c9f0-4630-88db-0719da719d88
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:40:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:47:INFO:
[92mINFO [0m:      Received: evaluate message 32d2fed2-866c-42c5-80ce-a811f82f8e8c
02/17/2025 16:25:47:INFO:Received: evaluate message 32d2fed2-866c-42c5-80ce-a811f82f8e8c

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:18:INFO:
[92mINFO [0m:      Received: train message dd33eb56-ac52-417c-8b48-176a7c182e6e
02/17/2025 16:26:18:INFO:Received: train message dd33eb56-ac52-417c-8b48-176a7c182e6e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:27:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:59:INFO:
[92mINFO [0m:      Received: evaluate message 92595145-2a9f-437e-9fc2-1a44e30fad41
02/17/2025 16:27:59:INFO:Received: evaluate message 92595145-2a9f-437e-9fc2-1a44e30fad41
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:28:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:46:INFO:
[92mINFO [0m:      Received: train message 682a384c-1209-4c6b-94ef-33eb54040b41
02/17/2025 16:28:46:INFO:Received: train message 682a384c-1209-4c6b-94ef-33eb54040b41
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:29:59:INFO:
[92mINFO [0m:      Received: evaluate message da4fd405-7e3f-47fb-bdda-3fb767e7a471
02/17/2025 16:29:59:INFO:Received: evaluate message da4fd405-7e3f-47fb-bdda-3fb767e7a471

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:01:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:31:04:INFO:
[92mINFO [0m:      Received: train message bc87135e-6c2a-4057-890a-94eb200c4d31
02/17/2025 16:31:04:INFO:Received: train message bc87135e-6c2a-4057-890a-94eb200c4d31
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:38:INFO:
[92mINFO [0m:      Received: evaluate message 085a7c77-c4f3-4d74-8168-952ece315411
02/17/2025 16:32:38:INFO:Received: evaluate message 085a7c77-c4f3-4d74-8168-952ece315411
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:32:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:20:INFO:
[92mINFO [0m:      Received: train message b11761a4-3f26-4d6a-8055-1be7c00a0903
02/17/2025 16:33:20:INFO:Received: train message b11761a4-3f26-4d6a-8055-1be7c00a0903
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:34:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:34:47:INFO:
[92mINFO [0m:      Received: evaluate message fbd1ff08-d52c-4256-bd1b-a613695c7c87
02/17/2025 16:34:47:INFO:Received: evaluate message fbd1ff08-d52c-4256-bd1b-a613695c7c87

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:34:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:07:INFO:
[92mINFO [0m:      Received: train message 274614f9-7510-4e7b-a338-12a1d938c553
02/17/2025 16:35:07:INFO:Received: train message 274614f9-7510-4e7b-a338-12a1d938c553
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:35:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:43:INFO:
[92mINFO [0m:      Received: evaluate message d6de2a11-d573-45ee-a150-6b5ce7634f32
02/17/2025 16:36:43:INFO:Received: evaluate message d6de2a11-d573-45ee-a150-6b5ce7634f32
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:36:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:25:INFO:
[92mINFO [0m:      Received: train message ddeab958-2ec1-4ada-9392-f9396a390b25
02/17/2025 16:37:25:INFO:Received: train message ddeab958-2ec1-4ada-9392-f9396a390b25
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:38:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:37:INFO:
[92mINFO [0m:      Received: evaluate message 8847883b-2dab-4323-80bb-f40bb27ba291
02/17/2025 16:38:37:INFO:Received: evaluate message 8847883b-2dab-4323-80bb-f40bb27ba291

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:38:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:13:INFO:
[92mINFO [0m:      Received: train message 1e039282-53ea-4d88-becd-3889cf989edf
02/17/2025 16:39:13:INFO:Received: train message 1e039282-53ea-4d88-becd-3889cf989edf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:39:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:36:INFO:
[92mINFO [0m:      Received: evaluate message ca495c67-1ab2-4e43-836e-fa0408a3a271
02/17/2025 16:40:36:INFO:Received: evaluate message ca495c67-1ab2-4e43-836e-fa0408a3a271
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:40:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:02:INFO:
[92mINFO [0m:      Received: train message 7502fd16-0125-4387-ac68-712aeb243d3a
02/17/2025 16:41:02:INFO:Received: train message 7502fd16-0125-4387-ac68-712aeb243d3a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:41:34:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:21:INFO:
[92mINFO [0m:      Received: evaluate message 0fcc84af-e49a-4db1-ae03-21a8778b4145
02/17/2025 16:42:21:INFO:Received: evaluate message 0fcc84af-e49a-4db1-ae03-21a8778b4145

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:42:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:50:INFO:
[92mINFO [0m:      Received: train message d3098f1d-d2d1-41fd-b387-d4e4bbc7bc6f
02/17/2025 16:42:50:INFO:Received: train message d3098f1d-d2d1-41fd-b387-d4e4bbc7bc6f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:43:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:22:INFO:
[92mINFO [0m:      Received: evaluate message 7f74b42f-ab5f-4357-a2d4-e3c81f23f95f
02/17/2025 16:44:22:INFO:Received: evaluate message 7f74b42f-ab5f-4357-a2d4-e3c81f23f95f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:44:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:43:INFO:
[92mINFO [0m:      Received: train message e724f0e8-ea38-4453-94d8-85d672895c1e
02/17/2025 16:44:43:INFO:Received: train message e724f0e8-ea38-4453-94d8-85d672895c1e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:45:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:46:INFO:
[92mINFO [0m:      Received: evaluate message 48f8a4a9-fef2-4b71-ac9c-7a20a4c7231d
02/17/2025 16:45:46:INFO:Received: evaluate message 48f8a4a9-fef2-4b71-ac9c-7a20a4c7231d

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:45:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:52:INFO:
[92mINFO [0m:      Received: train message 126f6551-ab8f-4fe3-8c76-8569315d2c6b
02/17/2025 16:46:52:INFO:Received: train message 126f6551-ab8f-4fe3-8c76-8569315d2c6b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:47:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:05:INFO:
[92mINFO [0m:      Received: evaluate message 7e3055b1-1358-4f50-8675-b29dad08875d
02/17/2025 16:48:05:INFO:Received: evaluate message 7e3055b1-1358-4f50-8675-b29dad08875d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:48:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:54:INFO:
[92mINFO [0m:      Received: train message f66ea8cb-0ea0-4f25-b8c0-fc84c5b9c1b6
02/17/2025 16:48:54:INFO:Received: train message f66ea8cb-0ea0-4f25-b8c0-fc84c5b9c1b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:08:INFO:
[92mINFO [0m:      Received: evaluate message 3fbab871-baf4-4367-8b87-4630ed773ca4
02/17/2025 16:50:08:INFO:Received: evaluate message 3fbab871-baf4-4367-8b87-4630ed773ca4

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:50:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:14:INFO:
[92mINFO [0m:      Received: reconnect message 722d61a6-8efc-4be4-9374-372eb3898c8a
02/17/2025 16:50:14:INFO:Received: reconnect message 722d61a6-8efc-4be4-9374-372eb3898c8a
02/17/2025 16:50:14:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:50:14:INFO:Disconnect and shut down
