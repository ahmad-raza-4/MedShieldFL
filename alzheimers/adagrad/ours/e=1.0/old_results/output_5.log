nohup: ignoring input
02/14/2025 23:50:58:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:50:58:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:50:58:DEBUG:ChannelConnectivity.CONNECTING
02/14/2025 23:50:58:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739605858.227539 1504669 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:51:27:INFO:
[92mINFO [0m:      Received: train message 15d0ae3e-29b8-492c-aa4d-fa074e84a78a
02/14/2025 23:51:27:INFO:Received: train message 15d0ae3e-29b8-492c-aa4d-fa074e84a78a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:51:52:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:52:23:INFO:
[92mINFO [0m:      Received: evaluate message 160b941f-1421-44e8-9a42-58fc28daee44
02/14/2025 23:52:23:INFO:Received: evaluate message 160b941f-1421-44e8-9a42-58fc28daee44
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:52:25:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:53:24:INFO:
[92mINFO [0m:      Received: train message 9a286097-c8f7-427a-bedf-037321dd330e
02/14/2025 23:53:24:INFO:Received: train message 9a286097-c8f7-427a-bedf-037321dd330e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:53:54:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:54:36:INFO:
[92mINFO [0m:      Received: evaluate message 388bc919-f884-4170-9b65-25c202b75a42
02/14/2025 23:54:36:INFO:Received: evaluate message 388bc919-f884-4170-9b65-25c202b75a42
[92mINFO [0m:      Sent reply
02/14/2025 23:54:39:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:55:07:INFO:
[92mINFO [0m:      Received: train message 3b96d9a5-fe93-4222-87e2-2195f27573de
02/14/2025 23:55:07:INFO:Received: train message 3b96d9a5-fe93-4222-87e2-2195f27573de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:55:33:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:23:INFO:
[92mINFO [0m:      Received: evaluate message acfe7044-4a57-4ab3-accc-3f23faa203d7
02/14/2025 23:56:23:INFO:Received: evaluate message acfe7044-4a57-4ab3-accc-3f23faa203d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:56:26:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:50:INFO:
[92mINFO [0m:      Received: train message 29196372-452b-46fb-a527-ff4d678fb41d
02/14/2025 23:56:50:INFO:Received: train message 29196372-452b-46fb-a527-ff4d678fb41d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:57:16:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:11:INFO:
[92mINFO [0m:      Received: evaluate message 7c49b8fa-6d3f-459c-82ae-652a7600cb0d
02/14/2025 23:58:11:INFO:Received: evaluate message 7c49b8fa-6d3f-459c-82ae-652a7600cb0d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:58:15:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:47:INFO:
[92mINFO [0m:      Received: train message 6dc51e52-aad7-4b3d-a61c-bf359d8e6b17
02/14/2025 23:58:47:INFO:Received: train message 6dc51e52-aad7-4b3d-a61c-bf359d8e6b17
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:59:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:13:INFO:
[92mINFO [0m:      Received: evaluate message 80734038-5061-4e92-b031-1689e0230d3c
02/15/2025 00:00:13:INFO:Received: evaluate message 80734038-5061-4e92-b031-1689e0230d3c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:00:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:53:INFO:
[92mINFO [0m:      Received: train message 9ae3f263-1f1a-471d-ac8d-5f9c0034b997
02/15/2025 00:00:53:INFO:Received: train message 9ae3f263-1f1a-471d-ac8d-5f9c0034b997
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:01:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:06:INFO:
[92mINFO [0m:      Received: evaluate message 6259e380-e6b6-4db4-98d5-26da413190af
02/15/2025 00:02:06:INFO:Received: evaluate message 6259e380-e6b6-4db4-98d5-26da413190af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:02:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:49:INFO:
[92mINFO [0m:      Received: train message 05075d04-dbbf-47a0-bfa6-653cb09b64f8
02/15/2025 00:02:49:INFO:Received: train message 05075d04-dbbf-47a0-bfa6-653cb09b64f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:03:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:52:INFO:
[92mINFO [0m:      Received: evaluate message 584553d0-2790-481c-b36c-ef680005981f
02/15/2025 00:03:52:INFO:Received: evaluate message 584553d0-2790-481c-b36c-ef680005981f
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392], 'accuracy': [0.17279124315871774], 'auc': [0.36910406569460863], 'precision': [0.19215364459241882], 'recall': [0.17279124315871774], 'f1': [0.15428779652186775]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477], 'accuracy': [0.17279124315871774, 0.23534010946051603], 'auc': [0.36910406569460863, 0.37509675462346337], 'precision': [0.19215364459241882, 0.20470434628072817], 'recall': [0.17279124315871774, 0.23534010946051603], 'f1': [0.15428779652186775, 0.2139057332831779]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:03:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:04:53:INFO:
[92mINFO [0m:      Received: train message d6ff13de-ab01-4641-93a6-91b36b0a4101
02/15/2025 00:04:53:INFO:Received: train message d6ff13de-ab01-4641-93a6-91b36b0a4101
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:05:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:08:INFO:
[92mINFO [0m:      Received: evaluate message 11768b18-bb61-4201-ae92-340efd0bf679
02/15/2025 00:06:08:INFO:Received: evaluate message 11768b18-bb61-4201-ae92-340efd0bf679
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:06:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:49:INFO:
[92mINFO [0m:      Received: train message e6e20c06-165a-4c6c-89be-31dc6bc4cb1c
02/15/2025 00:06:49:INFO:Received: train message e6e20c06-165a-4c6c-89be-31dc6bc4cb1c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:07:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:00:INFO:
[92mINFO [0m:      Received: evaluate message 78140f18-812f-41c1-a3fe-9810842e82e3
02/15/2025 00:08:00:INFO:Received: evaluate message 78140f18-812f-41c1-a3fe-9810842e82e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:08:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:48:INFO:
[92mINFO [0m:      Received: train message 02344cab-d90f-443e-af8a-a8e927ac1d8b
02/15/2025 00:08:48:INFO:Received: train message 02344cab-d90f-443e-af8a-a8e927ac1d8b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:05:INFO:
[92mINFO [0m:      Received: evaluate message 55ae692b-fc65-4e13-b22b-d1a542c79356
02/15/2025 00:10:05:INFO:Received: evaluate message 55ae692b-fc65-4e13-b22b-d1a542c79356
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:10:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:27:INFO:
[92mINFO [0m:      Received: train message 7272ec05-073c-447f-b1dd-3153008d25f5
02/15/2025 00:10:27:INFO:Received: train message 7272ec05-073c-447f-b1dd-3153008d25f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:10:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:04:INFO:
[92mINFO [0m:      Received: evaluate message 19f8915d-8767-4bd8-851a-b2294b886e78
02/15/2025 00:12:04:INFO:Received: evaluate message 19f8915d-8767-4bd8-851a-b2294b886e78

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:12:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:26:INFO:
[92mINFO [0m:      Received: train message 229592dc-74c9-4a97-97ea-ae46eafc6ee4
02/15/2025 00:12:26:INFO:Received: train message 229592dc-74c9-4a97-97ea-ae46eafc6ee4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:12:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:57:INFO:
[92mINFO [0m:      Received: evaluate message d147c518-85a4-4ecb-96e3-518343abae9e
02/15/2025 00:13:57:INFO:Received: evaluate message d147c518-85a4-4ecb-96e3-518343abae9e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:13:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:32:INFO:
[92mINFO [0m:      Received: train message 9fe925f9-de85-47fb-8a96-400c7e31783d
02/15/2025 00:14:32:INFO:Received: train message 9fe925f9-de85-47fb-8a96-400c7e31783d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:15:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:53:INFO:
[92mINFO [0m:      Received: evaluate message 778128e9-56f5-4cdc-b1ae-3b462cff0bf2
02/15/2025 00:15:53:INFO:Received: evaluate message 778128e9-56f5-4cdc-b1ae-3b462cff0bf2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:15:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:16:26:INFO:
[92mINFO [0m:      Received: train message aba7d0ec-f4d2-4024-90cd-fded55d34316
02/15/2025 00:16:26:INFO:Received: train message aba7d0ec-f4d2-4024-90cd-fded55d34316
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:46:INFO:
[92mINFO [0m:      Received: evaluate message 970f75e5-3935-4381-b75b-ebeb4502362b
02/15/2025 00:17:46:INFO:Received: evaluate message 970f75e5-3935-4381-b75b-ebeb4502362b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:17:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:18:26:INFO:
[92mINFO [0m:      Received: train message 07325817-4a22-4e3b-ab4d-82cf97d963d5
02/15/2025 00:18:26:INFO:Received: train message 07325817-4a22-4e3b-ab4d-82cf97d963d5

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:27:INFO:
[92mINFO [0m:      Received: evaluate message 8b0bab52-994a-46cf-b96b-23216b852e1a
02/15/2025 00:19:27:INFO:Received: evaluate message 8b0bab52-994a-46cf-b96b-23216b852e1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:20:15:INFO:
[92mINFO [0m:      Received: train message 550f00b3-568c-41fe-827a-4ccaea19a5fc
02/15/2025 00:20:15:INFO:Received: train message 550f00b3-568c-41fe-827a-4ccaea19a5fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:20:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:25:INFO:
[92mINFO [0m:      Received: evaluate message d26587a9-9bf0-444d-a58f-0d2ea0f02841
02/15/2025 00:21:25:INFO:Received: evaluate message d26587a9-9bf0-444d-a58f-0d2ea0f02841
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:12:INFO:
[92mINFO [0m:      Received: train message 771ca7fd-f989-425a-8fd0-4f8a0b41b525
02/15/2025 00:22:12:INFO:Received: train message 771ca7fd-f989-425a-8fd0-4f8a0b41b525
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:22:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:23:15:INFO:
[92mINFO [0m:      Received: evaluate message ccf822fe-28bf-4dd1-9167-125c24e73db8
02/15/2025 00:23:15:INFO:Received: evaluate message ccf822fe-28bf-4dd1-9167-125c24e73db8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:23:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:10:INFO:
[92mINFO [0m:      Received: train message 18c3db54-370a-4d2b-81b1-879dbf43e59e
02/15/2025 00:24:10:INFO:Received: train message 18c3db54-370a-4d2b-81b1-879dbf43e59e
Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:24:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:25:29:INFO:
[92mINFO [0m:      Received: evaluate message ddfeeb03-728a-4a8f-99ae-e9f35b15cc5a
02/15/2025 00:25:29:INFO:Received: evaluate message ddfeeb03-728a-4a8f-99ae-e9f35b15cc5a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:25:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:11:INFO:
[92mINFO [0m:      Received: train message 0e00fe99-a340-47f8-bd16-713201fa9312
02/15/2025 00:26:11:INFO:Received: train message 0e00fe99-a340-47f8-bd16-713201fa9312
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:26:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:33:INFO:
[92mINFO [0m:      Received: evaluate message f40ea1ea-98d9-428b-a128-5eddda2e480f
02/15/2025 00:27:33:INFO:Received: evaluate message f40ea1ea-98d9-428b-a128-5eddda2e480f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:27:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:28:16:INFO:
[92mINFO [0m:      Received: train message dcfb7d93-d756-4a16-a050-7f9afed159b9
02/15/2025 00:28:16:INFO:Received: train message dcfb7d93-d756-4a16-a050-7f9afed159b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:28:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:26:INFO:
[92mINFO [0m:      Received: evaluate message 4b5f763c-fd88-4c06-932d-acd3f72b3ac1
02/15/2025 00:29:26:INFO:Received: evaluate message 4b5f763c-fd88-4c06-932d-acd3f72b3ac1
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:30:11:INFO:
[92mINFO [0m:      Received: train message 0d41dd22-0e41-4e69-8b3a-e108d84830bf
02/15/2025 00:30:11:INFO:Received: train message 0d41dd22-0e41-4e69-8b3a-e108d84830bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:14:INFO:
[92mINFO [0m:      Received: evaluate message 170f9be1-ba09-4148-ab3b-bcbe1e06cc5b
02/15/2025 00:31:14:INFO:Received: evaluate message 170f9be1-ba09-4148-ab3b-bcbe1e06cc5b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:32:04:INFO:
[92mINFO [0m:      Received: train message 264a3edd-fc94-4444-a290-7cc2924389b8
02/15/2025 00:32:04:INFO:Received: train message 264a3edd-fc94-4444-a290-7cc2924389b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:15:INFO:
[92mINFO [0m:      Received: evaluate message 7fea716e-7e98-4fbf-8e05-1d03bec6a35d
02/15/2025 00:33:15:INFO:Received: evaluate message 7fea716e-7e98-4fbf-8e05-1d03bec6a35d

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:53:INFO:
[92mINFO [0m:      Received: train message 61a8ec9f-f6cf-4381-9e34-0cd9c0d74dcf
02/15/2025 00:33:53:INFO:Received: train message 61a8ec9f-f6cf-4381-9e34-0cd9c0d74dcf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:34:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:34:55:INFO:
[92mINFO [0m:      Received: evaluate message 70df5159-5830-4b31-878d-7a6dac8b2089
02/15/2025 00:34:55:INFO:Received: evaluate message 70df5159-5830-4b31-878d-7a6dac8b2089
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:34:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:55:INFO:
[92mINFO [0m:      Received: train message 3fc0c519-3811-4306-a7aa-1da91d10000c
02/15/2025 00:35:55:INFO:Received: train message 3fc0c519-3811-4306-a7aa-1da91d10000c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:36:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:06:INFO:
[92mINFO [0m:      Received: evaluate message b406a6f8-61bf-427b-9ec9-c1b8db7a1fc6
02/15/2025 00:37:06:INFO:Received: evaluate message b406a6f8-61bf-427b-9ec9-c1b8db7a1fc6

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:37:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:41:INFO:
[92mINFO [0m:      Received: train message 4634eb3c-edaf-412a-8c6f-7475c68251f1
02/15/2025 00:37:41:INFO:Received: train message 4634eb3c-edaf-412a-8c6f-7475c68251f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:38:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:38:53:INFO:
[92mINFO [0m:      Received: evaluate message a78488c4-4898-4386-8f17-83e808c70d75
02/15/2025 00:38:53:INFO:Received: evaluate message a78488c4-4898-4386-8f17-83e808c70d75
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:38:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:39:40:INFO:
[92mINFO [0m:      Received: train message 8f8e233a-132a-43df-b52a-cd3389d7a2b5
02/15/2025 00:39:40:INFO:Received: train message 8f8e233a-132a-43df-b52a-cd3389d7a2b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:40:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:54:INFO:
[92mINFO [0m:      Received: evaluate message 62f4bcb1-12c5-4496-a422-8723f7f8ca2d
02/15/2025 00:40:54:INFO:Received: evaluate message 62f4bcb1-12c5-4496-a422-8723f7f8ca2d

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:41:28:INFO:
[92mINFO [0m:      Received: train message b69a79be-f015-41c1-9f69-03c582e22af0
02/15/2025 00:41:28:INFO:Received: train message b69a79be-f015-41c1-9f69-03c582e22af0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:23:INFO:
[92mINFO [0m:      Received: evaluate message c5b9ca38-8c76-4258-92f7-3f9081e0e27c
02/15/2025 00:42:23:INFO:Received: evaluate message c5b9ca38-8c76-4258-92f7-3f9081e0e27c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:42:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:43:09:INFO:
[92mINFO [0m:      Received: train message 084385c1-fe1b-4149-b899-e489845e43e1
02/15/2025 00:43:09:INFO:Received: train message 084385c1-fe1b-4149-b899-e489845e43e1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:31:INFO:
[92mINFO [0m:      Received: evaluate message efffdc82-fcff-4575-ac12-65344163f1c7
02/15/2025 00:44:31:INFO:Received: evaluate message efffdc82-fcff-4575-ac12-65344163f1c7

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:45:02:INFO:
[92mINFO [0m:      Received: train message a0dcfc7c-0e5c-415c-b0d5-34985aed3988
02/15/2025 00:45:02:INFO:Received: train message a0dcfc7c-0e5c-415c-b0d5-34985aed3988
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:12:INFO:
[92mINFO [0m:      Received: evaluate message 7d4c79e3-ab57-43f7-b3d5-21b569e48586
02/15/2025 00:46:12:INFO:Received: evaluate message 7d4c79e3-ab57-43f7-b3d5-21b569e48586
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:47:03:INFO:
[92mINFO [0m:      Received: train message e489531d-4d52-4f48-ac31-e246f4364000
02/15/2025 00:47:03:INFO:Received: train message e489531d-4d52-4f48-ac31-e246f4364000

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:47:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:08:INFO:
[92mINFO [0m:      Received: evaluate message 343dd03d-25cf-40ae-a62d-93da4d3f1f8f
02/15/2025 00:48:08:INFO:Received: evaluate message 343dd03d-25cf-40ae-a62d-93da4d3f1f8f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:17:INFO:
[92mINFO [0m:      Received: reconnect message 7195631d-9442-491f-be5e-850d723f79d1
02/15/2025 00:48:17:INFO:Received: reconnect message 7195631d-9442-491f-be5e-850d723f79d1
02/15/2025 00:48:17:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 00:48:17:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}



Final client history:
{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}

