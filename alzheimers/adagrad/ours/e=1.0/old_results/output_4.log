nohup: ignoring input
02/14/2025 23:51:00:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:51:00:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:51:00:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739605860.976171 1504747 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:51:31:INFO:
[92mINFO [0m:      Received: train message 1363caeb-81db-4dbc-8869-ba979a590e33
02/14/2025 23:51:31:INFO:Received: train message 1363caeb-81db-4dbc-8869-ba979a590e33
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:52:01:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:52:43:INFO:
[92mINFO [0m:      Received: evaluate message 776b9062-c581-42fb-9a86-6e839fe9dc3f
02/14/2025 23:52:43:INFO:Received: evaluate message 776b9062-c581-42fb-9a86-6e839fe9dc3f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:52:46:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:53:09:INFO:
[92mINFO [0m:      Received: train message 28c93f83-df88-4b58-ad8a-60187b1c8484
02/14/2025 23:53:09:INFO:Received: train message 28c93f83-df88-4b58-ad8a-60187b1c8484
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:53:42:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:54:39:INFO:
[92mINFO [0m:      Received: evaluate message 9ac678ed-f92b-412c-a655-31f7ba01fc57
02/14/2025 23:54:39:INFO:Received: evaluate message 9ac678ed-f92b-412c-a655-31f7ba01fc57
[92mINFO [0m:      Sent reply
02/14/2025 23:54:41:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:55:13:INFO:
[92mINFO [0m:      Received: train message ef2ee81f-4a3e-463a-adbd-cdd01c2118b6
02/14/2025 23:55:13:INFO:Received: train message ef2ee81f-4a3e-463a-adbd-cdd01c2118b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:55:44:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:13:INFO:
[92mINFO [0m:      Received: evaluate message 1f4ae90f-a163-4361-b9d7-9c554fa4fa20
02/14/2025 23:56:13:INFO:Received: evaluate message 1f4ae90f-a163-4361-b9d7-9c554fa4fa20
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:56:15:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:43:INFO:
[92mINFO [0m:      Received: train message 662a5db4-bf96-43d1-9caa-de5ab7da0017
02/14/2025 23:56:43:INFO:Received: train message 662a5db4-bf96-43d1-9caa-de5ab7da0017
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:57:14:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:57:56:INFO:
[92mINFO [0m:      Received: evaluate message 11afda7a-1f42-452b-b2d7-3df017369e99
02/14/2025 23:57:56:INFO:Received: evaluate message 11afda7a-1f42-452b-b2d7-3df017369e99
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:57:59:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:50:INFO:
[92mINFO [0m:      Received: train message 4b3dde47-8cce-4c56-9931-9faa7537ca18
02/14/2025 23:58:50:INFO:Received: train message 4b3dde47-8cce-4c56-9931-9faa7537ca18
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:59:27:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:59:56:INFO:
[92mINFO [0m:      Received: evaluate message fb81adf1-2db6-4c8a-a66e-4a965e769730
02/14/2025 23:59:56:INFO:Received: evaluate message fb81adf1-2db6-4c8a-a66e-4a965e769730
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:00:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:38:INFO:
[92mINFO [0m:      Received: train message 3aff9e99-57f6-40f4-843a-17a3e4351181
02/15/2025 00:00:38:INFO:Received: train message 3aff9e99-57f6-40f4-843a-17a3e4351181
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:01:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:09:INFO:
[92mINFO [0m:      Received: evaluate message dcdf6753-f764-4e2a-81c1-2a5150773ea9
02/15/2025 00:02:09:INFO:Received: evaluate message dcdf6753-f764-4e2a-81c1-2a5150773ea9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:02:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:52:INFO:
[92mINFO [0m:      Received: train message e10a15d5-d20d-4ac0-97ef-e620de84eda2
02/15/2025 00:02:52:INFO:Received: train message e10a15d5-d20d-4ac0-97ef-e620de84eda2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:03:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:04:06:INFO:
[92mINFO [0m:      Received: evaluate message 3e554fdf-df33-49f5-b18b-e104c5a08f97
02/15/2025 00:04:06:INFO:Received: evaluate message 3e554fdf-df33-49f5-b18b-e104c5a08f97
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392], 'accuracy': [0.17279124315871774], 'auc': [0.36910406569460863], 'precision': [0.19215364459241882], 'recall': [0.17279124315871774], 'f1': [0.15428779652186775]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477], 'accuracy': [0.17279124315871774, 0.23534010946051603], 'auc': [0.36910406569460863, 0.37509675462346337], 'precision': [0.19215364459241882, 0.20470434628072817], 'recall': [0.17279124315871774, 0.23534010946051603], 'f1': [0.15428779652186775, 0.2139057332831779]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:04:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:04:29:INFO:
[92mINFO [0m:      Received: train message 4e94acd8-a77b-42f4-ab71-2c09b70bb78a
02/15/2025 00:04:29:INFO:Received: train message 4e94acd8-a77b-42f4-ab71-2c09b70bb78a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:05:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:05:59:INFO:
[92mINFO [0m:      Received: evaluate message 0db42cab-3c15-4f48-8ce9-0d1530cdb672
02/15/2025 00:05:59:INFO:Received: evaluate message 0db42cab-3c15-4f48-8ce9-0d1530cdb672
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:06:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:53:INFO:
[92mINFO [0m:      Received: train message 5644455b-34c3-46ba-aba6-f221b057df9e
02/15/2025 00:06:53:INFO:Received: train message 5644455b-34c3-46ba-aba6-f221b057df9e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:07:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:04:INFO:
[92mINFO [0m:      Received: evaluate message 65fdb9ad-62a6-4d99-830d-ffa94b6a5207
02/15/2025 00:08:04:INFO:Received: evaluate message 65fdb9ad-62a6-4d99-830d-ffa94b6a5207
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:08:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:39:INFO:
[92mINFO [0m:      Received: train message 105f610a-d2ff-4d62-b558-414d601e8ee5
02/15/2025 00:08:39:INFO:Received: train message 105f610a-d2ff-4d62-b558-414d601e8ee5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:05:INFO:
[92mINFO [0m:      Received: evaluate message afba1bd8-bc26-4b62-80e2-a2a782bb4429
02/15/2025 00:10:05:INFO:Received: evaluate message afba1bd8-bc26-4b62-80e2-a2a782bb4429
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:10:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:34:INFO:
[92mINFO [0m:      Received: train message 5c032b5b-0895-42ad-9b5c-fe263369bfb4
02/15/2025 00:10:34:INFO:Received: train message 5c032b5b-0895-42ad-9b5c-fe263369bfb4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:11:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:11:54:INFO:
[92mINFO [0m:      Received: evaluate message 60a859f9-04be-4b60-938a-420935e69507
02/15/2025 00:11:54:INFO:Received: evaluate message 60a859f9-04be-4b60-938a-420935e69507

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:11:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:44:INFO:
[92mINFO [0m:      Received: train message e94d7507-f9e1-4cdc-a116-192052a9662b
02/15/2025 00:12:44:INFO:Received: train message e94d7507-f9e1-4cdc-a116-192052a9662b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:13:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:57:INFO:
[92mINFO [0m:      Received: evaluate message d4cbe636-4bc0-403f-b0d8-94e8325308b8
02/15/2025 00:13:57:INFO:Received: evaluate message d4cbe636-4bc0-403f-b0d8-94e8325308b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:14:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:31:INFO:
[92mINFO [0m:      Received: train message 66967042-46e1-48b7-bf72-56d91fa2ed29
02/15/2025 00:14:31:INFO:Received: train message 66967042-46e1-48b7-bf72-56d91fa2ed29
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:15:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:35:INFO:
[92mINFO [0m:      Received: evaluate message 6e2c7128-f73a-4004-b7b9-c42ebdeb3062
02/15/2025 00:15:35:INFO:Received: evaluate message 6e2c7128-f73a-4004-b7b9-c42ebdeb3062
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:15:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:16:26:INFO:
[92mINFO [0m:      Received: train message 373b9344-a2a8-41e7-8730-1e7eebed3b45
02/15/2025 00:16:26:INFO:Received: train message 373b9344-a2a8-41e7-8730-1e7eebed3b45
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:35:INFO:
[92mINFO [0m:      Received: evaluate message 212a3802-65d8-45f5-961b-37d56d0ef3a2
02/15/2025 00:17:35:INFO:Received: evaluate message 212a3802-65d8-45f5-961b-37d56d0ef3a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:17:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:18:27:INFO:
[92mINFO [0m:      Received: train message b8e31ece-3055-4c22-a6d9-e3295d60260b
02/15/2025 00:18:27:INFO:Received: train message b8e31ece-3055-4c22-a6d9-e3295d60260b

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:27:INFO:
[92mINFO [0m:      Received: evaluate message f94d00db-aa95-4e16-a683-34bda3f5d495
02/15/2025 00:19:27:INFO:Received: evaluate message f94d00db-aa95-4e16-a683-34bda3f5d495
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:20:19:INFO:
[92mINFO [0m:      Received: train message 5e3484c7-d6f2-4698-9eeb-3df4f4b72dfa
02/15/2025 00:20:19:INFO:Received: train message 5e3484c7-d6f2-4698-9eeb-3df4f4b72dfa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:20:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:25:INFO:
[92mINFO [0m:      Received: evaluate message 611f2b3c-bdfa-446b-8f27-582d4ad56f07
02/15/2025 00:21:25:INFO:Received: evaluate message 611f2b3c-bdfa-446b-8f27-582d4ad56f07
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:12:INFO:
[92mINFO [0m:      Received: train message a4e1f500-e759-4537-bc62-b5ba49b503f7
02/15/2025 00:22:12:INFO:Received: train message a4e1f500-e759-4537-bc62-b5ba49b503f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:22:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:23:33:INFO:
[92mINFO [0m:      Received: evaluate message b1086f6e-8e3c-450f-b4b4-9d4a6b0c0d5e
02/15/2025 00:23:33:INFO:Received: evaluate message b1086f6e-8e3c-450f-b4b4-9d4a6b0c0d5e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:23:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:10:INFO:
[92mINFO [0m:      Received: train message 454dec67-8198-4852-a69c-918fa955ad27
02/15/2025 00:24:10:INFO:Received: train message 454dec67-8198-4852-a69c-918fa955ad27
Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:24:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:25:18:INFO:
[92mINFO [0m:      Received: evaluate message 5ed83ea1-590a-43d2-b18c-17d2ba3ed8ac
02/15/2025 00:25:18:INFO:Received: evaluate message 5ed83ea1-590a-43d2-b18c-17d2ba3ed8ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:25:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:14:INFO:
[92mINFO [0m:      Received: train message a5aaa9e1-0fb8-42be-acc8-15263c79d2f0
02/15/2025 00:26:14:INFO:Received: train message a5aaa9e1-0fb8-42be-acc8-15263c79d2f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:26:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:09:INFO:
[92mINFO [0m:      Received: evaluate message 845f588b-9080-481a-989d-fdab9a59cd5a
02/15/2025 00:27:09:INFO:Received: evaluate message 845f588b-9080-481a-989d-fdab9a59cd5a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:27:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:28:13:INFO:
[92mINFO [0m:      Received: train message 1f3539de-14a9-49b2-b55c-dca4d0ff5e4c
02/15/2025 00:28:13:INFO:Received: train message 1f3539de-14a9-49b2-b55c-dca4d0ff5e4c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:28:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:22:INFO:
[92mINFO [0m:      Received: evaluate message 4fdaff35-22cb-4e03-84be-850b8efa9cd8
02/15/2025 00:29:22:INFO:Received: evaluate message 4fdaff35-22cb-4e03-84be-850b8efa9cd8
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:30:02:INFO:
[92mINFO [0m:      Received: train message 2534aa60-4046-49b2-9315-3080defcc4d0
02/15/2025 00:30:02:INFO:Received: train message 2534aa60-4046-49b2-9315-3080defcc4d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:20:INFO:
[92mINFO [0m:      Received: evaluate message e6659916-53b8-4a96-b0ac-0a9814336b35
02/15/2025 00:31:20:INFO:Received: evaluate message e6659916-53b8-4a96-b0ac-0a9814336b35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:46:INFO:
[92mINFO [0m:      Received: train message df87d830-29c8-4490-874e-a97f6753f12d
02/15/2025 00:31:46:INFO:Received: train message df87d830-29c8-4490-874e-a97f6753f12d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:20:INFO:
[92mINFO [0m:      Received: evaluate message 105702e0-aecb-41a2-967e-141fead7db1e
02/15/2025 00:33:20:INFO:Received: evaluate message 105702e0-aecb-41a2-967e-141fead7db1e

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:42:INFO:
[92mINFO [0m:      Received: train message 4d00d3bc-1af4-4a90-84be-53364eca7115
02/15/2025 00:33:42:INFO:Received: train message 4d00d3bc-1af4-4a90-84be-53364eca7115
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:34:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:04:INFO:
[92mINFO [0m:      Received: evaluate message 244f9827-c5d4-483d-9b58-993980ca06e9
02/15/2025 00:35:04:INFO:Received: evaluate message 244f9827-c5d4-483d-9b58-993980ca06e9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:35:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:55:INFO:
[92mINFO [0m:      Received: train message b7b9daaf-d520-4d38-b72f-fb49d3e797dc
02/15/2025 00:35:55:INFO:Received: train message b7b9daaf-d520-4d38-b72f-fb49d3e797dc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:36:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:36:59:INFO:
[92mINFO [0m:      Received: evaluate message f5cad954-e3d6-45b8-90ca-3b9d3147febe
02/15/2025 00:36:59:INFO:Received: evaluate message f5cad954-e3d6-45b8-90ca-3b9d3147febe

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:37:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:48:INFO:
[92mINFO [0m:      Received: train message f2ecd192-7376-4f34-8805-050885bdd067
02/15/2025 00:37:48:INFO:Received: train message f2ecd192-7376-4f34-8805-050885bdd067
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:38:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:39:01:INFO:
[92mINFO [0m:      Received: evaluate message c0f8127d-3196-4018-978c-f0e8f3492beb
02/15/2025 00:39:01:INFO:Received: evaluate message c0f8127d-3196-4018-978c-f0e8f3492beb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:39:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:39:25:INFO:
[92mINFO [0m:      Received: train message 4a2e0192-c2ac-45db-8892-a59f542ab06a
02/15/2025 00:39:25:INFO:Received: train message 4a2e0192-c2ac-45db-8892-a59f542ab06a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:39:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:52:INFO:
[92mINFO [0m:      Received: evaluate message 3b1efd93-d317-4e56-95d5-5bcfadee18a0
02/15/2025 00:40:52:INFO:Received: evaluate message 3b1efd93-d317-4e56-95d5-5bcfadee18a0

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:41:21:INFO:
[92mINFO [0m:      Received: train message d1755f74-1530-4711-8f61-8b8f72b7eab0
02/15/2025 00:41:21:INFO:Received: train message d1755f74-1530-4711-8f61-8b8f72b7eab0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:45:INFO:
[92mINFO [0m:      Received: evaluate message 9ddcca78-2d2b-4ad0-a078-8fd7192c458c
02/15/2025 00:42:45:INFO:Received: evaluate message 9ddcca78-2d2b-4ad0-a078-8fd7192c458c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:42:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:43:24:INFO:
[92mINFO [0m:      Received: train message 2ff1efd3-619b-4bdc-afc2-4dc0b12f25e7
02/15/2025 00:43:24:INFO:Received: train message 2ff1efd3-619b-4bdc-afc2-4dc0b12f25e7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:23:INFO:
[92mINFO [0m:      Received: evaluate message b4cd3e20-2ce4-485f-a1a3-aeeaba4723af
02/15/2025 00:44:23:INFO:Received: evaluate message b4cd3e20-2ce4-485f-a1a3-aeeaba4723af

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:45:08:INFO:
[92mINFO [0m:      Received: train message aa7acb46-b3d4-4921-b108-c0f28a4511bb
02/15/2025 00:45:08:INFO:Received: train message aa7acb46-b3d4-4921-b108-c0f28a4511bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:21:INFO:
[92mINFO [0m:      Received: evaluate message fac8a6aa-4608-40b2-b69a-d7c51f7922c6
02/15/2025 00:46:21:INFO:Received: evaluate message fac8a6aa-4608-40b2-b69a-d7c51f7922c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:58:INFO:
[92mINFO [0m:      Received: train message 8c4f5694-596c-44d0-bd50-64c1bf5ae545
02/15/2025 00:46:58:INFO:Received: train message 8c4f5694-596c-44d0-bd50-64c1bf5ae545

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:47:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:00:INFO:
[92mINFO [0m:      Received: evaluate message 151834e5-669a-4cd7-8058-66d9d6c5dd87
02/15/2025 00:48:00:INFO:Received: evaluate message 151834e5-669a-4cd7-8058-66d9d6c5dd87
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:17:INFO:
[92mINFO [0m:      Received: reconnect message ee65899d-651f-45f8-bbcd-d347f4de3017
02/15/2025 00:48:17:INFO:Received: reconnect message ee65899d-651f-45f8-bbcd-d347f4de3017
02/15/2025 00:48:17:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 00:48:17:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}



Final client history:
{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132, 1.0323693126882771], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532, 0.5955649765426512], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691, 0.3368747031114691]}

