nohup: ignoring input
02/14/2025 23:50:57:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:50:57:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:50:57:DEBUG:ChannelConnectivity.CONNECTING
02/14/2025 23:50:57:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739605857.967404 1504603 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:51:13:INFO:
[92mINFO [0m:      Received: train message a1e578b7-a8d5-4097-ab04-60bc80733062
02/14/2025 23:51:13:INFO:Received: train message a1e578b7-a8d5-4097-ab04-60bc80733062
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:51:30:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:52:44:INFO:
[92mINFO [0m:      Received: evaluate message 0d551e05-2988-43f3-aab2-022a4b54a453
02/14/2025 23:52:44:INFO:Received: evaluate message 0d551e05-2988-43f3-aab2-022a4b54a453
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:52:47:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:52:59:INFO:
[92mINFO [0m:      Received: train message 8f479f43-52ef-4a04-838f-0462ef749f8e
02/14/2025 23:52:59:INFO:Received: train message 8f479f43-52ef-4a04-838f-0462ef749f8e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:53:15:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:54:33:INFO:
[92mINFO [0m:      Received: evaluate message b6d8367e-4d1e-436c-adbe-e5d77dbbb86e
02/14/2025 23:54:33:INFO:Received: evaluate message b6d8367e-4d1e-436c-adbe-e5d77dbbb86e
[92mINFO [0m:      Sent reply
02/14/2025 23:54:36:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:55:05:INFO:
[92mINFO [0m:      Received: train message afd4cbd8-4d05-41c5-87d2-289f4fb0ed21
02/14/2025 23:55:05:INFO:Received: train message afd4cbd8-4d05-41c5-87d2-289f4fb0ed21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:55:25:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:15:INFO:
[92mINFO [0m:      Received: evaluate message fdf54e96-3c1f-4063-af7a-c05582a92b6c
02/14/2025 23:56:15:INFO:Received: evaluate message fdf54e96-3c1f-4063-af7a-c05582a92b6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:56:18:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:56:48:INFO:
[92mINFO [0m:      Received: train message cd8128fd-9784-4a0f-98fe-a44d24136bed
02/14/2025 23:56:48:INFO:Received: train message cd8128fd-9784-4a0f-98fe-a44d24136bed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:57:08:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:08:INFO:
[92mINFO [0m:      Received: evaluate message a1441487-0c67-47ed-8906-a2ab0a5dcf34
02/14/2025 23:58:08:INFO:Received: evaluate message a1441487-0c67-47ed-8906-a2ab0a5dcf34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/14/2025 23:58:10:INFO:Sent reply
[92mINFO [0m:      
02/14/2025 23:58:50:INFO:
[92mINFO [0m:      Received: train message 6cb9929b-c7ea-4f6b-a1f1-4e3937a3cffd
02/14/2025 23:58:50:INFO:Received: train message 6cb9929b-c7ea-4f6b-a1f1-4e3937a3cffd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:59:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:13:INFO:
[92mINFO [0m:      Received: evaluate message 017fe496-7073-41f6-85cb-ba35afda9f48
02/15/2025 00:00:13:INFO:Received: evaluate message 017fe496-7073-41f6-85cb-ba35afda9f48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:00:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:48:INFO:
[92mINFO [0m:      Received: train message 7126d206-96bb-4324-abb5-3fa0e220966f
02/15/2025 00:00:48:INFO:Received: train message 7126d206-96bb-4324-abb5-3fa0e220966f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:01:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:01:56:INFO:
[92mINFO [0m:      Received: evaluate message 1d23b9a0-1011-48a6-82b0-958daf5e3ae5
02/15/2025 00:01:56:INFO:Received: evaluate message 1d23b9a0-1011-48a6-82b0-958daf5e3ae5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:01:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:41:INFO:
[92mINFO [0m:      Received: train message 2ef6bca1-002c-43bd-8ad4-b838d3343e57
02/15/2025 00:02:41:INFO:Received: train message 2ef6bca1-002c-43bd-8ad4-b838d3343e57
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:03:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:04:10:INFO:
[92mINFO [0m:      Received: evaluate message 5302d51e-f0a9-4056-af47-624e4aabbd18
02/15/2025 00:04:10:INFO:Received: evaluate message 5302d51e-f0a9-4056-af47-624e4aabbd18
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392], 'accuracy': [0.17279124315871774], 'auc': [0.36910406569460863], 'precision': [0.19215364459241882], 'recall': [0.17279124315871774], 'f1': [0.15428779652186775]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477], 'accuracy': [0.17279124315871774, 0.23534010946051603], 'auc': [0.36910406569460863, 0.37509675462346337], 'precision': [0.19215364459241882, 0.20470434628072817], 'recall': [0.17279124315871774, 0.23534010946051603], 'f1': [0.15428779652186775, 0.2139057332831779]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:04:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:04:49:INFO:
[92mINFO [0m:      Received: train message dc1d3790-c174-4bf1-81f4-4b95c739b708
02/15/2025 00:04:49:INFO:Received: train message dc1d3790-c174-4bf1-81f4-4b95c739b708
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:05:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:08:INFO:
[92mINFO [0m:      Received: evaluate message d7bac61a-b64e-4d7e-a54a-114570fe96ab
02/15/2025 00:06:08:INFO:Received: evaluate message d7bac61a-b64e-4d7e-a54a-114570fe96ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:06:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:53:INFO:
[92mINFO [0m:      Received: train message 3b9c7faf-88fd-4aa9-aa5f-b2218c99aca5
02/15/2025 00:06:53:INFO:Received: train message 3b9c7faf-88fd-4aa9-aa5f-b2218c99aca5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:07:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:08:INFO:
[92mINFO [0m:      Received: evaluate message 485feb08-cfc7-4cd9-b798-a3849169b56e
02/15/2025 00:08:08:INFO:Received: evaluate message 485feb08-cfc7-4cd9-b798-a3849169b56e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:08:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:42:INFO:
[92mINFO [0m:      Received: train message 3ac27ddd-c9a6-4802-a995-0db13288271f
02/15/2025 00:08:42:INFO:Received: train message 3ac27ddd-c9a6-4802-a995-0db13288271f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:04:INFO:
[92mINFO [0m:      Received: evaluate message 24c6fa86-98ff-4f99-ae71-b2ac5f79e544
02/15/2025 00:10:04:INFO:Received: evaluate message 24c6fa86-98ff-4f99-ae71-b2ac5f79e544
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:10:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:40:INFO:
[92mINFO [0m:      Received: train message d0c37b1c-5ae8-46ab-bf2d-7a61803e5e66
02/15/2025 00:10:40:INFO:Received: train message d0c37b1c-5ae8-46ab-bf2d-7a61803e5e66
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:11:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:07:INFO:
[92mINFO [0m:      Received: evaluate message 843af103-9391-4b06-a745-bec926c87aa1
02/15/2025 00:12:07:INFO:Received: evaluate message 843af103-9391-4b06-a745-bec926c87aa1

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:12:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:46:INFO:
[92mINFO [0m:      Received: train message fd758b01-6022-42e3-b705-ee38e6297532
02/15/2025 00:12:46:INFO:Received: train message fd758b01-6022-42e3-b705-ee38e6297532
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:13:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:44:INFO:
[92mINFO [0m:      Received: evaluate message 6116c517-ff5a-4ea2-a4de-6594648e39c4
02/15/2025 00:13:44:INFO:Received: evaluate message 6116c517-ff5a-4ea2-a4de-6594648e39c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:13:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:27:INFO:
[92mINFO [0m:      Received: train message 000896c2-d720-42d0-be30-b9e8546b6a0a
02/15/2025 00:14:27:INFO:Received: train message 000896c2-d720-42d0-be30-b9e8546b6a0a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:14:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:52:INFO:
[92mINFO [0m:      Received: evaluate message fa1927f1-fa5c-40aa-8560-129f71fe93cb
02/15/2025 00:15:52:INFO:Received: evaluate message fa1927f1-fa5c-40aa-8560-129f71fe93cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:15:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:16:33:INFO:
[92mINFO [0m:      Received: train message 48275dc8-8b77-4711-8369-0a3dc9228a1f
02/15/2025 00:16:33:INFO:Received: train message 48275dc8-8b77-4711-8369-0a3dc9228a1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:45:INFO:
[92mINFO [0m:      Received: evaluate message dd24f3b9-7d68-4997-8686-9bd194d7ef48
02/15/2025 00:17:45:INFO:Received: evaluate message dd24f3b9-7d68-4997-8686-9bd194d7ef48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:17:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:18:27:INFO:
[92mINFO [0m:      Received: train message e9b52d77-3986-4354-9ae8-f1c15b43cdba
02/15/2025 00:18:27:INFO:Received: train message e9b52d77-3986-4354-9ae8-f1c15b43cdba

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:39:INFO:
[92mINFO [0m:      Received: evaluate message 9414e76b-55a7-45af-800b-7675363117d9
02/15/2025 00:19:39:INFO:Received: evaluate message 9414e76b-55a7-45af-800b-7675363117d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:20:01:INFO:
[92mINFO [0m:      Received: train message aa5f63e5-e45d-4b9a-9b84-1169b70ba3ca
02/15/2025 00:20:01:INFO:Received: train message aa5f63e5-e45d-4b9a-9b84-1169b70ba3ca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:20:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:33:INFO:
[92mINFO [0m:      Received: evaluate message 811f0c8a-c626-444c-a921-5aeb696b950a
02/15/2025 00:21:33:INFO:Received: evaluate message 811f0c8a-c626-444c-a921-5aeb696b950a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:08:INFO:
[92mINFO [0m:      Received: train message 21d516e3-22ff-4fc9-86c0-5ac706d25a9b
02/15/2025 00:22:08:INFO:Received: train message 21d516e3-22ff-4fc9-86c0-5ac706d25a9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:22:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:23:32:INFO:
[92mINFO [0m:      Received: evaluate message 84900281-d0a4-422a-ae68-aba4bd4b88b0
02/15/2025 00:23:32:INFO:Received: evaluate message 84900281-d0a4-422a-ae68-aba4bd4b88b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:23:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:10:INFO:
[92mINFO [0m:      Received: train message 438b888c-948a-4d48-857f-da747219df96
02/15/2025 00:24:10:INFO:Received: train message 438b888c-948a-4d48-857f-da747219df96
Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:24:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:25:18:INFO:
[92mINFO [0m:      Received: evaluate message e5e316b7-f31d-4eee-ac6c-1b97a30bff64
02/15/2025 00:25:18:INFO:Received: evaluate message e5e316b7-f31d-4eee-ac6c-1b97a30bff64
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:25:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:09:INFO:
[92mINFO [0m:      Received: train message 21d9f0d6-5af3-4a0a-b0a8-efbeb01d1c61
02/15/2025 00:26:09:INFO:Received: train message 21d9f0d6-5af3-4a0a-b0a8-efbeb01d1c61
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:26:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:24:INFO:
[92mINFO [0m:      Received: evaluate message c9a893bd-62f0-4f4f-a653-ee282e9914eb
02/15/2025 00:27:24:INFO:Received: evaluate message c9a893bd-62f0-4f4f-a653-ee282e9914eb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:27:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:59:INFO:
[92mINFO [0m:      Received: train message 86b28654-e118-43c5-80d3-591b54844351
02/15/2025 00:27:59:INFO:Received: train message 86b28654-e118-43c5-80d3-591b54844351
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:28:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:15:INFO:
[92mINFO [0m:      Received: evaluate message c99225f3-5cb6-4f55-9e23-39281c2fdb1a
02/15/2025 00:29:15:INFO:Received: evaluate message c99225f3-5cb6-4f55-9e23-39281c2fdb1a
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:30:06:INFO:
[92mINFO [0m:      Received: train message 983566c7-c290-4296-9b2a-dfb49ebec9ab
02/15/2025 00:30:06:INFO:Received: train message 983566c7-c290-4296-9b2a-dfb49ebec9ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:28:INFO:
[92mINFO [0m:      Received: evaluate message 6ae082c5-3048-4b4e-b25b-19ecbee99e3b
02/15/2025 00:31:28:INFO:Received: evaluate message 6ae082c5-3048-4b4e-b25b-19ecbee99e3b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:32:08:INFO:
[92mINFO [0m:      Received: train message c0fa56c6-8e9e-4fbd-b492-f1251c6c2eb0
02/15/2025 00:32:08:INFO:Received: train message c0fa56c6-8e9e-4fbd-b492-f1251c6c2eb0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:20:INFO:
[92mINFO [0m:      Received: evaluate message af78f296-e2b2-4487-8ed4-8ee483dfc0b3
02/15/2025 00:33:20:INFO:Received: evaluate message af78f296-e2b2-4487-8ed4-8ee483dfc0b3

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:59:INFO:
[92mINFO [0m:      Received: train message 205b714d-07af-4506-903f-3de4137fc9b9
02/15/2025 00:33:59:INFO:Received: train message 205b714d-07af-4506-903f-3de4137fc9b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:34:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:13:INFO:
[92mINFO [0m:      Received: evaluate message 1c62a0c7-d2d4-4032-b230-7dc62f962d28
02/15/2025 00:35:13:INFO:Received: evaluate message 1c62a0c7-d2d4-4032-b230-7dc62f962d28
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:35:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:41:INFO:
[92mINFO [0m:      Received: train message 0d5cdb88-c06c-43e9-9dac-c24705b59476
02/15/2025 00:35:41:INFO:Received: train message 0d5cdb88-c06c-43e9-9dac-c24705b59476
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:36:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:36:54:INFO:
[92mINFO [0m:      Received: evaluate message e09cd9dd-fe55-496c-b016-39c704f47a0e
02/15/2025 00:36:54:INFO:Received: evaluate message e09cd9dd-fe55-496c-b016-39c704f47a0e

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:36:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:41:INFO:
[92mINFO [0m:      Received: train message b37e1015-a8d9-4de1-82f6-1b768ef2690c
02/15/2025 00:37:41:INFO:Received: train message b37e1015-a8d9-4de1-82f6-1b768ef2690c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:38:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:38:47:INFO:
[92mINFO [0m:      Received: evaluate message 9a5dcee6-8536-443c-bb6c-34227ca59f5d
02/15/2025 00:38:47:INFO:Received: evaluate message 9a5dcee6-8536-443c-bb6c-34227ca59f5d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:38:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:39:38:INFO:
[92mINFO [0m:      Received: train message 0c77216b-ea22-40a3-a42d-995d5cb69116
02/15/2025 00:39:38:INFO:Received: train message 0c77216b-ea22-40a3-a42d-995d5cb69116
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:39:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:48:INFO:
[92mINFO [0m:      Received: evaluate message 68744644-62d5-4307-97e1-fbc5933903c2
02/15/2025 00:40:48:INFO:Received: evaluate message 68744644-62d5-4307-97e1-fbc5933903c2

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:41:15:INFO:
[92mINFO [0m:      Received: train message 6a75de26-de90-4f1c-9cf6-378f332d3238
02/15/2025 00:41:15:INFO:Received: train message 6a75de26-de90-4f1c-9cf6-378f332d3238
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:45:INFO:
[92mINFO [0m:      Received: evaluate message e40d7b58-b660-4cf4-afd6-ceb1bb50b10e
02/15/2025 00:42:45:INFO:Received: evaluate message e40d7b58-b660-4cf4-afd6-ceb1bb50b10e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:42:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:43:00:INFO:
[92mINFO [0m:      Received: train message 23804cfe-f132-4dea-b64e-ab2a1d89ad85
02/15/2025 00:43:00:INFO:Received: train message 23804cfe-f132-4dea-b64e-ab2a1d89ad85
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:35:INFO:
[92mINFO [0m:      Received: evaluate message 634868c7-f944-41b0-a558-8fcdaa35b699
02/15/2025 00:44:35:INFO:Received: evaluate message 634868c7-f944-41b0-a558-8fcdaa35b699

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:45:02:INFO:
[92mINFO [0m:      Received: train message 2b8ff945-2d7f-4ec3-b36a-695d9dab64cf
02/15/2025 00:45:02:INFO:Received: train message 2b8ff945-2d7f-4ec3-b36a-695d9dab64cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:25:INFO:
[92mINFO [0m:      Received: evaluate message 074bce87-49d2-41e9-98e2-985ec9ffbaff
02/15/2025 00:46:25:INFO:Received: evaluate message 074bce87-49d2-41e9-98e2-985ec9ffbaff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:47:04:INFO:
[92mINFO [0m:      Received: train message 8f3003de-574c-4801-9330-ac48610636dc
02/15/2025 00:47:04:INFO:Received: train message 8f3003de-574c-4801-9330-ac48610636dc

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4384056039859392, 1.3550396099791477, 1.2946625155709142, 1.2465404058639848, 1.2106313411848353, 1.1849049930930418, 1.1634695036882903, 1.1441112056759022, 1.1290853583691547, 1.1166281381372924, 1.105884568312841, 1.0972558669134265, 1.0888619571108664, 1.0817084629336216, 1.0751324237744448, 1.0695832177379152, 1.0651007886973092, 1.0615411912956865, 1.0575112288029889, 1.0545533381000545, 1.0512122876761574, 1.0485076562234255, 1.0463568585472613, 1.0441328102578586, 1.0422173712382194, 1.0402091458218465, 1.0385302253958768, 1.0362769179832572, 1.0343519414748132], 'accuracy': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'auc': [0.36910406569460863, 0.37509675462346337, 0.3779542569966736, 0.38177753677992665, 0.3880435744093905, 0.3932820581950237, 0.4011433641467148, 0.4105750132256827, 0.42088979153113415, 0.4323512937978751, 0.43960340213558136, 0.45036550013211196, 0.46438156600818675, 0.47349946442242485, 0.48734530793509956, 0.49921185388027645, 0.5063025904201363, 0.5162856379275027, 0.5267839639716863, 0.5334796107207552, 0.5423155099836511, 0.5493078085570385, 0.5561437848039745, 0.562087475846842, 0.5686509594655133, 0.5750562922402556, 0.5785115322118943, 0.5834974041965699, 0.5911820076988532], 'precision': [0.19215364459241882, 0.20470434628072817, 0.2806098613359041, 0.3575905199830141, 0.43332992714507046, 0.42632434265717356, 0.4424414571582553, 0.46673739626793986, 0.47010074378279154, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.4255282152572456, 0.4255282152572456, 0.4255282152572456, 0.48410338863548535, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.5134893988869981, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.48410338863548535, 0.6010568897123156, 0.48410338863548535], 'recall': [0.17279124315871774, 0.23534010946051603, 0.3713838936669273, 0.4683346364347146, 0.4949179046129789, 0.4988272087568413, 0.5011727912431587, 0.5027365129007036, 0.5027365129007036, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5019546520719312, 0.5011727912431587], 'f1': [0.15428779652186775, 0.2139057332831779, 0.2905145503531547, 0.33029946775985886, 0.33991078930265345, 0.34089088311522664, 0.34292664159747815, 0.34575095669124195, 0.3417990887054476, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.33515072539310226, 0.33515072539310226, 0.33515072539310226, 0.3368747031114691, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3385920048438915, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3368747031114691, 0.3372295534420434, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:47:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:13:INFO:
[92mINFO [0m:      Received: evaluate message 43577810-93f3-498b-93ff-6dc165f86c27
02/15/2025 00:48:13:INFO:Received: evaluate message 43577810-93f3-498b-93ff-6dc165f86c27
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:17:INFO:
[92mINFO [0m:      Received: reconnect message 10df5a4d-e08b-4f11-a7ef-5f99df4d8116
02/15/2025 00:48:17:INFO:Received: reconnect message 10df5a4d-e08b-4f11-a7ef-5f99df4d8116
02/15/2025 00:48:17:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 00:48:17:INFO:Disconnect and shut down
