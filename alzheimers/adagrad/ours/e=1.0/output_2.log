nohup: ignoring input
02/17/2025 15:45:32:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:45:32:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:45:32:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739835932.692265  742268 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:45:59:INFO:
[92mINFO [0m:      Received: train message 524d4ecb-aa7f-4c8b-9824-aa30981a9b61
02/17/2025 15:45:59:INFO:Received: train message 524d4ecb-aa7f-4c8b-9824-aa30981a9b61
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:46:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:47:41:INFO:
[92mINFO [0m:      Received: evaluate message 6dd7f97f-a247-4aa6-8849-c0843950f6e8
02/17/2025 15:47:41:INFO:Received: evaluate message 6dd7f97f-a247-4aa6-8849-c0843950f6e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:47:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:48:13:INFO:
[92mINFO [0m:      Received: train message e87eb605-ddb8-4e83-93f8-74b65121123b
02/17/2025 15:48:14:INFO:Received: train message e87eb605-ddb8-4e83-93f8-74b65121123b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:48:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:49:50:INFO:
[92mINFO [0m:      Received: evaluate message 3a243007-0590-4161-9a73-1c0e01a91b6c
02/17/2025 15:49:50:INFO:Received: evaluate message 3a243007-0590-4161-9a73-1c0e01a91b6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:49:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:50:39:INFO:
[92mINFO [0m:      Received: train message a2cf6fe6-63ec-416e-8225-e6d07930dc09
02/17/2025 15:50:39:INFO:Received: train message a2cf6fe6-63ec-416e-8225-e6d07930dc09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:51:09:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:09:INFO:
[92mINFO [0m:      Received: evaluate message 284a382f-e2e8-46df-8776-fefa9e84417c
02/17/2025 15:52:09:INFO:Received: evaluate message 284a382f-e2e8-46df-8776-fefa9e84417c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:56:INFO:
[92mINFO [0m:      Received: train message bf3fd45c-323a-4e59-b312-90ddffbd7d65
02/17/2025 15:52:56:INFO:Received: train message bf3fd45c-323a-4e59-b312-90ddffbd7d65
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:53:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:32:INFO:
[92mINFO [0m:      Received: evaluate message cad2943a-dd17-4b32-97cf-5182c34951b2
02/17/2025 15:54:32:INFO:Received: evaluate message cad2943a-dd17-4b32-97cf-5182c34951b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:22:INFO:
[92mINFO [0m:      Received: train message 60a966db-cbe0-4759-ab2e-da501ff90527
02/17/2025 15:55:22:INFO:Received: train message 60a966db-cbe0-4759-ab2e-da501ff90527
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:55:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:56:50:INFO:
[92mINFO [0m:      Received: evaluate message f2f0f27b-cf13-4d6b-a6e7-5fec39bf0f25
02/17/2025 15:56:50:INFO:Received: evaluate message f2f0f27b-cf13-4d6b-a6e7-5fec39bf0f25
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:56:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:35:INFO:
[92mINFO [0m:      Received: train message b52c65c8-bd59-4a85-9ebb-41565bd3c20c
02/17/2025 15:57:35:INFO:Received: train message b52c65c8-bd59-4a85-9ebb-41565bd3c20c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:02:INFO:
[92mINFO [0m:      Received: evaluate message 48c14ff0-163b-41da-89c9-cfe6b382ea25
02/17/2025 15:59:02:INFO:Received: evaluate message 48c14ff0-163b-41da-89c9-cfe6b382ea25
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:48:INFO:
[92mINFO [0m:      Received: train message 7a47d624-e895-4b93-9181-9bf0350462bb
02/17/2025 15:59:48:INFO:Received: train message 7a47d624-e895-4b93-9181-9bf0350462bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:00:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:18:INFO:
[92mINFO [0m:      Received: evaluate message 5978b810-6a7f-4569-825f-ccaf81f66d11
02/17/2025 16:01:18:INFO:Received: evaluate message 5978b810-6a7f-4569-825f-ccaf81f66d11
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866], 'accuracy': [0.49960906958561374], 'auc': [0.6223554252585197], 'precision': [0.3853113538182961], 'recall': [0.49960906958561374], 'f1': [0.34049198829300026]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282], 'accuracy': [0.49960906958561374, 0.5222830336200156], 'auc': [0.6223554252585197, 0.6407563691851618], 'precision': [0.3853113538182961, 0.4289287570845902], 'recall': [0.49960906958561374, 0.5222830336200156], 'f1': [0.34049198829300026, 0.4252045406151577]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:54:INFO:
[92mINFO [0m:      Received: train message 7eb8bf82-ce25-4f11-8397-dc2e044de95a
02/17/2025 16:01:54:INFO:Received: train message 7eb8bf82-ce25-4f11-8397-dc2e044de95a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:02:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:20:INFO:
[92mINFO [0m:      Received: evaluate message bd953ce7-a7ca-4de0-8082-8d1f392e20ef
02/17/2025 16:03:20:INFO:Received: evaluate message bd953ce7-a7ca-4de0-8082-8d1f392e20ef
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:03:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:18:INFO:
[92mINFO [0m:      Received: train message eeed40a3-14af-4e19-ae4e-32c69ff67603
02/17/2025 16:04:18:INFO:Received: train message eeed40a3-14af-4e19-ae4e-32c69ff67603
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:04:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:05:17:INFO:
[92mINFO [0m:      Received: evaluate message ea90211f-3dd7-4e35-9977-4025577e034f
02/17/2025 16:05:17:INFO:Received: evaluate message ea90211f-3dd7-4e35-9977-4025577e034f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:05:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:36:INFO:
[92mINFO [0m:      Received: train message bfdcd2b7-d38c-464b-9b02-efdb3e9b0de3
02/17/2025 16:06:36:INFO:Received: train message bfdcd2b7-d38c-464b-9b02-efdb3e9b0de3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:07:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:07:56:INFO:
[92mINFO [0m:      Received: evaluate message 6c2eea59-6fd7-4b3e-bcbb-02cf6cc855f8
02/17/2025 16:07:56:INFO:Received: evaluate message 6c2eea59-6fd7-4b3e-bcbb-02cf6cc855f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:52:INFO:
[92mINFO [0m:      Received: train message 2e78bb2d-bd96-4c74-a040-aaef0694861b
02/17/2025 16:08:53:INFO:Received: train message 2e78bb2d-bd96-4c74-a040-aaef0694861b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:09:20:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:28:INFO:
[92mINFO [0m:      Received: evaluate message cbdc9e71-8a18-41b3-89bd-ddaa71d97275
02/17/2025 16:10:28:INFO:Received: evaluate message cbdc9e71-8a18-41b3-89bd-ddaa71d97275
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:10:31:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:54:INFO:
[92mINFO [0m:      Received: train message 2620d547-e8f7-4b38-9529-3476c294b535
02/17/2025 16:10:54:INFO:Received: train message 2620d547-e8f7-4b38-9529-3476c294b535

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:11:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:12:29:INFO:
[92mINFO [0m:      Received: evaluate message af39c45c-0433-4c26-b3c9-4d53f94cff16
02/17/2025 16:12:29:INFO:Received: evaluate message af39c45c-0433-4c26-b3c9-4d53f94cff16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:12:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:19:INFO:
[92mINFO [0m:      Received: train message 31b23302-74ed-47f3-8602-cceaf25a79b5
02/17/2025 16:13:19:INFO:Received: train message 31b23302-74ed-47f3-8602-cceaf25a79b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:13:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:56:INFO:
[92mINFO [0m:      Received: evaluate message 239d7b0b-2727-43b3-bfe3-41a9bd3817b2
02/17/2025 16:14:56:INFO:Received: evaluate message 239d7b0b-2727-43b3-bfe3-41a9bd3817b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:15:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:25:INFO:
[92mINFO [0m:      Received: train message 72f57697-f5b6-4dd8-9eb1-b581937e49f0
02/17/2025 16:15:25:INFO:Received: train message 72f57697-f5b6-4dd8-9eb1-b581937e49f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:15:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:58:INFO:
[92mINFO [0m:      Received: evaluate message b7ba29e7-4993-41e3-a468-d4aefdeab6b2
02/17/2025 16:16:58:INFO:Received: evaluate message b7ba29e7-4993-41e3-a468-d4aefdeab6b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:17:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:17:22:INFO:
[92mINFO [0m:      Received: train message c696a711-5f9f-46df-8cc3-558e1c7c3804
02/17/2025 16:17:22:INFO:Received: train message c696a711-5f9f-46df-8cc3-558e1c7c3804
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:17:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:19:13:INFO:
[92mINFO [0m:      Received: evaluate message facb4eb5-385d-4a80-9932-a51b3f41661e
02/17/2025 16:19:13:INFO:Received: evaluate message facb4eb5-385d-4a80-9932-a51b3f41661e
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:19:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:19:46:INFO:
[92mINFO [0m:      Received: train message 496b2e1e-f9d7-4279-b33d-e12cd8aa1e01
02/17/2025 16:19:46:INFO:Received: train message 496b2e1e-f9d7-4279-b33d-e12cd8aa1e01
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:20:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:15:INFO:
[92mINFO [0m:      Received: evaluate message 43d823dc-3220-4d6a-821f-2c38823ab859
02/17/2025 16:21:15:INFO:Received: evaluate message 43d823dc-3220-4d6a-821f-2c38823ab859
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:21:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:54:INFO:
[92mINFO [0m:      Received: train message a3b5c5c7-c2d6-42fa-a46c-3204ccffd9ab
02/17/2025 16:21:54:INFO:Received: train message a3b5c5c7-c2d6-42fa-a46c-3204ccffd9ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:28:INFO:
[92mINFO [0m:      Received: evaluate message c5252e27-6f6d-47f8-b2d5-edb913f445f3
02/17/2025 16:23:28:INFO:Received: evaluate message c5252e27-6f6d-47f8-b2d5-edb913f445f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:24:06:INFO:
[92mINFO [0m:      Received: train message 59831a72-2710-4965-995c-aab490e0c595
02/17/2025 16:24:06:INFO:Received: train message 59831a72-2710-4965-995c-aab490e0c595
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:40:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:44:INFO:
[92mINFO [0m:      Received: evaluate message bb118164-862c-41d5-b07d-dc990217eb9f
02/17/2025 16:25:44:INFO:Received: evaluate message bb118164-862c-41d5-b07d-dc990217eb9f

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:18:INFO:
[92mINFO [0m:      Received: train message bff46317-d9a6-4de9-8f1b-20ac7dff8b23
02/17/2025 16:26:18:INFO:Received: train message bff46317-d9a6-4de9-8f1b-20ac7dff8b23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:26:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:03:INFO:
[92mINFO [0m:      Received: evaluate message 813bb8f5-ab6d-430d-8ea1-6699541f9139
02/17/2025 16:28:03:INFO:Received: evaluate message 813bb8f5-ab6d-430d-8ea1-6699541f9139
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:28:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:46:INFO:
[92mINFO [0m:      Received: train message 72269b18-79ef-416f-97f1-d4a54f41a049
02/17/2025 16:28:46:INFO:Received: train message 72269b18-79ef-416f-97f1-d4a54f41a049
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:04:INFO:
[92mINFO [0m:      Received: evaluate message d2e02ad3-5dd8-4c87-8026-e48d9a876262
02/17/2025 16:30:04:INFO:Received: evaluate message d2e02ad3-5dd8-4c87-8026-e48d9a876262

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:49:INFO:
[92mINFO [0m:      Received: train message c382971c-1d35-4253-b4da-3c2d5da40a39
02/17/2025 16:30:49:INFO:Received: train message c382971c-1d35-4253-b4da-3c2d5da40a39
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:19:INFO:
[92mINFO [0m:      Received: evaluate message 6f876b16-05da-4272-85bd-7906eac3fb40
02/17/2025 16:32:19:INFO:Received: evaluate message 6f876b16-05da-4272-85bd-7906eac3fb40
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:32:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:17:INFO:
[92mINFO [0m:      Received: train message 4f4d9b7b-468a-48c2-8d7b-52d18ac0a309
02/17/2025 16:33:17:INFO:Received: train message 4f4d9b7b-468a-48c2-8d7b-52d18ac0a309
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:33:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:34:35:INFO:
[92mINFO [0m:      Received: evaluate message a2a10b84-a9cd-468e-852d-c3dd0fcb93fe
02/17/2025 16:34:35:INFO:Received: evaluate message a2a10b84-a9cd-468e-852d-c3dd0fcb93fe

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:34:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:11:INFO:
[92mINFO [0m:      Received: train message 20438682-3969-4325-a7d5-0a07ea2a296a
02/17/2025 16:35:11:INFO:Received: train message 20438682-3969-4325-a7d5-0a07ea2a296a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:35:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:43:INFO:
[92mINFO [0m:      Received: evaluate message 51d9910e-4f20-485e-a813-d7069346294c
02/17/2025 16:36:43:INFO:Received: evaluate message 51d9910e-4f20-485e-a813-d7069346294c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:36:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:10:INFO:
[92mINFO [0m:      Received: train message 78d2fa61-337c-42cc-acaf-6d72924a69d2
02/17/2025 16:37:10:INFO:Received: train message 78d2fa61-337c-42cc-acaf-6d72924a69d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:37:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:33:INFO:
[92mINFO [0m:      Received: evaluate message 913f9b24-4a5c-4bca-b2b7-7eb2227e0520
02/17/2025 16:38:33:INFO:Received: evaluate message 913f9b24-4a5c-4bca-b2b7-7eb2227e0520

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:38:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:32:INFO:
[92mINFO [0m:      Received: train message b70a2d42-6718-44b0-8c54-fe86f8417a3e
02/17/2025 16:39:32:INFO:Received: train message b70a2d42-6718-44b0-8c54-fe86f8417a3e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:39:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:39:INFO:
[92mINFO [0m:      Received: evaluate message 99765df0-b229-48a0-9842-a9f2288997f4
02/17/2025 16:40:39:INFO:Received: evaluate message 99765df0-b229-48a0-9842-a9f2288997f4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:40:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:21:INFO:
[92mINFO [0m:      Received: train message 0e393e92-b453-4aaa-8d86-09a327292eff
02/17/2025 16:41:21:INFO:Received: train message 0e393e92-b453-4aaa-8d86-09a327292eff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:41:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:17:INFO:
[92mINFO [0m:      Received: evaluate message 793e0ffb-b073-4aa7-9585-082bfc7c2298
02/17/2025 16:42:17:INFO:Received: evaluate message 793e0ffb-b073-4aa7-9585-082bfc7c2298

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:42:20:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:08:INFO:
[92mINFO [0m:      Received: train message 678b2bdb-5a14-436c-a65f-8e77a3d88e2c
02/17/2025 16:43:08:INFO:Received: train message 678b2bdb-5a14-436c-a65f-8e77a3d88e2c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:43:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:22:INFO:
[92mINFO [0m:      Received: evaluate message c7000136-aade-4b22-854c-f9da5c4dd83a
02/17/2025 16:44:22:INFO:Received: evaluate message c7000136-aade-4b22-854c-f9da5c4dd83a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:44:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:01:INFO:
[92mINFO [0m:      Received: train message 290986c9-7887-4202-b3db-bf8f0895e514
02/17/2025 16:45:01:INFO:Received: train message 290986c9-7887-4202-b3db-bf8f0895e514
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:45:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:06:INFO:
[92mINFO [0m:      Received: evaluate message 9ef7c95d-3f8e-4039-8507-d47f7e23fddb
02/17/2025 16:46:06:INFO:Received: evaluate message 9ef7c95d-3f8e-4039-8507-d47f7e23fddb

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:46:09:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:56:INFO:
[92mINFO [0m:      Received: train message 35a7db24-9860-4770-accb-56e3a9a6115f
02/17/2025 16:46:56:INFO:Received: train message 35a7db24-9860-4770-accb-56e3a9a6115f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:47:19:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:47:58:INFO:
[92mINFO [0m:      Received: evaluate message 47e88b52-a2b7-44f9-ad6e-7a5ebd2e295e
02/17/2025 16:47:58:INFO:Received: evaluate message 47e88b52-a2b7-44f9-ad6e-7a5ebd2e295e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:48:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:50:INFO:
[92mINFO [0m:      Received: train message 424df3d0-d2e7-4acd-a5ac-67ff541ed79a
02/17/2025 16:48:50:INFO:Received: train message 424df3d0-d2e7-4acd-a5ac-67ff541ed79a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:11:INFO:
[92mINFO [0m:      Received: evaluate message d05f4812-f732-4998-86be-0eebbcd72820
02/17/2025 16:50:11:INFO:Received: evaluate message d05f4812-f732-4998-86be-0eebbcd72820

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:50:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:14:INFO:
[92mINFO [0m:      Received: reconnect message 6b4fa0f8-1051-4df9-a7bc-10db2d74200a
02/17/2025 16:50:14:INFO:Received: reconnect message 6b4fa0f8-1051-4df9-a7bc-10db2d74200a
02/17/2025 16:50:14:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:50:14:INFO:Disconnect and shut down

{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}



Final client history:
{'loss': [1.058434859395866, 1.0341333382944282, 1.0536215163656657, 1.0330583057522866, 1.0246491269081062, 1.0398576592821176, 1.0297076206266926, 1.0690933547046055, 1.0283009547381965, 1.055065617186656, 1.0704958346134243, 1.0486507828203937, 1.0879653361367472, 1.1521604853369838, 1.0969555534544728, 1.0911984867281612, 1.1380859228481623, 1.128525823611035, 1.145277946511687, 1.1126798994378992, 1.1003727803852896, 1.07725133417732, 1.0972818723407176, 1.1122499263500962, 1.0596906767402243, 1.0346130378270542, 1.038649117601766, 1.0737049435618522, 1.0383350294581422, 1.0529207979635489], 'accuracy': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'auc': [0.6223554252585197, 0.6407563691851618, 0.6598273756662514, 0.6433403885342202, 0.6413754464174344, 0.6616599588651499, 0.6482974621646056, 0.6559984315678573, 0.6393935896364227, 0.6487574482172322, 0.6495972075606995, 0.6388055217647274, 0.6498496673038935, 0.6640601004836278, 0.6658161243654821, 0.6668287696904851, 0.6777234959696334, 0.6896029000474665, 0.6916106210816164, 0.6924954526286149, 0.6855326835295614, 0.6899093949982411, 0.6969664209453343, 0.7014638703515592, 0.6947881230795094, 0.68671707117551, 0.6867887968217191, 0.6931075292339248, 0.690774118726632, 0.6932271436834092], 'precision': [0.3853113538182961, 0.4289287570845902, 0.4298049537021516, 0.44788407461451657, 0.44692658612817715, 0.44863215295727515, 0.4441287496200894, 0.45438777323061913, 0.4480062133799768, 0.44688844351929824, 0.44760111477337, 0.4451239770717358, 0.4429462037271536, 0.4255301882404757, 0.4339801248778995, 0.4330927062338725, 0.4326365371284536, 0.4361638131675796, 0.4345827761922282, 0.43409108974584765, 0.44191463010150944, 0.4518421603401097, 0.43985796872832816, 0.4350621144991747, 0.45443483000647833, 0.4541400152917303, 0.44882470014489, 0.4344034347515477, 0.4393216925802126, 0.42869832232925603], 'recall': [0.49960906958561374, 0.5222830336200156, 0.5199374511336982, 0.544175136825645, 0.5418295543393276, 0.54573885848319, 0.5379202501954652, 0.5488663017982799, 0.5347928068803753, 0.5410476935105551, 0.5433932759968726, 0.5332290852228303, 0.5387021110242377, 0.5230648944487881, 0.5285379202501954, 0.527756059421423, 0.5285379202501954, 0.5324472243940579, 0.5324472243940579, 0.5308835027365129, 0.5387021110242377, 0.54573885848319, 0.5355746677091477, 0.5316653635652854, 0.5449569976544175, 0.5347928068803753, 0.5324472243940579, 0.5293197810789679, 0.5293197810789679, 0.5215011727912432], 'f1': [0.34049198829300026, 0.4252045406151577, 0.4132347706604684, 0.47310021196628393, 0.48051579862817606, 0.4752123047702001, 0.480383451894813, 0.4768256809963051, 0.48629900755377325, 0.4823042916747311, 0.48270209495516475, 0.4833102707865385, 0.4770130621152753, 0.4446068287682307, 0.46908664376762427, 0.46774494993573695, 0.4524352039195876, 0.46273596820810897, 0.458415917992503, 0.4664419124700152, 0.47492832145950237, 0.48969946760976707, 0.4756916363664831, 0.4686525999620888, 0.4934080615290738, 0.4911651398343582, 0.48671650701461916, 0.46935548008670824, 0.47747464714261734, 0.46548231714706734]}

