nohup: ignoring input
02/14/2025 23:58:38:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:58:38:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:58:38:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739606318.544401 1515749 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:58:55:INFO:
[92mINFO [0m:      Received: train message 2f853469-88b6-449c-84e6-53507198fbe0
02/14/2025 23:58:55:INFO:Received: train message 2f853469-88b6-449c-84e6-53507198fbe0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/14/2025 23:59:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:48:INFO:
[92mINFO [0m:      Received: evaluate message d5332c29-2017-480f-8173-9923205e768c
02/15/2025 00:00:48:INFO:Received: evaluate message d5332c29-2017-480f-8173-9923205e768c
[92mINFO [0m:      Sent reply
02/15/2025 00:00:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:01:27:INFO:
[92mINFO [0m:      Received: train message d8f71b39-2932-4e15-a310-5922f7ce10a8
02/15/2025 00:01:27:INFO:Received: train message d8f71b39-2932-4e15-a310-5922f7ce10a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:02:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:02:55:INFO:
[92mINFO [0m:      Received: evaluate message ee62844c-f9da-4aee-8960-80fe98ffdae8
02/15/2025 00:02:55:INFO:Received: evaluate message ee62844c-f9da-4aee-8960-80fe98ffdae8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:03:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:47:INFO:
[92mINFO [0m:      Received: train message 4879e5bf-18cd-4c48-bcc7-e19747aac5e4
02/15/2025 00:03:47:INFO:Received: train message 4879e5bf-18cd-4c48-bcc7-e19747aac5e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:04:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:05:23:INFO:
[92mINFO [0m:      Received: evaluate message 393eebda-7bae-4fad-8bf7-efcca30fc6bf
02/15/2025 00:05:23:INFO:Received: evaluate message 393eebda-7bae-4fad-8bf7-efcca30fc6bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:05:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:02:INFO:
[92mINFO [0m:      Received: train message 83ee300b-b5d9-49d3-8583-a2715a33f700
02/15/2025 00:06:02:INFO:Received: train message 83ee300b-b5d9-49d3-8583-a2715a33f700
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:07:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:07:49:INFO:
[92mINFO [0m:      Received: evaluate message 612202fa-a387-4955-9e43-732b122d42a7
02/15/2025 00:07:49:INFO:Received: evaluate message 612202fa-a387-4955-9e43-732b122d42a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:07:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:20:INFO:
[92mINFO [0m:      Received: train message 6a5e1847-c834-4a31-8677-945b59d251ea
02/15/2025 00:08:20:INFO:Received: train message 6a5e1847-c834-4a31-8677-945b59d251ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:04:INFO:
[92mINFO [0m:      Received: evaluate message b8fa9306-3834-436c-99a3-bf16807a1a8d
02/15/2025 00:10:04:INFO:Received: evaluate message b8fa9306-3834-436c-99a3-bf16807a1a8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:10:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:49:INFO:
[92mINFO [0m:      Received: train message 9790fa44-0241-408e-bb33-1cc3021c0f50
02/15/2025 00:10:49:INFO:Received: train message 9790fa44-0241-408e-bb33-1cc3021c0f50
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:11:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:28:INFO:
[92mINFO [0m:      Received: evaluate message 29ef18a3-87a9-494c-956c-e66a70ae71df
02/15/2025 00:12:28:INFO:Received: evaluate message 29ef18a3-87a9-494c-956c-e66a70ae71df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:12:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:12:INFO:
[92mINFO [0m:      Received: train message 74380de9-82c5-493d-be1d-499f821a7ceb
02/15/2025 00:13:12:INFO:Received: train message 74380de9-82c5-493d-be1d-499f821a7ceb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:14:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:52:INFO:
[92mINFO [0m:      Received: evaluate message 80d7d4b1-258d-45e7-9124-9cf79acd8c20
02/15/2025 00:14:52:INFO:Received: evaluate message 80d7d4b1-258d-45e7-9124-9cf79acd8c20
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904], 'accuracy': [0.5105551211884285], 'auc': [0.5353700758405713], 'precision': [0.4273732942807158], 'recall': [0.5105551211884285], 'f1': [0.46264894441049187]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877], 'accuracy': [0.5105551211884285, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766], 'precision': [0.4273732942807158, 0.41654227475272265], 'recall': [0.5105551211884285, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:14:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:34:INFO:
[92mINFO [0m:      Received: train message 06b28904-b42d-44c4-b352-f868a6b54596
02/15/2025 00:15:34:INFO:Received: train message 06b28904-b42d-44c4-b352-f868a6b54596
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:14:INFO:
[92mINFO [0m:      Received: evaluate message ee43cc1b-cbff-4966-85cd-4d55d01b373f
02/15/2025 00:17:14:INFO:Received: evaluate message ee43cc1b-cbff-4966-85cd-4d55d01b373f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:17:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:50:INFO:
[92mINFO [0m:      Received: train message 797a0a2c-530d-419e-abe1-3a0edce8ba2e
02/15/2025 00:17:50:INFO:Received: train message 797a0a2c-530d-419e-abe1-3a0edce8ba2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:32:INFO:
[92mINFO [0m:      Received: evaluate message 2be52e55-612a-41ee-9e8a-47f0998f6478
02/15/2025 00:19:32:INFO:Received: evaluate message 2be52e55-612a-41ee-9e8a-47f0998f6478
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:20:14:INFO:
[92mINFO [0m:      Received: train message 11dc65eb-82fa-4f0c-b449-c1d9345d2c4b
02/15/2025 00:20:14:INFO:Received: train message 11dc65eb-82fa-4f0c-b449-c1d9345d2c4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:21:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:53:INFO:
[92mINFO [0m:      Received: evaluate message b19fda69-f8d4-4eb2-8053-70e77cff9c19
02/15/2025 00:21:53:INFO:Received: evaluate message b19fda69-f8d4-4eb2-8053-70e77cff9c19
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:37:INFO:
[92mINFO [0m:      Received: train message fdb1515d-a12f-4c70-aacd-c805e16dbbf5
02/15/2025 00:22:37:INFO:Received: train message fdb1515d-a12f-4c70-aacd-c805e16dbbf5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:23:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:07:INFO:
[92mINFO [0m:      Received: evaluate message 3874dbbd-03f7-40a5-b031-f2e25cb15548
02/15/2025 00:24:07:INFO:Received: evaluate message 3874dbbd-03f7-40a5-b031-f2e25cb15548
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:24:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:53:INFO:
[92mINFO [0m:      Received: train message 220faba8-e616-4d74-b97a-83c72edcf769
02/15/2025 00:24:53:INFO:Received: train message 220faba8-e616-4d74-b97a-83c72edcf769

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:25:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:24:INFO:
[92mINFO [0m:      Received: evaluate message 3fd9e60d-8538-4970-80dc-988471f3297d
02/15/2025 00:26:24:INFO:Received: evaluate message 3fd9e60d-8538-4970-80dc-988471f3297d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:26:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:15:INFO:
[92mINFO [0m:      Received: train message 8f224852-b815-4a1b-affb-db9fc26797c0
02/15/2025 00:27:15:INFO:Received: train message 8f224852-b815-4a1b-affb-db9fc26797c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:28:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:28:56:INFO:
[92mINFO [0m:      Received: evaluate message 8af72b55-773e-4095-9405-be57e546dd1d
02/15/2025 00:28:56:INFO:Received: evaluate message 8af72b55-773e-4095-9405-be57e546dd1d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:43:INFO:
[92mINFO [0m:      Received: train message 3150b274-7839-4d7f-91ad-7d8f0b5eba0a
02/15/2025 00:29:43:INFO:Received: train message 3150b274-7839-4d7f-91ad-7d8f0b5eba0a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:03:INFO:
[92mINFO [0m:      Received: evaluate message 202ebf64-e5bd-4449-b3b9-244f0cf86bde
02/15/2025 00:31:03:INFO:Received: evaluate message 202ebf64-e5bd-4449-b3b9-244f0cf86bde
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:46:INFO:
[92mINFO [0m:      Received: train message ce8b760c-437a-4dcf-9ff1-dcc3c62813ad
02/15/2025 00:31:46:INFO:Received: train message ce8b760c-437a-4dcf-9ff1-dcc3c62813ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:18:INFO:
[92mINFO [0m:      Received: evaluate message d03f19c5-077d-42be-8bc9-be17a8a652fa
02/15/2025 00:33:18:INFO:Received: evaluate message d03f19c5-077d-42be-8bc9-be17a8a652fa
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:34:08:INFO:
[92mINFO [0m:      Received: train message dee93be5-e990-476a-9afd-b71b7e598db4
02/15/2025 00:34:08:INFO:Received: train message dee93be5-e990-476a-9afd-b71b7e598db4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:35:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:46:INFO:
[92mINFO [0m:      Received: evaluate message 7a053f53-e100-496f-91b7-2001e1f9528d
02/15/2025 00:35:46:INFO:Received: evaluate message 7a053f53-e100-496f-91b7-2001e1f9528d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:35:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:36:13:INFO:
[92mINFO [0m:      Received: train message 12671667-59e9-4e46-bab2-51a23ddd1cae
02/15/2025 00:36:13:INFO:Received: train message 12671667-59e9-4e46-bab2-51a23ddd1cae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:37:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:56:INFO:
[92mINFO [0m:      Received: evaluate message d184eb5e-d269-4ec2-803b-01a75f85de46
02/15/2025 00:37:56:INFO:Received: evaluate message d184eb5e-d269-4ec2-803b-01a75f85de46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:38:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:38:13:INFO:
[92mINFO [0m:      Received: train message c6a6daad-b007-4f57-a410-99d4a7ae5164
02/15/2025 00:38:13:INFO:Received: train message c6a6daad-b007-4f57-a410-99d4a7ae5164
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:39:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:01:INFO:
[92mINFO [0m:      Received: evaluate message 28b739b2-c2f0-4006-a614-f66931208e9b
02/15/2025 00:40:01:INFO:Received: evaluate message 28b739b2-c2f0-4006-a614-f66931208e9b

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:30:INFO:
[92mINFO [0m:      Received: train message d9f0200a-baed-4618-935e-f40a21838b07
02/15/2025 00:40:30:INFO:Received: train message d9f0200a-baed-4618-935e-f40a21838b07
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:11:INFO:
[92mINFO [0m:      Received: evaluate message 5f30b185-55b2-43a3-a1f0-aa577f55693a
02/15/2025 00:42:11:INFO:Received: evaluate message 5f30b185-55b2-43a3-a1f0-aa577f55693a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:42:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:33:INFO:
[92mINFO [0m:      Received: train message 6565eb8f-eb0f-45f9-8501-bd2199c63846
02/15/2025 00:42:33:INFO:Received: train message 6565eb8f-eb0f-45f9-8501-bd2199c63846
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:16:INFO:
[92mINFO [0m:      Received: evaluate message 438d94d7-afdf-40da-bec3-15e57abe7280
02/15/2025 00:44:16:INFO:Received: evaluate message 438d94d7-afdf-40da-bec3-15e57abe7280

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:53:INFO:
[92mINFO [0m:      Received: train message 2e31bd96-4fc6-403f-ae54-56e85924a614
02/15/2025 00:44:53:INFO:Received: train message 2e31bd96-4fc6-403f-ae54-56e85924a614
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:33:INFO:
[92mINFO [0m:      Received: evaluate message d65d6f82-3285-40fd-921b-95970aead02f
02/15/2025 00:46:33:INFO:Received: evaluate message d65d6f82-3285-40fd-921b-95970aead02f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:47:06:INFO:
[92mINFO [0m:      Received: train message 40287409-7b66-4740-92c9-c460b1a8110b
02/15/2025 00:47:06:INFO:Received: train message 40287409-7b66-4740-92c9-c460b1a8110b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:48:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:42:INFO:
[92mINFO [0m:      Received: evaluate message 2c843701-4068-4b3b-a4b8-63cb3b593b82
02/15/2025 00:48:42:INFO:Received: evaluate message 2c843701-4068-4b3b-a4b8-63cb3b593b82

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:28:INFO:
[92mINFO [0m:      Received: train message 2b61ad89-615c-4015-9cc9-aa087382a697
02/15/2025 00:49:28:INFO:Received: train message 2b61ad89-615c-4015-9cc9-aa087382a697
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:50:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:50:56:INFO:
[92mINFO [0m:      Received: evaluate message 16bf9a43-1adf-485b-ab57-297019510ea0
02/15/2025 00:50:56:INFO:Received: evaluate message 16bf9a43-1adf-485b-ab57-297019510ea0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:21:INFO:
[92mINFO [0m:      Received: train message 4dd86022-0540-4259-9975-4be8a06e9af7
02/15/2025 00:51:21:INFO:Received: train message 4dd86022-0540-4259-9975-4be8a06e9af7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:52:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:11:INFO:
[92mINFO [0m:      Received: evaluate message 6c708a09-0b87-4cbc-ab8c-91471dc69335
02/15/2025 00:53:11:INFO:Received: evaluate message 6c708a09-0b87-4cbc-ab8c-91471dc69335

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:47:INFO:
[92mINFO [0m:      Received: train message 1e99e327-d9ed-4247-97bd-0c91fcd45b7a
02/15/2025 00:53:47:INFO:Received: train message 1e99e327-d9ed-4247-97bd-0c91fcd45b7a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:54:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:23:INFO:
[92mINFO [0m:      Received: evaluate message a907a36b-e455-4842-89e0-81f1fe619ff4
02/15/2025 00:55:23:INFO:Received: evaluate message a907a36b-e455-4842-89e0-81f1fe619ff4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:56:09:INFO:
[92mINFO [0m:      Received: train message 793190c8-1311-43a3-89a9-7588173d5d3b
02/15/2025 00:56:09:INFO:Received: train message 793190c8-1311-43a3-89a9-7588173d5d3b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:57:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:42:INFO:
[92mINFO [0m:      Received: evaluate message 9b7b5cb6-0e1d-4612-b957-3f9a530b8989
02/15/2025 00:57:42:INFO:Received: evaluate message 9b7b5cb6-0e1d-4612-b957-3f9a530b8989

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:57:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:25:INFO:
[92mINFO [0m:      Received: train message df6a4907-7d2e-4d68-ba2b-c29ab82d8ad8
02/15/2025 00:58:25:INFO:Received: train message df6a4907-7d2e-4d68-ba2b-c29ab82d8ad8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:59:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:04:INFO:
[92mINFO [0m:      Received: evaluate message 705e1c27-f405-40c0-a914-ddde936623cc
02/15/2025 01:00:04:INFO:Received: evaluate message 705e1c27-f405-40c0-a914-ddde936623cc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:00:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:35:INFO:
[92mINFO [0m:      Received: train message e604c231-068a-4dfb-96c3-50798a043c38
02/15/2025 01:00:35:INFO:Received: train message e604c231-068a-4dfb-96c3-50798a043c38
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:07:INFO:
[92mINFO [0m:      Received: evaluate message c8de4ed8-a2ec-46b4-8845-fc350f74774f
02/15/2025 01:02:07:INFO:Received: evaluate message c8de4ed8-a2ec-46b4-8845-fc350f74774f

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:53:INFO:
[92mINFO [0m:      Received: train message 6548df84-e8c9-43bb-a85f-87bb16f7af70
02/15/2025 01:02:53:INFO:Received: train message 6548df84-e8c9-43bb-a85f-87bb16f7af70
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:28:INFO:
[92mINFO [0m:      Received: evaluate message ff6e6a5e-0467-4f41-a54c-c78463969b98
02/15/2025 01:04:28:INFO:Received: evaluate message ff6e6a5e-0467-4f41-a54c-c78463969b98
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:05:15:INFO:
[92mINFO [0m:      Received: train message eaae1ea9-525e-43ed-beda-0a25fd526ebf
02/15/2025 01:05:15:INFO:Received: train message eaae1ea9-525e-43ed-beda-0a25fd526ebf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:06:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:53:INFO:
[92mINFO [0m:      Received: evaluate message 66bf06ee-31ff-4426-b52a-b6f07375b0af
02/15/2025 01:06:53:INFO:Received: evaluate message 66bf06ee-31ff-4426-b52a-b6f07375b0af

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:07:00:INFO:
[92mINFO [0m:      Received: reconnect message a943bb6b-5814-4dd4-8748-8bb4b3c4aa96
02/15/2025 01:07:00:INFO:Received: reconnect message a943bb6b-5814-4dd4-8748-8bb4b3c4aa96
02/15/2025 01:07:00:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:07:00:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456, 1.0000020092217785], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172, 0.7112468503315255], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236, 0.41428740286963217], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704, 0.39244624060601346]}



Final client history:
{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456, 1.0000020092217785], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172, 0.7112468503315255], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236, 0.41428740286963217], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704, 0.39244624060601346]}

