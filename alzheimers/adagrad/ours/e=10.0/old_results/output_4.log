nohup: ignoring input
02/14/2025 23:58:36:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:58:36:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:58:36:DEBUG:ChannelConnectivity.CONNECTING
02/14/2025 23:58:36:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739606317.015308 1515684 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:59:12:INFO:
[92mINFO [0m:      Received: train message 0171a437-a53f-42e3-bb0e-beb228240191
02/14/2025 23:59:12:INFO:Received: train message 0171a437-a53f-42e3-bb0e-beb228240191
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:00:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:39:INFO:
[92mINFO [0m:      Received: evaluate message 264d0390-d0ac-46b8-9834-dd7d0cef35ba
02/15/2025 00:00:39:INFO:Received: evaluate message 264d0390-d0ac-46b8-9834-dd7d0cef35ba
[92mINFO [0m:      Sent reply
02/15/2025 00:00:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:01:31:INFO:
[92mINFO [0m:      Received: train message a89f4d27-efdc-4d40-827a-e2a8c9cd5d81
02/15/2025 00:01:31:INFO:Received: train message a89f4d27-efdc-4d40-827a-e2a8c9cd5d81
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:02:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:13:INFO:
[92mINFO [0m:      Received: evaluate message f217a386-0ff3-45a6-a892-e71a41a230fd
02/15/2025 00:03:13:INFO:Received: evaluate message f217a386-0ff3-45a6-a892-e71a41a230fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:03:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:45:INFO:
[92mINFO [0m:      Received: train message caab62d9-a83a-43a8-bc11-354bc9263ad8
02/15/2025 00:03:45:INFO:Received: train message caab62d9-a83a-43a8-bc11-354bc9263ad8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:04:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:05:32:INFO:
[92mINFO [0m:      Received: evaluate message cc665f26-1264-461b-a55f-e5fb5005032a
02/15/2025 00:05:32:INFO:Received: evaluate message cc665f26-1264-461b-a55f-e5fb5005032a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:05:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:13:INFO:
[92mINFO [0m:      Received: train message 7f57182f-b07f-4c5b-aaed-d0547463efea
02/15/2025 00:06:13:INFO:Received: train message 7f57182f-b07f-4c5b-aaed-d0547463efea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:07:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:07:29:INFO:
[92mINFO [0m:      Received: evaluate message 267dc23d-107a-4f3f-8fbc-f38ae4bdf440
02/15/2025 00:07:29:INFO:Received: evaluate message 267dc23d-107a-4f3f-8fbc-f38ae4bdf440
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:07:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:26:INFO:
[92mINFO [0m:      Received: train message cd45a66c-c557-46be-8e1c-9e08ef87d009
02/15/2025 00:08:26:INFO:Received: train message cd45a66c-c557-46be-8e1c-9e08ef87d009
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:00:INFO:
[92mINFO [0m:      Received: evaluate message 78254da9-1983-4078-b471-26719793b4f9
02/15/2025 00:10:00:INFO:Received: evaluate message 78254da9-1983-4078-b471-26719793b4f9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:10:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:41:INFO:
[92mINFO [0m:      Received: train message 2c9019ce-d216-4661-b744-adbea348cc46
02/15/2025 00:10:41:INFO:Received: train message 2c9019ce-d216-4661-b744-adbea348cc46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:11:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:30:INFO:
[92mINFO [0m:      Received: evaluate message 1fe499bd-c4ab-43d6-b322-19a135d208d0
02/15/2025 00:12:30:INFO:Received: evaluate message 1fe499bd-c4ab-43d6-b322-19a135d208d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:12:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:03:INFO:
[92mINFO [0m:      Received: train message 8d53691d-d089-4950-a7f6-4861afbf02dc
02/15/2025 00:13:03:INFO:Received: train message 8d53691d-d089-4950-a7f6-4861afbf02dc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:13:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:52:INFO:
[92mINFO [0m:      Received: evaluate message a985c2a8-2757-4ec3-a447-48962e67f5ee
02/15/2025 00:14:52:INFO:Received: evaluate message a985c2a8-2757-4ec3-a447-48962e67f5ee
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904], 'accuracy': [0.5105551211884285], 'auc': [0.5353700758405713], 'precision': [0.4273732942807158], 'recall': [0.5105551211884285], 'f1': [0.46264894441049187]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877], 'accuracy': [0.5105551211884285, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766], 'precision': [0.4273732942807158, 0.41654227475272265], 'recall': [0.5105551211884285, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:14:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:25:INFO:
[92mINFO [0m:      Received: train message f6c91e9f-279f-4307-8f60-d2b61474f78b
02/15/2025 00:15:25:INFO:Received: train message f6c91e9f-279f-4307-8f60-d2b61474f78b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:16:52:INFO:
[92mINFO [0m:      Received: evaluate message 137d2790-6cd2-4249-8db1-6d4eac9f0a36
02/15/2025 00:16:52:INFO:Received: evaluate message 137d2790-6cd2-4249-8db1-6d4eac9f0a36
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:16:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:53:INFO:
[92mINFO [0m:      Received: train message 4a77b58f-aac5-4841-a2e6-bc1117c40b67
02/15/2025 00:17:53:INFO:Received: train message 4a77b58f-aac5-4841-a2e6-bc1117c40b67
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:30:INFO:
[92mINFO [0m:      Received: evaluate message 5b602a40-1966-4a61-a18b-e5dd493aa4ab
02/15/2025 00:19:30:INFO:Received: evaluate message 5b602a40-1966-4a61-a18b-e5dd493aa4ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:20:02:INFO:
[92mINFO [0m:      Received: train message 2e8a7a91-e396-47f7-916a-e0a1fc3f0672
02/15/2025 00:20:02:INFO:Received: train message 2e8a7a91-e396-47f7-916a-e0a1fc3f0672
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:20:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:53:INFO:
[92mINFO [0m:      Received: evaluate message 25c033fc-7449-42b4-af0c-ce53bed76993
02/15/2025 00:21:53:INFO:Received: evaluate message 25c033fc-7449-42b4-af0c-ce53bed76993
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:26:INFO:
[92mINFO [0m:      Received: train message 531972d6-0b59-4339-8a0f-9cad27dc308a
02/15/2025 00:22:26:INFO:Received: train message 531972d6-0b59-4339-8a0f-9cad27dc308a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:23:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:04:INFO:
[92mINFO [0m:      Received: evaluate message 3a6c5eb2-c312-4b3c-9baa-03a2a52a2e8b
02/15/2025 00:24:04:INFO:Received: evaluate message 3a6c5eb2-c312-4b3c-9baa-03a2a52a2e8b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:24:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:36:INFO:
[92mINFO [0m:      Received: train message 5579cccb-01db-441b-a685-b7d7bf2360e7
02/15/2025 00:24:36:INFO:Received: train message 5579cccb-01db-441b-a685-b7d7bf2360e7

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:25:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:30:INFO:
[92mINFO [0m:      Received: evaluate message d1d9e4b2-053e-404e-8d82-eb25d451941d
02/15/2025 00:26:30:INFO:Received: evaluate message d1d9e4b2-053e-404e-8d82-eb25d451941d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:26:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:57:INFO:
[92mINFO [0m:      Received: train message fa79f622-9144-4a35-8975-b88da84d6eea
02/15/2025 00:26:57:INFO:Received: train message fa79f622-9144-4a35-8975-b88da84d6eea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:27:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:00:INFO:
[92mINFO [0m:      Received: evaluate message 60f2db46-928b-4dec-a582-012477bc85fb
02/15/2025 00:29:00:INFO:Received: evaluate message 60f2db46-928b-4dec-a582-012477bc85fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:35:INFO:
[92mINFO [0m:      Received: train message 7b796b83-de6c-494a-9321-9363feaef92d
02/15/2025 00:29:35:INFO:Received: train message 7b796b83-de6c-494a-9321-9363feaef92d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:20:INFO:
[92mINFO [0m:      Received: evaluate message ecbed99b-529c-4d54-95b4-b6a3eddeac24
02/15/2025 00:31:20:INFO:Received: evaluate message ecbed99b-529c-4d54-95b4-b6a3eddeac24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:56:INFO:
[92mINFO [0m:      Received: train message ae8c7c6d-7205-4347-81da-37b40565c1a7
02/15/2025 00:31:56:INFO:Received: train message ae8c7c6d-7205-4347-81da-37b40565c1a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:26:INFO:
[92mINFO [0m:      Received: evaluate message 3ff7e2f2-38e8-4bab-beb4-32b577f953d3
02/15/2025 00:33:26:INFO:Received: evaluate message 3ff7e2f2-38e8-4bab-beb4-32b577f953d3
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:34:08:INFO:
[92mINFO [0m:      Received: train message 87b76898-b5ca-48ae-9c8f-4c5a4272cf88
02/15/2025 00:34:08:INFO:Received: train message 87b76898-b5ca-48ae-9c8f-4c5a4272cf88
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:35:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:43:INFO:
[92mINFO [0m:      Received: evaluate message e516867f-8623-40ca-b3e2-9b1f7cfd4e92
02/15/2025 00:35:43:INFO:Received: evaluate message e516867f-8623-40ca-b3e2-9b1f7cfd4e92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:35:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:36:25:INFO:
[92mINFO [0m:      Received: train message bca5cf91-64dd-4c1b-b7b7-3671da3fea98
02/15/2025 00:36:25:INFO:Received: train message bca5cf91-64dd-4c1b-b7b7-3671da3fea98
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:37:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:56:INFO:
[92mINFO [0m:      Received: evaluate message 4081ac11-1454-4f3e-a881-3b41b9b2e5e5
02/15/2025 00:37:56:INFO:Received: evaluate message 4081ac11-1454-4f3e-a881-3b41b9b2e5e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:38:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:38:31:INFO:
[92mINFO [0m:      Received: train message a28aec85-2e81-4c20-b4fd-39e9a459a775
02/15/2025 00:38:31:INFO:Received: train message a28aec85-2e81-4c20-b4fd-39e9a459a775
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:39:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:00:INFO:
[92mINFO [0m:      Received: evaluate message 237788bc-c298-4abb-8af2-9ab91120500c
02/15/2025 00:40:00:INFO:Received: evaluate message 237788bc-c298-4abb-8af2-9ab91120500c

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:38:INFO:
[92mINFO [0m:      Received: train message a1ae3ae9-1961-4972-a3a6-4e5921b797ae
02/15/2025 00:40:38:INFO:Received: train message a1ae3ae9-1961-4972-a3a6-4e5921b797ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:01:INFO:
[92mINFO [0m:      Received: evaluate message d28051b7-5c94-4307-bd10-b8be43c6a9eb
02/15/2025 00:42:01:INFO:Received: evaluate message d28051b7-5c94-4307-bd10-b8be43c6a9eb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:42:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:47:INFO:
[92mINFO [0m:      Received: train message 82e20b6d-6c75-4e89-9a19-b973cf6ddd54
02/15/2025 00:42:47:INFO:Received: train message 82e20b6d-6c75-4e89-9a19-b973cf6ddd54
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:15:INFO:
[92mINFO [0m:      Received: evaluate message 98b578b3-f8f6-474b-aa9f-0617c59cfd2c
02/15/2025 00:44:15:INFO:Received: evaluate message 98b578b3-f8f6-474b-aa9f-0617c59cfd2c

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:31:INFO:
[92mINFO [0m:      Received: train message 25e3e1fb-735f-4941-9744-684ec7260393
02/15/2025 00:44:31:INFO:Received: train message 25e3e1fb-735f-4941-9744-684ec7260393
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:33:INFO:
[92mINFO [0m:      Received: evaluate message b5e69ead-62f7-4171-81c2-614767bb314f
02/15/2025 00:46:33:INFO:Received: evaluate message b5e69ead-62f7-4171-81c2-614767bb314f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:47:00:INFO:
[92mINFO [0m:      Received: train message dcf2a83b-671f-4b0b-b82b-d57bbb6b2a55
02/15/2025 00:47:00:INFO:Received: train message dcf2a83b-671f-4b0b-b82b-d57bbb6b2a55
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:47:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:44:INFO:
[92mINFO [0m:      Received: evaluate message c9f8f3e6-a1c6-4f7d-bb2b-496bdeaf1b68
02/15/2025 00:48:44:INFO:Received: evaluate message c9f8f3e6-a1c6-4f7d-bb2b-496bdeaf1b68

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:19:INFO:
[92mINFO [0m:      Received: train message 5253ab18-2c2a-44f0-982e-a2839fa8239c
02/15/2025 00:49:19:INFO:Received: train message 5253ab18-2c2a-44f0-982e-a2839fa8239c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:50:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:07:INFO:
[92mINFO [0m:      Received: evaluate message 36d154d8-c500-4900-ae62-23790adca76d
02/15/2025 00:51:07:INFO:Received: evaluate message 36d154d8-c500-4900-ae62-23790adca76d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:41:INFO:
[92mINFO [0m:      Received: train message 75bb87d2-fa71-4587-aad0-59095872f045
02/15/2025 00:51:41:INFO:Received: train message 75bb87d2-fa71-4587-aad0-59095872f045
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:52:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:03:INFO:
[92mINFO [0m:      Received: evaluate message 74f133f9-cc9b-4922-9d19-9d603bba1444
02/15/2025 00:53:03:INFO:Received: evaluate message 74f133f9-cc9b-4922-9d19-9d603bba1444

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:48:INFO:
[92mINFO [0m:      Received: train message ccf03c58-481a-437f-8ad0-d48b9ff16a43
02/15/2025 00:53:48:INFO:Received: train message ccf03c58-481a-437f-8ad0-d48b9ff16a43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:54:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:29:INFO:
[92mINFO [0m:      Received: evaluate message ac7c2ce7-f504-45ad-b745-254e24da68cb
02/15/2025 00:55:29:INFO:Received: evaluate message ac7c2ce7-f504-45ad-b745-254e24da68cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:57:INFO:
[92mINFO [0m:      Received: train message c2b31bed-f79f-47fa-bc0c-fb9a37f0bb0f
02/15/2025 00:55:57:INFO:Received: train message c2b31bed-f79f-47fa-bc0c-fb9a37f0bb0f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:56:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:53:INFO:
[92mINFO [0m:      Received: evaluate message 684d882f-5c2c-4526-87e8-500cc404236b
02/15/2025 00:57:53:INFO:Received: evaluate message 684d882f-5c2c-4526-87e8-500cc404236b

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:57:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:10:INFO:
[92mINFO [0m:      Received: train message 463715d6-2e18-4282-989d-8621ca1e7030
02/15/2025 00:58:10:INFO:Received: train message 463715d6-2e18-4282-989d-8621ca1e7030
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:59:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:59:58:INFO:
[92mINFO [0m:      Received: evaluate message dd2beecb-d211-43df-9939-6ce0ae6c0b5e
02/15/2025 00:59:58:INFO:Received: evaluate message dd2beecb-d211-43df-9939-6ce0ae6c0b5e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:00:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:38:INFO:
[92mINFO [0m:      Received: train message 2efc54a7-eb36-4556-a2d4-205c615e8ba3
02/15/2025 01:00:38:INFO:Received: train message 2efc54a7-eb36-4556-a2d4-205c615e8ba3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:19:INFO:
[92mINFO [0m:      Received: evaluate message 10c294e0-129d-4864-9b22-a7f2d989a09c
02/15/2025 01:02:19:INFO:Received: evaluate message 10c294e0-129d-4864-9b22-a7f2d989a09c

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:03:02:INFO:
[92mINFO [0m:      Received: train message df1eee34-e907-43e6-97a1-c9245a62244d
02/15/2025 01:03:02:INFO:Received: train message df1eee34-e907-43e6-97a1-c9245a62244d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:30:INFO:
[92mINFO [0m:      Received: evaluate message 3e0357e3-908c-46dc-8327-22a9a70ee602
02/15/2025 01:04:30:INFO:Received: evaluate message 3e0357e3-908c-46dc-8327-22a9a70ee602
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:05:02:INFO:
[92mINFO [0m:      Received: train message df487114-6ba8-423f-af18-360ce7584bc9
02/15/2025 01:05:02:INFO:Received: train message df487114-6ba8-423f-af18-360ce7584bc9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:05:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:53:INFO:
[92mINFO [0m:      Received: evaluate message 58f3342c-b2f1-449a-953a-48fcee581485
02/15/2025 01:06:53:INFO:Received: evaluate message 58f3342c-b2f1-449a-953a-48fcee581485

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:07:00:INFO:
[92mINFO [0m:      Received: reconnect message c03e6a52-9879-4016-af61-af83c4e8939a
02/15/2025 01:07:00:INFO:Received: reconnect message c03e6a52-9879-4016-af61-af83c4e8939a
02/15/2025 01:07:00:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:07:00:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456, 1.0000020092217785], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172, 0.7112468503315255], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236, 0.41428740286963217], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704, 0.39244624060601346]}



Final client history:
{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456, 1.0000020092217785], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172, 0.7112468503315255], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236, 0.41428740286963217], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704, 0.39244624060601346]}

