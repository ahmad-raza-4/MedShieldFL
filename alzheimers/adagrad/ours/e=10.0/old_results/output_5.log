nohup: ignoring input
02/14/2025 23:58:32:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/14/2025 23:58:33:DEBUG:ChannelConnectivity.IDLE
02/14/2025 23:58:33:DEBUG:ChannelConnectivity.CONNECTING
02/14/2025 23:58:33:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739606313.041139 1515592 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/14/2025 23:59:16:INFO:
[92mINFO [0m:      Received: train message 06e19fc9-df45-445c-a40f-c91ac4fe7209
02/14/2025 23:59:16:INFO:Received: train message 06e19fc9-df45-445c-a40f-c91ac4fe7209
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:00:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:00:29:INFO:
[92mINFO [0m:      Received: evaluate message 13625bd9-0759-4db2-82f6-93945c3132f4
02/15/2025 00:00:29:INFO:Received: evaluate message 13625bd9-0759-4db2-82f6-93945c3132f4
[92mINFO [0m:      Sent reply
02/15/2025 00:00:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:01:31:INFO:
[92mINFO [0m:      Received: train message c8fa3ede-ccc6-430d-8361-f6bd90a71b0e
02/15/2025 00:01:31:INFO:Received: train message c8fa3ede-ccc6-430d-8361-f6bd90a71b0e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:02:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:09:INFO:
[92mINFO [0m:      Received: evaluate message 75a354f8-cbd4-441e-a743-355a82d2d19c
02/15/2025 00:03:09:INFO:Received: evaluate message 75a354f8-cbd4-441e-a743-355a82d2d19c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:03:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:03:52:INFO:
[92mINFO [0m:      Received: train message 68a14559-30dc-4c57-8021-ba149ab1a13d
02/15/2025 00:03:52:INFO:Received: train message 68a14559-30dc-4c57-8021-ba149ab1a13d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:04:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:05:32:INFO:
[92mINFO [0m:      Received: evaluate message 54ce4a5e-6a07-4b7d-b198-c20839e0b4ee
02/15/2025 00:05:32:INFO:Received: evaluate message 54ce4a5e-6a07-4b7d-b198-c20839e0b4ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:05:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:06:13:INFO:
[92mINFO [0m:      Received: train message cca2f555-8693-4829-bb46-3a9ebda537be
02/15/2025 00:06:13:INFO:Received: train message cca2f555-8693-4829-bb46-3a9ebda537be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:07:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:07:46:INFO:
[92mINFO [0m:      Received: evaluate message ccad1bfd-7f40-412d-8794-ad887ba80e70
02/15/2025 00:07:46:INFO:Received: evaluate message ccad1bfd-7f40-412d-8794-ad887ba80e70
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:07:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:08:31:INFO:
[92mINFO [0m:      Received: train message b3e7a0cb-743d-4d6e-859f-c506f622e439
02/15/2025 00:08:31:INFO:Received: train message b3e7a0cb-743d-4d6e-859f-c506f622e439
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:09:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:09:53:INFO:
[92mINFO [0m:      Received: evaluate message ddc909a8-b4ac-426b-8ead-ea9960226c00
02/15/2025 00:09:53:INFO:Received: evaluate message ddc909a8-b4ac-426b-8ead-ea9960226c00
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:09:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:10:41:INFO:
[92mINFO [0m:      Received: train message dd8fca18-1052-4db2-bc50-8c03fed3a6df
02/15/2025 00:10:41:INFO:Received: train message dd8fca18-1052-4db2-bc50-8c03fed3a6df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:11:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:12:23:INFO:
[92mINFO [0m:      Received: evaluate message a14cc949-0cda-459c-b3e9-ce88afa9f9f1
02/15/2025 00:12:23:INFO:Received: evaluate message a14cc949-0cda-459c-b3e9-ce88afa9f9f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:12:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:13:09:INFO:
[92mINFO [0m:      Received: train message ff4c7b9e-bf71-4bdf-bb56-75757c847190
02/15/2025 00:13:09:INFO:Received: train message ff4c7b9e-bf71-4bdf-bb56-75757c847190
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:13:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:14:34:INFO:
[92mINFO [0m:      Received: evaluate message 576f9647-50e1-448d-92fe-87610e514187
02/15/2025 00:14:34:INFO:Received: evaluate message 576f9647-50e1-448d-92fe-87610e514187
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904], 'accuracy': [0.5105551211884285], 'auc': [0.5353700758405713], 'precision': [0.4273732942807158], 'recall': [0.5105551211884285], 'f1': [0.46264894441049187]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877], 'accuracy': [0.5105551211884285, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766], 'precision': [0.4273732942807158, 0.41654227475272265], 'recall': [0.5105551211884285, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:14:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:15:20:INFO:
[92mINFO [0m:      Received: train message 12f9f6c6-1ce9-4e20-9746-1e273bc25b48
02/15/2025 00:15:20:INFO:Received: train message 12f9f6c6-1ce9-4e20-9746-1e273bc25b48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:16:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:00:INFO:
[92mINFO [0m:      Received: evaluate message 888cb07e-252d-40c5-b3a3-0d922d9b9e43
02/15/2025 00:17:00:INFO:Received: evaluate message 888cb07e-252d-40c5-b3a3-0d922d9b9e43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:17:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:17:54:INFO:
[92mINFO [0m:      Received: train message 9b3645c3-fd9a-48d9-b8a2-e4f35a624cb8
02/15/2025 00:17:54:INFO:Received: train message 9b3645c3-fd9a-48d9-b8a2-e4f35a624cb8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:18:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:19:34:INFO:
[92mINFO [0m:      Received: evaluate message 85ae2834-139a-43f9-9dd7-25d5756e9564
02/15/2025 00:19:34:INFO:Received: evaluate message 85ae2834-139a-43f9-9dd7-25d5756e9564
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:19:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:20:07:INFO:
[92mINFO [0m:      Received: train message 597191e9-1156-4dd2-8eca-c6384424502f
02/15/2025 00:20:07:INFO:Received: train message 597191e9-1156-4dd2-8eca-c6384424502f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:20:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:21:50:INFO:
[92mINFO [0m:      Received: evaluate message 9d9bb071-134b-42b8-8382-676d737745a9
02/15/2025 00:21:50:INFO:Received: evaluate message 9d9bb071-134b-42b8-8382-676d737745a9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:21:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:22:20:INFO:
[92mINFO [0m:      Received: train message 40cf389b-9436-4549-9b17-4fe016a66bf8
02/15/2025 00:22:20:INFO:Received: train message 40cf389b-9436-4549-9b17-4fe016a66bf8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:23:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:13:INFO:
[92mINFO [0m:      Received: evaluate message 62bf2ab8-8416-408e-b197-670cb8e574f0
02/15/2025 00:24:13:INFO:Received: evaluate message 62bf2ab8-8416-408e-b197-670cb8e574f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:24:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:24:55:INFO:
[92mINFO [0m:      Received: train message 9a8506f3-8c31-4394-84c3-90e90f9e4fae
02/15/2025 00:24:55:INFO:Received: train message 9a8506f3-8c31-4394-84c3-90e90f9e4fae

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:25:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:26:30:INFO:
[92mINFO [0m:      Received: evaluate message 9b441496-d89e-4268-9d17-e9bf705e3be0
02/15/2025 00:26:30:INFO:Received: evaluate message 9b441496-d89e-4268-9d17-e9bf705e3be0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:26:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:27:01:INFO:
[92mINFO [0m:      Received: train message 17b8a740-79e0-4ed4-905b-99839a02f909
02/15/2025 00:27:01:INFO:Received: train message 17b8a740-79e0-4ed4-905b-99839a02f909
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:27:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:28:56:INFO:
[92mINFO [0m:      Received: evaluate message 6ccc1fe1-f404-4c96-b56b-8fb2c67e2c0a
02/15/2025 00:28:56:INFO:Received: evaluate message 6ccc1fe1-f404-4c96-b56b-8fb2c67e2c0a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:29:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:29:41:INFO:
[92mINFO [0m:      Received: train message fcae9aa4-9ed5-4808-b43c-da6255fc3765
02/15/2025 00:29:41:INFO:Received: train message fcae9aa4-9ed5-4808-b43c-da6255fc3765
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:30:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:03:INFO:
[92mINFO [0m:      Received: evaluate message 7ac0e63a-a657-48b8-b36a-36c0b0903d84
02/15/2025 00:31:03:INFO:Received: evaluate message 7ac0e63a-a657-48b8-b36a-36c0b0903d84
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:31:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:31:54:INFO:
[92mINFO [0m:      Received: train message 35255a6d-6c48-4d2d-b35a-ecc8767c619a
02/15/2025 00:31:54:INFO:Received: train message 35255a6d-6c48-4d2d-b35a-ecc8767c619a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:32:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:33:29:INFO:
[92mINFO [0m:      Received: evaluate message 42f06fa8-7f68-4d95-ab8f-7bd74f96585f
02/15/2025 00:33:29:INFO:Received: evaluate message 42f06fa8-7f68-4d95-ab8f-7bd74f96585f
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:33:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:34:08:INFO:
[92mINFO [0m:      Received: train message fa55f571-6511-46e4-b6db-e634fc911251
02/15/2025 00:34:08:INFO:Received: train message fa55f571-6511-46e4-b6db-e634fc911251
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:34:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:35:30:INFO:
[92mINFO [0m:      Received: evaluate message 43da7586-566c-4a59-b9e0-a3bac34847d6
02/15/2025 00:35:30:INFO:Received: evaluate message 43da7586-566c-4a59-b9e0-a3bac34847d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:35:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:36:28:INFO:
[92mINFO [0m:      Received: train message a811e9b5-c222-4dec-883f-592b7c40a789
02/15/2025 00:36:28:INFO:Received: train message a811e9b5-c222-4dec-883f-592b7c40a789
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:37:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:37:43:INFO:
[92mINFO [0m:      Received: evaluate message 1f45b233-f81a-4945-a9d7-b190c1d33367
02/15/2025 00:37:43:INFO:Received: evaluate message 1f45b233-f81a-4945-a9d7-b190c1d33367
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:37:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:38:37:INFO:
[92mINFO [0m:      Received: train message dc9b9ff8-55b1-4431-a4f2-09ab2c943438
02/15/2025 00:38:37:INFO:Received: train message dc9b9ff8-55b1-4431-a4f2-09ab2c943438
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:39:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:03:INFO:
[92mINFO [0m:      Received: evaluate message c6b0d634-1451-4e4c-8418-e6a6e8ccc0a7
02/15/2025 00:40:03:INFO:Received: evaluate message c6b0d634-1451-4e4c-8418-e6a6e8ccc0a7

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:40:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:40:42:INFO:
[92mINFO [0m:      Received: train message e875a8fc-b36a-44ba-b784-15727d69772a
02/15/2025 00:40:42:INFO:Received: train message e875a8fc-b36a-44ba-b784-15727d69772a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:41:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:41:54:INFO:
[92mINFO [0m:      Received: evaluate message 19e268e0-726c-478c-a46f-0b2d59ad4655
02/15/2025 00:41:54:INFO:Received: evaluate message 19e268e0-726c-478c-a46f-0b2d59ad4655
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:41:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:42:49:INFO:
[92mINFO [0m:      Received: train message c3687d8c-3d17-4317-aa40-d25141eff58f
02/15/2025 00:42:49:INFO:Received: train message c3687d8c-3d17-4317-aa40-d25141eff58f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:43:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:07:INFO:
[92mINFO [0m:      Received: evaluate message f15bf93c-b995-4674-8cf0-560f5eac30c3
02/15/2025 00:44:07:INFO:Received: evaluate message f15bf93c-b995-4674-8cf0-560f5eac30c3

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:44:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:44:35:INFO:
[92mINFO [0m:      Received: train message a34570ce-3a74-4bf7-b5a0-7f5d7e232cc7
02/15/2025 00:44:35:INFO:Received: train message a34570ce-3a74-4bf7-b5a0-7f5d7e232cc7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:45:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:46:11:INFO:
[92mINFO [0m:      Received: evaluate message 8640e982-59fd-47ad-8c8e-b140de35cb51
02/15/2025 00:46:11:INFO:Received: evaluate message 8640e982-59fd-47ad-8c8e-b140de35cb51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:46:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:47:10:INFO:
[92mINFO [0m:      Received: train message ea208cb5-b569-41a0-9cff-fb3ce15f1a7e
02/15/2025 00:47:10:INFO:Received: train message ea208cb5-b569-41a0-9cff-fb3ce15f1a7e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:47:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:48:48:INFO:
[92mINFO [0m:      Received: evaluate message 50675064-ccea-45dd-98b3-96cd1901d93b
02/15/2025 00:48:48:INFO:Received: evaluate message 50675064-ccea-45dd-98b3-96cd1901d93b

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:48:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:28:INFO:
[92mINFO [0m:      Received: train message 43aa57d9-5d6a-488e-a0c3-043d30482b04
02/15/2025 00:49:28:INFO:Received: train message 43aa57d9-5d6a-488e-a0c3-043d30482b04
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:50:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:02:INFO:
[92mINFO [0m:      Received: evaluate message d82cdf09-4686-4ca0-8312-f7080e7174b2
02/15/2025 00:51:02:INFO:Received: evaluate message d82cdf09-4686-4ca0-8312-f7080e7174b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:47:INFO:
[92mINFO [0m:      Received: train message bdc9daf1-9b83-42ea-a68f-8836a5f7c8b5
02/15/2025 00:51:47:INFO:Received: train message bdc9daf1-9b83-42ea-a68f-8836a5f7c8b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:52:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:07:INFO:
[92mINFO [0m:      Received: evaluate message 444c5005-f804-4878-aebb-b529a8628034
02/15/2025 00:53:07:INFO:Received: evaluate message 444c5005-f804-4878-aebb-b529a8628034

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:48:INFO:
[92mINFO [0m:      Received: train message 61910172-664d-42b9-89ef-8312399051a9
02/15/2025 00:53:48:INFO:Received: train message 61910172-664d-42b9-89ef-8312399051a9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:54:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:29:INFO:
[92mINFO [0m:      Received: evaluate message 2f278bd0-e0b1-49d5-915b-ed0b5d8355be
02/15/2025 00:55:29:INFO:Received: evaluate message 2f278bd0-e0b1-49d5-915b-ed0b5d8355be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:50:INFO:
[92mINFO [0m:      Received: train message 8acc298b-3a8e-4612-a230-92f185e01d64
02/15/2025 00:55:50:INFO:Received: train message 8acc298b-3a8e-4612-a230-92f185e01d64
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:56:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:39:INFO:
[92mINFO [0m:      Received: evaluate message 5f93c446-d678-4881-b93d-da4b1ddca777
02/15/2025 00:57:39:INFO:Received: evaluate message 5f93c446-d678-4881-b93d-da4b1ddca777

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:57:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:25:INFO:
[92mINFO [0m:      Received: train message a94dcfd4-2b40-4f4d-9eec-ec6ed01c39a5
02/15/2025 00:58:25:INFO:Received: train message a94dcfd4-2b40-4f4d-9eec-ec6ed01c39a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:59:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:08:INFO:
[92mINFO [0m:      Received: evaluate message 214bbeb6-d8d7-4c40-904c-9ba606e26077
02/15/2025 01:00:08:INFO:Received: evaluate message 214bbeb6-d8d7-4c40-904c-9ba606e26077
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:00:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:49:INFO:
[92mINFO [0m:      Received: train message 37776670-f8c3-4c9c-8b7b-9da1ac04ffa8
02/15/2025 01:00:49:INFO:Received: train message 37776670-f8c3-4c9c-8b7b-9da1ac04ffa8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:21:INFO:
[92mINFO [0m:      Received: evaluate message 03efe50f-ee05-4514-80fa-d387831dcf02
02/15/2025 01:02:21:INFO:Received: evaluate message 03efe50f-ee05-4514-80fa-d387831dcf02

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:53:INFO:
[92mINFO [0m:      Received: train message 034a4f2c-6367-4b99-8fd0-5e42dd71a748
02/15/2025 01:02:53:INFO:Received: train message 034a4f2c-6367-4b99-8fd0-5e42dd71a748
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:37:INFO:
[92mINFO [0m:      Received: evaluate message 097e2102-cf01-4676-9401-9c72c843a469
02/15/2025 01:04:37:INFO:Received: evaluate message 097e2102-cf01-4676-9401-9c72c843a469
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:05:14:INFO:
[92mINFO [0m:      Received: train message 3defe6ab-5385-4b90-912a-7ab81fba573c
02/15/2025 01:05:14:INFO:Received: train message 3defe6ab-5385-4b90-912a-7ab81fba573c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:06:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:53:INFO:
[92mINFO [0m:      Received: evaluate message ac4d6c84-2c04-48f2-af73-a62bc3eda761
02/15/2025 01:06:53:INFO:Received: evaluate message ac4d6c84-2c04-48f2-af73-a62bc3eda761

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:07:00:INFO:
[92mINFO [0m:      Received: reconnect message 1ec5f7d8-8859-46d1-b1f3-91ff58bc7f17
02/15/2025 01:07:00:INFO:Received: reconnect message 1ec5f7d8-8859-46d1-b1f3-91ff58bc7f17
02/15/2025 01:07:00:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:07:00:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456, 1.0000020092217785], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172, 0.7112468503315255], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236, 0.41428740286963217], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704, 0.39244624060601346]}



Final client history:
{'loss': [1.2801453032653904, 1.217973143910877, 1.170207204718064, 1.135790583116175, 1.1106440614591453, 1.0902098179609165, 1.0758761755650261, 1.0631974960696986, 1.0533220912442718, 1.0453464993282255, 1.0381587676651503, 1.0328380013928924, 1.0283386081899861, 1.0249555944184758, 1.020520821225373, 1.0167003916612913, 1.014935942065259, 1.0126074164802903, 1.010201390503532, 1.0085955203930972, 1.0073504066728258, 1.005863340130255, 1.0058758409811799, 1.005364303014724, 1.006562125505741, 1.0067206634218755, 1.0047544517770608, 1.002834329668482, 1.0018638614754456, 1.0000020092217785], 'accuracy': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'auc': [0.5353700758405713, 0.5567528693752766, 0.5715496904263248, 0.5836443508074672, 0.5971389800948795, 0.6113165914027084, 0.6231008951816877, 0.6349996000509412, 0.6455107337629921, 0.6514594550946208, 0.6601608354634753, 0.666459563956981, 0.6732249263091874, 0.6777195607059904, 0.6824471864116053, 0.6854281682925932, 0.6875413274047038, 0.6899976540325783, 0.6941288362472724, 0.6967074601289561, 0.7006321446885323, 0.7034832407696976, 0.705346482792835, 0.707517796553561, 0.7087238520724715, 0.7104546651007042, 0.7100797902688201, 0.7099185930685544, 0.7117895986470172, 0.7112468503315255], 'precision': [0.4273732942807158, 0.41654227475272265, 0.4240389568372002, 0.42622037659623, 0.429696000551097, 0.4288964079122722, 0.417930256339405, 0.4200125609130963, 0.4231496209800698, 0.42312563278230453, 0.4224574588925216, 0.4245598377323367, 0.4284084490114479, 0.4284084490114479, 0.4313138807046674, 0.43175818929582643, 0.4344267011489584, 0.43437011109407747, 0.43437011109407747, 0.43437011109407747, 0.43006245362301143, 0.433040227748703, 0.4330530850245938, 0.43068850243347917, 0.43958329814258007, 0.43259937905326706, 0.433040227748703, 0.4191264227840638, 0.42144223183894236, 0.41428740286963217], 'recall': [0.5105551211884285, 0.5129007036747459, 0.5230648944487881, 0.5238467552775606, 0.5230648944487881, 0.5222830336200156, 0.5168100078186083, 0.5160281469898358, 0.5136825645035183, 0.5152462861610634, 0.5129007036747459, 0.5113369820172009, 0.5105551211884285, 0.5105551211884285, 0.5113369820172009, 0.5121188428459734, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5129007036747459, 0.5144644253322909, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5136825645035183, 0.5144644253322909, 0.5113369820172009, 0.5121188428459734, 0.509773260359656], 'f1': [0.46264894441049187, 0.449317578856086, 0.4381507539933754, 0.42660099672835594, 0.4163533647884961, 0.4142670689521651, 0.40269721059612984, 0.3993853287831024, 0.38831710930972174, 0.39229613178049333, 0.38682079570675704, 0.3816534203442328, 0.37891219881493166, 0.37891219881493166, 0.38037101712319726, 0.3838995216845655, 0.3843530245930965, 0.38533274330372164, 0.38533274330372164, 0.38533274330372164, 0.3855064626316991, 0.3884349150761539, 0.38938671368421873, 0.3889248727115417, 0.3872194991986635, 0.385961751289347, 0.3884349150761539, 0.3887214313908112, 0.3901074344358704, 0.39244624060601346]}

