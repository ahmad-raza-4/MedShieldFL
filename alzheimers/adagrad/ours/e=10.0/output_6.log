nohup: ignoring input
02/18/2025 05:14:48:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/18/2025 05:14:48:DEBUG:ChannelConnectivity.IDLE
02/18/2025 05:14:48:DEBUG:ChannelConnectivity.CONNECTING
02/18/2025 05:14:48:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739884488.415768 1531204 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/18/2025 05:15:09:INFO:
[92mINFO [0m:      Received: train message d068c8c6-6aeb-48cd-8d31-eba67963ee01
02/18/2025 05:15:09:INFO:Received: train message d068c8c6-6aeb-48cd-8d31-eba67963ee01
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:15:35:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:16:39:INFO:
[92mINFO [0m:      Received: evaluate message dbd3466c-ba7d-4b4c-a13d-2a0b8d047963
02/18/2025 05:16:39:INFO:Received: evaluate message dbd3466c-ba7d-4b4c-a13d-2a0b8d047963
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:16:43:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:17:04:INFO:
[92mINFO [0m:      Received: train message 38d58a13-9df4-4811-a532-612e74f135da
02/18/2025 05:17:04:INFO:Received: train message 38d58a13-9df4-4811-a532-612e74f135da
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:17:29:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:18:28:INFO:
[92mINFO [0m:      Received: evaluate message 18743d5c-f0f7-4a36-bd32-04c41b979135
02/18/2025 05:18:28:INFO:Received: evaluate message 18743d5c-f0f7-4a36-bd32-04c41b979135
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:18:31:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:19:07:INFO:
[92mINFO [0m:      Received: train message 6576b89e-68b7-4890-a7e3-656b5ae54cc7
02/18/2025 05:19:07:INFO:Received: train message 6576b89e-68b7-4890-a7e3-656b5ae54cc7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:19:35:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:20:24:INFO:
[92mINFO [0m:      Received: evaluate message f7493a74-8a2c-40ad-9000-33961e8cf677
02/18/2025 05:20:24:INFO:Received: evaluate message f7493a74-8a2c-40ad-9000-33961e8cf677
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:20:27:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:21:03:INFO:
[92mINFO [0m:      Received: train message 6538fce6-cbd9-4237-9151-d07024d7bff3
02/18/2025 05:21:03:INFO:Received: train message 6538fce6-cbd9-4237-9151-d07024d7bff3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:21:32:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:22:16:INFO:
[92mINFO [0m:      Received: evaluate message 479e11a9-a850-4cca-98ea-fcff358380ce
02/18/2025 05:22:16:INFO:Received: evaluate message 479e11a9-a850-4cca-98ea-fcff358380ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:22:19:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:22:46:INFO:
[92mINFO [0m:      Received: train message 0557df16-2c32-45b8-8def-7f64411a0dbf
02/18/2025 05:22:46:INFO:Received: train message 0557df16-2c32-45b8-8def-7f64411a0dbf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:23:12:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:23:54:INFO:
[92mINFO [0m:      Received: evaluate message 689234f2-7993-452a-9414-831454d32be1
02/18/2025 05:23:54:INFO:Received: evaluate message 689234f2-7993-452a-9414-831454d32be1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:23:56:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:24:36:INFO:
[92mINFO [0m:      Received: train message dce4e0e3-7fff-493d-8185-a97bfa747e03
02/18/2025 05:24:36:INFO:Received: train message dce4e0e3-7fff-493d-8185-a97bfa747e03
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:25:05:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:25:50:INFO:
[92mINFO [0m:      Received: evaluate message f4b248c9-b1b3-40e2-bc71-409e7faff78b
02/18/2025 05:25:50:INFO:Received: evaluate message f4b248c9-b1b3-40e2-bc71-409e7faff78b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:25:52:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:26:16:INFO:
[92mINFO [0m:      Received: train message d0faf081-b0b3-48e9-8dc6-278e8b28f435
02/18/2025 05:26:16:INFO:Received: train message d0faf081-b0b3-48e9-8dc6-278e8b28f435
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:26:43:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:27:38:INFO:
[92mINFO [0m:      Received: evaluate message 6cfc60c0-3d41-464e-a28b-96b564b15858
02/18/2025 05:27:38:INFO:Received: evaluate message 6cfc60c0-3d41-464e-a28b-96b564b15858
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867], 'accuracy': [0.5050820953870211], 'auc': [0.6443169234724522], 'precision': [0.410454559693444], 'recall': [0.5050820953870211], 'f1': [0.36834332377725654]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418], 'accuracy': [0.5050820953870211, 0.5160281469898358], 'auc': [0.6443169234724522, 0.6900686554032975], 'precision': [0.410454559693444, 0.4128538264344963], 'recall': [0.5050820953870211, 0.5160281469898358], 'f1': [0.36834332377725654, 0.4106662499521769]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:27:41:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:28:13:INFO:
[92mINFO [0m:      Received: train message 6752df4d-7a8d-464c-a0f4-4e8b422e0812
02/18/2025 05:28:13:INFO:Received: train message 6752df4d-7a8d-464c-a0f4-4e8b422e0812
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:28:40:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:29:17:INFO:
[92mINFO [0m:      Received: evaluate message c89b4a68-9fcb-4a62-8e05-4871cbf18063
02/18/2025 05:29:17:INFO:Received: evaluate message c89b4a68-9fcb-4a62-8e05-4871cbf18063
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:29:19:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:29:56:INFO:
[92mINFO [0m:      Received: train message 1a6c637c-7abb-4794-88ac-1cd444699eda
02/18/2025 05:29:56:INFO:Received: train message 1a6c637c-7abb-4794-88ac-1cd444699eda
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:30:22:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:31:07:INFO:
[92mINFO [0m:      Received: evaluate message 901f8485-0538-4558-b9ad-d853da3b852f
02/18/2025 05:31:07:INFO:Received: evaluate message 901f8485-0538-4558-b9ad-d853da3b852f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:31:09:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:31:36:INFO:
[92mINFO [0m:      Received: train message 3066010d-2cd2-48f7-aa04-c6d734f299f1
02/18/2025 05:31:36:INFO:Received: train message 3066010d-2cd2-48f7-aa04-c6d734f299f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:32:00:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:33:04:INFO:
[92mINFO [0m:      Received: evaluate message 78279b77-99b1-4097-96b5-3ae88f16f9bb
02/18/2025 05:33:04:INFO:Received: evaluate message 78279b77-99b1-4097-96b5-3ae88f16f9bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:33:08:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:33:31:INFO:
[92mINFO [0m:      Received: train message 1d25d939-1496-45e6-a813-6ebaf02b85a7
02/18/2025 05:33:31:INFO:Received: train message 1d25d939-1496-45e6-a813-6ebaf02b85a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:33:57:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:34:41:INFO:
[92mINFO [0m:      Received: evaluate message 5188da45-feb3-42cb-822c-b2be7dec4dc1
02/18/2025 05:34:41:INFO:Received: evaluate message 5188da45-feb3-42cb-822c-b2be7dec4dc1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:34:43:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:35:13:INFO:
[92mINFO [0m:      Received: train message f9eee75a-8c5e-4d5a-8834-550269860427
02/18/2025 05:35:13:INFO:Received: train message f9eee75a-8c5e-4d5a-8834-550269860427

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:35:39:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:36:30:INFO:
[92mINFO [0m:      Received: evaluate message 19d45936-529b-4b0c-bacb-faf0185d9da4
02/18/2025 05:36:30:INFO:Received: evaluate message 19d45936-529b-4b0c-bacb-faf0185d9da4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:36:33:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:37:08:INFO:
[92mINFO [0m:      Received: train message b2df8fe2-7200-4006-a24e-776488023e12
02/18/2025 05:37:08:INFO:Received: train message b2df8fe2-7200-4006-a24e-776488023e12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:37:35:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:38:22:INFO:
[92mINFO [0m:      Received: evaluate message a1f92241-9a0b-454e-a1a9-dbff06e52618
02/18/2025 05:38:22:INFO:Received: evaluate message a1f92241-9a0b-454e-a1a9-dbff06e52618
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:38:24:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:38:58:INFO:
[92mINFO [0m:      Received: train message 46600524-0768-48ac-a418-9a9fe798b3e1
02/18/2025 05:38:58:INFO:Received: train message 46600524-0768-48ac-a418-9a9fe798b3e1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:39:30:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:39:59:INFO:
[92mINFO [0m:      Received: evaluate message b3b9d2f6-8f5c-49da-ba84-807f4e2fc909
02/18/2025 05:39:59:INFO:Received: evaluate message b3b9d2f6-8f5c-49da-ba84-807f4e2fc909
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:40:01:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:40:35:INFO:
[92mINFO [0m:      Received: train message b4c8e069-80d2-432c-a5f5-44a1ce84ed72
02/18/2025 05:40:35:INFO:Received: train message b4c8e069-80d2-432c-a5f5-44a1ce84ed72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:41:03:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:41:53:INFO:
[92mINFO [0m:      Received: evaluate message e783bb6e-4db6-4db1-88f9-2a7b2fa12f77
02/18/2025 05:41:53:INFO:Received: evaluate message e783bb6e-4db6-4db1-88f9-2a7b2fa12f77
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:41:56:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:42:19:INFO:
[92mINFO [0m:      Received: train message ef2d62f1-d4f9-43af-a174-11b159761f80
02/18/2025 05:42:19:INFO:Received: train message ef2d62f1-d4f9-43af-a174-11b159761f80
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:42:47:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:43:27:INFO:
[92mINFO [0m:      Received: evaluate message 69e68ee7-afe7-4408-97ca-f75df0a24786
02/18/2025 05:43:27:INFO:Received: evaluate message 69e68ee7-afe7-4408-97ca-f75df0a24786
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:43:29:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:44:10:INFO:
[92mINFO [0m:      Received: train message 81e7f2a8-bd50-491c-913a-46e44c09e9f8
02/18/2025 05:44:10:INFO:Received: train message 81e7f2a8-bd50-491c-913a-46e44c09e9f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:44:39:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:45:18:INFO:
[92mINFO [0m:      Received: evaluate message 914ab7f8-f8fd-492a-a539-5558cd86717b
02/18/2025 05:45:18:INFO:Received: evaluate message 914ab7f8-f8fd-492a-a539-5558cd86717b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:45:20:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:46:04:INFO:
[92mINFO [0m:      Received: train message d8c1bf03-262e-44b0-b417-6280670f7a81
02/18/2025 05:46:04:INFO:Received: train message d8c1bf03-262e-44b0-b417-6280670f7a81
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:46:33:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:47:05:INFO:
[92mINFO [0m:      Received: evaluate message 95af48a4-d86c-4abc-ad70-b42241428fa0
02/18/2025 05:47:05:INFO:Received: evaluate message 95af48a4-d86c-4abc-ad70-b42241428fa0

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:47:07:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:47:56:INFO:
[92mINFO [0m:      Received: train message 52e08060-ad7a-4dfb-ae91-21184b56916b
02/18/2025 05:47:56:INFO:Received: train message 52e08060-ad7a-4dfb-ae91-21184b56916b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:48:25:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:48:50:INFO:
[92mINFO [0m:      Received: evaluate message 9e0d22e1-803c-4f5e-a08b-67f51b6adc4b
02/18/2025 05:48:50:INFO:Received: evaluate message 9e0d22e1-803c-4f5e-a08b-67f51b6adc4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:48:52:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:49:42:INFO:
[92mINFO [0m:      Received: train message d86c4926-7a75-4e20-a1f6-496146ae300e
02/18/2025 05:49:42:INFO:Received: train message d86c4926-7a75-4e20-a1f6-496146ae300e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:50:09:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:50:54:INFO:
[92mINFO [0m:      Received: evaluate message 674be7a0-3b20-49d6-a31e-f20bb5c7a4ac
02/18/2025 05:50:54:INFO:Received: evaluate message 674be7a0-3b20-49d6-a31e-f20bb5c7a4ac

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:50:56:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:51:30:INFO:
[92mINFO [0m:      Received: train message 8b045a5e-55f0-44db-b992-4cc6fc143c22
02/18/2025 05:51:30:INFO:Received: train message 8b045a5e-55f0-44db-b992-4cc6fc143c22
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:51:59:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:52:35:INFO:
[92mINFO [0m:      Received: evaluate message 5a861163-2e3f-4dec-8d38-a225a55382f3
02/18/2025 05:52:35:INFO:Received: evaluate message 5a861163-2e3f-4dec-8d38-a225a55382f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:52:37:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:53:06:INFO:
[92mINFO [0m:      Received: train message 407b2a30-4b32-4bf0-b863-91e453b3e3f9
02/18/2025 05:53:06:INFO:Received: train message 407b2a30-4b32-4bf0-b863-91e453b3e3f9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:53:33:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:54:34:INFO:
[92mINFO [0m:      Received: evaluate message a8e27893-4905-49b3-8d0b-ca70daafd5ee
02/18/2025 05:54:34:INFO:Received: evaluate message a8e27893-4905-49b3-8d0b-ca70daafd5ee

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:54:37:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:54:56:INFO:
[92mINFO [0m:      Received: train message 3a565b8d-3449-483c-960a-99b03401fcf6
02/18/2025 05:54:56:INFO:Received: train message 3a565b8d-3449-483c-960a-99b03401fcf6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:55:21:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:56:26:INFO:
[92mINFO [0m:      Received: evaluate message e10530ff-3ece-4633-8bf6-3ba602017e12
02/18/2025 05:56:26:INFO:Received: evaluate message e10530ff-3ece-4633-8bf6-3ba602017e12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:56:28:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:56:48:INFO:
[92mINFO [0m:      Received: train message 8ab2d929-be22-4421-b929-9cc02e59793c
02/18/2025 05:56:48:INFO:Received: train message 8ab2d929-be22-4421-b929-9cc02e59793c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:57:15:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:58:01:INFO:
[92mINFO [0m:      Received: evaluate message 395e3138-7cf3-4088-9882-dcec267c1bc6
02/18/2025 05:58:01:INFO:Received: evaluate message 395e3138-7cf3-4088-9882-dcec267c1bc6

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:58:03:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:58:40:INFO:
[92mINFO [0m:      Received: train message a676ba3d-d96a-486a-b044-fa215ccab44a
02/18/2025 05:58:40:INFO:Received: train message a676ba3d-d96a-486a-b044-fa215ccab44a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 05:59:06:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 05:59:46:INFO:
[92mINFO [0m:      Received: evaluate message a98a8d10-f638-46c0-ab95-adaf621990ec
02/18/2025 05:59:46:INFO:Received: evaluate message a98a8d10-f638-46c0-ab95-adaf621990ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 05:59:49:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:00:17:INFO:
[92mINFO [0m:      Received: train message c79b97c1-f59a-4c1c-a7bb-dc369101dbc5
02/18/2025 06:00:17:INFO:Received: train message c79b97c1-f59a-4c1c-a7bb-dc369101dbc5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 06:00:44:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:01:52:INFO:
[92mINFO [0m:      Received: evaluate message 713dd408-5120-47b2-becc-7f120992d3fe
02/18/2025 06:01:52:INFO:Received: evaluate message 713dd408-5120-47b2-becc-7f120992d3fe

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483, 1.1413508930459817], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941, 0.7607988562955631], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726, 0.43740522502251294], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644, 0.47431811990218625]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 06:01:56:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:02:27:INFO:
[92mINFO [0m:      Received: train message 5396d989-25c2-4e1a-b825-cfb23f4952d6
02/18/2025 06:02:27:INFO:Received: train message 5396d989-25c2-4e1a-b825-cfb23f4952d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 06:02:55:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:03:31:INFO:
[92mINFO [0m:      Received: evaluate message 5e084b09-ebf3-4627-a230-003d58317d73
02/18/2025 06:03:31:INFO:Received: evaluate message 5e084b09-ebf3-4627-a230-003d58317d73
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 06:03:33:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:04:16:INFO:
[92mINFO [0m:      Received: train message 0fc47c90-1762-4a93-9b25-d7a502007456
02/18/2025 06:04:16:INFO:Received: train message 0fc47c90-1762-4a93-9b25-d7a502007456
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 06:04:43:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:05:27:INFO:
[92mINFO [0m:      Received: evaluate message 95f92c2c-80ce-4ab1-96db-0d08da57db31
02/18/2025 06:05:27:INFO:Received: evaluate message 95f92c2c-80ce-4ab1-96db-0d08da57db31

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483, 1.1413508930459817, 1.1582641067497426], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941, 0.7607988562955631, 0.7606399208147002], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726, 0.43740522502251294, 0.4334211208115929], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644, 0.47431811990218625, 0.4697980468117578]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483, 1.1413508930459817, 1.1582641067497426, 1.159490987041017], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941, 0.7607988562955631, 0.7606399208147002, 0.7604735387110503], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726, 0.43740522502251294, 0.4334211208115929, 0.433248737172464], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644, 0.47431811990218625, 0.4697980468117578, 0.4694611347310364]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 06:05:31:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:06:05:INFO:
[92mINFO [0m:      Received: train message 7bcf055d-ceb0-4725-a12d-d52a8595ada0
02/18/2025 06:06:05:INFO:Received: train message 7bcf055d-ceb0-4725-a12d-d52a8595ada0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 06:06:33:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:07:13:INFO:
[92mINFO [0m:      Received: evaluate message 2fd621aa-29cb-4f77-b01f-1937847797fe
02/18/2025 06:07:13:INFO:Received: evaluate message 2fd621aa-29cb-4f77-b01f-1937847797fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 06:07:16:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:07:54:INFO:
[92mINFO [0m:      Received: train message b8f2f43b-b2d2-4eab-a6da-bbf4f191f4a0
02/18/2025 06:07:54:INFO:Received: train message b8f2f43b-b2d2-4eab-a6da-bbf4f191f4a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/18/2025 06:08:22:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:09:07:INFO:
[92mINFO [0m:      Received: evaluate message a9ee32c3-c269-4b51-a7e7-e7471557e637
02/18/2025 06:09:07:INFO:Received: evaluate message a9ee32c3-c269-4b51-a7e7-e7471557e637

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483, 1.1413508930459817, 1.1582641067497426, 1.159490987041017, 1.1730798039238746], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941, 0.7607988562955631, 0.7606399208147002, 0.7604735387110503, 0.7613870066159021], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726, 0.43740522502251294, 0.4334211208115929, 0.433248737172464, 0.4335628945385825], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644, 0.47431811990218625, 0.4697980468117578, 0.4694611347310364, 0.4678383809916569]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483, 1.1413508930459817, 1.1582641067497426, 1.159490987041017, 1.1730798039238746, 1.1354056051506298], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477, 0.5363565285379203], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941, 0.7607988562955631, 0.7606399208147002, 0.7604735387110503, 0.7613870066159021, 0.764284431359177], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726, 0.43740522502251294, 0.4334211208115929, 0.433248737172464, 0.4335628945385825, 0.436414912702961], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477, 0.5363565285379203], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644, 0.47431811990218625, 0.4697980468117578, 0.4694611347310364, 0.4678383809916569, 0.47242838712182095]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/18/2025 06:09:11:INFO:Sent reply
[92mINFO [0m:      
02/18/2025 06:09:11:INFO:
[92mINFO [0m:      Received: reconnect message 04761ae0-393d-4165-aaa0-18e6b48606cc
02/18/2025 06:09:11:INFO:Received: reconnect message 04761ae0-393d-4165-aaa0-18e6b48606cc
02/18/2025 06:09:11:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/18/2025 06:09:11:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483, 1.1413508930459817, 1.1582641067497426, 1.159490987041017, 1.1730798039238746, 1.1354056051506298, 1.1409532328058352], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477, 0.5363565285379203, 0.5347928068803753], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941, 0.7607988562955631, 0.7606399208147002, 0.7604735387110503, 0.7613870066159021, 0.764284431359177, 0.7633470562370275], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726, 0.43740522502251294, 0.4334211208115929, 0.433248737172464, 0.4335628945385825, 0.436414912702961, 0.43554413602735165], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477, 0.5363565285379203, 0.5347928068803753], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644, 0.47431811990218625, 0.4697980468117578, 0.4694611347310364, 0.4678383809916569, 0.47242838712182095, 0.47264999846991423]}



Final client history:
{'loss': [1.1029971625489867, 1.0468588136340418, 1.0864389609134784, 1.1294109460615944, 1.0916231931234543, 1.0462876048006053, 1.0808289029152716, 1.0953077887817693, 1.1036169934403253, 1.1033575444411634, 1.1577058014597532, 1.1508214221138615, 1.1209409002963224, 1.1423409848216923, 1.0879938682603874, 1.1359865445471817, 1.106757615580048, 1.0962082032023974, 1.140676367180342, 1.1483438873402863, 1.1198040381178807, 1.147387962680575, 1.1719025570121568, 1.163382215738483, 1.1413508930459817, 1.1582641067497426, 1.159490987041017, 1.1730798039238746, 1.1354056051506298, 1.1409532328058352], 'accuracy': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477, 0.5363565285379203, 0.5347928068803753], 'auc': [0.6443169234724522, 0.6900686554032975, 0.7049978142103296, 0.7180931530109057, 0.7195515720744825, 0.7144009874835487, 0.7190066570036651, 0.7266722143058666, 0.7298370541083002, 0.7345061328579674, 0.7413268128178472, 0.7436086376365336, 0.7450218480142393, 0.7480110318503037, 0.7454910015575634, 0.748462247705928, 0.747908776158926, 0.7455097780734842, 0.7484676102037753, 0.7485924269130858, 0.7517971906544579, 0.7553027997314081, 0.757521598990653, 0.7569986033455941, 0.7607988562955631, 0.7606399208147002, 0.7604735387110503, 0.7613870066159021, 0.764284431359177, 0.7633470562370275], 'precision': [0.410454559693444, 0.4128538264344963, 0.4171886445308626, 0.41238749716493844, 0.4261026138202121, 0.4461383930148949, 0.4376152516022086, 0.4331578838428575, 0.43940686617216196, 0.43162397906757055, 0.4342983503216241, 0.43885441964119504, 0.43026895837400964, 0.4271863830124589, 0.44361233264337957, 0.43675227799260025, 0.43810190670805094, 0.43887709101644207, 0.42393029945659966, 0.428789724675333, 0.43186654865642937, 0.4297005227687967, 0.42195450155445263, 0.42853976523902726, 0.43740522502251294, 0.4334211208115929, 0.433248737172464, 0.4335628945385825, 0.436414912702961, 0.43554413602735165], 'recall': [0.5050820953870211, 0.5160281469898358, 0.5207193119624707, 0.5183737294761532, 0.5285379202501954, 0.5402658326817826, 0.5371383893666928, 0.5355746677091477, 0.5418295543393276, 0.5347928068803753, 0.5347928068803753, 0.5379202501954652, 0.5308835027365129, 0.5293197810789679, 0.5418295543393276, 0.5387021110242377, 0.5355746677091477, 0.5371383893666928, 0.5285379202501954, 0.5316653635652854, 0.5301016419077405, 0.5301016419077405, 0.5254104769351056, 0.5301016419077405, 0.5379202501954652, 0.5340109460516028, 0.5332290852228303, 0.5355746677091477, 0.5363565285379203, 0.5347928068803753], 'f1': [0.36834332377725654, 0.4106662499521769, 0.4256298275014733, 0.4228982678310284, 0.4475604860255536, 0.4846086199775348, 0.46855303001909515, 0.4594442239967913, 0.4623670298166921, 0.4595579925843732, 0.4478085677420827, 0.4586034455866221, 0.4641225821688153, 0.4595853799681536, 0.48030993301641495, 0.4694230634188063, 0.4761810003114271, 0.47656377346525725, 0.4567704246395666, 0.46091253981496444, 0.46917313808799144, 0.4656290278510847, 0.45099780250345195, 0.46301054493811644, 0.47431811990218625, 0.4697980468117578, 0.4694611347310364, 0.4678383809916569, 0.47242838712182095, 0.47264999846991423]}

