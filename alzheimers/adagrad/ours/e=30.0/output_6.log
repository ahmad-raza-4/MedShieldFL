nohup: ignoring input
02/15/2025 00:47:36:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 00:47:36:DEBUG:ChannelConnectivity.IDLE
02/15/2025 00:47:36:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 00:47:36:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739609256.936432 1569769 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 00:48:19:INFO:
[92mINFO [0m:      Received: train message 2bcd0931-ff25-44f1-b026-bf85cfa4c940
02/15/2025 00:48:19:INFO:Received: train message 2bcd0931-ff25-44f1-b026-bf85cfa4c940
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:48:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:32:INFO:
[92mINFO [0m:      Received: evaluate message 79688ebc-00f6-4bc4-bbf9-b45d0bd8007b
02/15/2025 00:49:32:INFO:Received: evaluate message 79688ebc-00f6-4bc4-bbf9-b45d0bd8007b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:49:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:50:20:INFO:
[92mINFO [0m:      Received: train message bb8a9657-53c2-4424-b6e5-0c9ed76af058
02/15/2025 00:50:20:INFO:Received: train message bb8a9657-53c2-4424-b6e5-0c9ed76af058
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:50:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:46:INFO:
[92mINFO [0m:      Received: evaluate message 2992b1a2-e94d-4b66-bd29-9b451706da5c
02/15/2025 00:51:46:INFO:Received: evaluate message 2992b1a2-e94d-4b66-bd29-9b451706da5c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:52:24:INFO:
[92mINFO [0m:      Received: train message a7deba6b-544e-4f37-819f-da9f4772bc33
02/15/2025 00:52:24:INFO:Received: train message a7deba6b-544e-4f37-819f-da9f4772bc33
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:52:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:43:INFO:
[92mINFO [0m:      Received: evaluate message b37f4b5d-fae4-4fa6-a33b-c79e75208c30
02/15/2025 00:53:43:INFO:Received: evaluate message b37f4b5d-fae4-4fa6-a33b-c79e75208c30
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:54:26:INFO:
[92mINFO [0m:      Received: train message 6424c19c-8b5e-4455-9bd0-806d98f31095
02/15/2025 00:54:26:INFO:Received: train message 6424c19c-8b5e-4455-9bd0-806d98f31095
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:54:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:39:INFO:
[92mINFO [0m:      Received: evaluate message f3c9c410-6f70-43e8-90b5-7a23c489eddf
02/15/2025 00:55:39:INFO:Received: evaluate message f3c9c410-6f70-43e8-90b5-7a23c489eddf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:56:35:INFO:
[92mINFO [0m:      Received: train message d6337be6-316f-4b1d-a208-c3c14b05fbb3
02/15/2025 00:56:35:INFO:Received: train message d6337be6-316f-4b1d-a208-c3c14b05fbb3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:57:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:48:INFO:
[92mINFO [0m:      Received: evaluate message b90d2fd2-97be-4ade-b465-98e66beb803e
02/15/2025 00:57:48:INFO:Received: evaluate message b90d2fd2-97be-4ade-b465-98e66beb803e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:57:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:17:INFO:
[92mINFO [0m:      Received: train message 98b22684-3e4b-4095-95b4-88f344e313b0
02/15/2025 00:58:17:INFO:Received: train message 98b22684-3e4b-4095-95b4-88f344e313b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:58:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:02:INFO:
[92mINFO [0m:      Received: evaluate message e262739c-8947-41e9-af8c-c534fd8946a8
02/15/2025 01:00:02:INFO:Received: evaluate message e262739c-8947-41e9-af8c-c534fd8946a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:00:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:31:INFO:
[92mINFO [0m:      Received: train message 441ef705-17ac-4f65-9945-96ea4e1cb39c
02/15/2025 01:00:31:INFO:Received: train message 441ef705-17ac-4f65-9945-96ea4e1cb39c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:03:INFO:
[92mINFO [0m:      Received: evaluate message 212a9979-454f-490b-8d10-21f86e2f996e
02/15/2025 01:02:03:INFO:Received: evaluate message 212a9979-454f-490b-8d10-21f86e2f996e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497], 'accuracy': [0.4988272087568413], 'auc': [0.5004293699052179], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784], 'accuracy': [0.4988272087568413, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665], 'precision': [0.24999954080217573, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:54:INFO:
[92mINFO [0m:      Received: train message 760ace65-eea8-4bbe-9ae1-9e98eb05f8ba
02/15/2025 01:02:54:INFO:Received: train message 760ace65-eea8-4bbe-9ae1-9e98eb05f8ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:03:INFO:
[92mINFO [0m:      Received: evaluate message 393a71c0-3217-4525-af38-f145a3c32825
02/15/2025 01:04:03:INFO:Received: evaluate message 393a71c0-3217-4525-af38-f145a3c32825
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:56:INFO:
[92mINFO [0m:      Received: train message b3806bae-d3fc-4b93-ab31-b5d7d3a2e0fc
02/15/2025 01:04:56:INFO:Received: train message b3806bae-d3fc-4b93-ab31-b5d7d3a2e0fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:05:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:21:INFO:
[92mINFO [0m:      Received: evaluate message 871d5d6b-81bb-40c7-a7be-c6ab0dc5b3bb
02/15/2025 01:06:21:INFO:Received: evaluate message 871d5d6b-81bb-40c7-a7be-c6ab0dc5b3bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:07:01:INFO:
[92mINFO [0m:      Received: train message e7f3f842-0675-46ea-adf9-8aae2a403fb1
02/15/2025 01:07:01:INFO:Received: train message e7f3f842-0675-46ea-adf9-8aae2a403fb1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:07:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:21:INFO:
[92mINFO [0m:      Received: evaluate message e40177b5-c1fa-43a2-aab5-21e6fe93ed2a
02/15/2025 01:08:21:INFO:Received: evaluate message e40177b5-c1fa-43a2-aab5-21e6fe93ed2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:08:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:56:INFO:
[92mINFO [0m:      Received: train message e7a1e2b1-0c11-4fde-98af-2f03f2fa8b78
02/15/2025 01:08:56:INFO:Received: train message e7a1e2b1-0c11-4fde-98af-2f03f2fa8b78
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:09:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:13:INFO:
[92mINFO [0m:      Received: evaluate message c440063b-68b8-4711-9bf2-2caf56f2f1c3
02/15/2025 01:10:13:INFO:Received: evaluate message c440063b-68b8-4711-9bf2-2caf56f2f1c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:10:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:50:INFO:
[92mINFO [0m:      Received: train message 60398a3c-2e6e-4544-af51-706a851a7e3c
02/15/2025 01:10:50:INFO:Received: train message 60398a3c-2e6e-4544-af51-706a851a7e3c

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:11:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:08:INFO:
[92mINFO [0m:      Received: evaluate message 70c765da-2565-4789-937a-c94445627904
02/15/2025 01:12:08:INFO:Received: evaluate message 70c765da-2565-4789-937a-c94445627904
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:12:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:51:INFO:
[92mINFO [0m:      Received: train message af6c4f81-1ebe-4907-9803-cc6e0403fc8c
02/15/2025 01:12:51:INFO:Received: train message af6c4f81-1ebe-4907-9803-cc6e0403fc8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:13:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:13:47:INFO:
[92mINFO [0m:      Received: evaluate message ac382c46-c536-4b81-9bc6-be8537e58397
02/15/2025 01:13:47:INFO:Received: evaluate message ac382c46-c536-4b81-9bc6-be8537e58397
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:13:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:41:INFO:
[92mINFO [0m:      Received: train message e68c19dc-df95-44be-985a-a1412102a76d
02/15/2025 01:14:41:INFO:Received: train message e68c19dc-df95-44be-985a-a1412102a76d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:15:51:INFO:
[92mINFO [0m:      Received: evaluate message 7a080fb0-613a-41c1-97d8-5f4e0abb36da
02/15/2025 01:15:51:INFO:Received: evaluate message 7a080fb0-613a-41c1-97d8-5f4e0abb36da
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:15:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:23:INFO:
[92mINFO [0m:      Received: train message f21c9be3-9244-4213-ab5a-e0f7f5089bd4
02/15/2025 01:16:23:INFO:Received: train message f21c9be3-9244-4213-ab5a-e0f7f5089bd4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:16:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:05:INFO:
[92mINFO [0m:      Received: evaluate message e6e297b3-6e8d-44d9-b124-80e7a00d0b9e
02/15/2025 01:18:05:INFO:Received: evaluate message e6e297b3-6e8d-44d9-b124-80e7a00d0b9e
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:38:INFO:
[92mINFO [0m:      Received: train message 07e858b7-78e1-4d50-acb5-4424ab5f8e8b
02/15/2025 01:18:38:INFO:Received: train message 07e858b7-78e1-4d50-acb5-4424ab5f8e8b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:12:INFO:
[92mINFO [0m:      Received: evaluate message 217e4b60-254f-4151-a240-7fe829befa5a
02/15/2025 01:20:12:INFO:Received: evaluate message 217e4b60-254f-4151-a240-7fe829befa5a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:55:INFO:
[92mINFO [0m:      Received: train message 979debd4-94f0-47eb-9ab9-2206ca4efae8
02/15/2025 01:20:55:INFO:Received: train message 979debd4-94f0-47eb-9ab9-2206ca4efae8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:21:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:43:INFO:
[92mINFO [0m:      Received: evaluate message f31d9089-b38a-439d-b7fb-cabbe58da6ac
02/15/2025 01:22:43:INFO:Received: evaluate message f31d9089-b38a-439d-b7fb-cabbe58da6ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:22:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:13:INFO:
[92mINFO [0m:      Received: train message 3ecdb6f5-4151-455f-add3-8e17dee4407c
02/15/2025 01:23:13:INFO:Received: train message 3ecdb6f5-4151-455f-add3-8e17dee4407c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:23:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:02:INFO:
[92mINFO [0m:      Received: evaluate message 9d4167ff-99ab-4539-9a62-de951ec449ea
02/15/2025 01:25:02:INFO:Received: evaluate message 9d4167ff-99ab-4539-9a62-de951ec449ea

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:47:INFO:
[92mINFO [0m:      Received: train message 30877038-44b6-4b1b-9b96-0cbf2adf05a4
02/15/2025 01:25:47:INFO:Received: train message 30877038-44b6-4b1b-9b96-0cbf2adf05a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:19:INFO:
[92mINFO [0m:      Received: evaluate message a91ea148-6040-4970-b444-68a1bbd39deb
02/15/2025 01:27:19:INFO:Received: evaluate message a91ea148-6040-4970-b444-68a1bbd39deb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:58:INFO:
[92mINFO [0m:      Received: train message 251c62fb-26a6-4529-9cb2-ee57baa28482
02/15/2025 01:27:58:INFO:Received: train message 251c62fb-26a6-4529-9cb2-ee57baa28482
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:27:INFO:
[92mINFO [0m:      Received: evaluate message afb98075-ceca-4345-abfe-02bd480c2190
02/15/2025 01:29:27:INFO:Received: evaluate message afb98075-ceca-4345-abfe-02bd480c2190

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:29:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:11:INFO:
[92mINFO [0m:      Received: train message dae5a0c4-3bff-4c10-81b2-3950711ebf68
02/15/2025 01:30:11:INFO:Received: train message dae5a0c4-3bff-4c10-81b2-3950711ebf68
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:30:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:23:INFO:
[92mINFO [0m:      Received: evaluate message dfe611fa-37f1-4b9a-840a-12bf1be5fd71
02/15/2025 01:31:23:INFO:Received: evaluate message dfe611fa-37f1-4b9a-840a-12bf1be5fd71
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:31:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:18:INFO:
[92mINFO [0m:      Received: train message 198b68e3-b5e6-499d-818d-3d358bbd99ff
02/15/2025 01:32:18:INFO:Received: train message 198b68e3-b5e6-499d-818d-3d358bbd99ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:32:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:54:INFO:
[92mINFO [0m:      Received: evaluate message b10ace5f-18a3-41ff-8f07-64b04777b6c0
02/15/2025 01:33:54:INFO:Received: evaluate message b10ace5f-18a3-41ff-8f07-64b04777b6c0

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:32:INFO:
[92mINFO [0m:      Received: train message 36b54418-6f87-4a6a-aeaa-99838043f2f0
02/15/2025 01:34:32:INFO:Received: train message 36b54418-6f87-4a6a-aeaa-99838043f2f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:35:47:INFO:
[92mINFO [0m:      Received: evaluate message 94a4164a-cb36-4ada-b9eb-f014c3f8664d
02/15/2025 01:35:47:INFO:Received: evaluate message 94a4164a-cb36-4ada-b9eb-f014c3f8664d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:35:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:46:INFO:
[92mINFO [0m:      Received: train message 01147e32-7e2d-49b8-80b4-fee93b5475f1
02/15/2025 01:36:46:INFO:Received: train message 01147e32-7e2d-49b8-80b4-fee93b5475f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:22:INFO:
[92mINFO [0m:      Received: evaluate message 5e2c79b4-58e3-496f-b234-e5008636c7c0
02/15/2025 01:38:22:INFO:Received: evaluate message 5e2c79b4-58e3-496f-b234-e5008636c7c0

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:38:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:39:INFO:
[92mINFO [0m:      Received: train message e62d444b-f74b-41d3-9dc7-c9dbf6242d6c
02/15/2025 01:38:39:INFO:Received: train message e62d444b-f74b-41d3-9dc7-c9dbf6242d6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:39:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:40:26:INFO:
[92mINFO [0m:      Received: evaluate message 2a6d47cc-70c8-4aca-81cf-92106f26a2f9
02/15/2025 01:40:26:INFO:Received: evaluate message 2a6d47cc-70c8-4aca-81cf-92106f26a2f9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:40:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:10:INFO:
[92mINFO [0m:      Received: train message 0810ab37-964e-4cf9-a453-f655673f093a
02/15/2025 01:41:10:INFO:Received: train message 0810ab37-964e-4cf9-a453-f655673f093a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:41:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:16:INFO:
[92mINFO [0m:      Received: evaluate message 1b31c4ca-0e31-4be2-87a8-6bf28968791c
02/15/2025 01:42:16:INFO:Received: evaluate message 1b31c4ca-0e31-4be2-87a8-6bf28968791c

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:42:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:03:INFO:
[92mINFO [0m:      Received: train message 0662751c-c40b-4561-8f26-7eabecb0e58f
02/15/2025 01:43:03:INFO:Received: train message 0662751c-c40b-4561-8f26-7eabecb0e58f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:43:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:36:INFO:
[92mINFO [0m:      Received: evaluate message 232ccc82-65c6-413d-acc8-e52bf698f6af
02/15/2025 01:44:36:INFO:Received: evaluate message 232ccc82-65c6-413d-acc8-e52bf698f6af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:44:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:11:INFO:
[92mINFO [0m:      Received: train message 5dc53452-42ba-45f4-803d-8b33848438a6
02/15/2025 01:45:11:INFO:Received: train message 5dc53452-42ba-45f4-803d-8b33848438a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:44:INFO:
[92mINFO [0m:      Received: evaluate message 60c77bed-dc63-4bfb-9b1a-cfbf3ec8b0db
02/15/2025 01:46:44:INFO:Received: evaluate message 60c77bed-dc63-4bfb-9b1a-cfbf3ec8b0db

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:47:32:INFO:
[92mINFO [0m:      Received: train message 0f743ceb-b00a-4f57-b95e-85f4f1aa0bfd
02/15/2025 01:47:32:INFO:Received: train message 0f743ceb-b00a-4f57-b95e-85f4f1aa0bfd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:48:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:34:INFO:
[92mINFO [0m:      Received: evaluate message 35828d47-9a6b-4494-9071-1fc27e0d0f83
02/15/2025 01:48:34:INFO:Received: evaluate message 35828d47-9a6b-4494-9071-1fc27e0d0f83
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:39:INFO:
[92mINFO [0m:      Received: train message a8f95265-0e3e-405e-8f31-d0045f2a6ee1
02/15/2025 01:49:39:INFO:Received: train message a8f95265-0e3e-405e-8f31-d0045f2a6ee1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:11:INFO:
[92mINFO [0m:      Received: evaluate message 368f1016-d3e0-4489-b0bc-e40c7b8ee14b
02/15/2025 01:51:11:INFO:Received: evaluate message 368f1016-d3e0-4489-b0bc-e40c7b8ee14b

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:51:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:13:INFO:
[92mINFO [0m:      Received: reconnect message 45484589-7746-46c3-a4f7-3801a886c027
02/15/2025 01:51:13:INFO:Received: reconnect message 45484589-7746-46c3-a4f7-3801a886c027
02/15/2025 01:51:13:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:51:13:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}



Final client history:
{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}

