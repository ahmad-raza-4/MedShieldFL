nohup: ignoring input
02/17/2025 15:49:22:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:49:22:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:49:22:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739836162.650144  749061 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:50:19:INFO:
[92mINFO [0m:      Received: train message 67a26a24-23dd-4eb4-874d-44ad5cbfa669
02/17/2025 15:50:19:INFO:Received: train message 67a26a24-23dd-4eb4-874d-44ad5cbfa669
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:50:58:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:22:INFO:
[92mINFO [0m:      Received: evaluate message 73f6c340-6c1a-4f92-ac2c-612abfce7826
02/17/2025 15:52:22:INFO:Received: evaluate message 73f6c340-6c1a-4f92-ac2c-612abfce7826
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:53:10:INFO:
[92mINFO [0m:      Received: train message a31dd73f-ee2e-464b-a47f-c6adeec09af3
02/17/2025 15:53:10:INFO:Received: train message a31dd73f-ee2e-464b-a47f-c6adeec09af3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:53:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:48:INFO:
[92mINFO [0m:      Received: evaluate message 696bcf04-9461-4889-90ef-438bf4b3c8fb
02/17/2025 15:54:48:INFO:Received: evaluate message 696bcf04-9461-4889-90ef-438bf4b3c8fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:40:INFO:
[92mINFO [0m:      Received: train message 8b863479-9253-4015-9a13-cfa9af17e229
02/17/2025 15:55:40:INFO:Received: train message 8b863479-9253-4015-9a13-cfa9af17e229
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:56:16:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:56:58:INFO:
[92mINFO [0m:      Received: evaluate message ce54bdec-ec42-4339-965c-d7d52476fe5c
02/17/2025 15:56:58:INFO:Received: evaluate message ce54bdec-ec42-4339-965c-d7d52476fe5c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:57:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:58:INFO:
[92mINFO [0m:      Received: train message 0cd7120d-9745-44b7-9d72-d291acdd63c2
02/17/2025 15:57:58:INFO:Received: train message 0cd7120d-9745-44b7-9d72-d291acdd63c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:38:INFO:
[92mINFO [0m:      Received: evaluate message d5ae61e2-4bd3-4215-b2c2-f12fd21847ec
02/17/2025 15:59:38:INFO:Received: evaluate message d5ae61e2-4bd3-4215-b2c2-f12fd21847ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:00:18:INFO:
[92mINFO [0m:      Received: train message 08c52738-1ccd-41f9-81ec-4c4e2630a547
02/17/2025 16:00:18:INFO:Received: train message 08c52738-1ccd-41f9-81ec-4c4e2630a547
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:00:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:58:INFO:
[92mINFO [0m:      Received: evaluate message ba9aaff8-770c-4821-9c40-ea264b4b5711
02/17/2025 16:01:58:INFO:Received: evaluate message ba9aaff8-770c-4821-9c40-ea264b4b5711
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:02:01:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:02:35:INFO:
[92mINFO [0m:      Received: train message 4afb26c7-78b8-4fc2-a3d2-0a99f15597c7
02/17/2025 16:02:35:INFO:Received: train message 4afb26c7-78b8-4fc2-a3d2-0a99f15597c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:03:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:53:INFO:
[92mINFO [0m:      Received: evaluate message 80a9a48e-7b27-45b4-a6cd-d499e10aa567
02/17/2025 16:03:53:INFO:Received: evaluate message 80a9a48e-7b27-45b4-a6cd-d499e10aa567
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:03:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:47:INFO:
[92mINFO [0m:      Received: train message ea67dfac-9ef3-465f-8987-f6122572788c
02/17/2025 16:04:47:INFO:Received: train message ea67dfac-9ef3-465f-8987-f6122572788c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:05:17:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:34:INFO:
[92mINFO [0m:      Received: evaluate message a55f3fae-c132-4d5c-b20b-b14576bae15a
02/17/2025 16:06:35:INFO:Received: evaluate message a55f3fae-c132-4d5c-b20b-b14576bae15a
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427], 'accuracy': [0.5019546520719312], 'auc': [0.6965369804720182], 'precision': [0.4088159391766196], 'recall': [0.5019546520719312], 'f1': [0.3521569235349163]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708], 'accuracy': [0.5019546520719312, 0.5301016419077405], 'auc': [0.6965369804720182, 0.6835920593265211], 'precision': [0.4088159391766196, 0.461975793404503], 'recall': [0.5019546520719312, 0.5301016419077405], 'f1': [0.3521569235349163, 0.4922183718303565]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:06:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:07:10:INFO:
[92mINFO [0m:      Received: train message 98c28096-641c-418a-87a9-3936c7e8f3fb
02/17/2025 16:07:10:INFO:Received: train message 98c28096-641c-418a-87a9-3936c7e8f3fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:07:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:42:INFO:
[92mINFO [0m:      Received: evaluate message 19a4bb6b-e8cd-4d8d-9e27-5de301c88f78
02/17/2025 16:08:42:INFO:Received: evaluate message 19a4bb6b-e8cd-4d8d-9e27-5de301c88f78
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:09:33:INFO:
[92mINFO [0m:      Received: train message 58e16493-a14e-4213-9aa3-e4c622f28b09
02/17/2025 16:09:33:INFO:Received: train message 58e16493-a14e-4213-9aa3-e4c622f28b09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:10:09:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:10:57:INFO:
[92mINFO [0m:      Received: evaluate message ed09a3f4-96b6-4bc3-a642-0a4540a0405a
02/17/2025 16:10:57:INFO:Received: evaluate message ed09a3f4-96b6-4bc3-a642-0a4540a0405a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:10:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:55:INFO:
[92mINFO [0m:      Received: train message 462c702f-93c4-4453-818e-7b88c7fcd674
02/17/2025 16:11:55:INFO:Received: train message 462c702f-93c4-4453-818e-7b88c7fcd674
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:12:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:24:INFO:
[92mINFO [0m:      Received: evaluate message 839b493a-4e46-41e0-b0c3-6330aa71277d
02/17/2025 16:13:24:INFO:Received: evaluate message 839b493a-4e46-41e0-b0c3-6330aa71277d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:13:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:09:INFO:
[92mINFO [0m:      Received: train message 814cadbc-2b8a-47b7-8b08-6e46741d4f47
02/17/2025 16:14:09:INFO:Received: train message 814cadbc-2b8a-47b7-8b08-6e46741d4f47
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:14:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:37:INFO:
[92mINFO [0m:      Received: evaluate message d09f42e0-2249-408b-bdaa-f17bb734acb0
02/17/2025 16:15:37:INFO:Received: evaluate message d09f42e0-2249-408b-bdaa-f17bb734acb0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:15:40:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:33:INFO:
[92mINFO [0m:      Received: train message 421b2841-c110-405c-b68b-bbd7b1ec35df
02/17/2025 16:16:33:INFO:Received: train message 421b2841-c110-405c-b68b-bbd7b1ec35df

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:17:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:01:INFO:
[92mINFO [0m:      Received: evaluate message 2e94e4d8-98c8-45d6-a96f-06d2445c250f
02/17/2025 16:18:01:INFO:Received: evaluate message 2e94e4d8-98c8-45d6-a96f-06d2445c250f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:18:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:30:INFO:
[92mINFO [0m:      Received: train message 34d8fb45-18d9-4a0a-8240-54bf171e87a7
02/17/2025 16:18:30:INFO:Received: train message 34d8fb45-18d9-4a0a-8240-54bf171e87a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:19:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:20:32:INFO:
[92mINFO [0m:      Received: evaluate message 6ac29139-2645-4a9a-9f40-e4905ebc869c
02/17/2025 16:20:32:INFO:Received: evaluate message 6ac29139-2645-4a9a-9f40-e4905ebc869c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:20:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:20:51:INFO:
[92mINFO [0m:      Received: train message 1feec369-e882-4393-a531-12ff3736f785
02/17/2025 16:20:51:INFO:Received: train message 1feec369-e882-4393-a531-12ff3736f785
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:21:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:02:INFO:
[92mINFO [0m:      Received: evaluate message 4c70c57a-ea5b-4935-8ad0-1a9409919706
02/17/2025 16:23:02:INFO:Received: evaluate message 4c70c57a-ea5b-4935-8ad0-1a9409919706
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:44:INFO:
[92mINFO [0m:      Received: train message 1f840210-aac4-4b66-b39b-349244829db3
02/17/2025 16:23:44:INFO:Received: train message 1f840210-aac4-4b66-b39b-349244829db3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:22:INFO:
[92mINFO [0m:      Received: evaluate message ae840e70-3ad8-4f46-a8ef-fc5063531689
02/17/2025 16:25:22:INFO:Received: evaluate message ae840e70-3ad8-4f46-a8ef-fc5063531689
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:12:INFO:
[92mINFO [0m:      Received: train message bc9bd6e7-fdbc-4308-96b4-0745b74b39c3
02/17/2025 16:26:12:INFO:Received: train message bc9bd6e7-fdbc-4308-96b4-0745b74b39c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:26:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:42:INFO:
[92mINFO [0m:      Received: evaluate message 6c7019b0-20ac-42d0-9d49-cf3db710d11f
02/17/2025 16:27:42:INFO:Received: evaluate message 6c7019b0-20ac-42d0-9d49-cf3db710d11f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:27:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:41:INFO:
[92mINFO [0m:      Received: train message f62c2bff-0556-4656-a812-59ae956651eb
02/17/2025 16:28:41:INFO:Received: train message f62c2bff-0556-4656-a812-59ae956651eb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:20:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:25:INFO:
[92mINFO [0m:      Received: evaluate message 31110efd-7a86-4fe0-84a8-e5b62219a9cc
02/17/2025 16:30:25:INFO:Received: evaluate message 31110efd-7a86-4fe0-84a8-e5b62219a9cc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:31:08:INFO:
[92mINFO [0m:      Received: train message f7a05221-04d0-4f83-ac0d-1ea5c21b0003
02/17/2025 16:31:08:INFO:Received: train message f7a05221-04d0-4f83-ac0d-1ea5c21b0003
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:01:INFO:
[92mINFO [0m:      Received: evaluate message e6197983-39c5-4b94-8962-0f85150038f2
02/17/2025 16:33:01:INFO:Received: evaluate message e6197983-39c5-4b94-8962-0f85150038f2

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:33:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:43:INFO:
[92mINFO [0m:      Received: train message 8d31c9a7-cdb9-4bcd-bbcc-3881203aa068
02/17/2025 16:33:43:INFO:Received: train message 8d31c9a7-cdb9-4bcd-bbcc-3881203aa068
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:34:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:13:INFO:
[92mINFO [0m:      Received: evaluate message 42dc44f9-1fc9-4337-bdf0-a07d32d0a966
02/17/2025 16:35:13:INFO:Received: evaluate message 42dc44f9-1fc9-4337-bdf0-a07d32d0a966
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:35:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:44:INFO:
[92mINFO [0m:      Received: train message da5400a3-0259-413d-9e4b-b340b9620cda
02/17/2025 16:35:44:INFO:Received: train message da5400a3-0259-413d-9e4b-b340b9620cda
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:36:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:19:INFO:
[92mINFO [0m:      Received: evaluate message 6cce48c9-7832-4d4a-aa88-a018181369e9
02/17/2025 16:37:19:INFO:Received: evaluate message 6cce48c9-7832-4d4a-aa88-a018181369e9

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:37:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:07:INFO:
[92mINFO [0m:      Received: train message f16ceb36-e872-4a7f-b1d1-eb6de3c24fb2
02/17/2025 16:38:07:INFO:Received: train message f16ceb36-e872-4a7f-b1d1-eb6de3c24fb2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:38:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:16:INFO:
[92mINFO [0m:      Received: evaluate message 4ad7f78b-3256-49aa-961d-78248ab211b0
02/17/2025 16:39:16:INFO:Received: evaluate message 4ad7f78b-3256-49aa-961d-78248ab211b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:39:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:47:INFO:
[92mINFO [0m:      Received: train message 0961ca72-8692-4c14-864f-fc5eeb80f2f8
02/17/2025 16:39:47:INFO:Received: train message 0961ca72-8692-4c14-864f-fc5eeb80f2f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:40:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:09:INFO:
[92mINFO [0m:      Received: evaluate message 3762803a-fe71-496a-8699-198a61bc65ed
02/17/2025 16:41:09:INFO:Received: evaluate message 3762803a-fe71-496a-8699-198a61bc65ed

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:41:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:08:INFO:
[92mINFO [0m:      Received: train message e822ffe2-64fe-4fed-9b1b-f8f57cc1a039
02/17/2025 16:42:08:INFO:Received: train message e822ffe2-64fe-4fed-9b1b-f8f57cc1a039
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:42:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:33:INFO:
[92mINFO [0m:      Received: evaluate message 7aa9d8ab-9929-4c76-9ac6-e1565f50e2b0
02/17/2025 16:43:33:INFO:Received: evaluate message 7aa9d8ab-9929-4c76-9ac6-e1565f50e2b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:43:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:07:INFO:
[92mINFO [0m:      Received: train message b95d89b9-d291-4e5c-8012-7bf90c473492
02/17/2025 16:44:07:INFO:Received: train message b95d89b9-d291-4e5c-8012-7bf90c473492
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:44:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:34:INFO:
[92mINFO [0m:      Received: evaluate message 3e930c93-6abe-4f74-91ec-82c2dcf6d232
02/17/2025 16:45:34:INFO:Received: evaluate message 3e930c93-6abe-4f74-91ec-82c2dcf6d232

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:45:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:10:INFO:
[92mINFO [0m:      Received: train message dc75c121-e353-42ff-a5c3-bcbdcf17addf
02/17/2025 16:46:10:INFO:Received: train message dc75c121-e353-42ff-a5c3-bcbdcf17addf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:46:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:47:41:INFO:
[92mINFO [0m:      Received: evaluate message 76381f26-cc3a-4f5b-a752-2d2414f1dc82
02/17/2025 16:47:41:INFO:Received: evaluate message 76381f26-cc3a-4f5b-a752-2d2414f1dc82
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:47:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:17:INFO:
[92mINFO [0m:      Received: train message 3768ee04-3240-4e44-93b3-d9913d556f9c
02/17/2025 16:48:17:INFO:Received: train message 3768ee04-3240-4e44-93b3-d9913d556f9c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:48:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:49:42:INFO:
[92mINFO [0m:      Received: evaluate message c19d26cf-086d-457e-8585-1a57c1c22959
02/17/2025 16:49:42:INFO:Received: evaluate message c19d26cf-086d-457e-8585-1a57c1c22959

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:49:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:23:INFO:
[92mINFO [0m:      Received: train message edb59a95-045b-407c-9bc2-c97e46334a99
02/17/2025 16:50:23:INFO:Received: train message edb59a95-045b-407c-9bc2-c97e46334a99
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:50:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:51:54:INFO:
[92mINFO [0m:      Received: evaluate message 999ee9c4-0c41-4be7-b517-92a74ff878a3
02/17/2025 16:51:54:INFO:Received: evaluate message 999ee9c4-0c41-4be7-b517-92a74ff878a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:51:56:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:52:36:INFO:
[92mINFO [0m:      Received: train message d567f54d-a931-476f-b080-56f21732f086
02/17/2025 16:52:36:INFO:Received: train message d567f54d-a931-476f-b080-56f21732f086
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:53:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:53:57:INFO:
[92mINFO [0m:      Received: evaluate message 41adb08f-1fe3-4e5d-8f47-3c2b3ad1e985
02/17/2025 16:53:57:INFO:Received: evaluate message 41adb08f-1fe3-4e5d-8f47-3c2b3ad1e985

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:54:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:54:37:INFO:
[92mINFO [0m:      Received: train message 5a73bc43-ec09-49b6-8c92-be10f4558a88
02/17/2025 16:54:37:INFO:Received: train message 5a73bc43-ec09-49b6-8c92-be10f4558a88
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:55:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:55:55:INFO:
[92mINFO [0m:      Received: evaluate message 31314a0b-03ae-4893-a156-a17bde195e3e
02/17/2025 16:55:55:INFO:Received: evaluate message 31314a0b-03ae-4893-a156-a17bde195e3e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:55:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:56:23:INFO:
[92mINFO [0m:      Received: train message e76d6892-f520-439f-b3a9-d0a6c6471c72
02/17/2025 16:56:23:INFO:Received: train message e76d6892-f520-439f-b3a9-d0a6c6471c72
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:56:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:57:54:INFO:
[92mINFO [0m:      Received: evaluate message fda5c1b6-9215-4cd1-9322-cdeac90abdfb
02/17/2025 16:57:54:INFO:Received: evaluate message fda5c1b6-9215-4cd1-9322-cdeac90abdfb

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:57:56:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:58:05:INFO:
[92mINFO [0m:      Received: reconnect message 7b36794b-8967-496c-8301-e3e4362b27ff
02/17/2025 16:58:05:INFO:Received: reconnect message 7b36794b-8967-496c-8301-e3e4362b27ff
02/17/2025 16:58:06:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:58:06:INFO:Disconnect and shut down

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}



Final client history:
{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}

