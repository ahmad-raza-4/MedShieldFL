nohup: ignoring input
02/15/2025 00:47:34:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 00:47:34:DEBUG:ChannelConnectivity.IDLE
02/15/2025 00:47:34:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 00:47:34:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739609254.753534 1569686 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 00:48:10:INFO:
[92mINFO [0m:      Received: train message e8730bb8-4cd2-4c21-843a-3ce88757f250
02/15/2025 00:48:10:INFO:Received: train message e8730bb8-4cd2-4c21-843a-3ce88757f250
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:48:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:40:INFO:
[92mINFO [0m:      Received: evaluate message d626ab0e-15c2-4532-b1fa-f1e9b485f4c2
02/15/2025 00:49:40:INFO:Received: evaluate message d626ab0e-15c2-4532-b1fa-f1e9b485f4c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:49:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:50:16:INFO:
[92mINFO [0m:      Received: train message f7bc8f65-ef85-496c-a739-ca3471c73e24
02/15/2025 00:50:16:INFO:Received: train message f7bc8f65-ef85-496c-a739-ca3471c73e24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:50:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:43:INFO:
[92mINFO [0m:      Received: evaluate message 6abbd1db-97b1-485b-89ff-7fffa4b510a1
02/15/2025 00:51:43:INFO:Received: evaluate message 6abbd1db-97b1-485b-89ff-7fffa4b510a1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:52:19:INFO:
[92mINFO [0m:      Received: train message aacf3a4c-d24a-41a2-9382-73a7de00c774
02/15/2025 00:52:19:INFO:Received: train message aacf3a4c-d24a-41a2-9382-73a7de00c774
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:52:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:45:INFO:
[92mINFO [0m:      Received: evaluate message 5565a30b-aad6-4aaa-9fde-0e192bca4534
02/15/2025 00:53:45:INFO:Received: evaluate message 5565a30b-aad6-4aaa-9fde-0e192bca4534
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:54:09:INFO:
[92mINFO [0m:      Received: train message 8a50749f-dca1-478d-addf-6375c25e5d08
02/15/2025 00:54:09:INFO:Received: train message 8a50749f-dca1-478d-addf-6375c25e5d08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:54:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:54:INFO:
[92mINFO [0m:      Received: evaluate message c23c10bd-1aef-4ed1-8a78-b6905a09c443
02/15/2025 00:55:54:INFO:Received: evaluate message c23c10bd-1aef-4ed1-8a78-b6905a09c443
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:56:24:INFO:
[92mINFO [0m:      Received: train message d42242fc-be07-4ff2-a666-36ce2388916d
02/15/2025 00:56:24:INFO:Received: train message d42242fc-be07-4ff2-a666-36ce2388916d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:57:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:59:INFO:
[92mINFO [0m:      Received: evaluate message fa213e3f-4795-41b0-b3e2-9e6b457fb87d
02/15/2025 00:57:59:INFO:Received: evaluate message fa213e3f-4795-41b0-b3e2-9e6b457fb87d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:58:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:39:INFO:
[92mINFO [0m:      Received: train message 1865b188-f3ae-48f4-a56d-f33009f1cd3e
02/15/2025 00:58:39:INFO:Received: train message 1865b188-f3ae-48f4-a56d-f33009f1cd3e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:59:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:06:INFO:
[92mINFO [0m:      Received: evaluate message 155b3bfc-e429-48b5-b416-3ce868636ef1
02/15/2025 01:00:06:INFO:Received: evaluate message 155b3bfc-e429-48b5-b416-3ce868636ef1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:00:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:49:INFO:
[92mINFO [0m:      Received: train message acfd200d-e874-4949-b6d5-16e2fa11a0d8
02/15/2025 01:00:49:INFO:Received: train message acfd200d-e874-4949-b6d5-16e2fa11a0d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:11:INFO:
[92mINFO [0m:      Received: evaluate message d2566f7b-7c9c-4f63-981b-d18427ea45eb
02/15/2025 01:02:11:INFO:Received: evaluate message d2566f7b-7c9c-4f63-981b-d18427ea45eb
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497], 'accuracy': [0.4988272087568413], 'auc': [0.5004293699052179], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784], 'accuracy': [0.4988272087568413, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665], 'precision': [0.24999954080217573, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:57:INFO:
[92mINFO [0m:      Received: train message f4e8f949-0ae8-4b89-b523-f92fadb662ec
02/15/2025 01:02:57:INFO:Received: train message f4e8f949-0ae8-4b89-b523-f92fadb662ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:22:INFO:
[92mINFO [0m:      Received: evaluate message 0d14037b-7416-4d78-add5-64f0f44bad46
02/15/2025 01:04:22:INFO:Received: evaluate message 0d14037b-7416-4d78-add5-64f0f44bad46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:05:02:INFO:
[92mINFO [0m:      Received: train message 86948680-c1e9-4e97-9205-9cfffb120a0c
02/15/2025 01:05:02:INFO:Received: train message 86948680-c1e9-4e97-9205-9cfffb120a0c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:05:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:16:INFO:
[92mINFO [0m:      Received: evaluate message 9d154644-3f36-4972-bd57-b7e145bb3374
02/15/2025 01:06:16:INFO:Received: evaluate message 9d154644-3f36-4972-bd57-b7e145bb3374
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:42:INFO:
[92mINFO [0m:      Received: train message 97028afd-5041-484a-9bf3-b18dc57ea43b
02/15/2025 01:06:42:INFO:Received: train message 97028afd-5041-484a-9bf3-b18dc57ea43b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:07:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:21:INFO:
[92mINFO [0m:      Received: evaluate message 66ec3143-98a2-4c00-ba35-1184ec5317c1
02/15/2025 01:08:21:INFO:Received: evaluate message 66ec3143-98a2-4c00-ba35-1184ec5317c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:08:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:35:INFO:
[92mINFO [0m:      Received: train message fd78be63-8136-465b-bb5d-a4157f4012d6
02/15/2025 01:08:35:INFO:Received: train message fd78be63-8136-465b-bb5d-a4157f4012d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:09:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:15:INFO:
[92mINFO [0m:      Received: evaluate message 2939aef8-3f62-4b86-8dfe-b22b799b8bde
02/15/2025 01:10:15:INFO:Received: evaluate message 2939aef8-3f62-4b86-8dfe-b22b799b8bde
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:10:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:52:INFO:
[92mINFO [0m:      Received: train message cd2f9117-44b1-4a65-bca6-05231943b0cf
02/15/2025 01:10:52:INFO:Received: train message cd2f9117-44b1-4a65-bca6-05231943b0cf

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:11:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:12:INFO:
[92mINFO [0m:      Received: evaluate message 62084bd7-7c34-461f-a771-4801ca360d18
02/15/2025 01:12:12:INFO:Received: evaluate message 62084bd7-7c34-461f-a771-4801ca360d18
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:12:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:41:INFO:
[92mINFO [0m:      Received: train message 9f7f2ed5-4176-4934-9f5a-6af1afae8c7e
02/15/2025 01:12:41:INFO:Received: train message 9f7f2ed5-4176-4934-9f5a-6af1afae8c7e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:13:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:06:INFO:
[92mINFO [0m:      Received: evaluate message 064bdddc-1120-40ae-b937-63f4482ef2d3
02/15/2025 01:14:06:INFO:Received: evaluate message 064bdddc-1120-40ae-b937-63f4482ef2d3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:14:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:35:INFO:
[92mINFO [0m:      Received: train message 76d6a418-5ae3-429e-a582-9194a3d3156f
02/15/2025 01:14:35:INFO:Received: train message 76d6a418-5ae3-429e-a582-9194a3d3156f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:15:57:INFO:
[92mINFO [0m:      Received: evaluate message 1fc53bc6-c11a-4004-8964-d36b7b9791a5
02/15/2025 01:15:57:INFO:Received: evaluate message 1fc53bc6-c11a-4004-8964-d36b7b9791a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:15:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:34:INFO:
[92mINFO [0m:      Received: train message 119a27a5-3bac-4887-8431-20b5fe3246a6
02/15/2025 01:16:34:INFO:Received: train message 119a27a5-3bac-4887-8431-20b5fe3246a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:08:INFO:
[92mINFO [0m:      Received: evaluate message 81ea874b-5445-41f0-a393-22fec0708aa8
02/15/2025 01:18:08:INFO:Received: evaluate message 81ea874b-5445-41f0-a393-22fec0708aa8
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:40:INFO:
[92mINFO [0m:      Received: train message e7f6d538-7a27-4430-a67d-e9ee6b3c05c9
02/15/2025 01:18:40:INFO:Received: train message e7f6d538-7a27-4430-a67d-e9ee6b3c05c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:12:INFO:
[92mINFO [0m:      Received: evaluate message a13f3c7f-528f-47b7-97e0-113c781ce6c8
02/15/2025 01:20:12:INFO:Received: evaluate message a13f3c7f-528f-47b7-97e0-113c781ce6c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:55:INFO:
[92mINFO [0m:      Received: train message 55cbde21-f954-47dd-a862-0c81a2c68b90
02/15/2025 01:20:55:INFO:Received: train message 55cbde21-f954-47dd-a862-0c81a2c68b90
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:21:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:35:INFO:
[92mINFO [0m:      Received: evaluate message 5ba4f407-99d6-4cd6-91a8-5964c75b19fc
02/15/2025 01:22:35:INFO:Received: evaluate message 5ba4f407-99d6-4cd6-91a8-5964c75b19fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:22:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:34:INFO:
[92mINFO [0m:      Received: train message 1ae1d5f7-e69f-41b1-a7ee-b1074225614d
02/15/2025 01:23:34:INFO:Received: train message 1ae1d5f7-e69f-41b1-a7ee-b1074225614d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:02:INFO:
[92mINFO [0m:      Received: evaluate message 6ec22cc4-b513-4937-a3a6-8532f47a12f5
02/15/2025 01:25:02:INFO:Received: evaluate message 6ec22cc4-b513-4937-a3a6-8532f47a12f5

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:47:INFO:
[92mINFO [0m:      Received: train message eae33a2c-e12b-489c-a3ca-9276a63a86ed
02/15/2025 01:25:47:INFO:Received: train message eae33a2c-e12b-489c-a3ca-9276a63a86ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:11:INFO:
[92mINFO [0m:      Received: evaluate message a0f5c287-81d0-440c-875a-a92b2e6a7a8b
02/15/2025 01:27:11:INFO:Received: evaluate message a0f5c287-81d0-440c-875a-a92b2e6a7a8b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:54:INFO:
[92mINFO [0m:      Received: train message 86e7f1e9-5324-4249-9736-d4d0a209d2e3
02/15/2025 01:27:54:INFO:Received: train message 86e7f1e9-5324-4249-9736-d4d0a209d2e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:12:INFO:
[92mINFO [0m:      Received: evaluate message 4f3450da-5944-4786-b1a7-a093a9c2887a
02/15/2025 01:29:12:INFO:Received: evaluate message 4f3450da-5944-4786-b1a7-a093a9c2887a

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:29:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:52:INFO:
[92mINFO [0m:      Received: train message 614d953d-f49b-429d-a863-cd920e46438b
02/15/2025 01:29:52:INFO:Received: train message 614d953d-f49b-429d-a863-cd920e46438b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:30:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:39:INFO:
[92mINFO [0m:      Received: evaluate message 1d97e25e-6b94-431c-9190-5025f908c1d9
02/15/2025 01:31:39:INFO:Received: evaluate message 1d97e25e-6b94-431c-9190-5025f908c1d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:31:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:07:INFO:
[92mINFO [0m:      Received: train message 8d96ed2f-f2b1-42e5-b567-c3ad8466390b
02/15/2025 01:32:07:INFO:Received: train message 8d96ed2f-f2b1-42e5-b567-c3ad8466390b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:32:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:42:INFO:
[92mINFO [0m:      Received: evaluate message 4caa322e-ff92-49fa-8e34-048613dba3fe
02/15/2025 01:33:42:INFO:Received: evaluate message 4caa322e-ff92-49fa-8e34-048613dba3fe

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:05:INFO:
[92mINFO [0m:      Received: train message 7f3d99aa-40ed-4750-a124-9383800e98d2
02/15/2025 01:34:05:INFO:Received: train message 7f3d99aa-40ed-4750-a124-9383800e98d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:34:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:35:54:INFO:
[92mINFO [0m:      Received: evaluate message da566e63-915a-4062-a93a-55e00836012d
02/15/2025 01:35:54:INFO:Received: evaluate message da566e63-915a-4062-a93a-55e00836012d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:35:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:30:INFO:
[92mINFO [0m:      Received: train message e5a2bfcf-ab37-4ecc-bbb9-7506c6c91fb9
02/15/2025 01:36:30:INFO:Received: train message e5a2bfcf-ab37-4ecc-bbb9-7506c6c91fb9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:13:INFO:
[92mINFO [0m:      Received: evaluate message 3c79a558-3f36-491b-9ce0-d93f99c85ae6
02/15/2025 01:38:13:INFO:Received: evaluate message 3c79a558-3f36-491b-9ce0-d93f99c85ae6

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:38:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:55:INFO:
[92mINFO [0m:      Received: train message a887dec5-189d-4427-b719-ea78417cfa36
02/15/2025 01:38:55:INFO:Received: train message a887dec5-189d-4427-b719-ea78417cfa36
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:39:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:40:24:INFO:
[92mINFO [0m:      Received: evaluate message 3a1dc481-c4ca-49b5-b378-de4ca127a446
02/15/2025 01:40:24:INFO:Received: evaluate message 3a1dc481-c4ca-49b5-b378-de4ca127a446
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:40:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:07:INFO:
[92mINFO [0m:      Received: train message a86ef0ca-31aa-4568-aad1-b12bbbb0e004
02/15/2025 01:41:07:INFO:Received: train message a86ef0ca-31aa-4568-aad1-b12bbbb0e004
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:41:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:32:INFO:
[92mINFO [0m:      Received: evaluate message d33e2c18-6a8d-4058-897d-a3959344965a
02/15/2025 01:42:32:INFO:Received: evaluate message d33e2c18-6a8d-4058-897d-a3959344965a

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:42:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:15:INFO:
[92mINFO [0m:      Received: train message 0ac01669-a056-4f28-8716-bac755fc18bb
02/15/2025 01:43:15:INFO:Received: train message 0ac01669-a056-4f28-8716-bac755fc18bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:43:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:36:INFO:
[92mINFO [0m:      Received: evaluate message 3895dc36-53ac-4ed6-bfd1-1709c6cd7c6d
02/15/2025 01:44:36:INFO:Received: evaluate message 3895dc36-53ac-4ed6-bfd1-1709c6cd7c6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:44:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:17:INFO:
[92mINFO [0m:      Received: train message 2335ac18-b3fc-42f6-8ee6-cc8e113bf39c
02/15/2025 01:45:17:INFO:Received: train message 2335ac18-b3fc-42f6-8ee6-cc8e113bf39c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:45:INFO:
[92mINFO [0m:      Received: evaluate message 35e4b6b9-ae6d-4b04-a412-70680aea41b3
02/15/2025 01:46:45:INFO:Received: evaluate message 35e4b6b9-ae6d-4b04-a412-70680aea41b3

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:47:32:INFO:
[92mINFO [0m:      Received: train message 761a62a2-61e0-4517-9b53-c628b1127c7d
02/15/2025 01:47:32:INFO:Received: train message 761a62a2-61e0-4517-9b53-c628b1127c7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:48:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:43:INFO:
[92mINFO [0m:      Received: evaluate message 53912c77-1d77-49a7-9f3c-dbb9ed4e08b3
02/15/2025 01:48:43:INFO:Received: evaluate message 53912c77-1d77-49a7-9f3c-dbb9ed4e08b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:48:INFO:
[92mINFO [0m:      Received: train message 0814c04d-bb3a-4ffc-9375-ae76f94ac15c
02/15/2025 01:49:48:INFO:Received: train message 0814c04d-bb3a-4ffc-9375-ae76f94ac15c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:59:INFO:
[92mINFO [0m:      Received: evaluate message ea7be9a6-23b5-4e7c-ab7c-95746d3685c6
02/15/2025 01:50:59:INFO:Received: evaluate message ea7be9a6-23b5-4e7c-ab7c-95746d3685c6

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:51:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:13:INFO:
[92mINFO [0m:      Received: reconnect message 9ba299fd-0384-4044-be75-547ae8ad6259
02/15/2025 01:51:13:INFO:Received: reconnect message 9ba299fd-0384-4044-be75-547ae8ad6259
02/15/2025 01:51:13:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:51:13:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}



Final client history:
{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}

