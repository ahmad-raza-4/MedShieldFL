nohup: ignoring input
02/15/2025 00:47:43:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 00:47:43:DEBUG:ChannelConnectivity.IDLE
02/15/2025 00:47:43:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739609263.951138 1570100 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 00:48:19:INFO:
[92mINFO [0m:      Received: train message e60d8740-12d8-4d62-92eb-f13388370a5b
02/15/2025 00:48:19:INFO:Received: train message e60d8740-12d8-4d62-92eb-f13388370a5b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:49:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:38:INFO:
[92mINFO [0m:      Received: evaluate message 30d72f06-827f-420f-9876-77d381243325
02/15/2025 00:49:38:INFO:Received: evaluate message 30d72f06-827f-420f-9876-77d381243325
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:49:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:50:22:INFO:
[92mINFO [0m:      Received: train message 8b91527d-621c-455e-a434-917a021bf7a4
02/15/2025 00:50:22:INFO:Received: train message 8b91527d-621c-455e-a434-917a021bf7a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:51:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:32:INFO:
[92mINFO [0m:      Received: evaluate message 17f04253-cd85-4e8d-885b-f16de06fdfc2
02/15/2025 00:51:32:INFO:Received: evaluate message 17f04253-cd85-4e8d-885b-f16de06fdfc2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:52:26:INFO:
[92mINFO [0m:      Received: train message 015cda7a-c9b9-488b-9ecd-d77268aa5ae2
02/15/2025 00:52:26:INFO:Received: train message 015cda7a-c9b9-488b-9ecd-d77268aa5ae2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:53:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:51:INFO:
[92mINFO [0m:      Received: evaluate message 48a4b2a4-0e86-40f3-b9bc-e79288e8bc07
02/15/2025 00:53:51:INFO:Received: evaluate message 48a4b2a4-0e86-40f3-b9bc-e79288e8bc07
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:54:24:INFO:
[92mINFO [0m:      Received: train message e42710f5-3500-48b1-855d-08cc5028fafa
02/15/2025 00:54:24:INFO:Received: train message e42710f5-3500-48b1-855d-08cc5028fafa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:55:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:39:INFO:
[92mINFO [0m:      Received: evaluate message 668149a7-a909-4d30-9096-62aeb55b3659
02/15/2025 00:55:39:INFO:Received: evaluate message 668149a7-a909-4d30-9096-62aeb55b3659
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:56:33:INFO:
[92mINFO [0m:      Received: train message be5bc62e-0134-4043-8256-bef745c998f9
02/15/2025 00:56:33:INFO:Received: train message be5bc62e-0134-4043-8256-bef745c998f9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:57:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:41:INFO:
[92mINFO [0m:      Received: evaluate message e2a14f89-a447-4f18-b47a-8322d9b3d440
02/15/2025 00:57:41:INFO:Received: evaluate message e2a14f89-a447-4f18-b47a-8322d9b3d440
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:57:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:37:INFO:
[92mINFO [0m:      Received: train message 6ac0a1ac-3851-476b-bf7c-43967fac84cc
02/15/2025 00:58:37:INFO:Received: train message 6ac0a1ac-3851-476b-bf7c-43967fac84cc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:59:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:59:52:INFO:
[92mINFO [0m:      Received: evaluate message 656bfeb5-1ceb-46f0-b29b-f1b75e68d32d
02/15/2025 00:59:52:INFO:Received: evaluate message 656bfeb5-1ceb-46f0-b29b-f1b75e68d32d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:59:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:48:INFO:
[92mINFO [0m:      Received: train message 01a95fa9-7c3f-46f6-8d23-b8740535072c
02/15/2025 01:00:48:INFO:Received: train message 01a95fa9-7c3f-46f6-8d23-b8740535072c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:17:INFO:
[92mINFO [0m:      Received: evaluate message 0d39fecd-ffb9-47f3-b160-b6d609393a39
02/15/2025 01:02:17:INFO:Received: evaluate message 0d39fecd-ffb9-47f3-b160-b6d609393a39
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497], 'accuracy': [0.4988272087568413], 'auc': [0.5004293699052179], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784], 'accuracy': [0.4988272087568413, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665], 'precision': [0.24999954080217573, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:39:INFO:
[92mINFO [0m:      Received: train message d84b5b97-d71b-4b87-953e-44f75305d160
02/15/2025 01:02:39:INFO:Received: train message d84b5b97-d71b-4b87-953e-44f75305d160
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:07:INFO:
[92mINFO [0m:      Received: evaluate message 4b716717-0ca1-40dc-83fe-2888f315e08b
02/15/2025 01:04:07:INFO:Received: evaluate message 4b716717-0ca1-40dc-83fe-2888f315e08b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:45:INFO:
[92mINFO [0m:      Received: train message fb96c19d-267b-4770-9080-d33a7a8566d0
02/15/2025 01:04:45:INFO:Received: train message fb96c19d-267b-4770-9080-d33a7a8566d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:05:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:00:INFO:
[92mINFO [0m:      Received: evaluate message 2909e154-111e-40e8-ae71-25f067963ae4
02/15/2025 01:06:00:INFO:Received: evaluate message 2909e154-111e-40e8-ae71-25f067963ae4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:07:00:INFO:
[92mINFO [0m:      Received: train message c7a806ed-1b65-4607-9c14-7abb6ca4e8cb
02/15/2025 01:07:00:INFO:Received: train message c7a806ed-1b65-4607-9c14-7abb6ca4e8cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:07:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:21:INFO:
[92mINFO [0m:      Received: evaluate message c082f256-a9aa-4f23-9791-f9e5cb00af89
02/15/2025 01:08:21:INFO:Received: evaluate message c082f256-a9aa-4f23-9791-f9e5cb00af89
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:08:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:48:INFO:
[92mINFO [0m:      Received: train message f878cf22-bb1e-4a09-b9f0-667bad3b159e
02/15/2025 01:08:48:INFO:Received: train message f878cf22-bb1e-4a09-b9f0-667bad3b159e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:09:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:09:55:INFO:
[92mINFO [0m:      Received: evaluate message 2008fad2-e474-4aea-abac-303529478d0f
02/15/2025 01:09:55:INFO:Received: evaluate message 2008fad2-e474-4aea-abac-303529478d0f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:09:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:50:INFO:
[92mINFO [0m:      Received: train message c38fe6f2-0e48-4db5-9bfc-90a8f692c6d7
02/15/2025 01:10:50:INFO:Received: train message c38fe6f2-0e48-4db5-9bfc-90a8f692c6d7

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:11:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:02:INFO:
[92mINFO [0m:      Received: evaluate message cf63bbcb-878c-461f-bbe7-624a1638cef5
02/15/2025 01:12:02:INFO:Received: evaluate message cf63bbcb-878c-461f-bbe7-624a1638cef5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:12:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:47:INFO:
[92mINFO [0m:      Received: train message f6087b86-f115-4c6d-a4da-8407dc862d7d
02/15/2025 01:12:47:INFO:Received: train message f6087b86-f115-4c6d-a4da-8407dc862d7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:13:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:09:INFO:
[92mINFO [0m:      Received: evaluate message 428cbe1b-0f10-45b8-ab0c-5320d903fc3d
02/15/2025 01:14:09:INFO:Received: evaluate message 428cbe1b-0f10-45b8-ab0c-5320d903fc3d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:14:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:32:INFO:
[92mINFO [0m:      Received: train message 21392eca-3e49-4859-a193-0ee576bb1dee
02/15/2025 01:14:32:INFO:Received: train message 21392eca-3e49-4859-a193-0ee576bb1dee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:15:51:INFO:
[92mINFO [0m:      Received: evaluate message b0cc0234-c8f8-4709-8e26-0fc6a1340b2c
02/15/2025 01:15:51:INFO:Received: evaluate message b0cc0234-c8f8-4709-8e26-0fc6a1340b2c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:15:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:47:INFO:
[92mINFO [0m:      Received: train message e20ebf46-57f2-481e-8788-73ea5d670bcc
02/15/2025 01:16:47:INFO:Received: train message e20ebf46-57f2-481e-8788-73ea5d670bcc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:17:57:INFO:
[92mINFO [0m:      Received: evaluate message a542d19b-acb8-4d15-ae7e-71e4a4d34144
02/15/2025 01:17:57:INFO:Received: evaluate message a542d19b-acb8-4d15-ae7e-71e4a4d34144
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:52:INFO:
[92mINFO [0m:      Received: train message f850bda5-1ad0-4c11-9734-fd98be394a56
02/15/2025 01:18:52:INFO:Received: train message f850bda5-1ad0-4c11-9734-fd98be394a56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:27:INFO:
[92mINFO [0m:      Received: evaluate message 3d84c624-ad8b-4f48-8e93-2859a9ed1582
02/15/2025 01:20:27:INFO:Received: evaluate message 3d84c624-ad8b-4f48-8e93-2859a9ed1582
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:02:INFO:
[92mINFO [0m:      Received: train message 68aa1e43-11fd-4a46-9064-3b8ec6305db9
02/15/2025 01:21:02:INFO:Received: train message 68aa1e43-11fd-4a46-9064-3b8ec6305db9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:40:INFO:
[92mINFO [0m:      Received: evaluate message 0faa6aa0-a955-42a1-b77c-2f4428992d62
02/15/2025 01:22:40:INFO:Received: evaluate message 0faa6aa0-a955-42a1-b77c-2f4428992d62
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:22:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:27:INFO:
[92mINFO [0m:      Received: train message b924c0a6-a110-4aee-8c3c-2f6b7ea4c24b
02/15/2025 01:23:27:INFO:Received: train message b924c0a6-a110-4aee-8c3c-2f6b7ea4c24b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:55:INFO:
[92mINFO [0m:      Received: evaluate message bb5a7061-6497-4527-ada6-f6fba11b1b20
02/15/2025 01:24:55:INFO:Received: evaluate message bb5a7061-6497-4527-ada6-f6fba11b1b20

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:24:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:43:INFO:
[92mINFO [0m:      Received: train message a3fb87eb-94c9-4703-8325-8a40605728b0
02/15/2025 01:25:43:INFO:Received: train message a3fb87eb-94c9-4703-8325-8a40605728b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:14:INFO:
[92mINFO [0m:      Received: evaluate message 02759934-c85b-484c-b816-dc688e99183e
02/15/2025 01:27:14:INFO:Received: evaluate message 02759934-c85b-484c-b816-dc688e99183e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:56:INFO:
[92mINFO [0m:      Received: train message 8bee13fd-1482-447c-9e42-6cb83691ad49
02/15/2025 01:27:56:INFO:Received: train message 8bee13fd-1482-447c-9e42-6cb83691ad49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:29:INFO:
[92mINFO [0m:      Received: evaluate message ac9e9ab1-590d-4a20-be19-b0eba3b09a44
02/15/2025 01:29:29:INFO:Received: evaluate message ac9e9ab1-590d-4a20-be19-b0eba3b09a44

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:29:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:07:INFO:
[92mINFO [0m:      Received: train message 67bed112-d5b5-40f2-b323-c84687a26ac5
02/15/2025 01:30:07:INFO:Received: train message 67bed112-d5b5-40f2-b323-c84687a26ac5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:30:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:40:INFO:
[92mINFO [0m:      Received: evaluate message e393c3eb-80ad-44de-95a5-4e3d6661bc9c
02/15/2025 01:31:40:INFO:Received: evaluate message e393c3eb-80ad-44de-95a5-4e3d6661bc9c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:31:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:24:INFO:
[92mINFO [0m:      Received: train message a37aff14-fdb9-4a4d-ace4-7bc38ab73667
02/15/2025 01:32:24:INFO:Received: train message a37aff14-fdb9-4a4d-ace4-7bc38ab73667
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:48:INFO:
[92mINFO [0m:      Received: evaluate message 4f47af89-43a8-49fe-aafd-4bc6637554d2
02/15/2025 01:33:48:INFO:Received: evaluate message 4f47af89-43a8-49fe-aafd-4bc6637554d2

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:34:INFO:
[92mINFO [0m:      Received: train message 3bfdbbfd-f92a-41b8-adbb-0306dc03e981
02/15/2025 01:34:34:INFO:Received: train message 3bfdbbfd-f92a-41b8-adbb-0306dc03e981
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:05:INFO:
[92mINFO [0m:      Received: evaluate message dc2e56ea-e4c8-4ee9-991e-185a8972a9e4
02/15/2025 01:36:05:INFO:Received: evaluate message dc2e56ea-e4c8-4ee9-991e-185a8972a9e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:46:INFO:
[92mINFO [0m:      Received: train message 3497818a-fb00-4eea-bc74-d7d1f651ffd3
02/15/2025 01:36:46:INFO:Received: train message 3497818a-fb00-4eea-bc74-d7d1f651ffd3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:10:INFO:
[92mINFO [0m:      Received: evaluate message 7658e681-d275-449d-a81d-58e50ee3ee78
02/15/2025 01:38:10:INFO:Received: evaluate message 7658e681-d275-449d-a81d-58e50ee3ee78

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:38:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:49:INFO:
[92mINFO [0m:      Received: train message 07dfb377-0c5e-4441-aaab-76abd757c872
02/15/2025 01:38:49:INFO:Received: train message 07dfb377-0c5e-4441-aaab-76abd757c872
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:39:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:40:25:INFO:
[92mINFO [0m:      Received: evaluate message f538a836-20c0-4981-8f81-3c9d3c3fa411
02/15/2025 01:40:25:INFO:Received: evaluate message f538a836-20c0-4981-8f81-3c9d3c3fa411
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:40:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:40:44:INFO:
[92mINFO [0m:      Received: train message 01d06784-c127-4886-aace-53f9f9ec7fbd
02/15/2025 01:40:44:INFO:Received: train message 01d06784-c127-4886-aace-53f9f9ec7fbd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:41:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:30:INFO:
[92mINFO [0m:      Received: evaluate message 9f785fd6-fafa-4fed-8ceb-9f47e9a9b094
02/15/2025 01:42:30:INFO:Received: evaluate message 9f785fd6-fafa-4fed-8ceb-9f47e9a9b094

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:42:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:12:INFO:
[92mINFO [0m:      Received: train message 5ed636ab-3dab-42fe-b4dc-8f660e5cb068
02/15/2025 01:43:12:INFO:Received: train message 5ed636ab-3dab-42fe-b4dc-8f660e5cb068
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:44:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:38:INFO:
[92mINFO [0m:      Received: evaluate message 560f48d4-6a73-431f-a1fa-ab95341e4376
02/15/2025 01:44:38:INFO:Received: evaluate message 560f48d4-6a73-431f-a1fa-ab95341e4376
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:44:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:25:INFO:
[92mINFO [0m:      Received: train message 7f84e5fd-9c10-46ba-96a7-64836ae02ca7
02/15/2025 01:45:25:INFO:Received: train message 7f84e5fd-9c10-46ba-96a7-64836ae02ca7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:46:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:42:INFO:
[92mINFO [0m:      Received: evaluate message 17db132d-dd66-44ea-8f2b-8369b0f56d90
02/15/2025 01:46:42:INFO:Received: evaluate message 17db132d-dd66-44ea-8f2b-8369b0f56d90

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:47:19:INFO:
[92mINFO [0m:      Received: train message d9d61c46-b9ed-4d46-a627-8f778199368b
02/15/2025 01:47:19:INFO:Received: train message d9d61c46-b9ed-4d46-a627-8f778199368b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:48:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:49:INFO:
[92mINFO [0m:      Received: evaluate message e7f8a525-110d-42c5-a3e3-6806e2e3862a
02/15/2025 01:48:49:INFO:Received: evaluate message e7f8a525-110d-42c5-a3e3-6806e2e3862a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:16:INFO:
[92mINFO [0m:      Received: train message 8199a6b5-7d6e-4b1a-88c6-e8894a3a3096
02/15/2025 01:49:16:INFO:Received: train message 8199a6b5-7d6e-4b1a-88c6-e8894a3a3096
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:51:INFO:
[92mINFO [0m:      Received: evaluate message 63eee491-3893-462c-8373-8fe277ee0004
02/15/2025 01:50:51:INFO:Received: evaluate message 63eee491-3893-462c-8373-8fe277ee0004

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:13:INFO:
[92mINFO [0m:      Received: reconnect message a2bcf9e7-d7f1-40f8-94a1-6cc64192e6d8
02/15/2025 01:51:13:INFO:Received: reconnect message a2bcf9e7-d7f1-40f8-94a1-6cc64192e6d8
02/15/2025 01:51:13:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:51:13:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}



Final client history:
{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}

