nohup: ignoring input
02/17/2025 15:49:34:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:49:34:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:49:34:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739836174.438502  749528 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:50:20:INFO:
[92mINFO [0m:      Received: train message c1cfa92d-de77-4867-bd77-60cba9064c84
02/17/2025 15:50:20:INFO:Received: train message c1cfa92d-de77-4867-bd77-60cba9064c84
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:51:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:51:59:INFO:
[92mINFO [0m:      Received: evaluate message fb776029-9bee-4266-9397-b7cc85b350fc
02/17/2025 15:51:59:INFO:Received: evaluate message fb776029-9bee-4266-9397-b7cc85b350fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:53:05:INFO:
[92mINFO [0m:      Received: train message 703489cf-d286-4eea-8d64-c0bed47f04a3
02/17/2025 15:53:05:INFO:Received: train message 703489cf-d286-4eea-8d64-c0bed47f04a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:54:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:57:INFO:
[92mINFO [0m:      Received: evaluate message 8267aada-9c05-4d1b-bdfb-a5edfa155f51
02/17/2025 15:54:57:INFO:Received: evaluate message 8267aada-9c05-4d1b-bdfb-a5edfa155f51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:55:01:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:33:INFO:
[92mINFO [0m:      Received: train message 61a95a51-80fa-4181-b6d0-e2e34fb6449f
02/17/2025 15:55:33:INFO:Received: train message 61a95a51-80fa-4181-b6d0-e2e34fb6449f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:56:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:19:INFO:
[92mINFO [0m:      Received: evaluate message c1f2b270-b82e-4c71-a99e-4a0df8d73d0b
02/17/2025 15:57:19:INFO:Received: evaluate message c1f2b270-b82e-4c71-a99e-4a0df8d73d0b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:57:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:57:INFO:
[92mINFO [0m:      Received: train message 052058f3-9b5f-44d1-9c5a-3c3411569343
02/17/2025 15:57:57:INFO:Received: train message 052058f3-9b5f-44d1-9c5a-3c3411569343
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:34:INFO:
[92mINFO [0m:      Received: evaluate message 9626d41e-c489-4532-ab08-d18cc9472e18
02/17/2025 15:59:34:INFO:Received: evaluate message 9626d41e-c489-4532-ab08-d18cc9472e18
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:00:12:INFO:
[92mINFO [0m:      Received: train message 28ce5017-a46b-4ce4-808d-49d7e883ae91
02/17/2025 16:00:12:INFO:Received: train message 28ce5017-a46b-4ce4-808d-49d7e883ae91
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:01:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:57:INFO:
[92mINFO [0m:      Received: evaluate message af4c4142-602d-4e69-9c21-5755eeb11743
02/17/2025 16:01:57:INFO:Received: evaluate message af4c4142-602d-4e69-9c21-5755eeb11743
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:02:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:02:20:INFO:
[92mINFO [0m:      Received: train message f1f0c84a-71de-4e44-bd1f-610077417e75
02/17/2025 16:02:20:INFO:Received: train message f1f0c84a-71de-4e44-bd1f-610077417e75
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:03:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:13:INFO:
[92mINFO [0m:      Received: evaluate message ea0be1c2-1cc4-4ae8-97f7-71d1360b845e
02/17/2025 16:04:13:INFO:Received: evaluate message ea0be1c2-1cc4-4ae8-97f7-71d1360b845e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:04:16:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:53:INFO:
[92mINFO [0m:      Received: train message a0391365-fd75-4b6c-a3bb-42297247c09d
02/17/2025 16:04:53:INFO:Received: train message a0391365-fd75-4b6c-a3bb-42297247c09d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:05:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:34:INFO:
[92mINFO [0m:      Received: evaluate message 0e977e10-e2f4-42b5-861e-2707b449bd29
02/17/2025 16:06:35:INFO:Received: evaluate message 0e977e10-e2f4-42b5-861e-2707b449bd29
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427], 'accuracy': [0.5019546520719312], 'auc': [0.6965369804720182], 'precision': [0.4088159391766196], 'recall': [0.5019546520719312], 'f1': [0.3521569235349163]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708], 'accuracy': [0.5019546520719312, 0.5301016419077405], 'auc': [0.6965369804720182, 0.6835920593265211], 'precision': [0.4088159391766196, 0.461975793404503], 'recall': [0.5019546520719312, 0.5301016419077405], 'f1': [0.3521569235349163, 0.4922183718303565]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:06:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:07:19:INFO:
[92mINFO [0m:      Received: train message 9a34e722-2cd3-4ef3-ad3c-004fe98de708
02/17/2025 16:07:19:INFO:Received: train message 9a34e722-2cd3-4ef3-ad3c-004fe98de708
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:08:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:59:INFO:
[92mINFO [0m:      Received: evaluate message f4f66ee8-e8cd-4958-b0c0-d9d8ebb8d392
02/17/2025 16:08:59:INFO:Received: evaluate message f4f66ee8-e8cd-4958-b0c0-d9d8ebb8d392
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:09:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:09:37:INFO:
[92mINFO [0m:      Received: train message c1097316-1346-4a5e-a941-4ae7d4d4f2fd
02/17/2025 16:09:37:INFO:Received: train message c1097316-1346-4a5e-a941-4ae7d4d4f2fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:10:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:07:INFO:
[92mINFO [0m:      Received: evaluate message ed85d2b9-f5a6-4057-8267-6a99a820211e
02/17/2025 16:11:07:INFO:Received: evaluate message ed85d2b9-f5a6-4057-8267-6a99a820211e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:11:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:37:INFO:
[92mINFO [0m:      Received: train message f45f1b99-a0e8-4f25-b38f-aa1ba515ed92
02/17/2025 16:11:37:INFO:Received: train message f45f1b99-a0e8-4f25-b38f-aa1ba515ed92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:12:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:26:INFO:
[92mINFO [0m:      Received: evaluate message 3e24b211-c320-412e-94dc-0b0ee26768f6
02/17/2025 16:13:26:INFO:Received: evaluate message 3e24b211-c320-412e-94dc-0b0ee26768f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:13:30:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:51:INFO:
[92mINFO [0m:      Received: train message abe6d812-c6e5-43a6-b624-e26ef920414b
02/17/2025 16:13:51:INFO:Received: train message abe6d812-c6e5-43a6-b624-e26ef920414b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:14:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:51:INFO:
[92mINFO [0m:      Received: evaluate message f697e5cf-c648-47af-ac4d-49992389ac29
02/17/2025 16:15:51:INFO:Received: evaluate message f697e5cf-c648-47af-ac4d-49992389ac29
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:15:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:10:INFO:
[92mINFO [0m:      Received: train message e8d3a46f-f44d-4717-a644-6fadb6b414bc
02/17/2025 16:16:10:INFO:Received: train message e8d3a46f-f44d-4717-a644-6fadb6b414bc

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:17:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:03:INFO:
[92mINFO [0m:      Received: evaluate message dbd681b8-f685-48cf-8853-8b2bce0c15b1
02/17/2025 16:18:03:INFO:Received: evaluate message dbd681b8-f685-48cf-8853-8b2bce0c15b1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:18:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:36:INFO:
[92mINFO [0m:      Received: train message ff1ac6b1-4f88-40ae-8941-db9645659cfd
02/17/2025 16:18:36:INFO:Received: train message ff1ac6b1-4f88-40ae-8941-db9645659cfd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:19:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:20:26:INFO:
[92mINFO [0m:      Received: evaluate message aa9f701a-d321-4fdf-85bc-bc8f2524a41e
02/17/2025 16:20:26:INFO:Received: evaluate message aa9f701a-d321-4fdf-85bc-bc8f2524a41e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:20:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:16:INFO:
[92mINFO [0m:      Received: train message 92b339ad-d4bb-47ff-afac-2ec591c6fa2b
02/17/2025 16:21:16:INFO:Received: train message 92b339ad-d4bb-47ff-afac-2ec591c6fa2b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:16:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:22:47:INFO:
[92mINFO [0m:      Received: evaluate message bb0aadcf-9b82-425b-a7e8-c93a9c2e4381
02/17/2025 16:22:47:INFO:Received: evaluate message bb0aadcf-9b82-425b-a7e8-c93a9c2e4381
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:22:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:44:INFO:
[92mINFO [0m:      Received: train message 827f9e81-4a05-4975-8f76-9bd4d71ec394
02/17/2025 16:23:44:INFO:Received: train message 827f9e81-4a05-4975-8f76-9bd4d71ec394
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:44:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:29:INFO:
[92mINFO [0m:      Received: evaluate message f168e344-9b5f-4329-a485-b0f4b8e402b9
02/17/2025 16:25:29:INFO:Received: evaluate message f168e344-9b5f-4329-a485-b0f4b8e402b9
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:10:INFO:
[92mINFO [0m:      Received: train message 8b3ce37a-edbe-4109-aa20-f8e309de9e96
02/17/2025 16:26:10:INFO:Received: train message 8b3ce37a-edbe-4109-aa20-f8e309de9e96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:27:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:42:INFO:
[92mINFO [0m:      Received: evaluate message 28672ff2-e833-4aa4-a1bc-3749cd7a86a6
02/17/2025 16:27:42:INFO:Received: evaluate message 28672ff2-e833-4aa4-a1bc-3749cd7a86a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:27:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:37:INFO:
[92mINFO [0m:      Received: train message 7e0c7893-3b76-4049-88d9-bd97f47bcc43
02/17/2025 16:28:37:INFO:Received: train message 7e0c7893-3b76-4049-88d9-bd97f47bcc43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:06:INFO:
[92mINFO [0m:      Received: evaluate message cdaa1514-02a4-4529-8b50-cd7e8504756d
02/17/2025 16:30:06:INFO:Received: evaluate message cdaa1514-02a4-4529-8b50-cd7e8504756d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:31:11:INFO:
[92mINFO [0m:      Received: train message 63a3ed91-b8b0-429d-ade0-5d6444c78c7f
02/17/2025 16:31:11:INFO:Received: train message 63a3ed91-b8b0-429d-ade0-5d6444c78c7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:32:13:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:58:INFO:
[92mINFO [0m:      Received: evaluate message 57b07929-9e51-4cc2-83d2-a5102bff8e81
02/17/2025 16:32:58:INFO:Received: evaluate message 57b07929-9e51-4cc2-83d2-a5102bff8e81

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:33:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:49:INFO:
[92mINFO [0m:      Received: train message 1257f0a4-be23-4461-87f9-9829c06d673c
02/17/2025 16:33:49:INFO:Received: train message 1257f0a4-be23-4461-87f9-9829c06d673c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:34:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:03:INFO:
[92mINFO [0m:      Received: evaluate message ccfffaaa-43f2-4b5b-ad58-4d0e10f0652b
02/17/2025 16:35:03:INFO:Received: evaluate message ccfffaaa-43f2-4b5b-ad58-4d0e10f0652b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:35:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:59:INFO:
[92mINFO [0m:      Received: train message b4843d58-fe33-421d-9353-e86fd25e886c
02/17/2025 16:35:59:INFO:Received: train message b4843d58-fe33-421d-9353-e86fd25e886c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:36:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:23:INFO:
[92mINFO [0m:      Received: evaluate message a62f5085-06b0-496b-b53a-d800c66c44bf
02/17/2025 16:37:23:INFO:Received: evaluate message a62f5085-06b0-496b-b53a-d800c66c44bf

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:37:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:56:INFO:
[92mINFO [0m:      Received: train message 8de14c61-9129-4233-819b-2a33c10345f5
02/17/2025 16:37:56:INFO:Received: train message 8de14c61-9129-4233-819b-2a33c10345f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:38:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:30:INFO:
[92mINFO [0m:      Received: evaluate message 7f9c32bb-7dbc-4c96-a23e-a9dce7e9008c
02/17/2025 16:39:30:INFO:Received: evaluate message 7f9c32bb-7dbc-4c96-a23e-a9dce7e9008c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:39:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:08:INFO:
[92mINFO [0m:      Received: train message a91fc444-2f91-4882-a312-42f58fb5fd1a
02/17/2025 16:40:08:INFO:Received: train message a91fc444-2f91-4882-a312-42f58fb5fd1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:40:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:29:INFO:
[92mINFO [0m:      Received: evaluate message 3c13446a-997b-4c14-83e5-b1557e526f96
02/17/2025 16:41:29:INFO:Received: evaluate message 3c13446a-997b-4c14-83e5-b1557e526f96

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:41:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:01:INFO:
[92mINFO [0m:      Received: train message 1878d97b-ad1b-4681-83c2-58fd387f6aae
02/17/2025 16:42:01:INFO:Received: train message 1878d97b-ad1b-4681-83c2-58fd387f6aae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:42:51:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:30:INFO:
[92mINFO [0m:      Received: evaluate message c3613263-daf4-4cde-9adf-535f04449722
02/17/2025 16:43:30:INFO:Received: evaluate message c3613263-daf4-4cde-9adf-535f04449722
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:43:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:11:INFO:
[92mINFO [0m:      Received: train message 078d6f44-e2f7-457e-ace1-6dcf75af12b6
02/17/2025 16:44:11:INFO:Received: train message 078d6f44-e2f7-457e-ace1-6dcf75af12b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:44:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:34:INFO:
[92mINFO [0m:      Received: evaluate message cda563ea-a19b-4fff-ba91-84b39868818a
02/17/2025 16:45:34:INFO:Received: evaluate message cda563ea-a19b-4fff-ba91-84b39868818a

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:45:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:58:INFO:
[92mINFO [0m:      Received: train message 043b8ba0-75f6-48b3-81f6-d96fdf09702d
02/17/2025 16:45:58:INFO:Received: train message 043b8ba0-75f6-48b3-81f6-d96fdf09702d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:46:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:47:31:INFO:
[92mINFO [0m:      Received: evaluate message e34e477a-797c-449d-bc40-1bac4b563e60
02/17/2025 16:47:31:INFO:Received: evaluate message e34e477a-797c-449d-bc40-1bac4b563e60
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:47:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:25:INFO:
[92mINFO [0m:      Received: train message 4d6f5fcd-37fe-4e7b-8022-4b76cb27e084
02/17/2025 16:48:25:INFO:Received: train message 4d6f5fcd-37fe-4e7b-8022-4b76cb27e084
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:49:56:INFO:
[92mINFO [0m:      Received: evaluate message 054de4b9-94dc-4a07-b29c-abf86e2bcf55
02/17/2025 16:49:56:INFO:Received: evaluate message 054de4b9-94dc-4a07-b29c-abf86e2bcf55

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:49:58:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:35:INFO:
[92mINFO [0m:      Received: train message ef2fc57f-ae51-416a-a95a-7f16c67e0b9b
02/17/2025 16:50:35:INFO:Received: train message ef2fc57f-ae51-416a-a95a-7f16c67e0b9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:51:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:51:44:INFO:
[92mINFO [0m:      Received: evaluate message fd9b2586-498d-4dd5-be4b-6b573ceef140
02/17/2025 16:51:44:INFO:Received: evaluate message fd9b2586-498d-4dd5-be4b-6b573ceef140
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:51:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:52:38:INFO:
[92mINFO [0m:      Received: train message 34b0584b-bcfe-4cea-9345-af03cd2dc30c
02/17/2025 16:52:38:INFO:Received: train message 34b0584b-bcfe-4cea-9345-af03cd2dc30c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:53:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:54:00:INFO:
[92mINFO [0m:      Received: evaluate message 2eeb28e8-7a8b-4a59-80c5-1b1701275f21
02/17/2025 16:54:00:INFO:Received: evaluate message 2eeb28e8-7a8b-4a59-80c5-1b1701275f21

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:54:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:54:21:INFO:
[92mINFO [0m:      Received: train message e4061e04-b93e-4b50-92fd-6f323607e021
02/17/2025 16:54:21:INFO:Received: train message e4061e04-b93e-4b50-92fd-6f323607e021
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:55:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:55:58:INFO:
[92mINFO [0m:      Received: evaluate message 687ac071-538a-4575-b93a-aa9fade9955e
02/17/2025 16:55:58:INFO:Received: evaluate message 687ac071-538a-4575-b93a-aa9fade9955e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:56:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:56:36:INFO:
[92mINFO [0m:      Received: train message 166867a3-f4c5-47d0-b1c8-8c5cfd5162c2
02/17/2025 16:56:36:INFO:Received: train message 166867a3-f4c5-47d0-b1c8-8c5cfd5162c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:57:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:57:44:INFO:
[92mINFO [0m:      Received: evaluate message 63924a76-843d-449f-aaf9-32ca5593770c
02/17/2025 16:57:44:INFO:Received: evaluate message 63924a76-843d-449f-aaf9-32ca5593770c

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:57:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:58:06:INFO:
[92mINFO [0m:      Received: reconnect message ce99ab6e-98f7-45b7-8513-ba37e9a0e539
02/17/2025 16:58:06:INFO:Received: reconnect message ce99ab6e-98f7-45b7-8513-ba37e9a0e539
02/17/2025 16:58:06:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:58:06:INFO:Disconnect and shut down

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}



Final client history:
{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}

