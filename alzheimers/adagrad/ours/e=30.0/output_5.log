nohup: ignoring input
02/17/2025 15:49:30:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:49:30:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:49:30:DEBUG:ChannelConnectivity.CONNECTING
02/17/2025 15:49:30:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739836170.064589  749364 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:50:19:INFO:
[92mINFO [0m:      Received: train message 8fec38c4-310d-4cee-9129-109beb92c47c
02/17/2025 15:50:19:INFO:Received: train message 8fec38c4-310d-4cee-9129-109beb92c47c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:51:09:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:16:INFO:
[92mINFO [0m:      Received: evaluate message 4959833d-6b43-405d-a7a1-d7c977f1f0de
02/17/2025 15:52:16:INFO:Received: evaluate message 4959833d-6b43-405d-a7a1-d7c977f1f0de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:57:INFO:
[92mINFO [0m:      Received: train message d80b2149-4010-421c-bd5e-d80c3cd79497
02/17/2025 15:52:57:INFO:Received: train message d80b2149-4010-421c-bd5e-d80c3cd79497
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:53:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:44:INFO:
[92mINFO [0m:      Received: evaluate message 3d1b2485-48d5-4bdc-9aee-479b5123800e
02/17/2025 15:54:44:INFO:Received: evaluate message 3d1b2485-48d5-4bdc-9aee-479b5123800e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:40:INFO:
[92mINFO [0m:      Received: train message 889c841e-1f6d-4538-8b7e-edffb9be6f63
02/17/2025 15:55:40:INFO:Received: train message 889c841e-1f6d-4538-8b7e-edffb9be6f63
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:56:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:11:INFO:
[92mINFO [0m:      Received: evaluate message 9398d0eb-bf12-40a2-bf4b-76de151a0fa8
02/17/2025 15:57:11:INFO:Received: evaluate message 9398d0eb-bf12-40a2-bf4b-76de151a0fa8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:57:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:52:INFO:
[92mINFO [0m:      Received: train message c969c12d-c1b0-488d-9d3e-189dbc8255fb
02/17/2025 15:57:52:INFO:Received: train message c969c12d-c1b0-488d-9d3e-189dbc8255fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:40:INFO:
[92mINFO [0m:      Received: evaluate message 72ca73ad-2857-43f1-80d8-ec37fe646b7f
02/17/2025 15:59:40:INFO:Received: evaluate message 72ca73ad-2857-43f1-80d8-ec37fe646b7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:00:18:INFO:
[92mINFO [0m:      Received: train message 8fc41fef-f219-4ffb-8af9-b3c0d9776a20
02/17/2025 16:00:18:INFO:Received: train message 8fc41fef-f219-4ffb-8af9-b3c0d9776a20
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:01:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:34:INFO:
[92mINFO [0m:      Received: evaluate message d544bb39-4363-4893-ab95-1c648ed8e9a3
02/17/2025 16:01:34:INFO:Received: evaluate message d544bb39-4363-4893-ab95-1c648ed8e9a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:02:39:INFO:
[92mINFO [0m:      Received: train message 3ca44529-6dcf-4671-ae4c-ee0569c2f80e
02/17/2025 16:02:39:INFO:Received: train message 3ca44529-6dcf-4671-ae4c-ee0569c2f80e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:03:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:03:52:INFO:
[92mINFO [0m:      Received: evaluate message 7e8619a4-738e-4f6a-b2b0-fc978305ce60
02/17/2025 16:03:52:INFO:Received: evaluate message 7e8619a4-738e-4f6a-b2b0-fc978305ce60
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:03:56:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:55:INFO:
[92mINFO [0m:      Received: train message aa990b06-47dd-4268-97cf-1b6a30432178
02/17/2025 16:04:55:INFO:Received: train message aa990b06-47dd-4268-97cf-1b6a30432178
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:05:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:28:INFO:
[92mINFO [0m:      Received: evaluate message dc9ef2e1-1067-4641-b824-957778a4e1a4
02/17/2025 16:06:28:INFO:Received: evaluate message dc9ef2e1-1067-4641-b824-957778a4e1a4
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427], 'accuracy': [0.5019546520719312], 'auc': [0.6965369804720182], 'precision': [0.4088159391766196], 'recall': [0.5019546520719312], 'f1': [0.3521569235349163]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708], 'accuracy': [0.5019546520719312, 0.5301016419077405], 'auc': [0.6965369804720182, 0.6835920593265211], 'precision': [0.4088159391766196, 0.461975793404503], 'recall': [0.5019546520719312, 0.5301016419077405], 'f1': [0.3521569235349163, 0.4922183718303565]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:06:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:07:20:INFO:
[92mINFO [0m:      Received: train message f8ec0487-236a-4f83-9fcd-4d3192bb0b9c
02/17/2025 16:07:20:INFO:Received: train message f8ec0487-236a-4f83-9fcd-4d3192bb0b9c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:08:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:56:INFO:
[92mINFO [0m:      Received: evaluate message d25348ba-ac01-4d95-b642-df8bdf666165
02/17/2025 16:08:56:INFO:Received: evaluate message d25348ba-ac01-4d95-b642-df8bdf666165
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:09:25:INFO:
[92mINFO [0m:      Received: train message 29e58dd1-5d86-4873-946c-c0b5d80f6702
02/17/2025 16:09:25:INFO:Received: train message 29e58dd1-5d86-4873-946c-c0b5d80f6702
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:10:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:17:INFO:
[92mINFO [0m:      Received: evaluate message e37207c1-f904-45ed-bb44-158abc3d58fb
02/17/2025 16:11:17:INFO:Received: evaluate message e37207c1-f904-45ed-bb44-158abc3d58fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:11:20:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:59:INFO:
[92mINFO [0m:      Received: train message 30382a4e-fee6-46a1-970b-9a853b6fec05
02/17/2025 16:11:59:INFO:Received: train message 30382a4e-fee6-46a1-970b-9a853b6fec05
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:12:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:23:INFO:
[92mINFO [0m:      Received: evaluate message 05d569de-7628-471b-adf5-ee8414dee769
02/17/2025 16:13:23:INFO:Received: evaluate message 05d569de-7628-471b-adf5-ee8414dee769
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:13:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:02:INFO:
[92mINFO [0m:      Received: train message 1afb88bd-c9d3-444e-9143-2aada733ff7e
02/17/2025 16:14:02:INFO:Received: train message 1afb88bd-c9d3-444e-9143-2aada733ff7e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:14:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:27:INFO:
[92mINFO [0m:      Received: evaluate message 1f546738-4bfd-4618-b6bd-ac10159006f6
02/17/2025 16:15:27:INFO:Received: evaluate message 1f546738-4bfd-4618-b6bd-ac10159006f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:15:31:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:29:INFO:
[92mINFO [0m:      Received: train message 72bf5b5e-cce2-4821-9e7f-588b311391ef
02/17/2025 16:16:29:INFO:Received: train message 72bf5b5e-cce2-4821-9e7f-588b311391ef

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:17:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:04:INFO:
[92mINFO [0m:      Received: evaluate message 0cb7d0b1-0db9-4b67-8e3c-65a06b1e2292
02/17/2025 16:18:04:INFO:Received: evaluate message 0cb7d0b1-0db9-4b67-8e3c-65a06b1e2292
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:18:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:40:INFO:
[92mINFO [0m:      Received: train message d346fa87-d29b-4724-9302-8d8dd50d4c64
02/17/2025 16:18:40:INFO:Received: train message d346fa87-d29b-4724-9302-8d8dd50d4c64
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:19:30:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:20:34:INFO:
[92mINFO [0m:      Received: evaluate message 3aa5fd50-5995-477e-8468-2e5651f1ac27
02/17/2025 16:20:34:INFO:Received: evaluate message 3aa5fd50-5995-477e-8468-2e5651f1ac27
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:20:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:18:INFO:
[92mINFO [0m:      Received: train message 43a69f6d-adf3-4311-937d-3fce551f484c
02/17/2025 16:21:18:INFO:Received: train message 43a69f6d-adf3-4311-937d-3fce551f484c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:04:INFO:
[92mINFO [0m:      Received: evaluate message 306db78e-0bdb-4699-9655-c87adfe97c3c
02/17/2025 16:23:04:INFO:Received: evaluate message 306db78e-0bdb-4699-9655-c87adfe97c3c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:07:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:40:INFO:
[92mINFO [0m:      Received: train message 17946739-3a1d-45c3-9c55-1f6a111abcd0
02/17/2025 16:23:40:INFO:Received: train message 17946739-3a1d-45c3-9c55-1f6a111abcd0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:11:INFO:
[92mINFO [0m:      Received: evaluate message 9ce0e1d8-068f-4719-8fc8-5061e7191ca4
02/17/2025 16:25:11:INFO:Received: evaluate message 9ce0e1d8-068f-4719-8fc8-5061e7191ca4
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:04:INFO:
[92mINFO [0m:      Received: train message 8e34e139-57f6-409d-a5ac-7d502fec5fdd
02/17/2025 16:26:04:INFO:Received: train message 8e34e139-57f6-409d-a5ac-7d502fec5fdd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:26:56:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:46:INFO:
[92mINFO [0m:      Received: evaluate message d4b986bb-aef2-4cf5-abb6-5fde9999f237
02/17/2025 16:27:46:INFO:Received: evaluate message d4b986bb-aef2-4cf5-abb6-5fde9999f237
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:27:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:42:INFO:
[92mINFO [0m:      Received: train message 8c4b96fa-1e41-4821-8810-2560699b09e2
02/17/2025 16:28:42:INFO:Received: train message 8c4b96fa-1e41-4821-8810-2560699b09e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:24:INFO:
[92mINFO [0m:      Received: evaluate message afce3cd3-aca9-41f4-9b38-6a74c9159f1a
02/17/2025 16:30:24:INFO:Received: evaluate message afce3cd3-aca9-41f4-9b38-6a74c9159f1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:31:00:INFO:
[92mINFO [0m:      Received: train message 03b54b06-24ba-4d46-89d8-93a09ffc2577
02/17/2025 16:31:00:INFO:Received: train message 03b54b06-24ba-4d46-89d8-93a09ffc2577
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:48:INFO:
[92mINFO [0m:      Received: evaluate message 6aa05356-d8be-449d-9e5b-6be2110734a6
02/17/2025 16:32:48:INFO:Received: evaluate message 6aa05356-d8be-449d-9e5b-6be2110734a6

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:32:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:40:INFO:
[92mINFO [0m:      Received: train message 3389b408-923d-4c1e-bf46-a6af298c8248
02/17/2025 16:33:40:INFO:Received: train message 3389b408-923d-4c1e-bf46-a6af298c8248
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:34:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:07:INFO:
[92mINFO [0m:      Received: evaluate message 0c04e873-1dd5-4b21-b31c-372eb23355fa
02/17/2025 16:35:07:INFO:Received: evaluate message 0c04e873-1dd5-4b21-b31c-372eb23355fa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:35:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:02:INFO:
[92mINFO [0m:      Received: train message bb9818ec-c480-4615-912d-867213c4ef5d
02/17/2025 16:36:02:INFO:Received: train message bb9818ec-c480-4615-912d-867213c4ef5d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:36:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:26:INFO:
[92mINFO [0m:      Received: evaluate message 1485f40f-d1d1-4a95-b466-d464e3ac2878
02/17/2025 16:37:26:INFO:Received: evaluate message 1485f40f-d1d1-4a95-b466-d464e3ac2878

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:37:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:09:INFO:
[92mINFO [0m:      Received: train message fd9e180f-08d0-4af0-8d1f-529e347ae5e7
02/17/2025 16:38:09:INFO:Received: train message fd9e180f-08d0-4af0-8d1f-529e347ae5e7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:38:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:25:INFO:
[92mINFO [0m:      Received: evaluate message 5d977c77-8e53-450d-9277-8fdc23930973
02/17/2025 16:39:25:INFO:Received: evaluate message 5d977c77-8e53-450d-9277-8fdc23930973
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:39:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:04:INFO:
[92mINFO [0m:      Received: train message b44698fd-99f2-428c-8e50-cb2a1fe5d2d9
02/17/2025 16:40:04:INFO:Received: train message b44698fd-99f2-428c-8e50-cb2a1fe5d2d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:40:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:29:INFO:
[92mINFO [0m:      Received: evaluate message af3133d3-9979-48cd-aedb-45711ab46a6f
02/17/2025 16:41:29:INFO:Received: evaluate message af3133d3-9979-48cd-aedb-45711ab46a6f

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:41:33:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:04:INFO:
[92mINFO [0m:      Received: train message b7a71766-707d-4cff-99b3-9a694f116a21
02/17/2025 16:42:04:INFO:Received: train message b7a71766-707d-4cff-99b3-9a694f116a21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:42:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:31:INFO:
[92mINFO [0m:      Received: evaluate message 849229e1-72af-425a-9334-f8701ec76089
02/17/2025 16:43:31:INFO:Received: evaluate message 849229e1-72af-425a-9334-f8701ec76089
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:43:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:03:INFO:
[92mINFO [0m:      Received: train message adf63417-7771-44f4-bb59-1814492b05a4
02/17/2025 16:44:03:INFO:Received: train message adf63417-7771-44f4-bb59-1814492b05a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:44:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:37:INFO:
[92mINFO [0m:      Received: evaluate message 685d5a6a-53f7-4789-8ab5-6dbddabe258e
02/17/2025 16:45:37:INFO:Received: evaluate message 685d5a6a-53f7-4789-8ab5-6dbddabe258e

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:45:41:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:15:INFO:
[92mINFO [0m:      Received: train message 9a841451-ee70-4b12-8833-5119faa81e5f
02/17/2025 16:46:15:INFO:Received: train message 9a841451-ee70-4b12-8833-5119faa81e5f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:46:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:47:36:INFO:
[92mINFO [0m:      Received: evaluate message b70474a9-2f24-488d-984b-602481165313
02/17/2025 16:47:36:INFO:Received: evaluate message b70474a9-2f24-488d-984b-602481165313
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:47:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:24:INFO:
[92mINFO [0m:      Received: train message 4c8fe124-be87-45ad-b1b0-b2a68a516fdf
02/17/2025 16:48:24:INFO:Received: train message 4c8fe124-be87-45ad-b1b0-b2a68a516fdf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:49:56:INFO:
[92mINFO [0m:      Received: evaluate message 498deb58-e827-4fba-9b93-de8ffbc8a405
02/17/2025 16:49:56:INFO:Received: evaluate message 498deb58-e827-4fba-9b93-de8ffbc8a405

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:49:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:31:INFO:
[92mINFO [0m:      Received: train message ce1414b3-a0e7-4276-9e31-902b3fde04f5
02/17/2025 16:50:31:INFO:Received: train message ce1414b3-a0e7-4276-9e31-902b3fde04f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:51:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:51:42:INFO:
[92mINFO [0m:      Received: evaluate message 5fcb7352-dcc0-4041-a089-26068a918e12
02/17/2025 16:51:42:INFO:Received: evaluate message 5fcb7352-dcc0-4041-a089-26068a918e12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:51:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:52:34:INFO:
[92mINFO [0m:      Received: train message ab28664d-b936-4fbb-955e-735d82071b01
02/17/2025 16:52:34:INFO:Received: train message ab28664d-b936-4fbb-955e-735d82071b01
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:53:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:54:02:INFO:
[92mINFO [0m:      Received: evaluate message b583b7b9-fbc7-4e33-b94b-ca78374d9eb5
02/17/2025 16:54:02:INFO:Received: evaluate message b583b7b9-fbc7-4e33-b94b-ca78374d9eb5

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:54:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:54:40:INFO:
[92mINFO [0m:      Received: train message eecd4bf8-04b7-4a33-bd06-d4a214f31ad1
02/17/2025 16:54:40:INFO:Received: train message eecd4bf8-04b7-4a33-bd06-d4a214f31ad1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:55:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:55:55:INFO:
[92mINFO [0m:      Received: evaluate message feae9362-1a43-473b-98fe-785a657198f7
02/17/2025 16:55:55:INFO:Received: evaluate message feae9362-1a43-473b-98fe-785a657198f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:55:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:56:23:INFO:
[92mINFO [0m:      Received: train message 9b730e14-20b4-4fd0-bc01-84eb4d2b9557
02/17/2025 16:56:23:INFO:Received: train message 9b730e14-20b4-4fd0-bc01-84eb4d2b9557
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:57:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:58:03:INFO:
[92mINFO [0m:      Received: evaluate message 94166da3-0ee3-4eb1-ad6e-5a8df7e6efaa
02/17/2025 16:58:03:INFO:Received: evaluate message 94166da3-0ee3-4eb1-ad6e-5a8df7e6efaa

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:58:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:58:06:INFO:
[92mINFO [0m:      Received: reconnect message 13960f14-89da-47a2-ab24-7bd486245af8
02/17/2025 16:58:06:INFO:Received: reconnect message 13960f14-89da-47a2-ab24-7bd486245af8
02/17/2025 16:58:06:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:58:06:INFO:Disconnect and shut down

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}



Final client history:
{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}

