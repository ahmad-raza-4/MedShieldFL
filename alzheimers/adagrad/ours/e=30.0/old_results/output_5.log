nohup: ignoring input
02/15/2025 00:47:40:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 00:47:40:DEBUG:ChannelConnectivity.IDLE
02/15/2025 00:47:40:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739609260.203938 1569945 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 00:48:09:INFO:
[92mINFO [0m:      Received: train message 19a056d7-13fe-40ff-a58c-a7ce1ddb48c7
02/15/2025 00:48:09:INFO:Received: train message 19a056d7-13fe-40ff-a58c-a7ce1ddb48c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:48:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:46:INFO:
[92mINFO [0m:      Received: evaluate message 2aacdc87-bc00-4191-9d57-d70f3ed30e67
02/15/2025 00:49:46:INFO:Received: evaluate message 2aacdc87-bc00-4191-9d57-d70f3ed30e67
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:49:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:50:22:INFO:
[92mINFO [0m:      Received: train message 1fdf5ef3-3261-40f6-8692-c6277c0b3b60
02/15/2025 00:50:22:INFO:Received: train message 1fdf5ef3-3261-40f6-8692-c6277c0b3b60
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:51:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:37:INFO:
[92mINFO [0m:      Received: evaluate message 336f96f1-cf3c-4bf3-b902-d7a7f69867fc
02/15/2025 00:51:37:INFO:Received: evaluate message 336f96f1-cf3c-4bf3-b902-d7a7f69867fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:52:20:INFO:
[92mINFO [0m:      Received: train message c7f4d78c-ba52-49c7-8a09-3938bcf473e0
02/15/2025 00:52:20:INFO:Received: train message c7f4d78c-ba52-49c7-8a09-3938bcf473e0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:52:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:43:INFO:
[92mINFO [0m:      Received: evaluate message 99661803-c8f1-40af-837b-177298e35f94
02/15/2025 00:53:43:INFO:Received: evaluate message 99661803-c8f1-40af-837b-177298e35f94
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:54:09:INFO:
[92mINFO [0m:      Received: train message 423b2009-777f-499e-8f06-cd303e7c500d
02/15/2025 00:54:09:INFO:Received: train message 423b2009-777f-499e-8f06-cd303e7c500d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:54:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:50:INFO:
[92mINFO [0m:      Received: evaluate message aeca0bd1-cd67-4060-baf1-6e5da2953896
02/15/2025 00:55:50:INFO:Received: evaluate message aeca0bd1-cd67-4060-baf1-6e5da2953896
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:56:36:INFO:
[92mINFO [0m:      Received: train message e7a9d713-72e2-47bb-af60-5e6a73fdbdba
02/15/2025 00:56:36:INFO:Received: train message e7a9d713-72e2-47bb-af60-5e6a73fdbdba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:57:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:52:INFO:
[92mINFO [0m:      Received: evaluate message ae21d728-f20d-4452-be5f-57e9b4d39cb9
02/15/2025 00:57:52:INFO:Received: evaluate message ae21d728-f20d-4452-be5f-57e9b4d39cb9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:57:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:21:INFO:
[92mINFO [0m:      Received: train message 422cc882-fbb6-432c-8def-045ed9f977e8
02/15/2025 00:58:21:INFO:Received: train message 422cc882-fbb6-432c-8def-045ed9f977e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:59:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:10:INFO:
[92mINFO [0m:      Received: evaluate message 2ca2742e-da2f-4c3f-afc7-c9696d53eff3
02/15/2025 01:00:10:INFO:Received: evaluate message 2ca2742e-da2f-4c3f-afc7-c9696d53eff3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:00:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:44:INFO:
[92mINFO [0m:      Received: train message 03fb1490-8280-4a8a-b241-a4ab1df4f755
02/15/2025 01:00:44:INFO:Received: train message 03fb1490-8280-4a8a-b241-a4ab1df4f755
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:15:INFO:
[92mINFO [0m:      Received: evaluate message 700b9d34-85ea-4299-a054-54ea17cc3ffe
02/15/2025 01:02:15:INFO:Received: evaluate message 700b9d34-85ea-4299-a054-54ea17cc3ffe
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497], 'accuracy': [0.4988272087568413], 'auc': [0.5004293699052179], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784], 'accuracy': [0.4988272087568413, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665], 'precision': [0.24999954080217573, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:50:INFO:
[92mINFO [0m:      Received: train message cd25e57d-69a5-4f33-bbfc-6d905545d0ba
02/15/2025 01:02:50:INFO:Received: train message cd25e57d-69a5-4f33-bbfc-6d905545d0ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:13:INFO:
[92mINFO [0m:      Received: evaluate message a6157917-acbc-4341-88e7-704fcdf7ba66
02/15/2025 01:04:13:INFO:Received: evaluate message a6157917-acbc-4341-88e7-704fcdf7ba66
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:54:INFO:
[92mINFO [0m:      Received: train message db6ba562-affc-4055-be78-f7a5eef9cd67
02/15/2025 01:04:54:INFO:Received: train message db6ba562-affc-4055-be78-f7a5eef9cd67
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:05:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:22:INFO:
[92mINFO [0m:      Received: evaluate message 28e79e9b-ff64-47cf-9521-c74120f8aac8
02/15/2025 01:06:22:INFO:Received: evaluate message 28e79e9b-ff64-47cf-9521-c74120f8aac8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:49:INFO:
[92mINFO [0m:      Received: train message bbc6bdaf-f3f2-4d5d-896e-9c7f647a9c5b
02/15/2025 01:06:49:INFO:Received: train message bbc6bdaf-f3f2-4d5d-896e-9c7f647a9c5b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:07:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:13:INFO:
[92mINFO [0m:      Received: evaluate message 8e691b46-28ef-448f-be6f-f1ddcef15151
02/15/2025 01:08:13:INFO:Received: evaluate message 8e691b46-28ef-448f-be6f-f1ddcef15151
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:08:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:59:INFO:
[92mINFO [0m:      Received: train message 7dda6c17-d41d-4c34-b2a7-b5cae309b2a5
02/15/2025 01:08:59:INFO:Received: train message 7dda6c17-d41d-4c34-b2a7-b5cae309b2a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:09:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:12:INFO:
[92mINFO [0m:      Received: evaluate message 8dae1f39-fa09-4016-ab20-57a961422642
02/15/2025 01:10:12:INFO:Received: evaluate message 8dae1f39-fa09-4016-ab20-57a961422642
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:10:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:49:INFO:
[92mINFO [0m:      Received: train message 94ce9dc5-d1b7-4424-a352-c5d16574cd7e
02/15/2025 01:10:49:INFO:Received: train message 94ce9dc5-d1b7-4424-a352-c5d16574cd7e

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:11:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:04:INFO:
[92mINFO [0m:      Received: evaluate message 358d325b-f5ab-49d5-8ce9-fe34938045ac
02/15/2025 01:12:04:INFO:Received: evaluate message 358d325b-f5ab-49d5-8ce9-fe34938045ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:12:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:43:INFO:
[92mINFO [0m:      Received: train message 1eea527e-2284-4b14-80fb-75d3069a4f89
02/15/2025 01:12:43:INFO:Received: train message 1eea527e-2284-4b14-80fb-75d3069a4f89
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:13:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:06:INFO:
[92mINFO [0m:      Received: evaluate message 8607587c-b8e4-47ab-94dc-3c04034990ee
02/15/2025 01:14:06:INFO:Received: evaluate message 8607587c-b8e4-47ab-94dc-3c04034990ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:14:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:32:INFO:
[92mINFO [0m:      Received: train message 41a1d309-7ca9-4f69-9340-99cc8b4e6180
02/15/2025 01:14:32:INFO:Received: train message 41a1d309-7ca9-4f69-9340-99cc8b4e6180
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:07:INFO:
[92mINFO [0m:      Received: evaluate message 21dbd492-7203-49f4-ad9b-ba90159c85ab
02/15/2025 01:16:07:INFO:Received: evaluate message 21dbd492-7203-49f4-ad9b-ba90159c85ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:16:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:35:INFO:
[92mINFO [0m:      Received: train message 4599277f-83cb-4101-b27c-3141fa7ed123
02/15/2025 01:16:35:INFO:Received: train message 4599277f-83cb-4101-b27c-3141fa7ed123
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:08:INFO:
[92mINFO [0m:      Received: evaluate message a1146300-8008-424a-8362-d5af752d3319
02/15/2025 01:18:08:INFO:Received: evaluate message a1146300-8008-424a-8362-d5af752d3319
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:52:INFO:
[92mINFO [0m:      Received: train message fbc34377-165d-44a0-979c-5eb6c634d784
02/15/2025 01:18:52:INFO:Received: train message fbc34377-165d-44a0-979c-5eb6c634d784
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:27:INFO:
[92mINFO [0m:      Received: evaluate message acb15605-486a-447f-bea7-8bd57a63faff
02/15/2025 01:20:27:INFO:Received: evaluate message acb15605-486a-447f-bea7-8bd57a63faff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:09:INFO:
[92mINFO [0m:      Received: train message b22ac3b2-d30e-4f57-b96a-f9fd5c572af9
02/15/2025 01:21:09:INFO:Received: train message b22ac3b2-d30e-4f57-b96a-f9fd5c572af9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:50:INFO:
[92mINFO [0m:      Received: evaluate message b811e63b-0ab7-445c-bd98-88e79cabce14
02/15/2025 01:22:50:INFO:Received: evaluate message b811e63b-0ab7-445c-bd98-88e79cabce14
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:22:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:31:INFO:
[92mINFO [0m:      Received: train message 1ec0a47a-5491-432c-b06c-fca99c66a207
02/15/2025 01:23:31:INFO:Received: train message 1ec0a47a-5491-432c-b06c-fca99c66a207
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:04:INFO:
[92mINFO [0m:      Received: evaluate message 9e5fafca-dac0-4919-95d2-f4db32793b90
02/15/2025 01:25:04:INFO:Received: evaluate message 9e5fafca-dac0-4919-95d2-f4db32793b90

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:26:INFO:
[92mINFO [0m:      Received: train message 64294b94-fe2d-4b82-9229-245b789647ca
02/15/2025 01:25:26:INFO:Received: train message 64294b94-fe2d-4b82-9229-245b789647ca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:19:INFO:
[92mINFO [0m:      Received: evaluate message 7d933539-e84d-4919-8894-21e883d007ec
02/15/2025 01:27:19:INFO:Received: evaluate message 7d933539-e84d-4919-8894-21e883d007ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:54:INFO:
[92mINFO [0m:      Received: train message f75fb356-a5a4-4e1d-b298-880e0628543e
02/15/2025 01:27:54:INFO:Received: train message f75fb356-a5a4-4e1d-b298-880e0628543e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:32:INFO:
[92mINFO [0m:      Received: evaluate message 988d87df-03f3-41e4-969a-5e53e6da5e5c
02/15/2025 01:29:32:INFO:Received: evaluate message 988d87df-03f3-41e4-969a-5e53e6da5e5c

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:29:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:00:INFO:
[92mINFO [0m:      Received: train message 418963d0-6868-4dd4-91fb-8bb29a5aacf1
02/15/2025 01:30:00:INFO:Received: train message 418963d0-6868-4dd4-91fb-8bb29a5aacf1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:30:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:21:INFO:
[92mINFO [0m:      Received: evaluate message 3481371d-9fd5-4036-b3ae-ee114ea898bd
02/15/2025 01:31:21:INFO:Received: evaluate message 3481371d-9fd5-4036-b3ae-ee114ea898bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:31:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:22:INFO:
[92mINFO [0m:      Received: train message 4b6a4ad9-fba1-4201-8504-d9f223fafdd5
02/15/2025 01:32:22:INFO:Received: train message 4b6a4ad9-fba1-4201-8504-d9f223fafdd5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:48:INFO:
[92mINFO [0m:      Received: evaluate message e10f5094-97a6-4d92-ab70-d7130ac27d90
02/15/2025 01:33:48:INFO:Received: evaluate message e10f5094-97a6-4d92-ab70-d7130ac27d90

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:17:INFO:
[92mINFO [0m:      Received: train message e9962df4-ab24-4516-a784-4b36aa47b2b5
02/15/2025 01:34:17:INFO:Received: train message e9962df4-ab24-4516-a784-4b36aa47b2b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:34:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:03:INFO:
[92mINFO [0m:      Received: evaluate message 12a9af87-7a50-4498-a7eb-50a5304d4fbc
02/15/2025 01:36:03:INFO:Received: evaluate message 12a9af87-7a50-4498-a7eb-50a5304d4fbc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:36:INFO:
[92mINFO [0m:      Received: train message 958b513b-8ba8-4bd2-9e65-de06d2784ba7
02/15/2025 01:36:36:INFO:Received: train message 958b513b-8ba8-4bd2-9e65-de06d2784ba7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:20:INFO:
[92mINFO [0m:      Received: evaluate message d966cb2f-91a8-4c86-bf78-9f5e05c68708
02/15/2025 01:38:20:INFO:Received: evaluate message d966cb2f-91a8-4c86-bf78-9f5e05c68708

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:38:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:02:INFO:
[92mINFO [0m:      Received: train message 63785c33-b2f6-43ce-a445-4cc8d630215e
02/15/2025 01:39:02:INFO:Received: train message 63785c33-b2f6-43ce-a445-4cc8d630215e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:39:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:40:25:INFO:
[92mINFO [0m:      Received: evaluate message ec847f36-7e45-4dee-ab9d-beb5e15f46ac
02/15/2025 01:40:25:INFO:Received: evaluate message ec847f36-7e45-4dee-ab9d-beb5e15f46ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:40:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:40:56:INFO:
[92mINFO [0m:      Received: train message d8f37ae1-1b0e-4300-ab53-4dad2c0a9fd1
02/15/2025 01:40:56:INFO:Received: train message d8f37ae1-1b0e-4300-ab53-4dad2c0a9fd1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:41:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:28:INFO:
[92mINFO [0m:      Received: evaluate message 2c73e32b-d243-449d-bade-e4e6113dd2bd
02/15/2025 01:42:28:INFO:Received: evaluate message 2c73e32b-d243-449d-bade-e4e6113dd2bd

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:42:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:16:INFO:
[92mINFO [0m:      Received: train message 4833e0f2-4ce9-48ea-82d9-28cf9863d603
02/15/2025 01:43:16:INFO:Received: train message 4833e0f2-4ce9-48ea-82d9-28cf9863d603
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:43:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:42:INFO:
[92mINFO [0m:      Received: evaluate message fca99fba-6d48-4d4b-a0ed-a2ac935700de
02/15/2025 01:44:42:INFO:Received: evaluate message fca99fba-6d48-4d4b-a0ed-a2ac935700de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:44:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:00:INFO:
[92mINFO [0m:      Received: train message c4949dac-abdd-45e3-9ba3-1e1c408f5b94
02/15/2025 01:45:00:INFO:Received: train message c4949dac-abdd-45e3-9ba3-1e1c408f5b94
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:38:INFO:
[92mINFO [0m:      Received: evaluate message 2ae55766-3c2d-4b3b-9a55-fa9b5828d1e8
02/15/2025 01:46:38:INFO:Received: evaluate message 2ae55766-3c2d-4b3b-9a55-fa9b5828d1e8

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:47:08:INFO:
[92mINFO [0m:      Received: train message 72080537-f230-4e3d-8789-1c0b30bf04d2
02/15/2025 01:47:08:INFO:Received: train message 72080537-f230-4e3d-8789-1c0b30bf04d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:55:INFO:
[92mINFO [0m:      Received: evaluate message 3da21329-f28d-4769-a874-bec6ffe5cac6
02/15/2025 01:48:55:INFO:Received: evaluate message 3da21329-f28d-4769-a874-bec6ffe5cac6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:46:INFO:
[92mINFO [0m:      Received: train message ac8b779c-8fdb-4a9f-81e5-e34928a8a807
02/15/2025 01:49:46:INFO:Received: train message ac8b779c-8fdb-4a9f-81e5-e34928a8a807
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:04:INFO:
[92mINFO [0m:      Received: evaluate message e0b0c30c-b645-4729-a9c2-74b946be5d18
02/15/2025 01:51:04:INFO:Received: evaluate message e0b0c30c-b645-4729-a9c2-74b946be5d18

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:51:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:13:INFO:
[92mINFO [0m:      Received: reconnect message 17f7f371-3adc-4cdd-9c9d-ad1140a1ec27
02/15/2025 01:51:13:INFO:Received: reconnect message 17f7f371-3adc-4cdd-9c9d-ad1140a1ec27
02/15/2025 01:51:13:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:51:13:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}



Final client history:
{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}

