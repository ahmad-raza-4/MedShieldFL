nohup: ignoring input
02/17/2025 15:49:28:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:49:28:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:49:28:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739836168.134488  749282 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:50:20:INFO:
[92mINFO [0m:      Received: train message df3d54fe-ccf7-4276-9a28-f164c29e1885
02/17/2025 15:50:20:INFO:Received: train message df3d54fe-ccf7-4276-9a28-f164c29e1885
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:51:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:16:INFO:
[92mINFO [0m:      Received: evaluate message 59ade08a-f671-4504-941f-6d71fe00b9aa
02/17/2025 15:52:16:INFO:Received: evaluate message 59ade08a-f671-4504-941f-6d71fe00b9aa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:53:13:INFO:
[92mINFO [0m:      Received: train message 6e91ca0a-7321-4610-8240-852c16ca2f8c
02/17/2025 15:53:13:INFO:Received: train message 6e91ca0a-7321-4610-8240-852c16ca2f8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:54:01:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:56:INFO:
[92mINFO [0m:      Received: evaluate message ad4bdc64-e3a1-4307-ae48-3329e93ecf71
02/17/2025 15:54:56:INFO:Received: evaluate message ad4bdc64-e3a1-4307-ae48-3329e93ecf71
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:55:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:42:INFO:
[92mINFO [0m:      Received: train message 5058b5e2-e5bd-4d35-9a8e-9603fcfd172e
02/17/2025 15:55:42:INFO:Received: train message 5058b5e2-e5bd-4d35-9a8e-9603fcfd172e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:56:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:13:INFO:
[92mINFO [0m:      Received: evaluate message 992ec09d-7024-403d-988b-6f8edc48dcfd
02/17/2025 15:57:13:INFO:Received: evaluate message 992ec09d-7024-403d-988b-6f8edc48dcfd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:57:16:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:35:INFO:
[92mINFO [0m:      Received: train message 96e6ebe1-d633-4411-9796-0ecdd50946a3
02/17/2025 15:57:35:INFO:Received: train message 96e6ebe1-d633-4411-9796-0ecdd50946a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:34:INFO:
[92mINFO [0m:      Received: evaluate message 5256d21f-e71e-48b5-b380-488670156cc3
02/17/2025 15:59:34:INFO:Received: evaluate message 5256d21f-e71e-48b5-b380-488670156cc3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:00:08:INFO:
[92mINFO [0m:      Received: train message 395dd135-1700-45da-9aa0-7e78cbcce5f0
02/17/2025 16:00:08:INFO:Received: train message 395dd135-1700-45da-9aa0-7e78cbcce5f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:00:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:54:INFO:
[92mINFO [0m:      Received: evaluate message 2c3df44f-d087-4d71-8a2c-aa8e91ea464b
02/17/2025 16:01:54:INFO:Received: evaluate message 2c3df44f-d087-4d71-8a2c-aa8e91ea464b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:02:13:INFO:
[92mINFO [0m:      Received: train message fd2ecd2f-23ac-4ba5-87f3-c5ffcf377027
02/17/2025 16:02:13:INFO:Received: train message fd2ecd2f-23ac-4ba5-87f3-c5ffcf377027
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:02:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:09:INFO:
[92mINFO [0m:      Received: evaluate message 58675a7f-5e72-43ea-93f7-ed24e60cf365
02/17/2025 16:04:09:INFO:Received: evaluate message 58675a7f-5e72-43ea-93f7-ed24e60cf365
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:04:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:48:INFO:
[92mINFO [0m:      Received: train message 54fd6413-d1ed-459e-930a-0cbf37aaa5bb
02/17/2025 16:04:48:INFO:Received: train message 54fd6413-d1ed-459e-930a-0cbf37aaa5bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:05:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:28:INFO:
[92mINFO [0m:      Received: evaluate message b6e9c28b-e34d-4ca4-8a02-ff96f7bbcc3e
02/17/2025 16:06:28:INFO:Received: evaluate message b6e9c28b-e34d-4ca4-8a02-ff96f7bbcc3e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427], 'accuracy': [0.5019546520719312], 'auc': [0.6965369804720182], 'precision': [0.4088159391766196], 'recall': [0.5019546520719312], 'f1': [0.3521569235349163]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708], 'accuracy': [0.5019546520719312, 0.5301016419077405], 'auc': [0.6965369804720182, 0.6835920593265211], 'precision': [0.4088159391766196, 0.461975793404503], 'recall': [0.5019546520719312, 0.5301016419077405], 'f1': [0.3521569235349163, 0.4922183718303565]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:06:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:58:INFO:
[92mINFO [0m:      Received: train message 0fc1c191-77c4-4bc1-9610-4bd3b7aa7671
02/17/2025 16:06:58:INFO:Received: train message 0fc1c191-77c4-4bc1-9610-4bd3b7aa7671
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:07:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:42:INFO:
[92mINFO [0m:      Received: evaluate message 0139dbde-88bf-436b-b315-8fd56214348f
02/17/2025 16:08:42:INFO:Received: evaluate message 0139dbde-88bf-436b-b315-8fd56214348f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:09:21:INFO:
[92mINFO [0m:      Received: train message 8fab43f2-5ace-44a8-9c69-21780dc9810c
02/17/2025 16:09:21:INFO:Received: train message 8fab43f2-5ace-44a8-9c69-21780dc9810c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:09:57:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:16:INFO:
[92mINFO [0m:      Received: evaluate message 0dbf52c5-aaf2-4b87-bc86-11830be7c197
02/17/2025 16:11:16:INFO:Received: evaluate message 0dbf52c5-aaf2-4b87-bc86-11830be7c197
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:11:19:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:50:INFO:
[92mINFO [0m:      Received: train message 390835fd-2e8c-4fcf-b025-8192a4636eef
02/17/2025 16:11:50:INFO:Received: train message 390835fd-2e8c-4fcf-b025-8192a4636eef
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:12:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:09:INFO:
[92mINFO [0m:      Received: evaluate message ab0f62e6-c211-4ae9-b7f7-9fc8f93f2ee8
02/17/2025 16:13:09:INFO:Received: evaluate message ab0f62e6-c211-4ae9-b7f7-9fc8f93f2ee8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:13:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:02:INFO:
[92mINFO [0m:      Received: train message f09bbe38-880a-489d-aa31-0ef436e8e535
02/17/2025 16:14:02:INFO:Received: train message f09bbe38-880a-489d-aa31-0ef436e8e535
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:14:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:47:INFO:
[92mINFO [0m:      Received: evaluate message c5186978-250b-4b8a-9dbd-fd104478429e
02/17/2025 16:15:47:INFO:Received: evaluate message c5186978-250b-4b8a-9dbd-fd104478429e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:15:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:14:INFO:
[92mINFO [0m:      Received: train message 4b38e245-9a22-4cd7-9b60-f4847e778be0
02/17/2025 16:16:14:INFO:Received: train message 4b38e245-9a22-4cd7-9b60-f4847e778be0

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:16:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:17:58:INFO:
[92mINFO [0m:      Received: evaluate message 5c3586e2-4d48-49d0-b555-b668d349b3f6
02/17/2025 16:17:58:INFO:Received: evaluate message 5c3586e2-4d48-49d0-b555-b668d349b3f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:18:01:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:29:INFO:
[92mINFO [0m:      Received: train message 490f7aaf-3ca6-457d-9a52-2bb048de846b
02/17/2025 16:18:29:INFO:Received: train message 490f7aaf-3ca6-457d-9a52-2bb048de846b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:19:12:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:20:21:INFO:
[92mINFO [0m:      Received: evaluate message 9bbe82f2-c4b3-47e5-ad89-4df59dfc0d59
02/17/2025 16:20:21:INFO:Received: evaluate message 9bbe82f2-c4b3-47e5-ad89-4df59dfc0d59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:20:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:16:INFO:
[92mINFO [0m:      Received: train message d0665d00-f4ef-481e-ab4b-9a914497b7d0
02/17/2025 16:21:16:INFO:Received: train message d0665d00-f4ef-481e-ab4b-9a914497b7d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:21:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:22:57:INFO:
[92mINFO [0m:      Received: evaluate message dd904f2b-658f-485e-af78-82faba7c177c
02/17/2025 16:22:57:INFO:Received: evaluate message dd904f2b-658f-485e-af78-82faba7c177c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:40:INFO:
[92mINFO [0m:      Received: train message 2223d954-dee4-4680-8e1d-916f43138c5d
02/17/2025 16:23:40:INFO:Received: train message 2223d954-dee4-4680-8e1d-916f43138c5d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:19:INFO:
[92mINFO [0m:      Received: evaluate message f3d73f08-6c3a-43e3-a7c8-a504be16c634
02/17/2025 16:25:19:INFO:Received: evaluate message f3d73f08-6c3a-43e3-a7c8-a504be16c634
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:13:INFO:
[92mINFO [0m:      Received: train message d6cf05d1-8e74-4e67-a858-3d7143198ff6
02/17/2025 16:26:13:INFO:Received: train message d6cf05d1-8e74-4e67-a858-3d7143198ff6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:27:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:27:58:INFO:
[92mINFO [0m:      Received: evaluate message e60f9e16-c83c-4362-a927-8ccafe51e241
02/17/2025 16:27:58:INFO:Received: evaluate message e60f9e16-c83c-4362-a927-8ccafe51e241
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:28:01:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:44:INFO:
[92mINFO [0m:      Received: train message 173c792e-8a9d-4390-8e16-e38de9c7d849
02/17/2025 16:28:44:INFO:Received: train message 173c792e-8a9d-4390-8e16-e38de9c7d849
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:07:INFO:
[92mINFO [0m:      Received: evaluate message f8cec3bd-c5fd-482e-9f28-eff34a8d90a7
02/17/2025 16:30:07:INFO:Received: evaluate message f8cec3bd-c5fd-482e-9f28-eff34a8d90a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:31:04:INFO:
[92mINFO [0m:      Received: train message 01108d03-1ab3-4976-aed4-fd397dd30f48
02/17/2025 16:31:04:INFO:Received: train message 01108d03-1ab3-4976-aed4-fd397dd30f48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:58:INFO:
[92mINFO [0m:      Received: evaluate message e94956c1-53e5-42f3-99f7-fbbe11793502
02/17/2025 16:32:58:INFO:Received: evaluate message e94956c1-53e5-42f3-99f7-fbbe11793502

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:33:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:52:INFO:
[92mINFO [0m:      Received: train message 34569714-691e-4b57-8228-132a14d7c1bb
02/17/2025 16:33:52:INFO:Received: train message 34569714-691e-4b57-8228-132a14d7c1bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:34:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:20:INFO:
[92mINFO [0m:      Received: evaluate message aff332b6-8a01-43bc-a689-509d1ae9eedc
02/17/2025 16:35:20:INFO:Received: evaluate message aff332b6-8a01-43bc-a689-509d1ae9eedc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:35:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:00:INFO:
[92mINFO [0m:      Received: train message 7d86cb08-af31-4eab-b98a-3434d84fbd22
02/17/2025 16:36:00:INFO:Received: train message 7d86cb08-af31-4eab-b98a-3434d84fbd22
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:36:35:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:28:INFO:
[92mINFO [0m:      Received: evaluate message 4ade9132-9125-4c59-a1ef-4a6463fb570d
02/17/2025 16:37:28:INFO:Received: evaluate message 4ade9132-9125-4c59-a1ef-4a6463fb570d

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:37:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:07:INFO:
[92mINFO [0m:      Received: train message 2330ec16-6d69-45f7-9f00-4c94ea567e2a
02/17/2025 16:38:07:INFO:Received: train message 2330ec16-6d69-45f7-9f00-4c94ea567e2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:38:43:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:19:INFO:
[92mINFO [0m:      Received: evaluate message 93adf133-8f5c-488c-b905-460d85da3260
02/17/2025 16:39:19:INFO:Received: evaluate message 93adf133-8f5c-488c-b905-460d85da3260
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:39:22:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:40:04:INFO:
[92mINFO [0m:      Received: train message c66080d3-88b5-45e7-8d37-39491a03da40
02/17/2025 16:40:04:INFO:Received: train message c66080d3-88b5-45e7-8d37-39491a03da40
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:40:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:25:INFO:
[92mINFO [0m:      Received: evaluate message 9b01ac71-0e51-471a-95fd-75fab3b71c23
02/17/2025 16:41:25:INFO:Received: evaluate message 9b01ac71-0e51-471a-95fd-75fab3b71c23

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:41:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:54:INFO:
[92mINFO [0m:      Received: train message d125b5d2-223a-44d2-89b5-ebb909b537bf
02/17/2025 16:41:54:INFO:Received: train message d125b5d2-223a-44d2-89b5-ebb909b537bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:42:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:33:INFO:
[92mINFO [0m:      Received: evaluate message 6ab31538-ae6a-4972-8a4c-3e75ac378065
02/17/2025 16:43:33:INFO:Received: evaluate message 6ab31538-ae6a-4972-8a4c-3e75ac378065
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:43:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:44:00:INFO:
[92mINFO [0m:      Received: train message c430f7b4-e060-447f-9e65-edd144cd5a8c
02/17/2025 16:44:00:INFO:Received: train message c430f7b4-e060-447f-9e65-edd144cd5a8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:44:34:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:27:INFO:
[92mINFO [0m:      Received: evaluate message 4eba59ed-e340-4199-b2c0-9d0a7e53adaa
02/17/2025 16:45:27:INFO:Received: evaluate message 4eba59ed-e340-4199-b2c0-9d0a7e53adaa

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:45:30:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:09:INFO:
[92mINFO [0m:      Received: train message ed4817af-19ae-4088-9393-dde8cc5c94bf
02/17/2025 16:46:09:INFO:Received: train message ed4817af-19ae-4088-9393-dde8cc5c94bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:46:48:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:47:43:INFO:
[92mINFO [0m:      Received: evaluate message 18e35b87-1e78-46d9-ad8b-988a5fc323b4
02/17/2025 16:47:43:INFO:Received: evaluate message 18e35b87-1e78-46d9-ad8b-988a5fc323b4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:47:45:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:25:INFO:
[92mINFO [0m:      Received: train message 6da88c16-8c4a-4143-a7c2-6c27176c93a2
02/17/2025 16:48:25:INFO:Received: train message 6da88c16-8c4a-4143-a7c2-6c27176c93a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:49:33:INFO:
[92mINFO [0m:      Received: evaluate message b1458061-2358-4ed3-bb09-7d3c7936caa3
02/17/2025 16:49:33:INFO:Received: evaluate message b1458061-2358-4ed3-bb09-7d3c7936caa3

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:49:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:35:INFO:
[92mINFO [0m:      Received: train message 638cbef7-adae-47e9-a547-e8dbbc6e69f3
02/17/2025 16:50:35:INFO:Received: train message 638cbef7-adae-47e9-a547-e8dbbc6e69f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:51:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:51:59:INFO:
[92mINFO [0m:      Received: evaluate message 4edfd15d-b159-450b-9bce-98ff999b791b
02/17/2025 16:51:59:INFO:Received: evaluate message 4edfd15d-b159-450b-9bce-98ff999b791b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:52:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:52:28:INFO:
[92mINFO [0m:      Received: train message feb11d1e-d7fd-4989-b0d2-02604e902522
02/17/2025 16:52:28:INFO:Received: train message feb11d1e-d7fd-4989-b0d2-02604e902522
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:53:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:53:53:INFO:
[92mINFO [0m:      Received: evaluate message 9611141c-1530-40ee-92be-3f5b0faf5ec5
02/17/2025 16:53:53:INFO:Received: evaluate message 9611141c-1530-40ee-92be-3f5b0faf5ec5

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:53:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:54:21:INFO:
[92mINFO [0m:      Received: train message 38a75465-cfa0-4972-87e6-e397ca8ddef9
02/17/2025 16:54:21:INFO:Received: train message 38a75465-cfa0-4972-87e6-e397ca8ddef9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:54:54:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:55:58:INFO:
[92mINFO [0m:      Received: evaluate message f5455435-df01-4b38-a6d5-82738adeb6e6
02/17/2025 16:55:58:INFO:Received: evaluate message f5455435-df01-4b38-a6d5-82738adeb6e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:56:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:56:20:INFO:
[92mINFO [0m:      Received: train message 41a8acb4-f996-4e6b-8fd4-7d164f718f9e
02/17/2025 16:56:20:INFO:Received: train message 41a8acb4-f996-4e6b-8fd4-7d164f718f9e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:56:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:57:59:INFO:
[92mINFO [0m:      Received: evaluate message 8ebac2a9-d7a9-4bda-bd5a-aa23bd86a740
02/17/2025 16:57:59:INFO:Received: evaluate message 8ebac2a9-d7a9-4bda-bd5a-aa23bd86a740

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:58:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:58:06:INFO:
[92mINFO [0m:      Received: reconnect message 48641f0d-5e45-4ff6-b119-2602a9e8b336
02/17/2025 16:58:06:INFO:Received: reconnect message 48641f0d-5e45-4ff6-b119-2602a9e8b336
02/17/2025 16:58:06:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:58:06:INFO:Disconnect and shut down

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}



Final client history:
{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}

