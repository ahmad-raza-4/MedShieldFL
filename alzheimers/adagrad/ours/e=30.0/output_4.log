nohup: ignoring input
02/17/2025 15:49:32:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/17/2025 15:49:32:DEBUG:ChannelConnectivity.IDLE
02/17/2025 15:49:32:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739836172.124284  749452 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/17/2025 15:50:02:INFO:
[92mINFO [0m:      Received: train message e0536b14-8f20-48b7-8b38-6f407ec9afcd
02/17/2025 15:50:02:INFO:Received: train message e0536b14-8f20-48b7-8b38-6f407ec9afcd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:50:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:52:22:INFO:
[92mINFO [0m:      Received: evaluate message 55f3ac87-c74c-4a94-b113-9eb8f11a0c68
02/17/2025 15:52:22:INFO:Received: evaluate message 55f3ac87-c74c-4a94-b113-9eb8f11a0c68
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:52:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:53:05:INFO:
[92mINFO [0m:      Received: train message a5bf91bc-4e60-4e81-ab0d-bb3db9a19204
02/17/2025 15:53:05:INFO:Received: train message a5bf91bc-4e60-4e81-ab0d-bb3db9a19204
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:54:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:54:52:INFO:
[92mINFO [0m:      Received: evaluate message 3a08551a-7db0-4e41-a708-1da7b87daecb
02/17/2025 15:54:52:INFO:Received: evaluate message 3a08551a-7db0-4e41-a708-1da7b87daecb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:54:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:55:18:INFO:
[92mINFO [0m:      Received: train message be63e052-0d06-4cc2-907b-198f1d35d831
02/17/2025 15:55:18:INFO:Received: train message be63e052-0d06-4cc2-907b-198f1d35d831
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:56:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:56:58:INFO:
[92mINFO [0m:      Received: evaluate message 9a337b49-8622-4697-b980-d4e433f9a8c7
02/17/2025 15:56:58:INFO:Received: evaluate message 9a337b49-8622-4697-b980-d4e433f9a8c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:57:02:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:57:57:INFO:
[92mINFO [0m:      Received: train message 50fc365b-9f70-4199-87cc-e6fa3c5110e0
02/17/2025 15:57:57:INFO:Received: train message 50fc365b-9f70-4199-87cc-e6fa3c5110e0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 15:58:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 15:59:22:INFO:
[92mINFO [0m:      Received: evaluate message ae49883a-ad15-42f5-b804-e3d7c1c0bd86
02/17/2025 15:59:22:INFO:Received: evaluate message ae49883a-ad15-42f5-b804-e3d7c1c0bd86
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 15:59:24:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:00:18:INFO:
[92mINFO [0m:      Received: train message b945cec5-3624-4806-abed-175491fae597
02/17/2025 16:00:18:INFO:Received: train message b945cec5-3624-4806-abed-175491fae597
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:01:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:01:44:INFO:
[92mINFO [0m:      Received: evaluate message b17b0546-eb87-4e05-a6b9-adf6dec8f93a
02/17/2025 16:01:44:INFO:Received: evaluate message b17b0546-eb87-4e05-a6b9-adf6dec8f93a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:01:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:02:40:INFO:
[92mINFO [0m:      Received: train message 0decf9b5-e1f7-41b2-9f97-4f03e3487801
02/17/2025 16:02:40:INFO:Received: train message 0decf9b5-e1f7-41b2-9f97-4f03e3487801
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:03:28:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:12:INFO:
[92mINFO [0m:      Received: evaluate message 12bf49bb-26a1-4720-8099-a3534f3e6682
02/17/2025 16:04:12:INFO:Received: evaluate message 12bf49bb-26a1-4720-8099-a3534f3e6682
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:04:15:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:04:53:INFO:
[92mINFO [0m:      Received: train message 1d2814b3-bbfd-4c25-b949-bd531e240100
02/17/2025 16:04:53:INFO:Received: train message 1d2814b3-bbfd-4c25-b949-bd531e240100
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:05:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:06:29:INFO:
[92mINFO [0m:      Received: evaluate message 00e576f4-0c9a-4e49-abd7-5f3a7340ddff
02/17/2025 16:06:29:INFO:Received: evaluate message 00e576f4-0c9a-4e49-abd7-5f3a7340ddff
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427], 'accuracy': [0.5019546520719312], 'auc': [0.6965369804720182], 'precision': [0.4088159391766196], 'recall': [0.5019546520719312], 'f1': [0.3521569235349163]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708], 'accuracy': [0.5019546520719312, 0.5301016419077405], 'auc': [0.6965369804720182, 0.6835920593265211], 'precision': [0.4088159391766196, 0.461975793404503], 'recall': [0.5019546520719312, 0.5301016419077405], 'f1': [0.3521569235349163, 0.4922183718303565]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:06:39:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:07:10:INFO:
[92mINFO [0m:      Received: train message 3631a99b-b8f9-4ae4-8338-a788f4a87952
02/17/2025 16:07:10:INFO:Received: train message 3631a99b-b8f9-4ae4-8338-a788f4a87952
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:08:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:08:46:INFO:
[92mINFO [0m:      Received: evaluate message 6c071576-1239-45b9-9749-96b986250344
02/17/2025 16:08:46:INFO:Received: evaluate message 6c071576-1239-45b9-9749-96b986250344
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:08:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:09:24:INFO:
[92mINFO [0m:      Received: train message 3e28cef5-46e1-45c4-9db0-88fc286a5cb2
02/17/2025 16:09:24:INFO:Received: train message 3e28cef5-46e1-45c4-9db0-88fc286a5cb2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:10:27:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:20:INFO:
[92mINFO [0m:      Received: evaluate message 918aecd5-fad9-4ba7-b318-5e589c1737fc
02/17/2025 16:11:20:INFO:Received: evaluate message 918aecd5-fad9-4ba7-b318-5e589c1737fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:11:23:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:11:51:INFO:
[92mINFO [0m:      Received: train message 48aeb730-88fe-4bed-9b24-51f5defcc203
02/17/2025 16:11:51:INFO:Received: train message 48aeb730-88fe-4bed-9b24-51f5defcc203
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:12:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:13:18:INFO:
[92mINFO [0m:      Received: evaluate message 2a7dacc8-935f-4cff-b5c8-4f7cc3ae2359
02/17/2025 16:13:18:INFO:Received: evaluate message 2a7dacc8-935f-4cff-b5c8-4f7cc3ae2359
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:13:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:14:09:INFO:
[92mINFO [0m:      Received: train message 3efd0402-a418-4155-a8b9-4d21543bafed
02/17/2025 16:14:09:INFO:Received: train message 3efd0402-a418-4155-a8b9-4d21543bafed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:15:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:15:51:INFO:
[92mINFO [0m:      Received: evaluate message eabab3df-b81e-497b-9d93-3ed8255c9d6c
02/17/2025 16:15:51:INFO:Received: evaluate message eabab3df-b81e-497b-9d93-3ed8255c9d6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:15:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:16:16:INFO:
[92mINFO [0m:      Received: train message 37be1413-9299-4063-b4bd-f51d6093309a
02/17/2025 16:16:16:INFO:Received: train message 37be1413-9299-4063-b4bd-f51d6093309a

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:17:10:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:17:49:INFO:
[92mINFO [0m:      Received: evaluate message 2a78091a-6e56-40ae-a9b9-7ef61cbc089e
02/17/2025 16:17:49:INFO:Received: evaluate message 2a78091a-6e56-40ae-a9b9-7ef61cbc089e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:17:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:18:45:INFO:
[92mINFO [0m:      Received: train message c384defb-05c1-4650-bf73-4a26909fb017
02/17/2025 16:18:45:INFO:Received: train message c384defb-05c1-4650-bf73-4a26909fb017
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:19:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:20:22:INFO:
[92mINFO [0m:      Received: evaluate message a10055a2-93d5-4b72-a792-edb45b03eb20
02/17/2025 16:20:22:INFO:Received: evaluate message a10055a2-93d5-4b72-a792-edb45b03eb20
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:20:25:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:21:13:INFO:
[92mINFO [0m:      Received: train message 884cdbb0-778e-40e4-acf9-ef0e5e57a9a6
02/17/2025 16:21:13:INFO:Received: train message 884cdbb0-778e-40e4-acf9-ef0e5e57a9a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:22:11:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:01:INFO:
[92mINFO [0m:      Received: evaluate message a840c038-5efa-48c4-93f9-eafe514332de
02/17/2025 16:23:01:INFO:Received: evaluate message a840c038-5efa-48c4-93f9-eafe514332de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:23:06:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:23:27:INFO:
[92mINFO [0m:      Received: train message 9b31e3c8-507b-4d01-a9be-64ca99dd5365
02/17/2025 16:23:27:INFO:Received: train message 9b31e3c8-507b-4d01-a9be-64ca99dd5365
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:24:18:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:25:28:INFO:
[92mINFO [0m:      Received: evaluate message 45a69d95-08ec-47a6-ac41-49ab7c3fad20
02/17/2025 16:25:28:INFO:Received: evaluate message 45a69d95-08ec-47a6-ac41-49ab7c3fad20
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:25:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:26:04:INFO:
[92mINFO [0m:      Received: train message 4cfce334-d2a9-48a4-b7b4-fd8d645b8275
02/17/2025 16:26:04:INFO:Received: train message 4cfce334-d2a9-48a4-b7b4-fd8d645b8275
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:27:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:00:INFO:
[92mINFO [0m:      Received: evaluate message ff5f0094-0166-4933-81ee-e23002ebacb6
02/17/2025 16:28:00:INFO:Received: evaluate message ff5f0094-0166-4933-81ee-e23002ebacb6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:28:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:28:30:INFO:
[92mINFO [0m:      Received: train message b497278a-6a8c-4950-be5d-d09cd48c17ce
02/17/2025 16:28:30:INFO:Received: train message b497278a-6a8c-4950-be5d-d09cd48c17ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:29:29:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:23:INFO:
[92mINFO [0m:      Received: evaluate message f62100b8-f1c8-4122-a1c3-9fbb5f102fec
02/17/2025 16:30:23:INFO:Received: evaluate message f62100b8-f1c8-4122-a1c3-9fbb5f102fec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:30:26:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:30:57:INFO:
[92mINFO [0m:      Received: train message 8a07ca7d-bb2f-4c97-b81c-4d802eed8550
02/17/2025 16:30:57:INFO:Received: train message 8a07ca7d-bb2f-4c97-b81c-4d802eed8550
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:31:53:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:32:49:INFO:
[92mINFO [0m:      Received: evaluate message 4de26f4c-3a40-4a07-bb16-2b81d5621618
02/17/2025 16:32:49:INFO:Received: evaluate message 4de26f4c-3a40-4a07-bb16-2b81d5621618

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:32:52:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:33:49:INFO:
[92mINFO [0m:      Received: train message 0a3e820f-2128-4204-8d38-c77c28a43569
02/17/2025 16:33:49:INFO:Received: train message 0a3e820f-2128-4204-8d38-c77c28a43569
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:34:36:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:35:05:INFO:
[92mINFO [0m:      Received: evaluate message 3c6a344f-e146-4e29-b0a3-f8404de161b7
02/17/2025 16:35:05:INFO:Received: evaluate message 3c6a344f-e146-4e29-b0a3-f8404de161b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:35:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:36:00:INFO:
[92mINFO [0m:      Received: train message 86a95e4c-8ee2-4291-8453-657ff592f0bb
02/17/2025 16:36:00:INFO:Received: train message 86a95e4c-8ee2-4291-8453-657ff592f0bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:36:46:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:37:28:INFO:
[92mINFO [0m:      Received: evaluate message cfe0d9c4-e9df-4aec-aba0-f173c2979abf
02/17/2025 16:37:28:INFO:Received: evaluate message cfe0d9c4-e9df-4aec-aba0-f173c2979abf

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:37:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:38:00:INFO:
[92mINFO [0m:      Received: train message e0d981a0-3287-4d0d-afbe-601db22107e6
02/17/2025 16:38:00:INFO:Received: train message e0d981a0-3287-4d0d-afbe-601db22107e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:38:47:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:12:INFO:
[92mINFO [0m:      Received: evaluate message ca82134c-8a78-4ae8-bdb5-92be948d56d2
02/17/2025 16:39:12:INFO:Received: evaluate message ca82134c-8a78-4ae8-bdb5-92be948d56d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:39:14:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:39:55:INFO:
[92mINFO [0m:      Received: train message eaf4a467-e90d-41aa-9ff0-1830c157ab42
02/17/2025 16:39:55:INFO:Received: train message eaf4a467-e90d-41aa-9ff0-1830c157ab42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:40:38:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:41:34:INFO:
[92mINFO [0m:      Received: evaluate message 7b234875-3eea-46cf-8d5e-8ab6723f91ce
02/17/2025 16:41:34:INFO:Received: evaluate message 7b234875-3eea-46cf-8d5e-8ab6723f91ce

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:41:37:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:42:00:INFO:
[92mINFO [0m:      Received: train message 0962362a-9241-483a-b3d1-945c2e254cd0
02/17/2025 16:42:00:INFO:Received: train message 0962362a-9241-483a-b3d1-945c2e254cd0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:42:49:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:30:INFO:
[92mINFO [0m:      Received: evaluate message 427198f7-561a-4ac5-87a1-fc9183247d8c
02/17/2025 16:43:30:INFO:Received: evaluate message 427198f7-561a-4ac5-87a1-fc9183247d8c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:43:34:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:43:49:INFO:
[92mINFO [0m:      Received: train message 9f69c3a1-adb4-46ee-9348-f6a1d6c5b85a
02/17/2025 16:43:49:INFO:Received: train message 9f69c3a1-adb4-46ee-9348-f6a1d6c5b85a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:44:32:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:45:38:INFO:
[92mINFO [0m:      Received: evaluate message b57503aa-5005-480c-a4b6-1c906fb4d786
02/17/2025 16:45:38:INFO:Received: evaluate message b57503aa-5005-480c-a4b6-1c906fb4d786

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:45:42:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:46:17:INFO:
[92mINFO [0m:      Received: train message 2da2ad1f-dbdb-49fa-ac8d-7a7d503a744c
02/17/2025 16:46:17:INFO:Received: train message 2da2ad1f-dbdb-49fa-ac8d-7a7d503a744c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:47:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:47:47:INFO:
[92mINFO [0m:      Received: evaluate message 9624c9d3-5a17-408a-b2fa-9277ff8a0510
02/17/2025 16:47:47:INFO:Received: evaluate message 9624c9d3-5a17-408a-b2fa-9277ff8a0510
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:47:50:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:48:17:INFO:
[92mINFO [0m:      Received: train message 6a2032a4-ac4a-48f3-895a-9c2f51994454
02/17/2025 16:48:17:INFO:Received: train message 6a2032a4-ac4a-48f3-895a-9c2f51994454
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:49:08:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:49:53:INFO:
[92mINFO [0m:      Received: evaluate message 747e5690-5742-421d-b971-1a96479b4cb3
02/17/2025 16:49:53:INFO:Received: evaluate message 747e5690-5742-421d-b971-1a96479b4cb3

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:49:55:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:50:20:INFO:
[92mINFO [0m:      Received: train message a3c0ac26-ced8-429f-baed-5ef78746509f
02/17/2025 16:50:20:INFO:Received: train message a3c0ac26-ced8-429f-baed-5ef78746509f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:51:05:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:51:59:INFO:
[92mINFO [0m:      Received: evaluate message 79c242b2-ad63-452b-9b33-174b36adfdd3
02/17/2025 16:51:59:INFO:Received: evaluate message 79c242b2-ad63-452b-9b33-174b36adfdd3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:52:03:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:52:18:INFO:
[92mINFO [0m:      Received: train message 4cfdf701-cc30-445d-a2a7-2f0a46ed47f3
02/17/2025 16:52:18:INFO:Received: train message 4cfdf701-cc30-445d-a2a7-2f0a46ed47f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:53:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:53:57:INFO:
[92mINFO [0m:      Received: evaluate message 74513fdf-81ed-471e-803a-64f7e67d38dc
02/17/2025 16:53:57:INFO:Received: evaluate message 74513fdf-81ed-471e-803a-64f7e67d38dc

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:54:00:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:54:34:INFO:
[92mINFO [0m:      Received: train message b100b8e2-d27a-4694-841e-3d59a7e90ac2
02/17/2025 16:54:34:INFO:Received: train message b100b8e2-d27a-4694-841e-3d59a7e90ac2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:55:19:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:55:55:INFO:
[92mINFO [0m:      Received: evaluate message 88d93dff-f8ec-4060-a3ba-b080207dba8d
02/17/2025 16:55:55:INFO:Received: evaluate message 88d93dff-f8ec-4060-a3ba-b080207dba8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:55:59:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:56:37:INFO:
[92mINFO [0m:      Received: train message 3ac9421c-e305-4df4-a2e1-6eeecae96a9c
02/17/2025 16:56:37:INFO:Received: train message 3ac9421c-e305-4df4-a2e1-6eeecae96a9c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/17/2025 16:57:21:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:58:01:INFO:
[92mINFO [0m:      Received: evaluate message 15761445-83d7-4560-9469-df4ed01693fa
02/17/2025 16:58:01:INFO:Received: evaluate message 15761445-83d7-4560-9469-df4ed01693fa

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/17/2025 16:58:04:INFO:Sent reply
[92mINFO [0m:      
02/17/2025 16:58:06:INFO:
[92mINFO [0m:      Received: reconnect message aaa2ded9-3241-4e14-814e-a49c679116d2
02/17/2025 16:58:06:INFO:Received: reconnect message aaa2ded9-3241-4e14-814e-a49c679116d2
02/17/2025 16:58:06:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/17/2025 16:58:06:INFO:Disconnect and shut down

{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}



Final client history:
{'loss': [1.1305018039864427, 0.9915772470950708, 1.1648563897824082, 1.0466597741986738, 1.148241737375416, 1.0641967954348548, 1.1320083865530328, 1.107989996275555, 1.1078047442007475, 1.0826559744373347, 1.1413210574493826, 1.138038889703758, 1.0563398472399708, 1.0743940855675698, 1.0936944837025873, 1.0919598198850422, 1.1096299480329368, 1.1283897627509134, 1.1283150949545258, 1.1367423536257413, 1.0756897296730292, 1.0994769219218798, 1.1254509527110979, 1.1394916475518222, 1.1648676878218542, 1.1255261691125507, 1.094075650466057, 1.0916713737621262, 1.069076407505629, 1.0734777398832707], 'accuracy': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.6965369804720182, 0.6835920593265211, 0.7117494183894532, 0.7112479223623938, 0.725291098437883, 0.7328371478621467, 0.7385992143527806, 0.7423369793551933, 0.7466571562813579, 0.7455803937093332, 0.7479130854730732, 0.7506965080522601, 0.7488619195828199, 0.7466850932219505, 0.7488694846251374, 0.7467170030427659, 0.7465041370261736, 0.7503527421955705, 0.7537941768795594, 0.7556333673478719, 0.7575593946776533, 0.7566818180690288, 0.7597422471652666, 0.7600499136547132, 0.7622189094944313, 0.7653079215282674, 0.767888725559831, 0.7711116201969671, 0.7719423692245099, 0.7722878636322325], 'precision': [0.4088159391766196, 0.461975793404503, 0.4070778716210185, 0.43768555143130466, 0.4228311571411994, 0.4450459184859455, 0.4279586381184888, 0.43535216468572985, 0.43726837703167465, 0.44639855421347835, 0.4340055882733993, 0.4314648621892967, 0.4485937487118673, 0.44526782741800625, 0.5848676542891614, 0.439560210901032, 0.4395015770259986, 0.5799941863846146, 0.5824096583028229, 0.537053629498672, 0.5300871793177622, 0.5863966436196579, 0.5495754817095225, 0.5637056902432779, 0.5839392422896885, 0.5878726432288425, 0.5970329048870627, 0.6034374495815974, 0.5771310997396796, 0.6022141276421745], 'recall': [0.5019546520719312, 0.5301016419077405, 0.5089913995308835, 0.5316653635652854, 0.5191555903049258, 0.5426114151681001, 0.527756059421423, 0.5340109460516028, 0.5363565285379203, 0.5426114151681001, 0.5332290852228303, 0.5308835027365129, 0.5410476935105551, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5363565285379203, 0.5363565285379203, 0.5402658326817826, 0.5418295543393276, 0.5543393275996873, 0.544175136825645, 0.5426114151681001, 0.5480844409695075, 0.5426114151681001, 0.5480844409695075, 0.5551211884284597, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'f1': [0.3521569235349163, 0.4922183718303565, 0.3932331082159528, 0.47489690728615946, 0.42115188396750275, 0.48045774919925965, 0.44878816135340316, 0.46610849931454124, 0.466987483827603, 0.48371149423060555, 0.45870481717945916, 0.4545813543855136, 0.48767429312863825, 0.48276491879450384, 0.4769359989587063, 0.4716505572418204, 0.4691195168067503, 0.4622087064594131, 0.4747628209265937, 0.474515643262412, 0.5010751364441269, 0.4834853877753442, 0.4757178849520703, 0.48537276606787005, 0.4723117824292989, 0.4866621169135657, 0.4998534350273592, 0.5032132146970555, 0.5031110168163134, 0.5058549924197282]}

