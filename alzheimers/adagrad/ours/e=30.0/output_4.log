nohup: ignoring input
02/15/2025 00:47:41:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 00:47:41:DEBUG:ChannelConnectivity.IDLE
02/15/2025 00:47:41:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739609262.007336 1570023 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 00:48:10:INFO:
[92mINFO [0m:      Received: train message 64a2e0c8-86c5-4147-be24-86baf9e67247
02/15/2025 00:48:10:INFO:Received: train message 64a2e0c8-86c5-4147-be24-86baf9e67247
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:48:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:49:46:INFO:
[92mINFO [0m:      Received: evaluate message 3f3d2082-3811-40a0-8c54-c8a7d8e746a8
02/15/2025 00:49:46:INFO:Received: evaluate message 3f3d2082-3811-40a0-8c54-c8a7d8e746a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:49:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:50:13:INFO:
[92mINFO [0m:      Received: train message c6524a71-b3ec-46d4-8f2b-3689b756dea4
02/15/2025 00:50:13:INFO:Received: train message c6524a71-b3ec-46d4-8f2b-3689b756dea4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:50:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:51:43:INFO:
[92mINFO [0m:      Received: evaluate message c9da48e3-c8fe-4952-b98a-c20ea9f027ac
02/15/2025 00:51:43:INFO:Received: evaluate message c9da48e3-c8fe-4952-b98a-c20ea9f027ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:51:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:52:24:INFO:
[92mINFO [0m:      Received: train message d22bf52c-8575-4af6-85b5-704f3ed42708
02/15/2025 00:52:24:INFO:Received: train message d22bf52c-8575-4af6-85b5-704f3ed42708
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:53:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:53:30:INFO:
[92mINFO [0m:      Received: evaluate message 5a07032d-1ced-43b9-a83e-d64b803a6c1f
02/15/2025 00:53:30:INFO:Received: evaluate message 5a07032d-1ced-43b9-a83e-d64b803a6c1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:53:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:54:20:INFO:
[92mINFO [0m:      Received: train message a95c15c5-6688-4ff4-a922-78509183273d
02/15/2025 00:54:20:INFO:Received: train message a95c15c5-6688-4ff4-a922-78509183273d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:55:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:55:46:INFO:
[92mINFO [0m:      Received: evaluate message ca311ee9-70dc-45e0-b3f2-b6d461de5f26
02/15/2025 00:55:46:INFO:Received: evaluate message ca311ee9-70dc-45e0-b3f2-b6d461de5f26
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:55:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:56:24:INFO:
[92mINFO [0m:      Received: train message 4266baf9-b82d-45e5-bfc9-18964e95b89c
02/15/2025 00:56:24:INFO:Received: train message 4266baf9-b82d-45e5-bfc9-18964e95b89c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:57:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:57:40:INFO:
[92mINFO [0m:      Received: evaluate message 3461df94-4108-41c2-bd2c-f04495e39ce8
02/15/2025 00:57:40:INFO:Received: evaluate message 3461df94-4108-41c2-bd2c-f04495e39ce8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 00:57:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 00:58:33:INFO:
[92mINFO [0m:      Received: train message 9bb7a924-2165-477f-a541-3a161399bcdb
02/15/2025 00:58:33:INFO:Received: train message 9bb7a924-2165-477f-a541-3a161399bcdb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 00:59:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:08:INFO:
[92mINFO [0m:      Received: evaluate message 7edbfa65-1557-478b-96c5-accee809884d
02/15/2025 01:00:08:INFO:Received: evaluate message 7edbfa65-1557-478b-96c5-accee809884d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:00:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:00:34:INFO:
[92mINFO [0m:      Received: train message 091487c5-c9c8-4f3b-960a-d4e84d254279
02/15/2025 01:00:34:INFO:Received: train message 091487c5-c9c8-4f3b-960a-d4e84d254279
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:01:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:15:INFO:
[92mINFO [0m:      Received: evaluate message cff25957-27d9-4ea6-87d5-94d39d330e12
02/15/2025 01:02:15:INFO:Received: evaluate message cff25957-27d9-4ea6-87d5-94d39d330e12
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497], 'accuracy': [0.4988272087568413], 'auc': [0.5004293699052179], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784], 'accuracy': [0.4988272087568413, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665], 'precision': [0.24999954080217573, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:02:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:02:52:INFO:
[92mINFO [0m:      Received: train message ba3f0349-34f4-46f8-959a-59e4ba08609e
02/15/2025 01:02:52:INFO:Received: train message ba3f0349-34f4-46f8-959a-59e4ba08609e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:03:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:24:INFO:
[92mINFO [0m:      Received: evaluate message b492c5f0-8d92-492f-a2e6-d5a095cbbbf2
02/15/2025 01:04:24:INFO:Received: evaluate message b492c5f0-8d92-492f-a2e6-d5a095cbbbf2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:04:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:04:39:INFO:
[92mINFO [0m:      Received: train message f5e0c695-0290-4ace-b573-5bed13e2d5cd
02/15/2025 01:04:39:INFO:Received: train message f5e0c695-0290-4ace-b573-5bed13e2d5cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:05:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:20:INFO:
[92mINFO [0m:      Received: evaluate message 3f781b32-067d-446b-bba1-e75e3a818f4c
02/15/2025 01:06:20:INFO:Received: evaluate message 3f781b32-067d-446b-bba1-e75e3a818f4c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:06:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:06:49:INFO:
[92mINFO [0m:      Received: train message ec30ec3c-d260-4680-bc67-09de8d80a95d
02/15/2025 01:06:49:INFO:Received: train message ec30ec3c-d260-4680-bc67-09de8d80a95d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:07:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:07:INFO:
[92mINFO [0m:      Received: evaluate message b8e26cbe-8f79-4086-b24a-72a8976c1479
02/15/2025 01:08:07:INFO:Received: evaluate message b8e26cbe-8f79-4086-b24a-72a8976c1479
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:08:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:08:40:INFO:
[92mINFO [0m:      Received: train message b72f1efb-1e1f-4a0b-babd-363fbb5b93f2
02/15/2025 01:08:40:INFO:Received: train message b72f1efb-1e1f-4a0b-babd-363fbb5b93f2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:09:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:09:56:INFO:
[92mINFO [0m:      Received: evaluate message 32c99d07-e7fd-468c-aa48-26ea4ecbbd2e
02/15/2025 01:09:56:INFO:Received: evaluate message 32c99d07-e7fd-468c-aa48-26ea4ecbbd2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:09:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:10:42:INFO:
[92mINFO [0m:      Received: train message 4dc380ad-5bb8-45f7-a3ea-5306e59470fb
02/15/2025 01:10:42:INFO:Received: train message 4dc380ad-5bb8-45f7-a3ea-5306e59470fb

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:11:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:13:INFO:
[92mINFO [0m:      Received: evaluate message f7c7735e-219c-466d-a033-b39060f6a9f1
02/15/2025 01:12:13:INFO:Received: evaluate message f7c7735e-219c-466d-a033-b39060f6a9f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:12:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:12:42:INFO:
[92mINFO [0m:      Received: train message e3102c36-52be-4a13-9b19-cc188010e0b6
02/15/2025 01:12:42:INFO:Received: train message e3102c36-52be-4a13-9b19-cc188010e0b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:13:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:09:INFO:
[92mINFO [0m:      Received: evaluate message 4fef1543-733f-4653-8a9f-d10e957fee93
02/15/2025 01:14:09:INFO:Received: evaluate message 4fef1543-733f-4653-8a9f-d10e957fee93
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:14:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:46:INFO:
[92mINFO [0m:      Received: train message 9952b7d2-2701-456d-bad8-6c9910c17ffc
02/15/2025 01:14:46:INFO:Received: train message 9952b7d2-2701-456d-bad8-6c9910c17ffc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:04:INFO:
[92mINFO [0m:      Received: evaluate message fb22948b-27e9-41d1-a786-dbc9bf53aa66
02/15/2025 01:16:04:INFO:Received: evaluate message fb22948b-27e9-41d1-a786-dbc9bf53aa66
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:16:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:42:INFO:
[92mINFO [0m:      Received: train message 285727d1-5f52-4729-b2a5-dd564857a9ae
02/15/2025 01:16:42:INFO:Received: train message 285727d1-5f52-4729-b2a5-dd564857a9ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:13:INFO:
[92mINFO [0m:      Received: evaluate message e1e832e1-b30c-4358-8e35-a9232540c0dd
02/15/2025 01:18:13:INFO:Received: evaluate message e1e832e1-b30c-4358-8e35-a9232540c0dd
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:46:INFO:
[92mINFO [0m:      Received: train message 3197b8a5-b030-4758-a60c-837cda00ca83
02/15/2025 01:18:46:INFO:Received: train message 3197b8a5-b030-4758-a60c-837cda00ca83
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:13:INFO:
[92mINFO [0m:      Received: evaluate message 8bffe5e4-fd8e-4fd3-b90d-7d22e271372b
02/15/2025 01:20:13:INFO:Received: evaluate message 8bffe5e4-fd8e-4fd3-b90d-7d22e271372b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:55:INFO:
[92mINFO [0m:      Received: train message f9419a0d-9eb9-46bd-a22d-a1f20e9a612e
02/15/2025 01:20:55:INFO:Received: train message f9419a0d-9eb9-46bd-a22d-a1f20e9a612e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:21:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:22:52:INFO:
[92mINFO [0m:      Received: evaluate message 8b8566bd-2cd1-4d1a-b63d-8887909d900c
02/15/2025 01:22:52:INFO:Received: evaluate message 8b8566bd-2cd1-4d1a-b63d-8887909d900c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:22:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:17:INFO:
[92mINFO [0m:      Received: train message 0580fa7b-6db0-4684-ae84-4eb08f844671
02/15/2025 01:23:17:INFO:Received: train message 0580fa7b-6db0-4684-ae84-4eb08f844671
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:56:INFO:
[92mINFO [0m:      Received: evaluate message 41c1f3df-97ee-4112-a7b6-cb6de5b4abba
02/15/2025 01:24:56:INFO:Received: evaluate message 41c1f3df-97ee-4112-a7b6-cb6de5b4abba

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:24:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:40:INFO:
[92mINFO [0m:      Received: train message 370b5a0e-75e6-44bc-b0b1-bae5ad63314f
02/15/2025 01:25:40:INFO:Received: train message 370b5a0e-75e6-44bc-b0b1-bae5ad63314f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:05:INFO:
[92mINFO [0m:      Received: evaluate message 9ec61d3a-e3ad-429e-8ebb-f12b67ec0f92
02/15/2025 01:27:05:INFO:Received: evaluate message 9ec61d3a-e3ad-429e-8ebb-f12b67ec0f92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:50:INFO:
[92mINFO [0m:      Received: train message deddc300-7983-4e8e-84f2-e10413bc22a5
02/15/2025 01:27:50:INFO:Received: train message deddc300-7983-4e8e-84f2-e10413bc22a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:33:INFO:
[92mINFO [0m:      Received: evaluate message 51d322bf-3f2b-498b-8076-4125133f9780
02/15/2025 01:29:33:INFO:Received: evaluate message 51d322bf-3f2b-498b-8076-4125133f9780

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:29:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:11:INFO:
[92mINFO [0m:      Received: train message 4c31214b-7ae0-43b6-9ac7-59b80a2c1edb
02/15/2025 01:30:11:INFO:Received: train message 4c31214b-7ae0-43b6-9ac7-59b80a2c1edb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:30:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:25:INFO:
[92mINFO [0m:      Received: evaluate message f4a44249-29f2-46ee-be09-e323919bf4c4
02/15/2025 01:31:25:INFO:Received: evaluate message f4a44249-29f2-46ee-be09-e323919bf4c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:31:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:15:INFO:
[92mINFO [0m:      Received: train message 46cc6719-3d8b-4639-a620-4d8800a40fc4
02/15/2025 01:32:15:INFO:Received: train message 46cc6719-3d8b-4639-a620-4d8800a40fc4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:54:INFO:
[92mINFO [0m:      Received: evaluate message 1db32908-ff59-4ae5-91f9-4ac7f429e52a
02/15/2025 01:33:54:INFO:Received: evaluate message 1db32908-ff59-4ae5-91f9-4ac7f429e52a

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:33:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:30:INFO:
[92mINFO [0m:      Received: train message c28b2e3c-2aba-4a1c-ab1b-0d6683641ae6
02/15/2025 01:34:30:INFO:Received: train message c28b2e3c-2aba-4a1c-ab1b-0d6683641ae6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:04:INFO:
[92mINFO [0m:      Received: evaluate message b5823545-b0fb-4302-a76c-1cb4a2f9ff3b
02/15/2025 01:36:04:INFO:Received: evaluate message b5823545-b0fb-4302-a76c-1cb4a2f9ff3b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:42:INFO:
[92mINFO [0m:      Received: train message 63008e1c-a402-4d23-8bf7-9fc80fc24af9
02/15/2025 01:36:42:INFO:Received: train message 63008e1c-a402-4d23-8bf7-9fc80fc24af9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:12:INFO:
[92mINFO [0m:      Received: evaluate message 9d78becf-9d2a-49b0-aeb1-09cf8cbad0d3
02/15/2025 01:38:12:INFO:Received: evaluate message 9d78becf-9d2a-49b0-aeb1-09cf8cbad0d3

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:38:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:39:INFO:
[92mINFO [0m:      Received: train message 2d88f61f-a371-42e4-8f63-bcc494f713f7
02/15/2025 01:38:39:INFO:Received: train message 2d88f61f-a371-42e4-8f63-bcc494f713f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:39:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:40:05:INFO:
[92mINFO [0m:      Received: evaluate message aee33f4e-1d16-4efe-a8fd-78dde97dede0
02/15/2025 01:40:05:INFO:Received: evaluate message aee33f4e-1d16-4efe-a8fd-78dde97dede0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:40:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:07:INFO:
[92mINFO [0m:      Received: train message 683a36c4-b574-45a1-bb11-b2cedeacea32
02/15/2025 01:41:07:INFO:Received: train message 683a36c4-b574-45a1-bb11-b2cedeacea32
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:41:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:19:INFO:
[92mINFO [0m:      Received: evaluate message 7e4379ec-650c-4951-9330-6c0dac2d47ef
02/15/2025 01:42:19:INFO:Received: evaluate message 7e4379ec-650c-4951-9330-6c0dac2d47ef

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:42:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:59:INFO:
[92mINFO [0m:      Received: train message aa59da94-a94e-476e-a636-0a7179d8b584
02/15/2025 01:42:59:INFO:Received: train message aa59da94-a94e-476e-a636-0a7179d8b584
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:43:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:45:INFO:
[92mINFO [0m:      Received: evaluate message 79a5c2fa-1ad6-42fd-8b00-b0fa8ddcd208
02/15/2025 01:44:45:INFO:Received: evaluate message 79a5c2fa-1ad6-42fd-8b00-b0fa8ddcd208
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:44:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:09:INFO:
[92mINFO [0m:      Received: train message 799fe153-cf2b-4ae8-b705-64a2ed0b8d3f
02/15/2025 01:45:09:INFO:Received: train message 799fe153-cf2b-4ae8-b705-64a2ed0b8d3f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:45:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:31:INFO:
[92mINFO [0m:      Received: evaluate message 78839d24-ecfd-4fbe-994b-2f27c3a85e7b
02/15/2025 01:46:31:INFO:Received: evaluate message 78839d24-ecfd-4fbe-994b-2f27c3a85e7b

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:47:34:INFO:
[92mINFO [0m:      Received: train message 953386a5-a9f7-434b-8fca-69c8e21e8662
02/15/2025 01:47:34:INFO:Received: train message 953386a5-a9f7-434b-8fca-69c8e21e8662
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:48:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:59:INFO:
[92mINFO [0m:      Received: evaluate message b157dd58-beb3-4def-8f7c-e8f348de8a08
02/15/2025 01:48:59:INFO:Received: evaluate message b157dd58-beb3-4def-8f7c-e8f348de8a08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:49:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:49:36:INFO:
[92mINFO [0m:      Received: train message 90c80964-e69c-4bfe-8a22-0885b8d72d33
02/15/2025 01:49:36:INFO:Received: train message 90c80964-e69c-4bfe-8a22-0885b8d72d33
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:50:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:57:INFO:
[92mINFO [0m:      Received: evaluate message c035cbf8-2536-45ca-90d3-2aff7d6d0221
02/15/2025 01:50:57:INFO:Received: evaluate message c035cbf8-2536-45ca-90d3-2aff7d6d0221

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:13:INFO:
[92mINFO [0m:      Received: reconnect message 18a8f409-afad-4efc-b3bb-cfc93866b706
02/15/2025 01:51:13:INFO:Received: reconnect message 18a8f409-afad-4efc-b3bb-cfc93866b706
02/15/2025 01:51:13:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 01:51:13:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}



Final client history:
{'loss': [1.2058249383908497, 1.1574853137659784, 1.1234915474600118, 1.0994388729077564, 1.0821013466467422, 1.0686665325075317, 1.0579408751138026, 1.0493917102642223, 1.044455025883929, 1.0383571505639775, 1.0347835622419128, 1.0324495461324494, 1.027916499653116, 1.0256349667615496, 1.0239284475675499, 1.0233403887014263, 1.0210476706417582, 1.0179764751907063, 1.0165869213809624, 1.015255855153183, 1.0129020319402544, 1.0151431251075511, 1.0159764715803146, 1.01314815932461, 1.0148416612183704, 1.0154970903896186, 1.0138044041334604, 1.012060859596664, 1.013165216505574, 1.011685040148093], 'accuracy': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'auc': [0.5004293699052179, 0.5623735130686665, 0.5951737799946877, 0.612644396196984, 0.6270697212278655, 0.6374241262171938, 0.6435452980672317, 0.648904454213816, 0.6558783441996208, 0.6663552271192316, 0.674809050825059, 0.6800752551311728, 0.6820138473640223, 0.6839242035006312, 0.6833225474244624, 0.6862912742634615, 0.6851225713992143, 0.6868483397214604, 0.6854327466097574, 0.6843822169782915, 0.6858631904310682, 0.6876514746586334, 0.68759878750197, 0.6881593343147202, 0.6908740322282004, 0.6938770366402662, 0.6948727950650468, 0.6944586385083702, 0.69694923429549, 0.69862750426247], 'precision': [0.24999954080217573, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.25039108324097514, 0.48410338863548535, 0.4263149910297899, 0.4263149910297899, 0.4010995225239844, 0.4010995225239844, 0.4010995225239844, 0.42671240768158236, 0.452131884160983, 0.42751480482310783, 0.41000458931812916, 0.41000458931812916, 0.41000458931812916, 0.41332922155526963, 0.4048787596062678, 0.41332922155526963, 0.4082706831609604, 0.4238219931186583, 0.4238219931186583, 0.425102590929484], 'recall': [0.4988272087568413, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5003909304143862, 0.5011727912431587, 0.5019546520719312, 0.5019546520719312, 0.5011727912431587, 0.5011727912431587, 0.5011727912431587, 0.5027365129007036, 0.5043002345582487, 0.5043002345582487, 0.5035183737294762, 0.5035183737294762, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5043002345582487, 0.5035183737294762, 0.5058639562157936, 0.5058639562157936, 0.5074276778733385], 'f1': [0.3330718973441611, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.333767791000737, 0.3368747031114691, 0.3389206512904971, 0.3389206512904971, 0.33856189446363516, 0.33856189446363516, 0.33856189446363516, 0.34211558998256714, 0.34677533244942577, 0.34835222097341123, 0.34812002645284723, 0.34812002645284723, 0.34812002645284723, 0.351152541044197, 0.3495232464440083, 0.351152541044197, 0.3519972633566007, 0.36047992242945576, 0.36047992242945576, 0.36384488809232807]}

