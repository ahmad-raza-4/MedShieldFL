nohup: ignoring input
02/10/2025 11:50:22:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/10/2025 11:50:22:DEBUG:ChannelConnectivity.IDLE
02/10/2025 11:50:22:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739217022.957981  749377 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/10/2025 11:50:52:INFO:
[92mINFO [0m:      Received: train message f550d7d6-2cc8-4d6f-9d1c-bdb2c2e54e85
02/10/2025 11:50:52:INFO:Received: train message f550d7d6-2cc8-4d6f-9d1c-bdb2c2e54e85
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:51:20:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:52:25:INFO:
[92mINFO [0m:      Received: evaluate message 6dc7b762-962a-42ab-a0c7-2811caddd661
02/10/2025 11:52:25:INFO:Received: evaluate message 6dc7b762-962a-42ab-a0c7-2811caddd661
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:52:29:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:52:55:INFO:
[92mINFO [0m:      Received: train message 1cfcc932-7980-4bf9-a5e6-d772536987f0
02/10/2025 11:52:55:INFO:Received: train message 1cfcc932-7980-4bf9-a5e6-d772536987f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:53:22:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:54:28:INFO:
[92mINFO [0m:      Received: evaluate message dc2f8a60-0c71-402a-8ee9-1cf336fd3dd3
02/10/2025 11:54:28:INFO:Received: evaluate message dc2f8a60-0c71-402a-8ee9-1cf336fd3dd3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:54:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:55:07:INFO:
[92mINFO [0m:      Received: train message 1a6c311b-2532-4c18-a718-9fcc66a5416c
02/10/2025 11:55:07:INFO:Received: train message 1a6c311b-2532-4c18-a718-9fcc66a5416c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:55:41:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:26:INFO:
[92mINFO [0m:      Received: evaluate message 84b91634-f373-4291-aec9-fbc7172805e3
02/10/2025 11:56:26:INFO:Received: evaluate message 84b91634-f373-4291-aec9-fbc7172805e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:56:30:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:59:INFO:
[92mINFO [0m:      Received: train message cd7641d2-76a9-44c2-877c-07f5f1c42226
02/10/2025 11:56:59:INFO:Received: train message cd7641d2-76a9-44c2-877c-07f5f1c42226
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:57:28:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:29:INFO:
[92mINFO [0m:      Received: evaluate message 79ff76e6-cee3-456c-9618-87f60a564737
02/10/2025 11:58:29:INFO:Received: evaluate message 79ff76e6-cee3-456c-9618-87f60a564737
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:58:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:58:INFO:
[92mINFO [0m:      Received: train message c39d30b6-722a-498a-b25a-3ba8db736f25
02/10/2025 11:58:58:INFO:Received: train message c39d30b6-722a-498a-b25a-3ba8db736f25
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:59:27:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:00:37:INFO:
[92mINFO [0m:      Received: evaluate message 7c1751e4-74ee-4a62-a8b9-1fd705a54ad7
02/10/2025 12:00:37:INFO:Received: evaluate message 7c1751e4-74ee-4a62-a8b9-1fd705a54ad7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:00:40:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:01:07:INFO:
[92mINFO [0m:      Received: train message a2969da2-5b56-4786-b772-36b0dc4f83ee
02/10/2025 12:01:07:INFO:Received: train message a2969da2-5b56-4786-b772-36b0dc4f83ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:01:35:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:02:33:INFO:
[92mINFO [0m:      Received: evaluate message 859c9446-1ace-4a54-be0b-6069602d8e49
02/10/2025 12:02:33:INFO:Received: evaluate message 859c9446-1ace-4a54-be0b-6069602d8e49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:02:36:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:03:14:INFO:
[92mINFO [0m:      Received: train message 31bb4af1-a907-4e2d-aaef-3dd3ef4c7e05
02/10/2025 12:03:14:INFO:Received: train message 31bb4af1-a907-4e2d-aaef-3dd3ef4c7e05
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:03:41:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:04:15:INFO:
[92mINFO [0m:      Received: evaluate message 2469e923-1f40-4b95-816b-be971fb30afd
02/10/2025 12:04:15:INFO:Received: evaluate message 2469e923-1f40-4b95-816b-be971fb30afd
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282], 'accuracy': [0.506645817044566], 'auc': [0.6996165186305552], 'precision': [0.394924318810202], 'recall': [0.506645817044566], 'f1': [0.40278177667270126]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696], 'accuracy': [0.506645817044566, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256], 'precision': [0.394924318810202, 0.4198365350542879], 'recall': [0.506645817044566, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:04:17:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:05:04:INFO:
[92mINFO [0m:      Received: train message 1e11779f-3f02-44d7-8f88-50221eb615b3
02/10/2025 12:05:04:INFO:Received: train message 1e11779f-3f02-44d7-8f88-50221eb615b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:05:35:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:06:35:INFO:
[92mINFO [0m:      Received: evaluate message c50273d3-e9d0-4692-abaa-409fa2cb5fc1
02/10/2025 12:06:35:INFO:Received: evaluate message c50273d3-e9d0-4692-abaa-409fa2cb5fc1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:06:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:07:02:INFO:
[92mINFO [0m:      Received: train message 6b702a75-2397-4da6-995e-809274249f2a
02/10/2025 12:07:02:INFO:Received: train message 6b702a75-2397-4da6-995e-809274249f2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:07:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:08:41:INFO:
[92mINFO [0m:      Received: evaluate message dbde7d97-30bb-4a99-a07b-2233d54a6ce6
02/10/2025 12:08:41:INFO:Received: evaluate message dbde7d97-30bb-4a99-a07b-2233d54a6ce6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:08:44:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:09:07:INFO:
[92mINFO [0m:      Received: train message 21bd9ccc-f25a-41c5-91a3-61e9ec156d62
02/10/2025 12:09:07:INFO:Received: train message 21bd9ccc-f25a-41c5-91a3-61e9ec156d62
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:09:37:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:10:36:INFO:
[92mINFO [0m:      Received: evaluate message e5dcc12c-15a0-4750-8b69-8ccbf2f22121
02/10/2025 12:10:36:INFO:Received: evaluate message e5dcc12c-15a0-4750-8b69-8ccbf2f22121
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:10:39:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:11:12:INFO:
[92mINFO [0m:      Received: train message 51ead81b-9efa-4fcf-a6b3-ce1c95aeab73
02/10/2025 12:11:12:INFO:Received: train message 51ead81b-9efa-4fcf-a6b3-ce1c95aeab73
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:11:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:12:29:INFO:
[92mINFO [0m:      Received: evaluate message a9fa7f4a-ce69-430d-8946-8c4e145ac1e8
02/10/2025 12:12:29:INFO:Received: evaluate message a9fa7f4a-ce69-430d-8946-8c4e145ac1e8

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:12:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:12:46:INFO:
[92mINFO [0m:      Received: train message 5e42c65b-71f4-4513-afc3-c96c87219a4f
02/10/2025 12:12:46:INFO:Received: train message 5e42c65b-71f4-4513-afc3-c96c87219a4f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:13:14:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:14:28:INFO:
[92mINFO [0m:      Received: evaluate message ade28162-b40a-4fe0-b015-252a439084fc
02/10/2025 12:14:28:INFO:Received: evaluate message ade28162-b40a-4fe0-b015-252a439084fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:14:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:15:07:INFO:
[92mINFO [0m:      Received: train message 3de70e17-1cbb-4dee-a9ab-e219ad467403
02/10/2025 12:15:07:INFO:Received: train message 3de70e17-1cbb-4dee-a9ab-e219ad467403
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:15:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:17:INFO:
[92mINFO [0m:      Received: evaluate message 898c695c-5c77-4287-adb4-dd9ab75db495
02/10/2025 12:16:17:INFO:Received: evaluate message 898c695c-5c77-4287-adb4-dd9ab75db495
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:16:20:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:58:INFO:
[92mINFO [0m:      Received: train message fb090198-6124-4993-b843-cf660e7caa70
02/10/2025 12:16:58:INFO:Received: train message fb090198-6124-4993-b843-cf660e7caa70
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:17:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:26:INFO:
[92mINFO [0m:      Received: evaluate message 51d63709-89c6-48b4-9d99-d7c29b6755ce
02/10/2025 12:18:26:INFO:Received: evaluate message 51d63709-89c6-48b4-9d99-d7c29b6755ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:18:29:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:19:03:INFO:
[92mINFO [0m:      Received: train message 667b98d3-76ee-4174-9cea-65d899439205
02/10/2025 12:19:03:INFO:Received: train message 667b98d3-76ee-4174-9cea-65d899439205
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:19:33:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:21:INFO:
[92mINFO [0m:      Received: evaluate message 7acf0613-6b1a-49d6-b4a5-1df7ed79496e
02/10/2025 12:20:21:INFO:Received: evaluate message 7acf0613-6b1a-49d6-b4a5-1df7ed79496e
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:20:24:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:46:INFO:
[92mINFO [0m:      Received: train message d7f44379-6be1-4bec-8570-e86c5af83f59
02/10/2025 12:20:46:INFO:Received: train message d7f44379-6be1-4bec-8570-e86c5af83f59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:21:12:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:22:INFO:
[92mINFO [0m:      Received: evaluate message b6b929d5-e62c-4c12-9ae7-26a877c0dc95
02/10/2025 12:22:22:INFO:Received: evaluate message b6b929d5-e62c-4c12-9ae7-26a877c0dc95
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:22:25:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:57:INFO:
[92mINFO [0m:      Received: train message 551fde50-4cab-48b8-9a1c-15f9f3c7accd
02/10/2025 12:22:57:INFO:Received: train message 551fde50-4cab-48b8-9a1c-15f9f3c7accd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:23:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:15:INFO:
[92mINFO [0m:      Received: evaluate message 1ed20f60-a75e-4f7a-a1ae-e8309787f8c4
02/10/2025 12:24:15:INFO:Received: evaluate message 1ed20f60-a75e-4f7a-a1ae-e8309787f8c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:24:18:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:55:INFO:
[92mINFO [0m:      Received: train message c8278f7f-d88b-463d-9a72-b2d6e6486f2c
02/10/2025 12:24:55:INFO:Received: train message c8278f7f-d88b-463d-9a72-b2d6e6486f2c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:25:24:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:19:INFO:
[92mINFO [0m:      Received: evaluate message 23c613fd-bed0-403d-aced-d22cae5424b3
02/10/2025 12:26:19:INFO:Received: evaluate message 23c613fd-bed0-403d-aced-d22cae5424b3

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:26:22:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:51:INFO:
[92mINFO [0m:      Received: train message 11d89582-b06d-4f20-b428-264d29d05215
02/10/2025 12:26:51:INFO:Received: train message 11d89582-b06d-4f20-b428-264d29d05215
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:27:20:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:27:55:INFO:
[92mINFO [0m:      Received: evaluate message dc1bf044-a219-4347-af10-12dda1e21d6c
02/10/2025 12:27:55:INFO:Received: evaluate message dc1bf044-a219-4347-af10-12dda1e21d6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:27:57:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:28:31:INFO:
[92mINFO [0m:      Received: train message 097e616a-0a38-4e5e-9bfa-67d709e3b523
02/10/2025 12:28:31:INFO:Received: train message 097e616a-0a38-4e5e-9bfa-67d709e3b523
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:28:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:05:INFO:
[92mINFO [0m:      Received: evaluate message 46b02311-fa12-4b8b-9d43-bd38bb9edff7
02/10/2025 12:30:05:INFO:Received: evaluate message 46b02311-fa12-4b8b-9d43-bd38bb9edff7

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:30:09:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:42:INFO:
[92mINFO [0m:      Received: train message 765d1dc5-31f5-45fe-a11f-8f252819761d
02/10/2025 12:30:42:INFO:Received: train message 765d1dc5-31f5-45fe-a11f-8f252819761d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:31:13:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:31:56:INFO:
[92mINFO [0m:      Received: evaluate message 24422fce-39c8-4960-99b3-08cf8f7dbc86
02/10/2025 12:31:56:INFO:Received: evaluate message 24422fce-39c8-4960-99b3-08cf8f7dbc86
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:31:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:32:31:INFO:
[92mINFO [0m:      Received: train message deaba7a8-9934-485b-9cc2-974b36f25c49
02/10/2025 12:32:31:INFO:Received: train message deaba7a8-9934-485b-9cc2-974b36f25c49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:33:00:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:33:47:INFO:
[92mINFO [0m:      Received: evaluate message b53b2bcf-079e-49c8-95b5-8f1c979f6fe9
02/10/2025 12:33:47:INFO:Received: evaluate message b53b2bcf-079e-49c8-95b5-8f1c979f6fe9

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:33:50:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:34:22:INFO:
[92mINFO [0m:      Received: train message 96c04788-7b1b-451c-964b-2c536bdb08ea
02/10/2025 12:34:22:INFO:Received: train message 96c04788-7b1b-451c-964b-2c536bdb08ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:34:51:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:35:32:INFO:
[92mINFO [0m:      Received: evaluate message 7a6a5f06-4a78-44d3-a75b-89790e0fcc59
02/10/2025 12:35:32:INFO:Received: evaluate message 7a6a5f06-4a78-44d3-a75b-89790e0fcc59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:35:34:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:36:24:INFO:
[92mINFO [0m:      Received: train message bb804645-8ced-4c30-9c68-60ee5ec535cd
02/10/2025 12:36:24:INFO:Received: train message bb804645-8ced-4c30-9c68-60ee5ec535cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:36:54:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:37:53:INFO:
[92mINFO [0m:      Received: evaluate message 046df850-707a-4944-aa08-f4859e913694
02/10/2025 12:37:53:INFO:Received: evaluate message 046df850-707a-4944-aa08-f4859e913694

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:37:57:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:38:20:INFO:
[92mINFO [0m:      Received: train message b38f90e5-205e-453b-a891-0845e24b8e47
02/10/2025 12:38:20:INFO:Received: train message b38f90e5-205e-453b-a891-0845e24b8e47
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:38:52:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:39:43:INFO:
[92mINFO [0m:      Received: evaluate message 8dbc175d-0f9c-47c8-b52e-82f418a59838
02/10/2025 12:39:43:INFO:Received: evaluate message 8dbc175d-0f9c-47c8-b52e-82f418a59838
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:39:46:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:40:30:INFO:
[92mINFO [0m:      Received: train message 67b7d52c-7d7b-4f80-b157-1eb881214318
02/10/2025 12:40:30:INFO:Received: train message 67b7d52c-7d7b-4f80-b157-1eb881214318
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:40:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:41:55:INFO:
[92mINFO [0m:      Received: evaluate message f8c95983-8b40-4297-b721-7eb61b4d987f
02/10/2025 12:41:55:INFO:Received: evaluate message f8c95983-8b40-4297-b721-7eb61b4d987f

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:41:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:42:21:INFO:
[92mINFO [0m:      Received: train message fd9d1b33-1e58-41cd-8f1e-10f429f10bba
02/10/2025 12:42:21:INFO:Received: train message fd9d1b33-1e58-41cd-8f1e-10f429f10bba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:42:52:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:44:00:INFO:
[92mINFO [0m:      Received: evaluate message 52fd8264-11b5-46a8-b05c-73205d838d56
02/10/2025 12:44:00:INFO:Received: evaluate message 52fd8264-11b5-46a8-b05c-73205d838d56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:44:03:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:44:33:INFO:
[92mINFO [0m:      Received: train message dd3073f5-62d2-455c-b60a-57efd91b0e4f
02/10/2025 12:44:33:INFO:Received: train message dd3073f5-62d2-455c-b60a-57efd91b0e4f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:45:05:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:45:57:INFO:
[92mINFO [0m:      Received: evaluate message 9842dc5d-74e0-449b-8917-de9b3509be21
02/10/2025 12:45:57:INFO:Received: evaluate message 9842dc5d-74e0-449b-8917-de9b3509be21

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:45:59:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:46:28:INFO:
[92mINFO [0m:      Received: train message e8233228-d7a4-4f82-aaec-31b6a3aed5db
02/10/2025 12:46:28:INFO:Received: train message e8233228-d7a4-4f82-aaec-31b6a3aed5db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:46:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:47:39:INFO:
[92mINFO [0m:      Received: evaluate message 26c3dffe-bc5c-4a94-89f7-a53acf7437d7
02/10/2025 12:47:39:INFO:Received: evaluate message 26c3dffe-bc5c-4a94-89f7-a53acf7437d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:47:43:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:48:16:INFO:
[92mINFO [0m:      Received: train message 21b49f45-14d2-40cb-88e6-33323158145b
02/10/2025 12:48:16:INFO:Received: train message 21b49f45-14d2-40cb-88e6-33323158145b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:48:45:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:42:INFO:
[92mINFO [0m:      Received: evaluate message 654552e5-dae3-4734-9224-62f5f57f2394
02/10/2025 12:49:42:INFO:Received: evaluate message 654552e5-dae3-4734-9224-62f5f57f2394

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:49:44:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:53:INFO:
[92mINFO [0m:      Received: reconnect message 40339b0e-ca5d-4525-9c12-fcd911086d43
02/10/2025 12:49:53:INFO:Received: reconnect message 40339b0e-ca5d-4525-9c12-fcd911086d43
02/10/2025 12:49:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/10/2025 12:49:53:INFO:Disconnect and shut down
