nohup: ignoring input
02/10/2025 11:50:22:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/10/2025 11:50:22:DEBUG:ChannelConnectivity.IDLE
02/10/2025 11:50:22:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739217022.350292  749292 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/10/2025 11:50:52:INFO:
[92mINFO [0m:      Received: train message 7c26bf90-696b-4f7b-a06e-bf342f02ab8a
02/10/2025 11:50:52:INFO:Received: train message 7c26bf90-696b-4f7b-a06e-bf342f02ab8a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:51:21:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:52:25:INFO:
[92mINFO [0m:      Received: evaluate message ccf68991-c181-4de9-ad57-b07e1a701dc7
02/10/2025 11:52:25:INFO:Received: evaluate message ccf68991-c181-4de9-ad57-b07e1a701dc7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:52:30:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:53:03:INFO:
[92mINFO [0m:      Received: train message f65eb581-1d35-4f0b-bfb6-acad64324bfa
02/10/2025 11:53:03:INFO:Received: train message f65eb581-1d35-4f0b-bfb6-acad64324bfa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:53:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:54:18:INFO:
[92mINFO [0m:      Received: evaluate message 9dbfd701-bac3-459d-9170-df1afac016d7
02/10/2025 11:54:18:INFO:Received: evaluate message 9dbfd701-bac3-459d-9170-df1afac016d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:54:21:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:55:07:INFO:
[92mINFO [0m:      Received: train message 50b3768a-2139-4fdc-bbb4-980775381694
02/10/2025 11:55:07:INFO:Received: train message 50b3768a-2139-4fdc-bbb4-980775381694
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:55:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:26:INFO:
[92mINFO [0m:      Received: evaluate message 4a0cb45e-d569-4737-bae3-93057d60e412
02/10/2025 11:56:26:INFO:Received: evaluate message 4a0cb45e-d569-4737-bae3-93057d60e412
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:56:30:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:59:INFO:
[92mINFO [0m:      Received: train message 35277a64-b9a6-45fc-8d86-e66fa7eac08a
02/10/2025 11:56:59:INFO:Received: train message 35277a64-b9a6-45fc-8d86-e66fa7eac08a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:57:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:27:INFO:
[92mINFO [0m:      Received: evaluate message 083806ce-dd68-4019-af79-ac55a282b059
02/10/2025 11:58:27:INFO:Received: evaluate message 083806ce-dd68-4019-af79-ac55a282b059
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:58:29:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:56:INFO:
[92mINFO [0m:      Received: train message 8b910d29-84b0-43d7-94ce-5b817fb587cf
02/10/2025 11:58:56:INFO:Received: train message 8b910d29-84b0-43d7-94ce-5b817fb587cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:59:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:00:15:INFO:
[92mINFO [0m:      Received: evaluate message 4514fca0-6c0f-4e9b-930e-971d3e7b3f3c
02/10/2025 12:00:15:INFO:Received: evaluate message 4514fca0-6c0f-4e9b-930e-971d3e7b3f3c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:00:17:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:01:08:INFO:
[92mINFO [0m:      Received: train message 3b42a1c5-341e-4b36-9f4a-5cd944437708
02/10/2025 12:01:08:INFO:Received: train message 3b42a1c5-341e-4b36-9f4a-5cd944437708
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:01:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:02:36:INFO:
[92mINFO [0m:      Received: evaluate message d6633814-4a5c-4f1a-9b5a-f9d7d157128b
02/10/2025 12:02:36:INFO:Received: evaluate message d6633814-4a5c-4f1a-9b5a-f9d7d157128b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:02:39:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:03:12:INFO:
[92mINFO [0m:      Received: train message 50a02ef5-6d35-4471-95f3-9c7375184e5d
02/10/2025 12:03:12:INFO:Received: train message 50a02ef5-6d35-4471-95f3-9c7375184e5d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:03:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:04:31:INFO:
[92mINFO [0m:      Received: evaluate message 73686f69-bbde-4cac-992a-6aa9f45b4e5b
02/10/2025 12:04:31:INFO:Received: evaluate message 73686f69-bbde-4cac-992a-6aa9f45b4e5b
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282], 'accuracy': [0.506645817044566], 'auc': [0.6996165186305552], 'precision': [0.394924318810202], 'recall': [0.506645817044566], 'f1': [0.40278177667270126]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696], 'accuracy': [0.506645817044566, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256], 'precision': [0.394924318810202, 0.4198365350542879], 'recall': [0.506645817044566, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:04:33:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:05:14:INFO:
[92mINFO [0m:      Received: train message 27e5f213-2001-4a13-8b4e-3eb553f8b2c6
02/10/2025 12:05:14:INFO:Received: train message 27e5f213-2001-4a13-8b4e-3eb553f8b2c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:05:46:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:06:21:INFO:
[92mINFO [0m:      Received: evaluate message 7900f738-ea3e-42a7-96e7-360d4c7265c5
02/10/2025 12:06:21:INFO:Received: evaluate message 7900f738-ea3e-42a7-96e7-360d4c7265c5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:06:23:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:06:50:INFO:
[92mINFO [0m:      Received: train message 59e0c99b-d682-4196-a5f4-82eecbd320b7
02/10/2025 12:06:50:INFO:Received: train message 59e0c99b-d682-4196-a5f4-82eecbd320b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:07:21:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:08:20:INFO:
[92mINFO [0m:      Received: evaluate message e425414e-0906-4a59-8d63-a788f3955f42
02/10/2025 12:08:20:INFO:Received: evaluate message e425414e-0906-4a59-8d63-a788f3955f42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:08:23:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:09:15:INFO:
[92mINFO [0m:      Received: train message aa98c2d4-dea9-429e-babd-e33088c58a97
02/10/2025 12:09:15:INFO:Received: train message aa98c2d4-dea9-429e-babd-e33088c58a97
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:09:46:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:10:30:INFO:
[92mINFO [0m:      Received: evaluate message c376f1ed-4563-4169-8581-2d0ff6b6a771
02/10/2025 12:10:30:INFO:Received: evaluate message c376f1ed-4563-4169-8581-2d0ff6b6a771
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:10:33:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:11:05:INFO:
[92mINFO [0m:      Received: train message 0f151956-8849-4ab7-9324-9f11db97c03a
02/10/2025 12:11:05:INFO:Received: train message 0f151956-8849-4ab7-9324-9f11db97c03a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:11:37:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:12:25:INFO:
[92mINFO [0m:      Received: evaluate message 2582a28d-5701-471f-b6f1-cda7861f629a
02/10/2025 12:12:25:INFO:Received: evaluate message 2582a28d-5701-471f-b6f1-cda7861f629a

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:12:28:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:12:57:INFO:
[92mINFO [0m:      Received: train message 9511fe9f-6c4c-4e0e-b3b1-fdb7e240fc25
02/10/2025 12:12:57:INFO:Received: train message 9511fe9f-6c4c-4e0e-b3b1-fdb7e240fc25
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:13:27:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:14:30:INFO:
[92mINFO [0m:      Received: evaluate message 60f4cd5a-9dd7-435f-8ac8-dae8e22bf1fe
02/10/2025 12:14:30:INFO:Received: evaluate message 60f4cd5a-9dd7-435f-8ac8-dae8e22bf1fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:14:33:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:14:50:INFO:
[92mINFO [0m:      Received: train message 7fbd42d7-e7fe-4bb6-82b1-ecdcd2907d40
02/10/2025 12:14:50:INFO:Received: train message 7fbd42d7-e7fe-4bb6-82b1-ecdcd2907d40
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:15:18:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:18:INFO:
[92mINFO [0m:      Received: evaluate message 07095b6f-25c9-498e-806e-dab4802af62b
02/10/2025 12:16:18:INFO:Received: evaluate message 07095b6f-25c9-498e-806e-dab4802af62b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:16:21:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:43:INFO:
[92mINFO [0m:      Received: train message 3fb32e09-5084-438c-8b97-6bb2b434863b
02/10/2025 12:16:43:INFO:Received: train message 3fb32e09-5084-438c-8b97-6bb2b434863b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:17:12:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:21:INFO:
[92mINFO [0m:      Received: evaluate message 20f2a8cc-c0d4-4e86-a3c0-bbdffd041579
02/10/2025 12:18:21:INFO:Received: evaluate message 20f2a8cc-c0d4-4e86-a3c0-bbdffd041579
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:18:23:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:45:INFO:
[92mINFO [0m:      Received: train message 869aeaff-c9e5-415f-9883-a3b33ff1b993
02/10/2025 12:18:45:INFO:Received: train message 869aeaff-c9e5-415f-9883-a3b33ff1b993
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:19:14:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:26:INFO:
[92mINFO [0m:      Received: evaluate message c5a756fc-00dc-490b-9a29-2393d4f69791
02/10/2025 12:20:26:INFO:Received: evaluate message c5a756fc-00dc-490b-9a29-2393d4f69791
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:20:30:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:21:03:INFO:
[92mINFO [0m:      Received: train message 4487c6ae-13f7-4311-87b1-3d87354f93a4
02/10/2025 12:21:03:INFO:Received: train message 4487c6ae-13f7-4311-87b1-3d87354f93a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:21:33:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:17:INFO:
[92mINFO [0m:      Received: evaluate message 5c780032-1665-4999-af68-2a53549156e7
02/10/2025 12:22:17:INFO:Received: evaluate message 5c780032-1665-4999-af68-2a53549156e7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:22:19:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:49:INFO:
[92mINFO [0m:      Received: train message 48a0ec90-28a4-4f8b-9ce6-476f8977d36b
02/10/2025 12:22:49:INFO:Received: train message 48a0ec90-28a4-4f8b-9ce6-476f8977d36b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:23:20:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:15:INFO:
[92mINFO [0m:      Received: evaluate message 88da3ecf-d249-43ae-98f5-e739b2651403
02/10/2025 12:24:15:INFO:Received: evaluate message 88da3ecf-d249-43ae-98f5-e739b2651403
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:24:18:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:55:INFO:
[92mINFO [0m:      Received: train message 17179006-76ed-400d-b37a-7da251f01827
02/10/2025 12:24:55:INFO:Received: train message 17179006-76ed-400d-b37a-7da251f01827
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:25:27:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:18:INFO:
[92mINFO [0m:      Received: evaluate message c9572af2-62ca-4065-9d24-fffc10e8fe64
02/10/2025 12:26:18:INFO:Received: evaluate message c9572af2-62ca-4065-9d24-fffc10e8fe64

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:26:20:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:44:INFO:
[92mINFO [0m:      Received: train message 353c074f-6174-4177-b970-7251b5705244
02/10/2025 12:26:44:INFO:Received: train message 353c074f-6174-4177-b970-7251b5705244
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:27:13:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:27:57:INFO:
[92mINFO [0m:      Received: evaluate message f3d64919-895f-4d9c-a420-9f595a877848
02/10/2025 12:27:57:INFO:Received: evaluate message f3d64919-895f-4d9c-a420-9f595a877848
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:27:59:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:28:46:INFO:
[92mINFO [0m:      Received: train message 8d0d5a21-e134-409d-b8e2-189279e1bf16
02/10/2025 12:28:46:INFO:Received: train message 8d0d5a21-e134-409d-b8e2-189279e1bf16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:29:16:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:29:57:INFO:
[92mINFO [0m:      Received: evaluate message 935a5d4d-bca4-4cb4-b40e-dca071c85766
02/10/2025 12:29:57:INFO:Received: evaluate message 935a5d4d-bca4-4cb4-b40e-dca071c85766

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:29:59:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:42:INFO:
[92mINFO [0m:      Received: train message 222546e7-11d8-430c-988e-7266582a904a
02/10/2025 12:30:42:INFO:Received: train message 222546e7-11d8-430c-988e-7266582a904a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:31:14:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:31:49:INFO:
[92mINFO [0m:      Received: evaluate message 36911420-5936-4e47-a6fe-7340680184ec
02/10/2025 12:31:49:INFO:Received: evaluate message 36911420-5936-4e47-a6fe-7340680184ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:31:51:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:32:06:INFO:
[92mINFO [0m:      Received: train message 3c483a42-d058-40a1-b173-c8e022ca94c2
02/10/2025 12:32:06:INFO:Received: train message 3c483a42-d058-40a1-b173-c8e022ca94c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:32:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:33:54:INFO:
[92mINFO [0m:      Received: evaluate message 2965d0f2-638f-4777-aae5-feccca5c8275
02/10/2025 12:33:54:INFO:Received: evaluate message 2965d0f2-638f-4777-aae5-feccca5c8275

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:33:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:34:28:INFO:
[92mINFO [0m:      Received: train message 4fdfa9b9-3206-4ae9-b633-7ed6d828742d
02/10/2025 12:34:28:INFO:Received: train message 4fdfa9b9-3206-4ae9-b633-7ed6d828742d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:34:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:35:36:INFO:
[92mINFO [0m:      Received: evaluate message f2dd0737-3fae-4d0e-9ee8-8b083111bb1b
02/10/2025 12:35:36:INFO:Received: evaluate message f2dd0737-3fae-4d0e-9ee8-8b083111bb1b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:35:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:36:22:INFO:
[92mINFO [0m:      Received: train message e623e8eb-fe3e-4159-a0fa-3ea01b788077
02/10/2025 12:36:22:INFO:Received: train message e623e8eb-fe3e-4159-a0fa-3ea01b788077
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:36:54:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:37:44:INFO:
[92mINFO [0m:      Received: evaluate message 3f02d54a-b506-43d3-83dd-dd90fcd1de31
02/10/2025 12:37:44:INFO:Received: evaluate message 3f02d54a-b506-43d3-83dd-dd90fcd1de31

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:37:47:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:38:26:INFO:
[92mINFO [0m:      Received: train message 37a03043-ca49-46e3-bc09-03a6deaf88e5
02/10/2025 12:38:26:INFO:Received: train message 37a03043-ca49-46e3-bc09-03a6deaf88e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:39:00:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:39:53:INFO:
[92mINFO [0m:      Received: evaluate message 03f603db-9190-4a1b-af3f-b61048dd4c8d
02/10/2025 12:39:53:INFO:Received: evaluate message 03f603db-9190-4a1b-af3f-b61048dd4c8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:39:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:40:23:INFO:
[92mINFO [0m:      Received: train message a6bd3601-b452-4cf9-bc1a-53ab9b3cd4a3
02/10/2025 12:40:23:INFO:Received: train message a6bd3601-b452-4cf9-bc1a-53ab9b3cd4a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:40:49:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:41:52:INFO:
[92mINFO [0m:      Received: evaluate message 5bb1bf77-d842-471b-9008-61ec63176dea
02/10/2025 12:41:52:INFO:Received: evaluate message 5bb1bf77-d842-471b-9008-61ec63176dea

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:41:55:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:42:26:INFO:
[92mINFO [0m:      Received: train message 37a303a0-adf1-49c5-a74a-42791cfafa13
02/10/2025 12:42:26:INFO:Received: train message 37a303a0-adf1-49c5-a74a-42791cfafa13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:42:57:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:43:59:INFO:
[92mINFO [0m:      Received: evaluate message 175a25c8-8a3b-4c37-9ab4-7d75541b8f45
02/10/2025 12:43:59:INFO:Received: evaluate message 175a25c8-8a3b-4c37-9ab4-7d75541b8f45
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:44:02:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:44:32:INFO:
[92mINFO [0m:      Received: train message b744364d-d6f4-42f0-81b4-9c6feb356778
02/10/2025 12:44:32:INFO:Received: train message b744364d-d6f4-42f0-81b4-9c6feb356778
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:45:05:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:45:47:INFO:
[92mINFO [0m:      Received: evaluate message c3fef2e8-d007-45f6-b731-855cecaeea43
02/10/2025 12:45:47:INFO:Received: evaluate message c3fef2e8-d007-45f6-b731-855cecaeea43

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:45:49:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:46:37:INFO:
[92mINFO [0m:      Received: train message 0fa62218-1311-4dde-9797-f5f0299762a2
02/10/2025 12:46:37:INFO:Received: train message 0fa62218-1311-4dde-9797-f5f0299762a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:47:11:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:47:51:INFO:
[92mINFO [0m:      Received: evaluate message c9a2d90c-c26c-468f-b88b-5561f10eaddf
02/10/2025 12:47:51:INFO:Received: evaluate message c9a2d90c-c26c-468f-b88b-5561f10eaddf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:47:54:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:48:11:INFO:
[92mINFO [0m:      Received: train message 1e0fe46c-8af7-4ffb-9ee5-9bf5c4efc424
02/10/2025 12:48:11:INFO:Received: train message 1e0fe46c-8af7-4ffb-9ee5-9bf5c4efc424
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:48:39:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:46:INFO:
[92mINFO [0m:      Received: evaluate message 109e5329-084b-4dbc-9615-9ffaec95e410
02/10/2025 12:49:46:INFO:Received: evaluate message 109e5329-084b-4dbc-9615-9ffaec95e410

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:49:48:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:53:INFO:
[92mINFO [0m:      Received: reconnect message 5e29a977-9c0c-4ab7-b4a8-657bb31a17ba
02/10/2025 12:49:53:INFO:Received: reconnect message 5e29a977-9c0c-4ab7-b4a8-657bb31a17ba
02/10/2025 12:49:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/10/2025 12:49:53:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088, 1.1039477369578394], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228, 0.7467077310319794], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085, 0.4791652346454203], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705, 0.4494452346094889]}



Final client history:
{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088, 1.1039477369578394], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228, 0.7467077310319794], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085, 0.4791652346454203], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705, 0.4494452346094889]}

