nohup: ignoring input
02/10/2025 11:50:23:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/10/2025 11:50:23:DEBUG:ChannelConnectivity.IDLE
02/10/2025 11:50:23:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739217023.865211  749458 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/10/2025 11:51:01:INFO:
[92mINFO [0m:      Received: train message dc78a671-0ba2-438e-8bd1-d89eb7810238
02/10/2025 11:51:01:INFO:Received: train message dc78a671-0ba2-438e-8bd1-d89eb7810238
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:51:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:52:12:INFO:
[92mINFO [0m:      Received: evaluate message 8d4f1527-9165-4929-99ff-a428188998d7
02/10/2025 11:52:12:INFO:Received: evaluate message 8d4f1527-9165-4929-99ff-a428188998d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:52:15:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:53:05:INFO:
[92mINFO [0m:      Received: train message c0d54fcb-c08a-4cf6-ab79-b2ae0b78e143
02/10/2025 11:53:05:INFO:Received: train message c0d54fcb-c08a-4cf6-ab79-b2ae0b78e143
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:53:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:54:28:INFO:
[92mINFO [0m:      Received: evaluate message 46058051-5ddf-46c5-ad17-897371887a7d
02/10/2025 11:54:28:INFO:Received: evaluate message 46058051-5ddf-46c5-ad17-897371887a7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:54:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:55:01:INFO:
[92mINFO [0m:      Received: train message 4759e62d-5124-4efb-9a7a-1e0ee81b1e27
02/10/2025 11:55:01:INFO:Received: train message 4759e62d-5124-4efb-9a7a-1e0ee81b1e27
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:55:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:22:INFO:
[92mINFO [0m:      Received: evaluate message 68da2f38-8732-49a6-a26c-f75cd17237ed
02/10/2025 11:56:22:INFO:Received: evaluate message 68da2f38-8732-49a6-a26c-f75cd17237ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:56:25:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:57:08:INFO:
[92mINFO [0m:      Received: train message a617f1d6-1c83-4fec-b166-5e2e1ce400d2
02/10/2025 11:57:08:INFO:Received: train message a617f1d6-1c83-4fec-b166-5e2e1ce400d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:57:40:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:34:INFO:
[92mINFO [0m:      Received: evaluate message 342ea4af-abf0-4fc6-8937-c833772c9e5b
02/10/2025 11:58:34:INFO:Received: evaluate message 342ea4af-abf0-4fc6-8937-c833772c9e5b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:58:36:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:59:13:INFO:
[92mINFO [0m:      Received: train message 955f1c04-b560-4c31-b2ef-ffa970583617
02/10/2025 11:59:13:INFO:Received: train message 955f1c04-b560-4c31-b2ef-ffa970583617
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:59:46:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:00:34:INFO:
[92mINFO [0m:      Received: evaluate message 296c4d79-462b-4e08-9f18-a50c6b2cd5c1
02/10/2025 12:00:34:INFO:Received: evaluate message 296c4d79-462b-4e08-9f18-a50c6b2cd5c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:00:36:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:01:11:INFO:
[92mINFO [0m:      Received: train message a652c019-6d5f-45f8-916b-6e6973b74325
02/10/2025 12:01:11:INFO:Received: train message a652c019-6d5f-45f8-916b-6e6973b74325
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:01:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:02:36:INFO:
[92mINFO [0m:      Received: evaluate message 079b02cc-42f9-405c-bcdf-0409747aab28
02/10/2025 12:02:36:INFO:Received: evaluate message 079b02cc-42f9-405c-bcdf-0409747aab28
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:02:39:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:03:03:INFO:
[92mINFO [0m:      Received: train message bce14dce-5b71-421b-8e09-4a8a027a65f7
02/10/2025 12:03:03:INFO:Received: train message bce14dce-5b71-421b-8e09-4a8a027a65f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:03:34:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:04:34:INFO:
[92mINFO [0m:      Received: evaluate message 325ece37-2c15-4184-8a46-4a3354d1e7c1
02/10/2025 12:04:34:INFO:Received: evaluate message 325ece37-2c15-4184-8a46-4a3354d1e7c1
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282], 'accuracy': [0.506645817044566], 'auc': [0.6996165186305552], 'precision': [0.394924318810202], 'recall': [0.506645817044566], 'f1': [0.40278177667270126]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696], 'accuracy': [0.506645817044566, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256], 'precision': [0.394924318810202, 0.4198365350542879], 'recall': [0.506645817044566, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:04:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:05:04:INFO:
[92mINFO [0m:      Received: train message b0eb0746-1bca-4c7e-b6ca-403c8f363197
02/10/2025 12:05:04:INFO:Received: train message b0eb0746-1bca-4c7e-b6ca-403c8f363197
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:05:39:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:06:27:INFO:
[92mINFO [0m:      Received: evaluate message cc9f8398-86a7-405e-88b5-12f3c51b193b
02/10/2025 12:06:27:INFO:Received: evaluate message cc9f8398-86a7-405e-88b5-12f3c51b193b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:06:30:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:07:11:INFO:
[92mINFO [0m:      Received: train message ebe05063-ed56-4089-b299-7b5e480ef803
02/10/2025 12:07:11:INFO:Received: train message ebe05063-ed56-4089-b299-7b5e480ef803
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:07:45:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:08:41:INFO:
[92mINFO [0m:      Received: evaluate message 9ddbdc39-974b-488c-9068-702833c3729d
02/10/2025 12:08:41:INFO:Received: evaluate message 9ddbdc39-974b-488c-9068-702833c3729d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:08:44:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:09:19:INFO:
[92mINFO [0m:      Received: train message be9503ef-1f52-4586-8a90-dc0bea0bed57
02/10/2025 12:09:19:INFO:Received: train message be9503ef-1f52-4586-8a90-dc0bea0bed57
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:09:52:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:10:28:INFO:
[92mINFO [0m:      Received: evaluate message cd82ac3a-f05e-43de-ba0f-179a742ed4cd
02/10/2025 12:10:28:INFO:Received: evaluate message cd82ac3a-f05e-43de-ba0f-179a742ed4cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:10:30:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:11:14:INFO:
[92mINFO [0m:      Received: train message 95c87a88-ae88-455b-9443-d9be21efad77
02/10/2025 12:11:14:INFO:Received: train message 95c87a88-ae88-455b-9443-d9be21efad77
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:11:47:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:12:16:INFO:
[92mINFO [0m:      Received: evaluate message 9af5e530-733c-45de-be5c-691b0d6d1308
02/10/2025 12:12:16:INFO:Received: evaluate message 9af5e530-733c-45de-be5c-691b0d6d1308

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:12:18:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:13:01:INFO:
[92mINFO [0m:      Received: train message 7a8dadb3-c5c3-4f25-b0ef-7661699b12ff
02/10/2025 12:13:01:INFO:Received: train message 7a8dadb3-c5c3-4f25-b0ef-7661699b12ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:13:36:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:14:22:INFO:
[92mINFO [0m:      Received: evaluate message 3496bbd4-ffbe-44e6-8702-19769da47e42
02/10/2025 12:14:22:INFO:Received: evaluate message 3496bbd4-ffbe-44e6-8702-19769da47e42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:14:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:15:07:INFO:
[92mINFO [0m:      Received: train message 7e46c5bc-f708-47ee-b059-17fd42559da8
02/10/2025 12:15:07:INFO:Received: train message 7e46c5bc-f708-47ee-b059-17fd42559da8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:15:40:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:16:INFO:
[92mINFO [0m:      Received: evaluate message 5084553b-c1ac-4648-ac23-8dc5060469e3
02/10/2025 12:16:16:INFO:Received: evaluate message 5084553b-c1ac-4648-ac23-8dc5060469e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:16:19:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:17:05:INFO:
[92mINFO [0m:      Received: train message 722fc8ba-6894-4c03-8e55-f26bc849b8a0
02/10/2025 12:17:05:INFO:Received: train message 722fc8ba-6894-4c03-8e55-f26bc849b8a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:17:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:24:INFO:
[92mINFO [0m:      Received: evaluate message 85616b7d-7628-4263-ae4e-7676169b872c
02/10/2025 12:18:24:INFO:Received: evaluate message 85616b7d-7628-4263-ae4e-7676169b872c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:18:27:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:50:INFO:
[92mINFO [0m:      Received: train message 411ba228-a310-4657-8809-a2d401c00ee7
02/10/2025 12:18:50:INFO:Received: train message 411ba228-a310-4657-8809-a2d401c00ee7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:19:22:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:20:INFO:
[92mINFO [0m:      Received: evaluate message 78ef82b2-6f83-48f8-bf2d-6219068a2e10
02/10/2025 12:20:20:INFO:Received: evaluate message 78ef82b2-6f83-48f8-bf2d-6219068a2e10
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:20:23:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:53:INFO:
[92mINFO [0m:      Received: train message 0ba29b3d-ff0a-424a-a699-8cefffd66c22
02/10/2025 12:20:53:INFO:Received: train message 0ba29b3d-ff0a-424a-a699-8cefffd66c22
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:21:25:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:21:INFO:
[92mINFO [0m:      Received: evaluate message 008b4ae8-2b9d-4420-9176-5186701f547f
02/10/2025 12:22:21:INFO:Received: evaluate message 008b4ae8-2b9d-4420-9176-5186701f547f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:22:24:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:54:INFO:
[92mINFO [0m:      Received: train message 3c7ee991-2485-44d8-b67a-928c6e1aa721
02/10/2025 12:22:54:INFO:Received: train message 3c7ee991-2485-44d8-b67a-928c6e1aa721
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:23:24:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:23:INFO:
[92mINFO [0m:      Received: evaluate message 208b64e2-fe99-4ce4-8495-94272195be7d
02/10/2025 12:24:23:INFO:Received: evaluate message 208b64e2-fe99-4ce4-8495-94272195be7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:24:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:57:INFO:
[92mINFO [0m:      Received: train message 069f1698-fce8-49b9-8e32-e0c4e3ff3f29
02/10/2025 12:24:57:INFO:Received: train message 069f1698-fce8-49b9-8e32-e0c4e3ff3f29
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:25:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:04:INFO:
[92mINFO [0m:      Received: evaluate message e25acdd0-4d05-4477-9536-1038bda36069
02/10/2025 12:26:04:INFO:Received: evaluate message e25acdd0-4d05-4477-9536-1038bda36069

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:26:06:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:51:INFO:
[92mINFO [0m:      Received: train message b05e4d69-bac6-4882-a240-34429a21f690
02/10/2025 12:26:51:INFO:Received: train message b05e4d69-bac6-4882-a240-34429a21f690
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:27:22:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:28:07:INFO:
[92mINFO [0m:      Received: evaluate message 06f1f25a-8887-4a9c-8808-2b3def07de63
02/10/2025 12:28:07:INFO:Received: evaluate message 06f1f25a-8887-4a9c-8808-2b3def07de63
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:28:09:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:28:35:INFO:
[92mINFO [0m:      Received: train message 97f044dc-f07f-43ac-8e90-06c5b43b1019
02/10/2025 12:28:35:INFO:Received: train message 97f044dc-f07f-43ac-8e90-06c5b43b1019
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:29:05:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:04:INFO:
[92mINFO [0m:      Received: evaluate message d132eab0-5253-4e2d-be20-61c20eef5aad
02/10/2025 12:30:04:INFO:Received: evaluate message d132eab0-5253-4e2d-be20-61c20eef5aad

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:30:06:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:38:INFO:
[92mINFO [0m:      Received: train message 29007a0d-7e46-4673-a9ac-2fb99a0d0aec
02/10/2025 12:30:38:INFO:Received: train message 29007a0d-7e46-4673-a9ac-2fb99a0d0aec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:31:13:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:31:46:INFO:
[92mINFO [0m:      Received: evaluate message fac395fd-648c-474f-9685-85a785b33483
02/10/2025 12:31:46:INFO:Received: evaluate message fac395fd-648c-474f-9685-85a785b33483
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:31:49:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:32:24:INFO:
[92mINFO [0m:      Received: train message a0c49ec8-ed87-4e71-b3ad-382f5d065859
02/10/2025 12:32:24:INFO:Received: train message a0c49ec8-ed87-4e71-b3ad-382f5d065859
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:32:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:33:35:INFO:
[92mINFO [0m:      Received: evaluate message 13a677b8-e941-4e1c-b6b1-f29833371c8f
02/10/2025 12:33:35:INFO:Received: evaluate message 13a677b8-e941-4e1c-b6b1-f29833371c8f

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:33:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:34:30:INFO:
[92mINFO [0m:      Received: train message e32ba92e-3639-496c-b415-3d9599a8e1b6
02/10/2025 12:34:30:INFO:Received: train message e32ba92e-3639-496c-b415-3d9599a8e1b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:35:01:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:35:52:INFO:
[92mINFO [0m:      Received: evaluate message ad1e7954-1fbe-47a8-90c5-e509e69ef257
02/10/2025 12:35:52:INFO:Received: evaluate message ad1e7954-1fbe-47a8-90c5-e509e69ef257
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:35:55:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:36:27:INFO:
[92mINFO [0m:      Received: train message 07beede7-c7a4-4038-b008-b208db416bbe
02/10/2025 12:36:27:INFO:Received: train message 07beede7-c7a4-4038-b008-b208db416bbe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:37:02:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:37:53:INFO:
[92mINFO [0m:      Received: evaluate message 0fa87ef0-56b8-41b7-8a31-6699c79a0e50
02/10/2025 12:37:53:INFO:Received: evaluate message 0fa87ef0-56b8-41b7-8a31-6699c79a0e50

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:37:57:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:38:30:INFO:
[92mINFO [0m:      Received: train message 16fce989-985e-4042-aa8a-010c404f29e6
02/10/2025 12:38:30:INFO:Received: train message 16fce989-985e-4042-aa8a-010c404f29e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:39:05:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:39:49:INFO:
[92mINFO [0m:      Received: evaluate message c9d142d7-f9b5-4705-9d6c-7db69a9950c7
02/10/2025 12:39:49:INFO:Received: evaluate message c9d142d7-f9b5-4705-9d6c-7db69a9950c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:39:51:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:40:17:INFO:
[92mINFO [0m:      Received: train message 4b1ada6b-69a2-4953-9842-675b66e2cf36
02/10/2025 12:40:17:INFO:Received: train message 4b1ada6b-69a2-4953-9842-675b66e2cf36
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:40:47:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:41:52:INFO:
[92mINFO [0m:      Received: evaluate message 5b72ebb2-644d-4f08-8a0c-c97db88da4b9
02/10/2025 12:41:52:INFO:Received: evaluate message 5b72ebb2-644d-4f08-8a0c-c97db88da4b9

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:41:55:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:42:28:INFO:
[92mINFO [0m:      Received: train message 7702c30b-9bcd-43ba-99f2-0df9b8ab8823
02/10/2025 12:42:28:INFO:Received: train message 7702c30b-9bcd-43ba-99f2-0df9b8ab8823
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:43:02:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:43:40:INFO:
[92mINFO [0m:      Received: evaluate message 5e66be71-343f-4ec0-a0b4-f31126c656b9
02/10/2025 12:43:40:INFO:Received: evaluate message 5e66be71-343f-4ec0-a0b4-f31126c656b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:43:43:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:44:27:INFO:
[92mINFO [0m:      Received: train message af93af74-ee81-4187-bfe0-aa60bc2980de
02/10/2025 12:44:27:INFO:Received: train message af93af74-ee81-4187-bfe0-aa60bc2980de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:45:01:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:46:00:INFO:
[92mINFO [0m:      Received: evaluate message ae705f69-7d01-44d9-94dc-c1f52cd21ae0
02/10/2025 12:46:00:INFO:Received: evaluate message ae705f69-7d01-44d9-94dc-c1f52cd21ae0

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:46:03:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:46:39:INFO:
[92mINFO [0m:      Received: train message e031b2bf-9b23-4015-9ab3-99a75d699b7f
02/10/2025 12:46:39:INFO:Received: train message e031b2bf-9b23-4015-9ab3-99a75d699b7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:47:15:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:47:54:INFO:
[92mINFO [0m:      Received: evaluate message f35f29a1-0405-40d8-a165-5300e0e3771a
02/10/2025 12:47:54:INFO:Received: evaluate message f35f29a1-0405-40d8-a165-5300e0e3771a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:47:57:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:48:27:INFO:
[92mINFO [0m:      Received: train message 8aaee867-e9db-4edc-80c5-849769baace5
02/10/2025 12:48:27:INFO:Received: train message 8aaee867-e9db-4edc-80c5-849769baace5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:48:59:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:48:INFO:
[92mINFO [0m:      Received: evaluate message 38f37049-568b-48b8-ab8b-dc71eee75b5c
02/10/2025 12:49:48:INFO:Received: evaluate message 38f37049-568b-48b8-ab8b-dc71eee75b5c

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:49:51:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:53:INFO:
[92mINFO [0m:      Received: reconnect message 9f30cd4c-ea04-4b02-bfaa-25a670c6f73e
02/10/2025 12:49:53:INFO:Received: reconnect message 9f30cd4c-ea04-4b02-bfaa-25a670c6f73e
02/10/2025 12:49:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/10/2025 12:49:53:INFO:Disconnect and shut down
