nohup: ignoring input
02/10/2025 11:50:28:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/10/2025 11:50:28:DEBUG:ChannelConnectivity.IDLE
02/10/2025 11:50:28:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739217028.748345  749728 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/10/2025 11:50:53:INFO:
[92mINFO [0m:      Received: train message 9d8a0ae6-84e0-4571-85ec-14709371c3b6
02/10/2025 11:50:53:INFO:Received: train message 9d8a0ae6-84e0-4571-85ec-14709371c3b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:51:41:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:52:11:INFO:
[92mINFO [0m:      Received: evaluate message a418bf65-aa57-484c-bcb2-ba08812e217a
02/10/2025 11:52:11:INFO:Received: evaluate message a418bf65-aa57-484c-bcb2-ba08812e217a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:52:15:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:53:00:INFO:
[92mINFO [0m:      Received: train message 73f2d2bd-4a95-4daa-827b-a0493ebc17f1
02/10/2025 11:53:00:INFO:Received: train message 73f2d2bd-4a95-4daa-827b-a0493ebc17f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:53:50:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:54:29:INFO:
[92mINFO [0m:      Received: evaluate message 0e0a69d1-be56-4c9f-81da-b3a6fbe479cb
02/10/2025 11:54:29:INFO:Received: evaluate message 0e0a69d1-be56-4c9f-81da-b3a6fbe479cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:54:33:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:54:54:INFO:
[92mINFO [0m:      Received: train message 07520fcf-a96e-4711-b81e-9a17db6a8134
02/10/2025 11:54:54:INFO:Received: train message 07520fcf-a96e-4711-b81e-9a17db6a8134
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:55:47:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:10:INFO:
[92mINFO [0m:      Received: evaluate message 9f7ce279-48ba-4b8e-8808-da5ecb08ad02
02/10/2025 11:56:10:INFO:Received: evaluate message 9f7ce279-48ba-4b8e-8808-da5ecb08ad02
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:56:13:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:57:09:INFO:
[92mINFO [0m:      Received: train message 449946bc-dd8a-4006-aefe-dd1376829035
02/10/2025 11:57:09:INFO:Received: train message 449946bc-dd8a-4006-aefe-dd1376829035
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:57:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:36:INFO:
[92mINFO [0m:      Received: evaluate message 91b8b073-546b-4dc3-b113-19c19c9d10b4
02/10/2025 11:58:36:INFO:Received: evaluate message 91b8b073-546b-4dc3-b113-19c19c9d10b4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:58:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:59:10:INFO:
[92mINFO [0m:      Received: train message a7dd7620-8acb-4cc3-996c-52f75074d93c
02/10/2025 11:59:10:INFO:Received: train message a7dd7620-8acb-4cc3-996c-52f75074d93c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:59:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:00:37:INFO:
[92mINFO [0m:      Received: evaluate message 4090e71d-0be2-4959-82ca-5a6100fad061
02/10/2025 12:00:37:INFO:Received: evaluate message 4090e71d-0be2-4959-82ca-5a6100fad061
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:00:41:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:01:15:INFO:
[92mINFO [0m:      Received: train message 9f276913-0bee-487a-a3c6-c993cfdfcc83
02/10/2025 12:01:15:INFO:Received: train message 9f276913-0bee-487a-a3c6-c993cfdfcc83
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:01:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:02:25:INFO:
[92mINFO [0m:      Received: evaluate message 03ae3609-1025-4ff0-baec-45eed900160a
02/10/2025 12:02:25:INFO:Received: evaluate message 03ae3609-1025-4ff0-baec-45eed900160a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:02:28:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:03:08:INFO:
[92mINFO [0m:      Received: train message 371844e2-cc37-423b-b3ba-99cb62fe3733
02/10/2025 12:03:08:INFO:Received: train message 371844e2-cc37-423b-b3ba-99cb62fe3733
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:03:54:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:04:34:INFO:
[92mINFO [0m:      Received: evaluate message 94c19ba8-579a-48ac-96a4-eb3deee9489e
02/10/2025 12:04:34:INFO:Received: evaluate message 94c19ba8-579a-48ac-96a4-eb3deee9489e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282], 'accuracy': [0.506645817044566], 'auc': [0.6996165186305552], 'precision': [0.394924318810202], 'recall': [0.506645817044566], 'f1': [0.40278177667270126]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696], 'accuracy': [0.506645817044566, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256], 'precision': [0.394924318810202, 0.4198365350542879], 'recall': [0.506645817044566, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:04:37:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:05:04:INFO:
[92mINFO [0m:      Received: train message b3f31d7e-0fcc-42c0-8dce-484bea93145e
02/10/2025 12:05:04:INFO:Received: train message b3f31d7e-0fcc-42c0-8dce-484bea93145e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:05:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:06:18:INFO:
[92mINFO [0m:      Received: evaluate message a7f8ade8-ae60-4421-a60f-dd90188fe23f
02/10/2025 12:06:18:INFO:Received: evaluate message a7f8ade8-ae60-4421-a60f-dd90188fe23f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:06:21:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:07:13:INFO:
[92mINFO [0m:      Received: train message f6136d0a-f2a9-4ff7-bb46-0e50d24a6bbf
02/10/2025 12:07:13:INFO:Received: train message f6136d0a-f2a9-4ff7-bb46-0e50d24a6bbf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:08:02:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:08:29:INFO:
[92mINFO [0m:      Received: evaluate message 3472003a-81f1-4eda-8699-216935f3ecf6
02/10/2025 12:08:29:INFO:Received: evaluate message 3472003a-81f1-4eda-8699-216935f3ecf6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:08:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:09:08:INFO:
[92mINFO [0m:      Received: train message 7e1cba90-5a8a-4f92-a012-6e5dfa0fee3c
02/10/2025 12:09:08:INFO:Received: train message 7e1cba90-5a8a-4f92-a012-6e5dfa0fee3c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:09:59:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:10:35:INFO:
[92mINFO [0m:      Received: evaluate message c9ea69b0-edbd-43e8-a6f1-8ba7ee18e2f6
02/10/2025 12:10:35:INFO:Received: evaluate message c9ea69b0-edbd-43e8-a6f1-8ba7ee18e2f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:10:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:10:53:INFO:
[92mINFO [0m:      Received: train message 10ac9fe7-8746-4b67-bad9-6d002fafbd12
02/10/2025 12:10:53:INFO:Received: train message 10ac9fe7-8746-4b67-bad9-6d002fafbd12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:11:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:12:25:INFO:
[92mINFO [0m:      Received: evaluate message 360d4e2b-5ad7-4bfe-8e5d-a4fc44e4bb6d
02/10/2025 12:12:25:INFO:Received: evaluate message 360d4e2b-5ad7-4bfe-8e5d-a4fc44e4bb6d

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:12:28:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:13:05:INFO:
[92mINFO [0m:      Received: train message bb52f575-6323-48e8-9c5e-bceac4b34623
02/10/2025 12:13:05:INFO:Received: train message bb52f575-6323-48e8-9c5e-bceac4b34623
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:13:53:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:14:29:INFO:
[92mINFO [0m:      Received: evaluate message 5153ac3b-767b-475a-8ea6-444308e9deaf
02/10/2025 12:14:29:INFO:Received: evaluate message 5153ac3b-767b-475a-8ea6-444308e9deaf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:14:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:15:01:INFO:
[92mINFO [0m:      Received: train message 443381ad-06fa-48ab-9de0-13dc0b06fad0
02/10/2025 12:15:01:INFO:Received: train message 443381ad-06fa-48ab-9de0-13dc0b06fad0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:15:49:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:26:INFO:
[92mINFO [0m:      Received: evaluate message b18084d8-1653-4c35-b66b-49046b7738f6
02/10/2025 12:16:26:INFO:Received: evaluate message b18084d8-1653-4c35-b66b-49046b7738f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:16:28:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:17:00:INFO:
[92mINFO [0m:      Received: train message 69db400d-84fa-471e-ae2a-5cab46ef958b
02/10/2025 12:17:00:INFO:Received: train message 69db400d-84fa-471e-ae2a-5cab46ef958b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:17:48:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:14:INFO:
[92mINFO [0m:      Received: evaluate message 3a77b3e8-d4d5-4811-97e4-4cc6c8f91af0
02/10/2025 12:18:14:INFO:Received: evaluate message 3a77b3e8-d4d5-4811-97e4-4cc6c8f91af0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:18:17:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:19:01:INFO:
[92mINFO [0m:      Received: train message b2e75bf4-2220-454c-8fe2-195db51df3ad
02/10/2025 12:19:01:INFO:Received: train message b2e75bf4-2220-454c-8fe2-195db51df3ad

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567]}

Step 1b: Recomputing FIM for epoch 15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:19:48:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:20:INFO:
[92mINFO [0m:      Received: evaluate message cba6948a-11ae-4bbb-a089-a37ca7e91055
02/10/2025 12:20:20:INFO:Received: evaluate message cba6948a-11ae-4bbb-a089-a37ca7e91055
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:20:23:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:49:INFO:
[92mINFO [0m:      Received: train message 13c19472-bbac-4d14-8300-99478d1013a9
02/10/2025 12:20:49:INFO:Received: train message 13c19472-bbac-4d14-8300-99478d1013a9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:21:37:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:05:INFO:
[92mINFO [0m:      Received: evaluate message 350fac65-e84b-495d-b5aa-2ab37dccd707
02/10/2025 12:22:05:INFO:Received: evaluate message 350fac65-e84b-495d-b5aa-2ab37dccd707
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:22:07:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:58:INFO:
[92mINFO [0m:      Received: train message 0619db73-abd0-46ce-aa3b-e9a1b767fb21
02/10/2025 12:22:58:INFO:Received: train message 0619db73-abd0-46ce-aa3b-e9a1b767fb21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:23:44:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:03:INFO:
[92mINFO [0m:      Received: evaluate message 33991806-453e-4862-9ecb-c0aa82592137
02/10/2025 12:24:03:INFO:Received: evaluate message 33991806-453e-4862-9ecb-c0aa82592137
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:24:06:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:54:INFO:
[92mINFO [0m:      Received: train message 3ddea0d6-9111-4abc-a0af-52c469fbf3f1
02/10/2025 12:24:54:INFO:Received: train message 3ddea0d6-9111-4abc-a0af-52c469fbf3f1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:25:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:09:INFO:
[92mINFO [0m:      Received: evaluate message 8586efc5-fc4a-414f-a9d4-6c8a0f467072
02/10/2025 12:26:09:INFO:Received: evaluate message 8586efc5-fc4a-414f-a9d4-6c8a0f467072
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:26:11:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:35:INFO:
[92mINFO [0m:      Received: train message 5fb3a8e3-0152-4e4b-89da-eb836638013b
02/10/2025 12:26:35:INFO:Received: train message 5fb3a8e3-0152-4e4b-89da-eb836638013b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:27:21:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:28:09:INFO:
[92mINFO [0m:      Received: evaluate message 1bf3041e-0912-4d56-b7dc-c5de515ca663
02/10/2025 12:28:09:INFO:Received: evaluate message 1bf3041e-0912-4d56-b7dc-c5de515ca663
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:28:11:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:28:40:INFO:
[92mINFO [0m:      Received: train message fa488f01-b07c-47f1-b201-ad74e46735fd
02/10/2025 12:28:40:INFO:Received: train message fa488f01-b07c-47f1-b201-ad74e46735fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:29:28:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:04:INFO:
[92mINFO [0m:      Received: evaluate message a60c8b0a-e840-4437-bd0e-94a78138e7d8
02/10/2025 12:30:04:INFO:Received: evaluate message a60c8b0a-e840-4437-bd0e-94a78138e7d8
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:30:07:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:26:INFO:
[92mINFO [0m:      Received: train message 4177308a-e4a0-457a-ba99-1baaa2b57907
02/10/2025 12:30:26:INFO:Received: train message 4177308a-e4a0-457a-ba99-1baaa2b57907
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:31:17:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:31:54:INFO:
[92mINFO [0m:      Received: evaluate message 69dd69ca-f44e-47ae-afcf-0306dc90b093
02/10/2025 12:31:54:INFO:Received: evaluate message 69dd69ca-f44e-47ae-afcf-0306dc90b093
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:31:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:32:31:INFO:
[92mINFO [0m:      Received: train message 971506ad-3aaa-4c14-8309-b79d4ca4d926
02/10/2025 12:32:31:INFO:Received: train message 971506ad-3aaa-4c14-8309-b79d4ca4d926
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:33:16:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:33:39:INFO:
[92mINFO [0m:      Received: evaluate message e5f0b214-06f1-4f44-b0a0-faf4a5fcf825
02/10/2025 12:33:39:INFO:Received: evaluate message e5f0b214-06f1-4f44-b0a0-faf4a5fcf825

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:33:41:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:34:28:INFO:
[92mINFO [0m:      Received: train message f4e17cb5-e0b6-44b1-8d07-eb611862c13a
02/10/2025 12:34:28:INFO:Received: train message f4e17cb5-e0b6-44b1-8d07-eb611862c13a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:35:16:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:35:44:INFO:
[92mINFO [0m:      Received: evaluate message 7db8905b-151a-4c3b-b651-e40ca4f70cad
02/10/2025 12:35:44:INFO:Received: evaluate message 7db8905b-151a-4c3b-b651-e40ca4f70cad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:35:46:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:36:22:INFO:
[92mINFO [0m:      Received: train message 1cbb4658-e580-471f-9462-478a454a7697
02/10/2025 12:36:22:INFO:Received: train message 1cbb4658-e580-471f-9462-478a454a7697
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:37:10:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:37:48:INFO:
[92mINFO [0m:      Received: evaluate message f3b66273-f6e9-4069-9c44-ef33b5c5c339
02/10/2025 12:37:48:INFO:Received: evaluate message f3b66273-f6e9-4069-9c44-ef33b5c5c339

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:37:51:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:38:24:INFO:
[92mINFO [0m:      Received: train message 17c4c260-1997-4fe9-b754-56b49f4a08e8
02/10/2025 12:38:24:INFO:Received: train message 17c4c260-1997-4fe9-b754-56b49f4a08e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:39:13:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:39:53:INFO:
[92mINFO [0m:      Received: evaluate message 765d304e-5471-42ba-a988-08817483e31a
02/10/2025 12:39:53:INFO:Received: evaluate message 765d304e-5471-42ba-a988-08817483e31a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:39:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:40:28:INFO:
[92mINFO [0m:      Received: train message ffcea0d1-a269-4e22-859f-daa830f680b3
02/10/2025 12:40:28:INFO:Received: train message ffcea0d1-a269-4e22-859f-daa830f680b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:41:14:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:41:56:INFO:
[92mINFO [0m:      Received: evaluate message d78ff60d-59cc-441d-b05e-b137f20d49eb
02/10/2025 12:41:56:INFO:Received: evaluate message d78ff60d-59cc-441d-b05e-b137f20d49eb

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:41:59:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:42:29:INFO:
[92mINFO [0m:      Received: train message 039527ab-d813-488a-b5de-f170d4ae224c
02/10/2025 12:42:29:INFO:Received: train message 039527ab-d813-488a-b5de-f170d4ae224c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:43:20:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:43:35:INFO:
[92mINFO [0m:      Received: evaluate message 5e8296b1-02b0-476c-a7b9-b593cb8fd7b9
02/10/2025 12:43:35:INFO:Received: evaluate message 5e8296b1-02b0-476c-a7b9-b593cb8fd7b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:43:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:44:36:INFO:
[92mINFO [0m:      Received: train message cc2862d5-c951-4f58-ab14-5a6b1358290f
02/10/2025 12:44:36:INFO:Received: train message cc2862d5-c951-4f58-ab14-5a6b1358290f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:45:25:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:46:02:INFO:
[92mINFO [0m:      Received: evaluate message 1e600323-b7e5-4200-805c-17d5a4dd5a3e
02/10/2025 12:46:02:INFO:Received: evaluate message 1e600323-b7e5-4200-805c-17d5a4dd5a3e

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:46:05:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:46:19:INFO:
[92mINFO [0m:      Received: train message fccca495-173a-4db8-b666-d9a5435ce248
02/10/2025 12:46:19:INFO:Received: train message fccca495-173a-4db8-b666-d9a5435ce248
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:47:11:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:47:53:INFO:
[92mINFO [0m:      Received: evaluate message 57a1f2c4-67d2-4091-b279-d7b61f3db1c7
02/10/2025 12:47:53:INFO:Received: evaluate message 57a1f2c4-67d2-4091-b279-d7b61f3db1c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:47:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:48:20:INFO:
[92mINFO [0m:      Received: train message 69d3bdd9-815f-452b-bf3e-4bad79393102
02/10/2025 12:48:20:INFO:Received: train message 69d3bdd9-815f-452b-bf3e-4bad79393102
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:49:10:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:48:INFO:
[92mINFO [0m:      Received: evaluate message fee86c74-780e-4863-af6f-ad1f698ba69e
02/10/2025 12:49:48:INFO:Received: evaluate message fee86c74-780e-4863-af6f-ad1f698ba69e

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:49:51:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:53:INFO:
[92mINFO [0m:      Received: reconnect message beb121f8-9393-48ec-a737-685d194cefe1
02/10/2025 12:49:53:INFO:Received: reconnect message beb121f8-9393-48ec-a737-685d194cefe1
02/10/2025 12:49:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/10/2025 12:49:53:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088, 1.1039477369578394], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228, 0.7467077310319794], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085, 0.4791652346454203], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705, 0.4494452346094889]}



Final client history:
{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088, 1.1039477369578394], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228, 0.7467077310319794], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085, 0.4791652346454203], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705, 0.4494452346094889]}

