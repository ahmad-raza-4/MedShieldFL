nohup: ignoring input
02/10/2025 11:50:24:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/10/2025 11:50:24:DEBUG:ChannelConnectivity.IDLE
02/10/2025 11:50:24:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739217024.950025  749533 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/10/2025 11:51:03:INFO:
[92mINFO [0m:      Received: train message 9587724e-6404-4058-8ddd-285e9e08278a
02/10/2025 11:51:03:INFO:Received: train message 9587724e-6404-4058-8ddd-285e9e08278a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:51:40:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:52:23:INFO:
[92mINFO [0m:      Received: evaluate message faf0b1df-a9b7-4b79-a1c7-a05720274273
02/10/2025 11:52:23:INFO:Received: evaluate message faf0b1df-a9b7-4b79-a1c7-a05720274273
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:52:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:53:01:INFO:
[92mINFO [0m:      Received: train message cf4be198-c50d-4b03-b804-a0d9b1db1658
02/10/2025 11:53:01:INFO:Received: train message cf4be198-c50d-4b03-b804-a0d9b1db1658
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:53:39:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:54:21:INFO:
[92mINFO [0m:      Received: evaluate message 0b705fed-51c7-4d5d-99ca-67783508643b
02/10/2025 11:54:21:INFO:Received: evaluate message 0b705fed-51c7-4d5d-99ca-67783508643b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:54:24:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:54:53:INFO:
[92mINFO [0m:      Received: train message ad4b17b3-b93f-402c-a5b4-82a469b2b31a
02/10/2025 11:54:53:INFO:Received: train message ad4b17b3-b93f-402c-a5b4-82a469b2b31a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:55:33:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:27:INFO:
[92mINFO [0m:      Received: evaluate message dba6e961-9704-4874-8aab-0af70f1c2ad1
02/10/2025 11:56:27:INFO:Received: evaluate message dba6e961-9704-4874-8aab-0af70f1c2ad1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:56:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:56:56:INFO:
[92mINFO [0m:      Received: train message f6c1eb5c-084f-4225-b5a6-abc5955297df
02/10/2025 11:56:56:INFO:Received: train message f6c1eb5c-084f-4225-b5a6-abc5955297df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:57:35:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:22:INFO:
[92mINFO [0m:      Received: evaluate message e74eaed2-bc7b-4534-92a7-056c3b66d7c5
02/10/2025 11:58:22:INFO:Received: evaluate message e74eaed2-bc7b-4534-92a7-056c3b66d7c5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 11:58:24:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 11:58:54:INFO:
[92mINFO [0m:      Received: train message 8e4f67c3-5a15-449d-b81d-cd457aa60a22
02/10/2025 11:58:54:INFO:Received: train message 8e4f67c3-5a15-449d-b81d-cd457aa60a22
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 11:59:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:00:37:INFO:
[92mINFO [0m:      Received: evaluate message b2fc71bf-b906-44e6-a01b-cf48da96014c
02/10/2025 12:00:37:INFO:Received: evaluate message b2fc71bf-b906-44e6-a01b-cf48da96014c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:00:41:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:00:53:INFO:
[92mINFO [0m:      Received: train message 6671c44a-c8a7-47e2-ac5c-7be8717a3f22
02/10/2025 12:00:53:INFO:Received: train message 6671c44a-c8a7-47e2-ac5c-7be8717a3f22
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:01:25:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:02:23:INFO:
[92mINFO [0m:      Received: evaluate message 817d86e0-4155-4187-b4b9-e249089a6dff
02/10/2025 12:02:23:INFO:Received: evaluate message 817d86e0-4155-4187-b4b9-e249089a6dff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:02:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:02:58:INFO:
[92mINFO [0m:      Received: train message 59719034-7170-4eec-b01a-92474e493b21
02/10/2025 12:02:58:INFO:Received: train message 59719034-7170-4eec-b01a-92474e493b21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:03:32:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:04:32:INFO:
[92mINFO [0m:      Received: evaluate message aa868fc3-6eda-476b-a6f3-d2045704ae25
02/10/2025 12:04:32:INFO:Received: evaluate message aa868fc3-6eda-476b-a6f3-d2045704ae25
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282], 'accuracy': [0.506645817044566], 'auc': [0.6996165186305552], 'precision': [0.394924318810202], 'recall': [0.506645817044566], 'f1': [0.40278177667270126]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696], 'accuracy': [0.506645817044566, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256], 'precision': [0.394924318810202, 0.4198365350542879], 'recall': [0.506645817044566, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:04:35:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:05:12:INFO:
[92mINFO [0m:      Received: train message 963222df-f625-46c3-8d4f-38417439d561
02/10/2025 12:05:12:INFO:Received: train message 963222df-f625-46c3-8d4f-38417439d561
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:05:52:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:06:28:INFO:
[92mINFO [0m:      Received: evaluate message baccc228-73db-49c1-a152-156ae344b264
02/10/2025 12:06:28:INFO:Received: evaluate message baccc228-73db-49c1-a152-156ae344b264
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:06:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:07:11:INFO:
[92mINFO [0m:      Received: train message 976e27ca-964d-45c2-b8d4-542aac7c2935
02/10/2025 12:07:11:INFO:Received: train message 976e27ca-964d-45c2-b8d4-542aac7c2935
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:07:49:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:08:32:INFO:
[92mINFO [0m:      Received: evaluate message 56c1a501-e0d2-45ee-bfb8-82f45aa294b5
02/10/2025 12:08:32:INFO:Received: evaluate message 56c1a501-e0d2-45ee-bfb8-82f45aa294b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:08:35:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:09:18:INFO:
[92mINFO [0m:      Received: train message a555ce9a-5add-4b7e-900a-b163bc8414a5
02/10/2025 12:09:18:INFO:Received: train message a555ce9a-5add-4b7e-900a-b163bc8414a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:09:56:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:10:37:INFO:
[92mINFO [0m:      Received: evaluate message 39e8b015-03f0-43cf-962b-be048a564b4b
02/10/2025 12:10:37:INFO:Received: evaluate message 39e8b015-03f0-43cf-962b-be048a564b4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:10:40:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:11:12:INFO:
[92mINFO [0m:      Received: train message 149811c6-b53e-48e7-b38f-5b844f0474d5
02/10/2025 12:11:12:INFO:Received: train message 149811c6-b53e-48e7-b38f-5b844f0474d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:11:49:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:12:14:INFO:
[92mINFO [0m:      Received: evaluate message d5776085-d6fd-46a8-b7ee-13e56d776d9c
02/10/2025 12:12:14:INFO:Received: evaluate message d5776085-d6fd-46a8-b7ee-13e56d776d9c

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:12:16:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:13:07:INFO:
[92mINFO [0m:      Received: train message c36f7719-feb3-479f-b62d-2ae371af3fa3
02/10/2025 12:13:07:INFO:Received: train message c36f7719-feb3-479f-b62d-2ae371af3fa3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:13:44:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:14:22:INFO:
[92mINFO [0m:      Received: evaluate message 6e36f9ab-62a8-4c02-9744-9e2f87ac420f
02/10/2025 12:14:22:INFO:Received: evaluate message 6e36f9ab-62a8-4c02-9744-9e2f87ac420f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:14:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:14:52:INFO:
[92mINFO [0m:      Received: train message 20b283f9-522c-4eb5-86c9-341f5d27398b
02/10/2025 12:14:52:INFO:Received: train message 20b283f9-522c-4eb5-86c9-341f5d27398b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:15:30:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:28:INFO:
[92mINFO [0m:      Received: evaluate message a65143ca-9467-4c14-a806-573368723bcc
02/10/2025 12:16:28:INFO:Received: evaluate message a65143ca-9467-4c14-a806-573368723bcc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:16:31:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:16:47:INFO:
[92mINFO [0m:      Received: train message d751ffbe-f820-4730-9025-4b98726969c6
02/10/2025 12:16:47:INFO:Received: train message d751ffbe-f820-4730-9025-4b98726969c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:17:22:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:25:INFO:
[92mINFO [0m:      Received: evaluate message 2c079b1a-9cc4-49c0-b379-0b8244ba0c7d
02/10/2025 12:18:25:INFO:Received: evaluate message 2c079b1a-9cc4-49c0-b379-0b8244ba0c7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:18:28:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:18:55:INFO:
[92mINFO [0m:      Received: train message 23f689c8-8630-4832-a07f-ba7eeb124fb8
02/10/2025 12:18:55:INFO:Received: train message 23f689c8-8630-4832-a07f-ba7eeb124fb8

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567]}

Step 1b: Recomputing FIM for epoch 15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:19:34:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:20:11:INFO:
[92mINFO [0m:      Received: evaluate message 5ed48fd9-60b9-48d5-9364-fc7abcb9cf24
02/10/2025 12:20:11:INFO:Received: evaluate message 5ed48fd9-60b9-48d5-9364-fc7abcb9cf24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:20:13:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:21:03:INFO:
[92mINFO [0m:      Received: train message 7ba9c074-cd83-404b-8f88-f43305116bb4
02/10/2025 12:21:03:INFO:Received: train message 7ba9c074-cd83-404b-8f88-f43305116bb4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:21:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:21:INFO:
[92mINFO [0m:      Received: evaluate message c72ae38f-2f4c-498c-b40c-4b91733302f8
02/10/2025 12:22:21:INFO:Received: evaluate message c72ae38f-2f4c-498c-b40c-4b91733302f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:22:24:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:22:38:INFO:
[92mINFO [0m:      Received: train message 7058b673-5daa-40d3-9173-d996eb01a602
02/10/2025 12:22:38:INFO:Received: train message 7058b673-5daa-40d3-9173-d996eb01a602
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:23:13:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:23:INFO:
[92mINFO [0m:      Received: evaluate message 6e86a0c9-84c9-4547-9ac7-bc59547e600b
02/10/2025 12:24:23:INFO:Received: evaluate message 6e86a0c9-84c9-4547-9ac7-bc59547e600b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:24:26:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:24:59:INFO:
[92mINFO [0m:      Received: train message 68b17d56-829c-4ee1-a51b-2212be9336ec
02/10/2025 12:24:59:INFO:Received: train message 68b17d56-829c-4ee1-a51b-2212be9336ec
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:25:34:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:14:INFO:
[92mINFO [0m:      Received: evaluate message 1186feeb-3708-4c3f-8f85-633fc190de81
02/10/2025 12:26:14:INFO:Received: evaluate message 1186feeb-3708-4c3f-8f85-633fc190de81
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:26:17:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:26:53:INFO:
[92mINFO [0m:      Received: train message c4fa66f0-e755-474c-bd8c-29f086d7f4f3
02/10/2025 12:26:53:INFO:Received: train message c4fa66f0-e755-474c-bd8c-29f086d7f4f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:27:27:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:27:51:INFO:
[92mINFO [0m:      Received: evaluate message eb5516c6-62bd-4fea-876c-6297f7c57557
02/10/2025 12:27:51:INFO:Received: evaluate message eb5516c6-62bd-4fea-876c-6297f7c57557
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:27:53:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:28:44:INFO:
[92mINFO [0m:      Received: train message 6f7c5922-570c-41d8-87ba-3485b8468663
02/10/2025 12:28:44:INFO:Received: train message 6f7c5922-570c-41d8-87ba-3485b8468663
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:29:22:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:06:INFO:
[92mINFO [0m:      Received: evaluate message ed3ffa5d-f589-4a56-8ca8-dcca81f98208
02/10/2025 12:30:06:INFO:Received: evaluate message ed3ffa5d-f589-4a56-8ca8-dcca81f98208
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:30:09:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:30:32:INFO:
[92mINFO [0m:      Received: train message 30171a49-a683-4b29-8a62-de58dabb38ed
02/10/2025 12:30:32:INFO:Received: train message 30171a49-a683-4b29-8a62-de58dabb38ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:31:11:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:31:40:INFO:
[92mINFO [0m:      Received: evaluate message f9ed0bce-0762-4282-b20c-8241104fafa4
02/10/2025 12:31:40:INFO:Received: evaluate message f9ed0bce-0762-4282-b20c-8241104fafa4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:31:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:32:22:INFO:
[92mINFO [0m:      Received: train message e9a5ede9-3817-4514-9681-3a9cdbe60a3e
02/10/2025 12:32:22:INFO:Received: train message e9a5ede9-3817-4514-9681-3a9cdbe60a3e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:32:58:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:33:40:INFO:
[92mINFO [0m:      Received: evaluate message ca83b7b7-b2e3-4bad-b052-2da70b84a26e
02/10/2025 12:33:40:INFO:Received: evaluate message ca83b7b7-b2e3-4bad-b052-2da70b84a26e

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:33:43:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:34:18:INFO:
[92mINFO [0m:      Received: train message 55eb9321-57b1-4afd-969e-0affbcfd4c09
02/10/2025 12:34:18:INFO:Received: train message 55eb9321-57b1-4afd-969e-0affbcfd4c09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:34:54:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:35:49:INFO:
[92mINFO [0m:      Received: evaluate message 1c900012-94ca-4cf7-a3df-cc6c2d76bf1c
02/10/2025 12:35:49:INFO:Received: evaluate message 1c900012-94ca-4cf7-a3df-cc6c2d76bf1c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:35:51:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:36:05:INFO:
[92mINFO [0m:      Received: train message fe4ca22d-a198-48f5-be8e-26c3349adf96
02/10/2025 12:36:05:INFO:Received: train message fe4ca22d-a198-48f5-be8e-26c3349adf96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:36:39:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:37:34:INFO:
[92mINFO [0m:      Received: evaluate message 30cc6100-e609-46b4-9097-7159f6ad8b9e
02/10/2025 12:37:34:INFO:Received: evaluate message 30cc6100-e609-46b4-9097-7159f6ad8b9e

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:37:38:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:38:32:INFO:
[92mINFO [0m:      Received: train message b087728c-0978-404d-960a-3b7240427d68
02/10/2025 12:38:32:INFO:Received: train message b087728c-0978-404d-960a-3b7240427d68
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:39:12:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:39:38:INFO:
[92mINFO [0m:      Received: evaluate message ac94b61f-9f70-479a-ad73-6a2450eb4ee8
02/10/2025 12:39:38:INFO:Received: evaluate message ac94b61f-9f70-479a-ad73-6a2450eb4ee8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:39:42:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:40:28:INFO:
[92mINFO [0m:      Received: train message 2d3b9054-2874-49af-ad5b-4460403feb82
02/10/2025 12:40:28:INFO:Received: train message 2d3b9054-2874-49af-ad5b-4460403feb82
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:41:05:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:41:34:INFO:
[92mINFO [0m:      Received: evaluate message 3ba742f3-e3cc-44ec-bccf-ea355c1a23c1
02/10/2025 12:41:34:INFO:Received: evaluate message 3ba742f3-e3cc-44ec-bccf-ea355c1a23c1

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:41:37:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:42:33:INFO:
[92mINFO [0m:      Received: train message c8d969ab-dbb6-44ec-b9a4-5e2748df2b16
02/10/2025 12:42:33:INFO:Received: train message c8d969ab-dbb6-44ec-b9a4-5e2748df2b16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:43:14:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:43:56:INFO:
[92mINFO [0m:      Received: evaluate message 330a6d20-e347-467f-839c-29df5a322119
02/10/2025 12:43:56:INFO:Received: evaluate message 330a6d20-e347-467f-839c-29df5a322119
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:43:59:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:44:35:INFO:
[92mINFO [0m:      Received: train message 76c42a19-d1b2-4098-8ba7-58d80b464651
02/10/2025 12:44:35:INFO:Received: train message 76c42a19-d1b2-4098-8ba7-58d80b464651
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:45:14:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:45:44:INFO:
[92mINFO [0m:      Received: evaluate message b8baacfd-26b7-45a3-ace4-7e9a0b13eb66
02/10/2025 12:45:44:INFO:Received: evaluate message b8baacfd-26b7-45a3-ace4-7e9a0b13eb66

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:45:46:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:46:28:INFO:
[92mINFO [0m:      Received: train message 80a19464-c2ad-4c33-98a5-84e6d8332c5c
02/10/2025 12:46:28:INFO:Received: train message 80a19464-c2ad-4c33-98a5-84e6d8332c5c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:47:10:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:47:40:INFO:
[92mINFO [0m:      Received: evaluate message c397bb46-b3ca-4e5b-80dd-d9c2a8007584
02/10/2025 12:47:40:INFO:Received: evaluate message c397bb46-b3ca-4e5b-80dd-d9c2a8007584
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:47:43:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:48:31:INFO:
[92mINFO [0m:      Received: train message 2df7ba06-a559-4cc1-95c2-31b5185cc111
02/10/2025 12:48:31:INFO:Received: train message 2df7ba06-a559-4cc1-95c2-31b5185cc111
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/10/2025 12:49:09:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:39:INFO:
[92mINFO [0m:      Received: evaluate message 546999c1-fa13-444b-ba94-37aa5d098f99
02/10/2025 12:49:39:INFO:Received: evaluate message 546999c1-fa13-444b-ba94-37aa5d098f99

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/10/2025 12:49:41:INFO:Sent reply
[92mINFO [0m:      
02/10/2025 12:49:53:INFO:
[92mINFO [0m:      Received: reconnect message ec6631f7-d0cd-457b-904f-c8112317a23c
02/10/2025 12:49:53:INFO:Received: reconnect message ec6631f7-d0cd-457b-904f-c8112317a23c
02/10/2025 12:49:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/10/2025 12:49:53:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088, 1.1039477369578394], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228, 0.7467077310319794], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085, 0.4791652346454203], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705, 0.4494452346094889]}



Final client history:
{'loss': [1.0626869424420282, 0.9988381409197696, 1.0640688277111099, 1.1235883715005477, 1.1801901162835748, 1.1354996979096794, 1.203894488041619, 1.0658839546767318, 1.0984257157656063, 1.1503324610260524, 1.2178940057195287, 1.1204007493042218, 1.1418560623097365, 1.1373494013479857, 1.090595050003493, 1.0996275490573648, 1.1633722452189048, 1.1350842574502082, 1.0894402364346085, 1.0879797244836242, 1.0900897646062164, 1.1091533665362516, 1.1562342335229, 1.0845045537292444, 1.145556606661071, 1.1755407020577795, 1.1184201170635746, 1.1322718420655025, 1.223313979752088, 1.1039477369578394], 'accuracy': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'auc': [0.6996165186305552, 0.6902793241426256, 0.6818678404066819, 0.6680193247197673, 0.6883221565022224, 0.7047192521663362, 0.7209086040079571, 0.7071830931228156, 0.6931176741368482, 0.6941767433892134, 0.6994047674135185, 0.7046451725558638, 0.7123070369718956, 0.7257159852093146, 0.7164632133594093, 0.7156123770954463, 0.7159598097230888, 0.7169621483765813, 0.7139075514798184, 0.6909485227170448, 0.7018433393690016, 0.7055107213591066, 0.7261013967837409, 0.7272081656889182, 0.726619916349315, 0.7150879654512318, 0.7234216508737501, 0.7322798643028332, 0.7455009874011228, 0.7467077310319794], 'precision': [0.394924318810202, 0.4198365350542879, 0.43363998968463574, 0.4250860048070752, 0.44240930352599994, 0.40269797493287574, 0.4301652158077647, 0.4377766183811023, 0.4263019287154631, 0.4244924877243116, 0.41172037578861354, 0.4343626451716475, 0.42154472347463956, 0.4027292274788598, 0.4431010789689965, 0.44435693793333403, 0.40602168514600107, 0.42052181071297196, 0.4444742477726096, 0.4418075837473564, 0.4396825943070712, 0.44578710309969033, 0.5014397599788073, 0.5713149193815459, 0.5234791233859095, 0.5464838762497025, 0.43561667671479243, 0.43288785475124064, 0.4731341338899085, 0.4791652346454203], 'recall': [0.506645817044566, 0.5230648944487881, 0.5254104769351056, 0.5183737294761532, 0.5105551211884285, 0.5113369820172009, 0.5160281469898358, 0.5379202501954652, 0.5269741985926505, 0.5238467552775606, 0.5144644253322909, 0.5324472243940579, 0.5222830336200156, 0.5136825645035183, 0.5410476935105551, 0.5426114151681001, 0.5136825645035183, 0.5230648944487881, 0.5410476935105551, 0.5293197810789679, 0.5301016419077405, 0.5418295543393276, 0.5308835027365129, 0.5324472243940579, 0.527756059421423, 0.5152462861610634, 0.5324472243940579, 0.5301016419077405, 0.5129007036747459, 0.5285379202501954], 'f1': [0.40278177667270126, 0.44439877497477875, 0.4155453357873727, 0.4051081500334658, 0.36889932215040455, 0.39623548489850285, 0.3915829929067406, 0.45873357591054725, 0.4456499078302641, 0.43756753205062415, 0.40391150098179807, 0.4545286101087458, 0.431701600737198, 0.412205318621567, 0.47190211843689456, 0.4730995016145806, 0.4037971485597097, 0.43556678405604626, 0.47655555778261177, 0.48015498077618035, 0.47801917883856776, 0.4793360252708951, 0.44854715943196666, 0.45240985154944213, 0.4254069687876042, 0.40297035886663346, 0.45130200290647215, 0.442409733338993, 0.39509260858570705, 0.4494452346094889]}

