nohup: ignoring input
02/15/2025 01:13:56:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:13:56:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:13:56:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 01:13:56:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739610836.310054 1652455 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:14:18:INFO:
[92mINFO [0m:      Received: train message b8b091b3-db1a-4c0e-944d-b0cfdd4683d5
02/15/2025 01:14:18:INFO:Received: train message b8b091b3-db1a-4c0e-944d-b0cfdd4683d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:14:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:13:INFO:
[92mINFO [0m:      Received: evaluate message 65082a4f-dc93-4b24-9947-8c774c151368
02/15/2025 01:16:13:INFO:Received: evaluate message 65082a4f-dc93-4b24-9947-8c774c151368
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:16:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:58:INFO:
[92mINFO [0m:      Received: train message 5a66b097-a8ec-4245-a76f-c61df31a85e6
02/15/2025 01:16:58:INFO:Received: train message 5a66b097-a8ec-4245-a76f-c61df31a85e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:12:INFO:
[92mINFO [0m:      Received: evaluate message 50cddb78-1297-497d-9271-7989cbe2a48a
02/15/2025 01:18:12:INFO:Received: evaluate message 50cddb78-1297-497d-9271-7989cbe2a48a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:19:09:INFO:
[92mINFO [0m:      Received: train message 0ef6397f-224b-43e5-94a5-15dad017abc9
02/15/2025 01:19:09:INFO:Received: train message 0ef6397f-224b-43e5-94a5-15dad017abc9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:49:INFO:
[92mINFO [0m:      Received: evaluate message 72a816f1-25b8-488c-a6be-93784edf7058
02/15/2025 01:20:49:INFO:Received: evaluate message 72a816f1-25b8-488c-a6be-93784edf7058
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:42:INFO:
[92mINFO [0m:      Received: train message dc5b3a3b-ef97-4058-ba00-1efc670bdc14
02/15/2025 01:21:42:INFO:Received: train message dc5b3a3b-ef97-4058-ba00-1efc670bdc14
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:11:INFO:
[92mINFO [0m:      Received: evaluate message 21209799-5122-4b08-96ab-f7ac261d9979
02/15/2025 01:23:11:INFO:Received: evaluate message 21209799-5122-4b08-96ab-f7ac261d9979
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:23:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:10:INFO:
[92mINFO [0m:      Received: train message c8d9ecfc-8369-4f5b-8dd5-be2284e92b23
02/15/2025 01:24:10:INFO:Received: train message c8d9ecfc-8369-4f5b-8dd5-be2284e92b23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:29:INFO:
[92mINFO [0m:      Received: evaluate message dedfe195-d702-488e-92c7-974c9f69d6bf
02/15/2025 01:25:29:INFO:Received: evaluate message dedfe195-d702-488e-92c7-974c9f69d6bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:26:05:INFO:
[92mINFO [0m:      Received: train message c16b14f4-89a8-4326-9b06-9aaf237be6b8
02/15/2025 01:26:05:INFO:Received: train message c16b14f4-89a8-4326-9b06-9aaf237be6b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:49:INFO:
[92mINFO [0m:      Received: evaluate message 5161b4f9-3e5c-4b13-bb51-df1ac42b1a13
02/15/2025 01:27:49:INFO:Received: evaluate message 5161b4f9-3e5c-4b13-bb51-df1ac42b1a13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:35:INFO:
[92mINFO [0m:      Received: train message 7d2d69de-f327-4391-92fd-123f0e88d200
02/15/2025 01:28:35:INFO:Received: train message 7d2d69de-f327-4391-92fd-123f0e88d200
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:29:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:54:INFO:
[92mINFO [0m:      Received: evaluate message a9f20930-e74c-497b-af54-14e331404c6a
02/15/2025 01:29:54:INFO:Received: evaluate message a9f20930-e74c-497b-af54-14e331404c6a
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563], 'accuracy': [0.4988272087568413], 'auc': [0.6039493601988546], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003], 'accuracy': [0.4988272087568413, 0.5058639562157936], 'auc': [0.6039493601988546, 0.6592898585275673], 'precision': [0.24999954080217573, 0.40000145617924], 'recall': [0.4988272087568413, 0.5058639562157936], 'f1': [0.3330718973441611, 0.38101002254991995]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:29:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:41:INFO:
[92mINFO [0m:      Received: train message f60a4af0-2340-4527-aefb-1455b9a40d99
02/15/2025 01:30:41:INFO:Received: train message f60a4af0-2340-4527-aefb-1455b9a40d99
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:31:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:25:INFO:
[92mINFO [0m:      Received: evaluate message 61fee011-92ed-4eca-98e6-b4f3d4ca3144
02/15/2025 01:32:25:INFO:Received: evaluate message 61fee011-92ed-4eca-98e6-b4f3d4ca3144
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:32:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:53:INFO:
[92mINFO [0m:      Received: train message 710320ff-698a-4c4f-b0e2-2c9e83cf5b80
02/15/2025 01:32:53:INFO:Received: train message 710320ff-698a-4c4f-b0e2-2c9e83cf5b80
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:19:INFO:
[92mINFO [0m:      Received: evaluate message 011e627b-1b91-4a3d-8eae-de4a7e0d39fb
02/15/2025 01:34:19:INFO:Received: evaluate message 011e627b-1b91-4a3d-8eae-de4a7e0d39fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:34:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:35:20:INFO:
[92mINFO [0m:      Received: train message d95b1c02-82cc-459b-8d6e-31cffc55f87c
02/15/2025 01:35:20:INFO:Received: train message d95b1c02-82cc-459b-8d6e-31cffc55f87c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:46:INFO:
[92mINFO [0m:      Received: evaluate message fb0f7902-473e-4175-8894-19e2e217d587
02/15/2025 01:36:46:INFO:Received: evaluate message fb0f7902-473e-4175-8894-19e2e217d587
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:16:INFO:
[92mINFO [0m:      Received: train message f94e82e5-97f4-44da-ba61-dc6f83369598
02/15/2025 01:37:16:INFO:Received: train message f94e82e5-97f4-44da-ba61-dc6f83369598
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:45:INFO:
[92mINFO [0m:      Received: evaluate message 5c74ed9c-ba21-4f22-a526-49ce69b22cd8
02/15/2025 01:38:45:INFO:Received: evaluate message 5c74ed9c-ba21-4f22-a526-49ce69b22cd8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:38:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:47:INFO:
[92mINFO [0m:      Received: train message fcc8f58e-b1cb-4579-a9ab-9a54f525f633
02/15/2025 01:39:47:INFO:Received: train message fcc8f58e-b1cb-4579-a9ab-9a54f525f633

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:21:INFO:
[92mINFO [0m:      Received: evaluate message 58c6a9ea-4298-4447-9962-541cde3c900c
02/15/2025 01:41:21:INFO:Received: evaluate message 58c6a9ea-4298-4447-9962-541cde3c900c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:42:04:INFO:
[92mINFO [0m:      Received: train message 7faa8b6b-fe43-410e-aa5e-31e84bc7a4e2
02/15/2025 01:42:04:INFO:Received: train message 7faa8b6b-fe43-410e-aa5e-31e84bc7a4e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:42:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:34:INFO:
[92mINFO [0m:      Received: evaluate message 551254d8-2220-4f53-8c41-bca50a65204f
02/15/2025 01:43:34:INFO:Received: evaluate message 551254d8-2220-4f53-8c41-bca50a65204f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:59:INFO:
[92mINFO [0m:      Received: train message d197cdc1-61a4-47ef-b892-575b46d9576d
02/15/2025 01:43:59:INFO:Received: train message d197cdc1-61a4-47ef-b892-575b46d9576d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:44:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:58:INFO:
[92mINFO [0m:      Received: evaluate message 6cda6b82-6491-4e86-974d-da2aa4a160e5
02/15/2025 01:45:58:INFO:Received: evaluate message 6cda6b82-6491-4e86-974d-da2aa4a160e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:35:INFO:
[92mINFO [0m:      Received: train message 9585ce04-2259-4308-8b26-12fe3e06b8ba
02/15/2025 01:46:35:INFO:Received: train message 9585ce04-2259-4308-8b26-12fe3e06b8ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:47:57:INFO:
[92mINFO [0m:      Received: evaluate message f9e1ea49-b352-43fa-a69c-5cc34f9d310b
02/15/2025 01:47:57:INFO:Received: evaluate message f9e1ea49-b352-43fa-a69c-5cc34f9d310b
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:57:INFO:
[92mINFO [0m:      Received: train message 73c44b35-1c8e-4ad1-9d7f-aa4573d77fc5
02/15/2025 01:48:57:INFO:Received: train message 73c44b35-1c8e-4ad1-9d7f-aa4573d77fc5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:49:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:30:INFO:
[92mINFO [0m:      Received: evaluate message 50e6d9b1-edc6-4dd7-aae1-ed966c37a07f
02/15/2025 01:50:30:INFO:Received: evaluate message 50e6d9b1-edc6-4dd7-aae1-ed966c37a07f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:12:INFO:
[92mINFO [0m:      Received: train message f162d645-a3f4-493c-8c27-f86752c9d2dc
02/15/2025 01:51:12:INFO:Received: train message f162d645-a3f4-493c-8c27-f86752c9d2dc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:51:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:52:40:INFO:
[92mINFO [0m:      Received: evaluate message 184609a8-d5b1-4981-8b24-bb3ad1f66d2a
02/15/2025 01:52:40:INFO:Received: evaluate message 184609a8-d5b1-4981-8b24-bb3ad1f66d2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:52:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:21:INFO:
[92mINFO [0m:      Received: train message 37f48cc9-09d6-4382-b06a-94bb107d4f10
02/15/2025 01:53:21:INFO:Received: train message 37f48cc9-09d6-4382-b06a-94bb107d4f10
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:53:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:54:46:INFO:
[92mINFO [0m:      Received: evaluate message e9438601-5144-4b9b-9233-5c3840ceb81a
02/15/2025 01:54:46:INFO:Received: evaluate message e9438601-5144-4b9b-9233-5c3840ceb81a

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:54:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:55:27:INFO:
[92mINFO [0m:      Received: train message 15898fb5-0d92-44f0-b7ff-e32adf1d631d
02/15/2025 01:55:27:INFO:Received: train message 15898fb5-0d92-44f0-b7ff-e32adf1d631d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:55:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:34:INFO:
[92mINFO [0m:      Received: evaluate message 7e72eaff-0fde-4daa-8fe3-3dad4a78a38d
02/15/2025 01:56:34:INFO:Received: evaluate message 7e72eaff-0fde-4daa-8fe3-3dad4a78a38d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:57:29:INFO:
[92mINFO [0m:      Received: train message c29b3237-ca0c-4ed2-93fd-2c6d4850f177
02/15/2025 01:57:29:INFO:Received: train message c29b3237-ca0c-4ed2-93fd-2c6d4850f177
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:57:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:56:INFO:
[92mINFO [0m:      Received: evaluate message de5fe8af-e022-4e53-9646-bbd600bebf2e
02/15/2025 01:58:56:INFO:Received: evaluate message de5fe8af-e022-4e53-9646-bbd600bebf2e

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:59:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:30:INFO:
[92mINFO [0m:      Received: train message d4e37371-b9c8-4f94-a6e0-dc321ef27dae
02/15/2025 01:59:30:INFO:Received: train message d4e37371-b9c8-4f94-a6e0-dc321ef27dae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:59:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:12:INFO:
[92mINFO [0m:      Received: evaluate message f12d9fd6-c956-4837-bdf2-f5ec9801c37d
02/15/2025 02:01:12:INFO:Received: evaluate message f12d9fd6-c956-4837-bdf2-f5ec9801c37d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:01:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:46:INFO:
[92mINFO [0m:      Received: train message 93f5bf63-04b5-46a4-9cb2-b81d20757bb4
02/15/2025 02:01:46:INFO:Received: train message 93f5bf63-04b5-46a4-9cb2-b81d20757bb4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:02:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:03:27:INFO:
[92mINFO [0m:      Received: evaluate message c513c4c5-b86b-4092-ace5-c433d746c31a
02/15/2025 02:03:27:INFO:Received: evaluate message c513c4c5-b86b-4092-ace5-c433d746c31a

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:03:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:02:INFO:
[92mINFO [0m:      Received: train message b18dc9e4-8439-437e-b65f-c68a51b5cfbd
02/15/2025 02:04:02:INFO:Received: train message b18dc9e4-8439-437e-b65f-c68a51b5cfbd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:04:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:29:INFO:
[92mINFO [0m:      Received: evaluate message 5568aa29-0a61-4842-8f09-69c2b36235db
02/15/2025 02:05:29:INFO:Received: evaluate message 5568aa29-0a61-4842-8f09-69c2b36235db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:05:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:54:INFO:
[92mINFO [0m:      Received: train message 82ada435-39b4-46b8-900b-b48fbf680da4
02/15/2025 02:05:54:INFO:Received: train message 82ada435-39b4-46b8-900b-b48fbf680da4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:43:INFO:
[92mINFO [0m:      Received: evaluate message 003fac95-a6b2-431b-8795-18c1e5d86648
02/15/2025 02:07:43:INFO:Received: evaluate message 003fac95-a6b2-431b-8795-18c1e5d86648

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:14:INFO:
[92mINFO [0m:      Received: train message 55988a19-2e26-4ab2-88e2-2f7f4298900a
02/15/2025 02:08:14:INFO:Received: train message 55988a19-2e26-4ab2-88e2-2f7f4298900a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:08:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:09:47:INFO:
[92mINFO [0m:      Received: evaluate message 193c279f-f5b5-4688-b813-d7537c7815cb
02/15/2025 02:09:47:INFO:Received: evaluate message 193c279f-f5b5-4688-b813-d7537c7815cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:09:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:35:INFO:
[92mINFO [0m:      Received: train message 0eb68de7-58c0-4a91-8ff9-1fb59621db2d
02/15/2025 02:10:35:INFO:Received: train message 0eb68de7-58c0-4a91-8ff9-1fb59621db2d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:11:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:11:53:INFO:
[92mINFO [0m:      Received: evaluate message c57eabf6-8bc2-4140-874e-40726ec13cd5
02/15/2025 02:11:53:INFO:Received: evaluate message c57eabf6-8bc2-4140-874e-40726ec13cd5

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:11:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:25:INFO:
[92mINFO [0m:      Received: train message 2a2c320b-52be-4889-ae4b-98a370c7c184
02/15/2025 02:12:25:INFO:Received: train message 2a2c320b-52be-4889-ae4b-98a370c7c184
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:12:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:15:INFO:
[92mINFO [0m:      Received: evaluate message 5a734322-66d8-4a7e-a95f-93409c6bcf3e
02/15/2025 02:14:15:INFO:Received: evaluate message 5a734322-66d8-4a7e-a95f-93409c6bcf3e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:14:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:47:INFO:
[92mINFO [0m:      Received: train message 268e2322-a110-4b83-a166-4acb5af15f23
02/15/2025 02:14:47:INFO:Received: train message 268e2322-a110-4b83-a166-4acb5af15f23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:15:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:23:INFO:
[92mINFO [0m:      Received: evaluate message bf5c0d4c-84fd-4c6e-b275-83ad6581e495
02/15/2025 02:16:23:INFO:Received: evaluate message bf5c0d4c-84fd-4c6e-b275-83ad6581e495

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:16:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:14:INFO:
[92mINFO [0m:      Received: train message e927a6b2-2cf1-4cb4-b735-4eae71e7a130
02/15/2025 02:17:14:INFO:Received: train message e927a6b2-2cf1-4cb4-b735-4eae71e7a130
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:17:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:30:INFO:
[92mINFO [0m:      Received: evaluate message b576192e-061b-47cc-a120-00767e2d35c1
02/15/2025 02:18:30:INFO:Received: evaluate message b576192e-061b-47cc-a120-00767e2d35c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:18:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:19:11:INFO:
[92mINFO [0m:      Received: train message 8d724a6d-79c5-4907-a3f6-6dddceffc292
02/15/2025 02:19:11:INFO:Received: train message 8d724a6d-79c5-4907-a3f6-6dddceffc292
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:47:INFO:
[92mINFO [0m:      Received: evaluate message d936b738-c13f-4483-9ed2-7df89ba9b9d6
02/15/2025 02:20:47:INFO:Received: evaluate message d936b738-c13f-4483-9ed2-7df89ba9b9d6

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:54:INFO:
[92mINFO [0m:      Received: reconnect message c8556b09-02ab-47ac-afef-923a19bee221
02/15/2025 02:20:54:INFO:Received: reconnect message c8556b09-02ab-47ac-afef-923a19bee221
02/15/2025 02:20:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:20:54:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}



Final client history:
{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}

