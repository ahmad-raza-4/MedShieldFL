nohup: ignoring input
02/15/2025 01:13:55:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:13:55:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:13:55:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 01:13:55:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/15/2025 01:13:55:INFO:
[92mINFO [0m:      Received: get_parameters message 3993c8ac-4d99-4435-a899-77f03e2c4b4c
02/15/2025 01:13:55:INFO:Received: get_parameters message 3993c8ac-4d99-4435-a899-77f03e2c4b4c
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739610835.984331 1652222 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      Sent reply
02/15/2025 01:14:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:14:37:INFO:
[92mINFO [0m:      Received: train message 7fd4fae0-6bf2-42da-955a-1f4cfb8b4f47
02/15/2025 01:14:37:INFO:Received: train message 7fd4fae0-6bf2-42da-955a-1f4cfb8b4f47
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:15:INFO:
[92mINFO [0m:      Received: evaluate message 3a1f4e3d-8abb-4daf-b4a5-1e666d9c6e57
02/15/2025 01:16:15:INFO:Received: evaluate message 3a1f4e3d-8abb-4daf-b4a5-1e666d9c6e57
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:16:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:36:INFO:
[92mINFO [0m:      Received: train message f6e39b0b-2856-4a2b-945f-daac32c684c9
02/15/2025 01:16:36:INFO:Received: train message f6e39b0b-2856-4a2b-945f-daac32c684c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:24:INFO:
[92mINFO [0m:      Received: evaluate message 3d957760-93ad-498f-92bc-07e39adf7df7
02/15/2025 01:18:24:INFO:Received: evaluate message 3d957760-93ad-498f-92bc-07e39adf7df7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:19:13:INFO:
[92mINFO [0m:      Received: train message 213259f3-356d-4e11-8f0c-497a450b78bc
02/15/2025 01:19:13:INFO:Received: train message 213259f3-356d-4e11-8f0c-497a450b78bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:27:INFO:
[92mINFO [0m:      Received: evaluate message e6f445f4-a565-458f-9955-62c6bb84ae8a
02/15/2025 01:20:27:INFO:Received: evaluate message e6f445f4-a565-458f-9955-62c6bb84ae8a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:38:INFO:
[92mINFO [0m:      Received: train message ccc0c707-e8e2-4e28-8d56-ffd65e17c266
02/15/2025 01:21:38:INFO:Received: train message ccc0c707-e8e2-4e28-8d56-ffd65e17c266
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:17:INFO:
[92mINFO [0m:      Received: evaluate message fef47bdf-9196-4549-bfff-72cd3f79177a
02/15/2025 01:23:17:INFO:Received: evaluate message fef47bdf-9196-4549-bfff-72cd3f79177a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:23:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:12:INFO:
[92mINFO [0m:      Received: train message 041d1de9-4e16-489c-99bb-f27fce55e58d
02/15/2025 01:24:12:INFO:Received: train message 041d1de9-4e16-489c-99bb-f27fce55e58d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:16:INFO:
[92mINFO [0m:      Received: evaluate message bd7a8250-7cdf-4b70-adfa-0295ec593961
02/15/2025 01:25:16:INFO:Received: evaluate message bd7a8250-7cdf-4b70-adfa-0295ec593961
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:26:26:INFO:
[92mINFO [0m:      Received: train message a5d1eec2-bb84-4bda-8577-e246f7359e7f
02/15/2025 01:26:26:INFO:Received: train message a5d1eec2-bb84-4bda-8577-e246f7359e7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:27:INFO:
[92mINFO [0m:      Received: evaluate message a0566d41-5c31-4d85-855b-4a14a385b891
02/15/2025 01:27:27:INFO:Received: evaluate message a0566d41-5c31-4d85-855b-4a14a385b891
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:11:INFO:
[92mINFO [0m:      Received: train message ccd0fa53-3dcb-47e3-bb86-d630cb9bbb84
02/15/2025 01:28:11:INFO:Received: train message ccd0fa53-3dcb-47e3-bb86-d630cb9bbb84
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:08:INFO:
[92mINFO [0m:      Received: evaluate message 8ff7b9c6-26ac-4b87-84d3-170d1782571f
02/15/2025 01:30:08:INFO:Received: evaluate message 8ff7b9c6-26ac-4b87-84d3-170d1782571f
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563], 'accuracy': [0.4988272087568413], 'auc': [0.6039493601988546], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003], 'accuracy': [0.4988272087568413, 0.5058639562157936], 'auc': [0.6039493601988546, 0.6592898585275673], 'precision': [0.24999954080217573, 0.40000145617924], 'recall': [0.4988272087568413, 0.5058639562157936], 'f1': [0.3330718973441611, 0.38101002254991995]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:30:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:51:INFO:
[92mINFO [0m:      Received: train message 1357afb3-9529-4932-9ccd-698a28fb3865
02/15/2025 01:30:51:INFO:Received: train message 1357afb3-9529-4932-9ccd-698a28fb3865
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:31:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:31:58:INFO:
[92mINFO [0m:      Received: evaluate message 6b5fa0d3-f74a-415c-be7a-0bb1d84f58d0
02/15/2025 01:31:58:INFO:Received: evaluate message 6b5fa0d3-f74a-415c-be7a-0bb1d84f58d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:32:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:05:INFO:
[92mINFO [0m:      Received: train message c776106f-41b8-4138-981a-071ea884516e
02/15/2025 01:33:05:INFO:Received: train message c776106f-41b8-4138-981a-071ea884516e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:08:INFO:
[92mINFO [0m:      Received: evaluate message 5d8ff084-2258-4ab3-a483-61d575cd6f8b
02/15/2025 01:34:08:INFO:Received: evaluate message 5d8ff084-2258-4ab3-a483-61d575cd6f8b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:34:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:35:16:INFO:
[92mINFO [0m:      Received: train message 30be2588-8a2f-444b-bc97-d3a21e6909b9
02/15/2025 01:35:16:INFO:Received: train message 30be2588-8a2f-444b-bc97-d3a21e6909b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:39:INFO:
[92mINFO [0m:      Received: evaluate message cb624b46-f9f2-4b7b-aec4-e86f4a6d4ce0
02/15/2025 01:36:39:INFO:Received: evaluate message cb624b46-f9f2-4b7b-aec4-e86f4a6d4ce0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:27:INFO:
[92mINFO [0m:      Received: train message fc1c95c4-800c-4c3f-915c-2d5d8cc13120
02/15/2025 01:37:27:INFO:Received: train message fc1c95c4-800c-4c3f-915c-2d5d8cc13120
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:37:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:43:INFO:
[92mINFO [0m:      Received: evaluate message 5cf73fa4-c778-4a8e-a3fa-beaa79f81909
02/15/2025 01:38:43:INFO:Received: evaluate message 5cf73fa4-c778-4a8e-a3fa-beaa79f81909
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:38:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:45:INFO:
[92mINFO [0m:      Received: train message d758276d-6099-4e8b-bf6b-17b7e79a11ec
02/15/2025 01:39:45:INFO:Received: train message d758276d-6099-4e8b-bf6b-17b7e79a11ec

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:23:INFO:
[92mINFO [0m:      Received: evaluate message a467ca34-f6a8-4c21-a4ad-a70485f1d621
02/15/2025 01:41:23:INFO:Received: evaluate message a467ca34-f6a8-4c21-a4ad-a70485f1d621
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:51:INFO:
[92mINFO [0m:      Received: train message e07705ec-54b0-43f8-b779-5d81fe019b81
02/15/2025 01:41:51:INFO:Received: train message e07705ec-54b0-43f8-b779-5d81fe019b81
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:42:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:39:INFO:
[92mINFO [0m:      Received: evaluate message 5eee70b3-4aab-487c-af66-4662214886f3
02/15/2025 01:43:39:INFO:Received: evaluate message 5eee70b3-4aab-487c-af66-4662214886f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:14:INFO:
[92mINFO [0m:      Received: train message 20904e0e-d101-478c-9f32-03ac699c1ca9
02/15/2025 01:44:14:INFO:Received: train message 20904e0e-d101-478c-9f32-03ac699c1ca9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:44:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:45:INFO:
[92mINFO [0m:      Received: evaluate message bd12f046-6d49-44ae-874b-ad9d445d86fc
02/15/2025 01:45:45:INFO:Received: evaluate message bd12f046-6d49-44ae-874b-ad9d445d86fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:45:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:24:INFO:
[92mINFO [0m:      Received: train message 7eef7116-62bb-497e-98ef-0c96d0ade70e
02/15/2025 01:46:24:INFO:Received: train message 7eef7116-62bb-497e-98ef-0c96d0ade70e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:46:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:13:INFO:
[92mINFO [0m:      Received: evaluate message 4d56a035-cecc-44f4-8f47-45ae8a4f0d61
02/15/2025 01:48:13:INFO:Received: evaluate message 4d56a035-cecc-44f4-8f47-45ae8a4f0d61
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:57:INFO:
[92mINFO [0m:      Received: train message 52a404bd-3027-42a5-9661-7c7413707069
02/15/2025 01:48:57:INFO:Received: train message 52a404bd-3027-42a5-9661-7c7413707069
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:49:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:21:INFO:
[92mINFO [0m:      Received: evaluate message 432446db-7e3f-48ec-89cd-bad08d430e6e
02/15/2025 01:50:21:INFO:Received: evaluate message 432446db-7e3f-48ec-89cd-bad08d430e6e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:07:INFO:
[92mINFO [0m:      Received: train message c0e71bbb-fae4-4f05-83a9-c83308c0212c
02/15/2025 01:51:07:INFO:Received: train message c0e71bbb-fae4-4f05-83a9-c83308c0212c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:51:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:52:27:INFO:
[92mINFO [0m:      Received: evaluate message d249ec48-04f2-46cf-ba62-37d314a5115d
02/15/2025 01:52:27:INFO:Received: evaluate message d249ec48-04f2-46cf-ba62-37d314a5115d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:52:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:21:INFO:
[92mINFO [0m:      Received: train message 9fc663cd-40c1-4e6c-8462-f3e4f6b31968
02/15/2025 01:53:21:INFO:Received: train message 9fc663cd-40c1-4e6c-8462-f3e4f6b31968
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:53:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:54:48:INFO:
[92mINFO [0m:      Received: evaluate message c4b574e8-476f-4dad-949c-c0d88dd46666
02/15/2025 01:54:48:INFO:Received: evaluate message c4b574e8-476f-4dad-949c-c0d88dd46666

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:54:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:55:28:INFO:
[92mINFO [0m:      Received: train message 812f3e58-b1cb-4c75-ba07-5870b9f743d5
02/15/2025 01:55:28:INFO:Received: train message 812f3e58-b1cb-4c75-ba07-5870b9f743d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:55:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:50:INFO:
[92mINFO [0m:      Received: evaluate message b9246641-ac8c-4784-9ec8-38fc6e86fd14
02/15/2025 01:56:50:INFO:Received: evaluate message b9246641-ac8c-4784-9ec8-38fc6e86fd14
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:57:22:INFO:
[92mINFO [0m:      Received: train message 4bc7fbc7-0c3d-496c-9df8-c598f399d480
02/15/2025 01:57:22:INFO:Received: train message 4bc7fbc7-0c3d-496c-9df8-c598f399d480
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:57:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:06:INFO:
[92mINFO [0m:      Received: evaluate message c36cb62c-6f7a-43b3-b155-c9b6bff2ce17
02/15/2025 01:59:06:INFO:Received: evaluate message c36cb62c-6f7a-43b3-b155-c9b6bff2ce17

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:59:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:39:INFO:
[92mINFO [0m:      Received: train message 566b202d-76e5-4878-affd-3764a1d2509c
02/15/2025 01:59:39:INFO:Received: train message 566b202d-76e5-4878-affd-3764a1d2509c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:00:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:07:INFO:
[92mINFO [0m:      Received: evaluate message 332b0fae-1747-4f71-898b-3d0867e18199
02/15/2025 02:01:07:INFO:Received: evaluate message 332b0fae-1747-4f71-898b-3d0867e18199
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:01:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:52:INFO:
[92mINFO [0m:      Received: train message 647f66ce-92cf-4d41-9383-832271e265b5
02/15/2025 02:01:52:INFO:Received: train message 647f66ce-92cf-4d41-9383-832271e265b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:02:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:03:19:INFO:
[92mINFO [0m:      Received: evaluate message 3a169011-d20d-4b62-b193-a2d49f19ca1b
02/15/2025 02:03:19:INFO:Received: evaluate message 3a169011-d20d-4b62-b193-a2d49f19ca1b

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:03:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:06:INFO:
[92mINFO [0m:      Received: train message 184d3474-32b9-4fbc-83a2-4d832bbd8368
02/15/2025 02:04:06:INFO:Received: train message 184d3474-32b9-4fbc-83a2-4d832bbd8368
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:04:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:32:INFO:
[92mINFO [0m:      Received: evaluate message 7ad449aa-e14e-4726-9c32-ef7ff9ccc466
02/15/2025 02:05:32:INFO:Received: evaluate message 7ad449aa-e14e-4726-9c32-ef7ff9ccc466
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:05:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:06:07:INFO:
[92mINFO [0m:      Received: train message 6ec17c15-5c22-44fe-8202-5443caeebefd
02/15/2025 02:06:07:INFO:Received: train message 6ec17c15-5c22-44fe-8202-5443caeebefd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:39:INFO:
[92mINFO [0m:      Received: evaluate message f51485bd-9aaa-4aca-abc3-e8a9b1bddc3c
02/15/2025 02:07:39:INFO:Received: evaluate message f51485bd-9aaa-4aca-abc3-e8a9b1bddc3c

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:25:INFO:
[92mINFO [0m:      Received: train message ba5af29b-0d80-4145-88e8-5b3ce771f646
02/15/2025 02:08:25:INFO:Received: train message ba5af29b-0d80-4145-88e8-5b3ce771f646
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:08:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:09:54:INFO:
[92mINFO [0m:      Received: evaluate message d7c5d641-339b-4c01-bb17-a948deb025e8
02/15/2025 02:09:54:INFO:Received: evaluate message d7c5d641-339b-4c01-bb17-a948deb025e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:10:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:20:INFO:
[92mINFO [0m:      Received: train message 5800610f-7700-4985-af01-4d2d9bede61e
02/15/2025 02:10:20:INFO:Received: train message 5800610f-7700-4985-af01-4d2d9bede61e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:10:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:11:59:INFO:
[92mINFO [0m:      Received: evaluate message 130c2620-03f8-4c5a-a833-e9ceed176e47
02/15/2025 02:11:59:INFO:Received: evaluate message 130c2620-03f8-4c5a-a833-e9ceed176e47

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:12:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:27:INFO:
[92mINFO [0m:      Received: train message bd480ebb-70ee-41dc-899b-c96c3a4f6319
02/15/2025 02:12:27:INFO:Received: train message bd480ebb-70ee-41dc-899b-c96c3a4f6319
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:12:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:17:INFO:
[92mINFO [0m:      Received: evaluate message 6f530ca9-dd7e-424d-8d42-dde9446fdb0e
02/15/2025 02:14:17:INFO:Received: evaluate message 6f530ca9-dd7e-424d-8d42-dde9446fdb0e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:14:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:57:INFO:
[92mINFO [0m:      Received: train message bb51f932-c716-4c6d-a502-8aa392e62b0d
02/15/2025 02:14:57:INFO:Received: train message bb51f932-c716-4c6d-a502-8aa392e62b0d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:15:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:23:INFO:
[92mINFO [0m:      Received: evaluate message bc178619-e78c-45f9-9292-36b7a8e1aa0a
02/15/2025 02:16:23:INFO:Received: evaluate message bc178619-e78c-45f9-9292-36b7a8e1aa0a

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:16:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:04:INFO:
[92mINFO [0m:      Received: train message 3f5603c6-d026-43e3-97ba-84acbf1eda03
02/15/2025 02:17:04:INFO:Received: train message 3f5603c6-d026-43e3-97ba-84acbf1eda03
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:17:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:33:INFO:
[92mINFO [0m:      Received: evaluate message 2a2c52a0-c33c-4f86-9d15-ab855b9446cd
02/15/2025 02:18:33:INFO:Received: evaluate message 2a2c52a0-c33c-4f86-9d15-ab855b9446cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:18:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:19:08:INFO:
[92mINFO [0m:      Received: train message af5e1244-bc8b-4cde-831f-637f2afdca5c
02/15/2025 02:19:08:INFO:Received: train message af5e1244-bc8b-4cde-831f-637f2afdca5c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:49:INFO:
[92mINFO [0m:      Received: evaluate message 5b02b4ad-5bae-4372-9718-8ab2f6c9d503
02/15/2025 02:20:49:INFO:Received: evaluate message 5b02b4ad-5bae-4372-9718-8ab2f6c9d503

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:54:INFO:
[92mINFO [0m:      Received: reconnect message ac07f2e6-5b66-424b-9f59-520c49236144
02/15/2025 02:20:54:INFO:Received: reconnect message ac07f2e6-5b66-424b-9f59-520c49236144
02/15/2025 02:20:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:20:54:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}



Final client history:
{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}

