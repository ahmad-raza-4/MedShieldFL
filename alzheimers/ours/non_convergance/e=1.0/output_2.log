nohup: ignoring input
02/15/2025 01:13:57:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:13:57:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:13:57:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739610837.619721 1653029 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:14:39:INFO:
[92mINFO [0m:      Received: train message 03115e92-db40-43fe-8915-157957e10b09
02/15/2025 01:14:39:INFO:Received: train message 03115e92-db40-43fe-8915-157957e10b09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:09:INFO:
[92mINFO [0m:      Received: evaluate message e81d25c4-72a3-4a51-99e4-95f8dcdd1c74
02/15/2025 01:16:09:INFO:Received: evaluate message e81d25c4-72a3-4a51-99e4-95f8dcdd1c74
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:16:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:55:INFO:
[92mINFO [0m:      Received: train message b6905e6f-225d-4c38-a24d-150557449b50
02/15/2025 01:16:55:INFO:Received: train message b6905e6f-225d-4c38-a24d-150557449b50
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:28:INFO:
[92mINFO [0m:      Received: evaluate message f7838c7b-1824-483f-9473-781303b9a974
02/15/2025 01:18:28:INFO:Received: evaluate message f7838c7b-1824-483f-9473-781303b9a974
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:19:04:INFO:
[92mINFO [0m:      Received: train message 33665bc7-8939-47fe-9b1f-7c6b5686615a
02/15/2025 01:19:04:INFO:Received: train message 33665bc7-8939-47fe-9b1f-7c6b5686615a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:47:INFO:
[92mINFO [0m:      Received: evaluate message a642af92-6572-4dbd-b6d9-ff60fe2e9697
02/15/2025 01:20:47:INFO:Received: evaluate message a642af92-6572-4dbd-b6d9-ff60fe2e9697
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:35:INFO:
[92mINFO [0m:      Received: train message e34d5c8f-4799-43b3-bccc-dddf7eb71d8d
02/15/2025 01:21:35:INFO:Received: train message e34d5c8f-4799-43b3-bccc-dddf7eb71d8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:17:INFO:
[92mINFO [0m:      Received: evaluate message ed4cea64-5d70-426f-9d58-b1cd8e5d52ce
02/15/2025 01:23:17:INFO:Received: evaluate message ed4cea64-5d70-426f-9d58-b1cd8e5d52ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:23:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:24:08:INFO:
[92mINFO [0m:      Received: train message 3116d91b-c7a9-4a3f-822f-977a8f604495
02/15/2025 01:24:08:INFO:Received: train message 3116d91b-c7a9-4a3f-822f-977a8f604495
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:35:INFO:
[92mINFO [0m:      Received: evaluate message c6d3197d-5842-438f-94b6-9762a29e3e3d
02/15/2025 01:25:35:INFO:Received: evaluate message c6d3197d-5842-438f-94b6-9762a29e3e3d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:26:08:INFO:
[92mINFO [0m:      Received: train message 71ee7c76-5528-4370-a219-06b6e13ff5b7
02/15/2025 01:26:08:INFO:Received: train message 71ee7c76-5528-4370-a219-06b6e13ff5b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:51:INFO:
[92mINFO [0m:      Received: evaluate message cd0832b5-621e-4391-bad7-dea7b3a78213
02/15/2025 01:27:51:INFO:Received: evaluate message cd0832b5-621e-4391-bad7-dea7b3a78213
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:27:INFO:
[92mINFO [0m:      Received: train message 90459ec7-0f25-40c0-83e4-67002381d36b
02/15/2025 01:28:27:INFO:Received: train message 90459ec7-0f25-40c0-83e4-67002381d36b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:54:INFO:
[92mINFO [0m:      Received: evaluate message 338dc17e-5e18-4429-99ef-ebb725e0020c
02/15/2025 01:29:54:INFO:Received: evaluate message 338dc17e-5e18-4429-99ef-ebb725e0020c
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563], 'accuracy': [0.4988272087568413], 'auc': [0.6039493601988546], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003], 'accuracy': [0.4988272087568413, 0.5058639562157936], 'auc': [0.6039493601988546, 0.6592898585275673], 'precision': [0.24999954080217573, 0.40000145617924], 'recall': [0.4988272087568413, 0.5058639562157936], 'f1': [0.3330718973441611, 0.38101002254991995]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:29:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:41:INFO:
[92mINFO [0m:      Received: train message 19cb7c71-d9ad-4012-8a74-c91b260a4452
02/15/2025 01:30:41:INFO:Received: train message 19cb7c71-d9ad-4012-8a74-c91b260a4452
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:31:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:23:INFO:
[92mINFO [0m:      Received: evaluate message cd0ce814-af96-45b0-9bf8-35ffe6dffd8d
02/15/2025 01:32:23:INFO:Received: evaluate message cd0ce814-af96-45b0-9bf8-35ffe6dffd8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:32:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:33:06:INFO:
[92mINFO [0m:      Received: train message 4323d575-cdb3-42f5-b265-b302d0e26399
02/15/2025 01:33:06:INFO:Received: train message 4323d575-cdb3-42f5-b265-b302d0e26399
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:33:INFO:
[92mINFO [0m:      Received: evaluate message 1a2154ca-7fe0-4831-8da0-5835f2a10dab
02/15/2025 01:34:33:INFO:Received: evaluate message 1a2154ca-7fe0-4831-8da0-5835f2a10dab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:34:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:35:12:INFO:
[92mINFO [0m:      Received: train message a857d966-b7fc-46a6-a98c-b55afd4e7a49
02/15/2025 01:35:12:INFO:Received: train message a857d966-b7fc-46a6-a98c-b55afd4e7a49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:37:INFO:
[92mINFO [0m:      Received: evaluate message 239bbe26-7983-4108-b3ba-e3107f1a75bc
02/15/2025 01:36:37:INFO:Received: evaluate message 239bbe26-7983-4108-b3ba-e3107f1a75bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:34:INFO:
[92mINFO [0m:      Received: train message 53d7eb0c-223a-4781-946a-645fcd3d8daa
02/15/2025 01:37:34:INFO:Received: train message 53d7eb0c-223a-4781-946a-645fcd3d8daa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:38:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:56:INFO:
[92mINFO [0m:      Received: evaluate message 1ecc8a3b-5f24-4336-8ced-a2f9c1e7c541
02/15/2025 01:38:56:INFO:Received: evaluate message 1ecc8a3b-5f24-4336-8ced-a2f9c1e7c541
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:22:INFO:
[92mINFO [0m:      Received: train message bda293db-4b55-44de-95e3-6611bab0cb19
02/15/2025 01:39:22:INFO:Received: train message bda293db-4b55-44de-95e3-6611bab0cb19

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:39:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:03:INFO:
[92mINFO [0m:      Received: evaluate message 358e7b08-273d-496e-a752-e968e434f607
02/15/2025 01:41:03:INFO:Received: evaluate message 358e7b08-273d-496e-a752-e968e434f607
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:48:INFO:
[92mINFO [0m:      Received: train message 33800fb2-df23-403c-b931-0f0f102a67a4
02/15/2025 01:41:48:INFO:Received: train message 33800fb2-df23-403c-b931-0f0f102a67a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:42:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:34:INFO:
[92mINFO [0m:      Received: evaluate message ab0cce73-7d82-4f72-a6cf-f65bd7c562c4
02/15/2025 01:43:34:INFO:Received: evaluate message ab0cce73-7d82-4f72-a6cf-f65bd7c562c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:05:INFO:
[92mINFO [0m:      Received: train message a07223dc-a7bf-4225-8348-ba394067d099
02/15/2025 01:44:05:INFO:Received: train message a07223dc-a7bf-4225-8348-ba394067d099
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:44:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:56:INFO:
[92mINFO [0m:      Received: evaluate message c16eb8ad-866f-4982-aa1b-b012f2a1ab3b
02/15/2025 01:45:56:INFO:Received: evaluate message c16eb8ad-866f-4982-aa1b-b012f2a1ab3b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:46:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:35:INFO:
[92mINFO [0m:      Received: train message 2e624738-0468-469f-ac65-ac0f75a8d154
02/15/2025 01:46:35:INFO:Received: train message 2e624738-0468-469f-ac65-ac0f75a8d154
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:15:INFO:
[92mINFO [0m:      Received: evaluate message 0a99e73a-4ef7-4d30-ad41-7435c970c180
02/15/2025 01:48:15:INFO:Received: evaluate message 0a99e73a-4ef7-4d30-ad41-7435c970c180
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:53:INFO:
[92mINFO [0m:      Received: train message da0a2626-d4e7-4c2a-ae98-d399fe1a4664
02/15/2025 01:48:53:INFO:Received: train message da0a2626-d4e7-4c2a-ae98-d399fe1a4664
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:49:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:32:INFO:
[92mINFO [0m:      Received: evaluate message 6d2c184b-349a-4854-9751-56b9452f989e
02/15/2025 01:50:32:INFO:Received: evaluate message 6d2c184b-349a-4854-9751-56b9452f989e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:45:INFO:
[92mINFO [0m:      Received: train message abe10960-1a28-409e-ad27-a6828b08ccf8
02/15/2025 01:50:45:INFO:Received: train message abe10960-1a28-409e-ad27-a6828b08ccf8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:51:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:52:37:INFO:
[92mINFO [0m:      Received: evaluate message 780f36e5-34c6-4398-8d62-5d0c43a64472
02/15/2025 01:52:37:INFO:Received: evaluate message 780f36e5-34c6-4398-8d62-5d0c43a64472
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:52:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:53:10:INFO:
[92mINFO [0m:      Received: train message a7835424-c64d-4c32-9379-f6b1017b9c7d
02/15/2025 01:53:10:INFO:Received: train message a7835424-c64d-4c32-9379-f6b1017b9c7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:53:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:54:27:INFO:
[92mINFO [0m:      Received: evaluate message eccc071c-df4c-4396-8c21-6fe3be29c601
02/15/2025 01:54:27:INFO:Received: evaluate message eccc071c-df4c-4396-8c21-6fe3be29c601

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:54:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:55:26:INFO:
[92mINFO [0m:      Received: train message e55188e5-139b-49f0-a0f5-f02b371cdd9b
02/15/2025 01:55:26:INFO:Received: train message e55188e5-139b-49f0-a0f5-f02b371cdd9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:56:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:50:INFO:
[92mINFO [0m:      Received: evaluate message 8882e1a9-a9de-4052-ba27-14ca27a9add7
02/15/2025 01:56:50:INFO:Received: evaluate message 8882e1a9-a9de-4052-ba27-14ca27a9add7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:57:27:INFO:
[92mINFO [0m:      Received: train message 6ab99139-8810-42b2-9cfa-4662410b92b8
02/15/2025 01:57:27:INFO:Received: train message 6ab99139-8810-42b2-9cfa-4662410b92b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:57:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:58:53:INFO:
[92mINFO [0m:      Received: evaluate message 43903707-70f9-40ae-ad58-cc3d46e6ab88
02/15/2025 01:58:53:INFO:Received: evaluate message 43903707-70f9-40ae-ad58-cc3d46e6ab88

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:58:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:47:INFO:
[92mINFO [0m:      Received: train message 1b1233b3-530f-42e4-84a9-090aceacd1bb
02/15/2025 01:59:47:INFO:Received: train message 1b1233b3-530f-42e4-84a9-090aceacd1bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:00:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:18:INFO:
[92mINFO [0m:      Received: evaluate message 1e1901bb-12ec-4d28-aad0-83b9db23ad62
02/15/2025 02:01:18:INFO:Received: evaluate message 1e1901bb-12ec-4d28-aad0-83b9db23ad62
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:01:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:54:INFO:
[92mINFO [0m:      Received: train message 3b893891-190d-47b7-aaab-2bf552c4e975
02/15/2025 02:01:54:INFO:Received: train message 3b893891-190d-47b7-aaab-2bf552c4e975
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:02:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:03:06:INFO:
[92mINFO [0m:      Received: evaluate message fbd4b6bd-7e53-4488-ae7f-65a422466fb6
02/15/2025 02:03:06:INFO:Received: evaluate message fbd4b6bd-7e53-4488-ae7f-65a422466fb6

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:03:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:04:07:INFO:
[92mINFO [0m:      Received: train message 2dbccc1a-4ff1-4a75-9ccb-9ff96ffb4f6b
02/15/2025 02:04:07:INFO:Received: train message 2dbccc1a-4ff1-4a75-9ccb-9ff96ffb4f6b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:04:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:23:INFO:
[92mINFO [0m:      Received: evaluate message b0ee30f6-f299-4a88-a634-a838cc8360c4
02/15/2025 02:05:23:INFO:Received: evaluate message b0ee30f6-f299-4a88-a634-a838cc8360c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:05:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:06:13:INFO:
[92mINFO [0m:      Received: train message 676de274-18c4-411a-b405-ea39a3ab8282
02/15/2025 02:06:13:INFO:Received: train message 676de274-18c4-411a-b405-ea39a3ab8282
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:19:INFO:
[92mINFO [0m:      Received: evaluate message 9e23d31e-f1e2-4955-a2cb-462f70da5f62
02/15/2025 02:07:19:INFO:Received: evaluate message 9e23d31e-f1e2-4955-a2cb-462f70da5f62

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:23:INFO:
[92mINFO [0m:      Received: train message 59bca7bc-174e-4a05-80a6-0aa40c954da9
02/15/2025 02:08:23:INFO:Received: train message 59bca7bc-174e-4a05-80a6-0aa40c954da9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:08:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:09:54:INFO:
[92mINFO [0m:      Received: evaluate message e349102b-8f16-4ffb-b5ce-9b32777ec879
02/15/2025 02:09:54:INFO:Received: evaluate message e349102b-8f16-4ffb-b5ce-9b32777ec879
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:10:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:35:INFO:
[92mINFO [0m:      Received: train message 379af05c-704c-44f9-a0bb-6b511b585533
02/15/2025 02:10:35:INFO:Received: train message 379af05c-704c-44f9-a0bb-6b511b585533
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:11:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:11:44:INFO:
[92mINFO [0m:      Received: evaluate message cdae66aa-4289-472e-8b17-7e7a1a7bc975
02/15/2025 02:11:44:INFO:Received: evaluate message cdae66aa-4289-472e-8b17-7e7a1a7bc975

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:11:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:40:INFO:
[92mINFO [0m:      Received: train message f68572ab-174b-4639-b045-7f20025432b2
02/15/2025 02:12:40:INFO:Received: train message f68572ab-174b-4639-b045-7f20025432b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:13:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:11:INFO:
[92mINFO [0m:      Received: evaluate message e8cc83fa-33fb-4c9b-91ac-42fb9d1e3b09
02/15/2025 02:14:11:INFO:Received: evaluate message e8cc83fa-33fb-4c9b-91ac-42fb9d1e3b09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:14:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:15:01:INFO:
[92mINFO [0m:      Received: train message 046dccc6-634b-4ba4-b84e-1333c1f3a5bc
02/15/2025 02:15:01:INFO:Received: train message 046dccc6-634b-4ba4-b84e-1333c1f3a5bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:15:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:29:INFO:
[92mINFO [0m:      Received: evaluate message b9e42083-76c1-44d5-a2f5-596dea199304
02/15/2025 02:16:29:INFO:Received: evaluate message b9e42083-76c1-44d5-a2f5-596dea199304

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:16:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:10:INFO:
[92mINFO [0m:      Received: train message 769ef4ed-3533-4065-a004-47c4c8fb5f02
02/15/2025 02:17:10:INFO:Received: train message 769ef4ed-3533-4065-a004-47c4c8fb5f02
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:17:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:19:INFO:
[92mINFO [0m:      Received: evaluate message b6c5f403-53b6-43c3-80b9-1af0c76c7520
02/15/2025 02:18:19:INFO:Received: evaluate message b6c5f403-53b6-43c3-80b9-1af0c76c7520
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:18:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:19:04:INFO:
[92mINFO [0m:      Received: train message e6ad4f32-48bd-419b-ab1a-245e53f7d5bf
02/15/2025 02:19:04:INFO:Received: train message e6ad4f32-48bd-419b-ab1a-245e53f7d5bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:26:INFO:
[92mINFO [0m:      Received: evaluate message 21e7c233-78bb-409e-be2a-0463ed5ce312
02/15/2025 02:20:26:INFO:Received: evaluate message 21e7c233-78bb-409e-be2a-0463ed5ce312

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:30:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:54:INFO:
[92mINFO [0m:      Received: reconnect message ba1b29ea-f793-4605-a66c-58ba5d90d80f
02/15/2025 02:20:54:INFO:Received: reconnect message ba1b29ea-f793-4605-a66c-58ba5d90d80f
02/15/2025 02:20:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:20:54:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}



Final client history:
{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}

