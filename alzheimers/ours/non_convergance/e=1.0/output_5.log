nohup: ignoring input
02/15/2025 01:13:59:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 01:13:59:DEBUG:ChannelConnectivity.IDLE
02/15/2025 01:13:59:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739610839.815982 1653973 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 01:14:37:INFO:
[92mINFO [0m:      Received: train message fa11682b-4e13-4944-8205-67474c34e893
02/15/2025 01:14:37:INFO:Received: train message fa11682b-4e13-4944-8205-67474c34e893
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:15:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:09:INFO:
[92mINFO [0m:      Received: evaluate message 419bc99d-8e7e-4ee4-8458-df3c8f561e5d
02/15/2025 01:16:09:INFO:Received: evaluate message 419bc99d-8e7e-4ee4-8458-df3c8f561e5d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:16:15:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:16:37:INFO:
[92mINFO [0m:      Received: train message bb48a57e-a2a2-4d46-839a-bdaa1a6e398d
02/15/2025 01:16:37:INFO:Received: train message bb48a57e-a2a2-4d46-839a-bdaa1a6e398d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:17:08:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:18:28:INFO:
[92mINFO [0m:      Received: evaluate message d640ad10-8255-49d7-af37-7fee24183aba
02/15/2025 01:18:28:INFO:Received: evaluate message d640ad10-8255-49d7-af37-7fee24183aba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:18:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:19:13:INFO:
[92mINFO [0m:      Received: train message bfc1cb00-3e04-4592-970f-eaf74be16c22
02/15/2025 01:19:13:INFO:Received: train message bfc1cb00-3e04-4592-970f-eaf74be16c22
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:19:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:20:50:INFO:
[92mINFO [0m:      Received: evaluate message 71019c71-e613-494d-90af-6e2b5e7a4209
02/15/2025 01:20:50:INFO:Received: evaluate message 71019c71-e613-494d-90af-6e2b5e7a4209
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:20:57:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:21:45:INFO:
[92mINFO [0m:      Received: train message da0e8b2e-a8a4-47e6-bd1a-41efdcb6c2af
02/15/2025 01:21:45:INFO:Received: train message da0e8b2e-a8a4-47e6-bd1a-41efdcb6c2af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:22:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:22:INFO:
[92mINFO [0m:      Received: evaluate message f51efe04-ee33-44b7-abab-df84c1a96b2e
02/15/2025 01:23:22:INFO:Received: evaluate message f51efe04-ee33-44b7-abab-df84c1a96b2e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:23:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:23:52:INFO:
[92mINFO [0m:      Received: train message f7951252-b992-4632-9ce1-10382ac8df46
02/15/2025 01:23:52:INFO:Received: train message f7951252-b992-4632-9ce1-10382ac8df46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:24:29:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:25:33:INFO:
[92mINFO [0m:      Received: evaluate message 3fc2c976-f6e1-416a-9ee7-d86b10016308
02/15/2025 01:25:33:INFO:Received: evaluate message 3fc2c976-f6e1-416a-9ee7-d86b10016308
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:25:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:26:19:INFO:
[92mINFO [0m:      Received: train message d0e9dba0-7a00-4201-ba48-42ea7c8cd375
02/15/2025 01:26:19:INFO:Received: train message d0e9dba0-7a00-4201-ba48-42ea7c8cd375
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:26:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:27:49:INFO:
[92mINFO [0m:      Received: evaluate message 85afd6c8-0879-4bb3-973e-1e657758e026
02/15/2025 01:27:49:INFO:Received: evaluate message 85afd6c8-0879-4bb3-973e-1e657758e026
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:27:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:28:13:INFO:
[92mINFO [0m:      Received: train message bcd0eb87-8d93-459a-b8cf-9d5e87e7d82c
02/15/2025 01:28:13:INFO:Received: train message bcd0eb87-8d93-459a-b8cf-9d5e87e7d82c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:28:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:29:58:INFO:
[92mINFO [0m:      Received: evaluate message 60fa43c8-79d1-4f21-80be-c7d5f2244038
02/15/2025 01:29:58:INFO:Received: evaluate message 60fa43c8-79d1-4f21-80be-c7d5f2244038
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563], 'accuracy': [0.4988272087568413], 'auc': [0.6039493601988546], 'precision': [0.24999954080217573], 'recall': [0.4988272087568413], 'f1': [0.3330718973441611]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003], 'accuracy': [0.4988272087568413, 0.5058639562157936], 'auc': [0.6039493601988546, 0.6592898585275673], 'precision': [0.24999954080217573, 0.40000145617924], 'recall': [0.4988272087568413, 0.5058639562157936], 'f1': [0.3330718973441611, 0.38101002254991995]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:30:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:30:45:INFO:
[92mINFO [0m:      Received: train message 43236a83-3de8-47ef-9b64-47a87323e890
02/15/2025 01:30:45:INFO:Received: train message 43236a83-3de8-47ef-9b64-47a87323e890
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:31:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:06:INFO:
[92mINFO [0m:      Received: evaluate message fd07c4bf-5a7c-49ec-a6cc-a41ce61941d4
02/15/2025 01:32:06:INFO:Received: evaluate message fd07c4bf-5a7c-49ec-a6cc-a41ce61941d4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:32:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:32:48:INFO:
[92mINFO [0m:      Received: train message 5ce28421-5002-4c3e-a9ae-b651f76f932f
02/15/2025 01:32:48:INFO:Received: train message 5ce28421-5002-4c3e-a9ae-b651f76f932f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:33:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:34:33:INFO:
[92mINFO [0m:      Received: evaluate message 2e66623a-40cd-485b-9c8b-9373d4cdc08a
02/15/2025 01:34:33:INFO:Received: evaluate message 2e66623a-40cd-485b-9c8b-9373d4cdc08a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:34:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:35:19:INFO:
[92mINFO [0m:      Received: train message 2a1b654c-a533-4997-8c4b-0f7155bfe521
02/15/2025 01:35:19:INFO:Received: train message 2a1b654c-a533-4997-8c4b-0f7155bfe521
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:35:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:36:46:INFO:
[92mINFO [0m:      Received: evaluate message 19ea2621-8d34-4eb9-b93b-c1283336e267
02/15/2025 01:36:46:INFO:Received: evaluate message 19ea2621-8d34-4eb9-b93b-c1283336e267
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:36:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:37:31:INFO:
[92mINFO [0m:      Received: train message b72f220f-1e07-44fe-a4f4-d8acf36754e4
02/15/2025 01:37:31:INFO:Received: train message b72f220f-1e07-44fe-a4f4-d8acf36754e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:38:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:38:59:INFO:
[92mINFO [0m:      Received: evaluate message 6599c2d7-76a0-470c-bc0c-4421275d26bf
02/15/2025 01:38:59:INFO:Received: evaluate message 6599c2d7-76a0-470c-bc0c-4421275d26bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:39:03:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:39:45:INFO:
[92mINFO [0m:      Received: train message f6ee3753-688a-4e33-8f0f-ce581cda4f69
02/15/2025 01:39:45:INFO:Received: train message f6ee3753-688a-4e33-8f0f-ce581cda4f69

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:40:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:21:INFO:
[92mINFO [0m:      Received: evaluate message bbe196f7-0572-42e2-80c8-12a79c56c5c0
02/15/2025 01:41:21:INFO:Received: evaluate message bbe196f7-0572-42e2-80c8-12a79c56c5c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:41:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:41:56:INFO:
[92mINFO [0m:      Received: train message 45771b49-fb5c-4ed5-9904-bf9ac84aa2d3
02/15/2025 01:41:56:INFO:Received: train message 45771b49-fb5c-4ed5-9904-bf9ac84aa2d3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:42:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:43:22:INFO:
[92mINFO [0m:      Received: evaluate message 925c6aa9-26e5-4e5e-a431-b173fcddd77a
02/15/2025 01:43:22:INFO:Received: evaluate message 925c6aa9-26e5-4e5e-a431-b173fcddd77a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:43:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:44:18:INFO:
[92mINFO [0m:      Received: train message 65d9f9a3-32e8-4b94-9e03-b2b27e88d2df
02/15/2025 01:44:18:INFO:Received: train message 65d9f9a3-32e8-4b94-9e03-b2b27e88d2df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:44:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:45:53:INFO:
[92mINFO [0m:      Received: evaluate message d3a268b0-ca1d-4b6d-a0a8-65dd0b14a902
02/15/2025 01:45:53:INFO:Received: evaluate message d3a268b0-ca1d-4b6d-a0a8-65dd0b14a902
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:45:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:46:32:INFO:
[92mINFO [0m:      Received: train message 2053297a-65b1-4e68-b84d-401efafbdeb6
02/15/2025 01:46:32:INFO:Received: train message 2053297a-65b1-4e68-b84d-401efafbdeb6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:47:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:06:INFO:
[92mINFO [0m:      Received: evaluate message 33355c12-2505-413a-a212-e78d2cda5e16
02/15/2025 01:48:06:INFO:Received: evaluate message 33355c12-2505-413a-a212-e78d2cda5e16
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:48:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:48:47:INFO:
[92mINFO [0m:      Received: train message 69588348-8501-4f30-a21b-788de03cb4ec
02/15/2025 01:48:47:INFO:Received: train message 69588348-8501-4f30-a21b-788de03cb4ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:49:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:50:22:INFO:
[92mINFO [0m:      Received: evaluate message a85aacfc-5876-4fce-9226-ec6ae66925fd
02/15/2025 01:50:22:INFO:Received: evaluate message a85aacfc-5876-4fce-9226-ec6ae66925fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:50:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:51:15:INFO:
[92mINFO [0m:      Received: train message da62f6f1-a442-4c59-b553-e6e8e1930ddf
02/15/2025 01:51:15:INFO:Received: train message da62f6f1-a442-4c59-b553-e6e8e1930ddf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:51:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:52:31:INFO:
[92mINFO [0m:      Received: evaluate message d346a540-6da7-420d-ac3b-2a51798753be
02/15/2025 01:52:31:INFO:Received: evaluate message d346a540-6da7-420d-ac3b-2a51798753be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:52:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:52:56:INFO:
[92mINFO [0m:      Received: train message 560a221c-6558-4e72-b131-e7f7557b5504
02/15/2025 01:52:56:INFO:Received: train message 560a221c-6558-4e72-b131-e7f7557b5504
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:53:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:54:39:INFO:
[92mINFO [0m:      Received: evaluate message 91634db4-ed5d-4e68-84e2-f5138d31130b
02/15/2025 01:54:39:INFO:Received: evaluate message 91634db4-ed5d-4e68-84e2-f5138d31130b

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:54:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:55:24:INFO:
[92mINFO [0m:      Received: train message 2963ff1b-59b4-4a68-b217-600e1ff49d9f
02/15/2025 01:55:24:INFO:Received: train message 2963ff1b-59b4-4a68-b217-600e1ff49d9f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:56:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:56:30:INFO:
[92mINFO [0m:      Received: evaluate message b27c1b08-de64-445e-a6f1-c33e59bcb62f
02/15/2025 01:56:30:INFO:Received: evaluate message b27c1b08-de64-445e-a6f1-c33e59bcb62f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:56:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:57:34:INFO:
[92mINFO [0m:      Received: train message aac39e98-e920-4f3e-b92d-e61bbf390924
02/15/2025 01:57:34:INFO:Received: train message aac39e98-e920-4f3e-b92d-e61bbf390924
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 01:58:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:08:INFO:
[92mINFO [0m:      Received: evaluate message e3dcbbb0-8909-44f3-b179-efec058ff65d
02/15/2025 01:59:08:INFO:Received: evaluate message e3dcbbb0-8909-44f3-b179-efec058ff65d

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 01:59:12:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 01:59:42:INFO:
[92mINFO [0m:      Received: train message f179d561-9e56-4e48-a182-be8553c55f13
02/15/2025 01:59:42:INFO:Received: train message f179d561-9e56-4e48-a182-be8553c55f13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:00:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:15:INFO:
[92mINFO [0m:      Received: evaluate message c9f18935-3fa5-4645-80a9-40cee391c3a4
02/15/2025 02:01:15:INFO:Received: evaluate message c9f18935-3fa5-4645-80a9-40cee391c3a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:01:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:01:43:INFO:
[92mINFO [0m:      Received: train message 778122f0-b3e1-407b-a45b-338975c55e56
02/15/2025 02:01:43:INFO:Received: train message 778122f0-b3e1-407b-a45b-338975c55e56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:02:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:03:27:INFO:
[92mINFO [0m:      Received: evaluate message e9984fcc-7b85-432e-b1fa-a9f483146b77
02/15/2025 02:03:27:INFO:Received: evaluate message e9984fcc-7b85-432e-b1fa-a9f483146b77

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:03:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:03:45:INFO:
[92mINFO [0m:      Received: train message 115e14f9-3cd5-4fca-b91a-bcba123cf791
02/15/2025 02:03:45:INFO:Received: train message 115e14f9-3cd5-4fca-b91a-bcba123cf791
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:04:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:05:29:INFO:
[92mINFO [0m:      Received: evaluate message 1cdea7c1-e014-4b2a-bb0a-9e59c387f1ed
02/15/2025 02:05:29:INFO:Received: evaluate message 1cdea7c1-e014-4b2a-bb0a-9e59c387f1ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:05:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:06:06:INFO:
[92mINFO [0m:      Received: train message 90054be5-4940-4a09-b5aa-2f1b9631f957
02/15/2025 02:06:06:INFO:Received: train message 90054be5-4940-4a09-b5aa-2f1b9631f957
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:06:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:07:45:INFO:
[92mINFO [0m:      Received: evaluate message 1a598d04-f5dc-433f-a3e9-876f4f64a841
02/15/2025 02:07:45:INFO:Received: evaluate message 1a598d04-f5dc-433f-a3e9-876f4f64a841

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:07:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:08:14:INFO:
[92mINFO [0m:      Received: train message 387ab026-33ac-4fde-9658-b3cc3bf8c945
02/15/2025 02:08:14:INFO:Received: train message 387ab026-33ac-4fde-9658-b3cc3bf8c945
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:08:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:09:49:INFO:
[92mINFO [0m:      Received: evaluate message c81b0def-0def-4778-a331-d8916f3571c3
02/15/2025 02:09:49:INFO:Received: evaluate message c81b0def-0def-4778-a331-d8916f3571c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:09:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:10:37:INFO:
[92mINFO [0m:      Received: train message fc756f6f-8428-4f9d-839b-2d83558c81e6
02/15/2025 02:10:37:INFO:Received: train message fc756f6f-8428-4f9d-839b-2d83558c81e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:11:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:11:56:INFO:
[92mINFO [0m:      Received: evaluate message c0d56eb6-3029-47f7-bede-b561139d09ae
02/15/2025 02:11:56:INFO:Received: evaluate message c0d56eb6-3029-47f7-bede-b561139d09ae

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:12:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:12:47:INFO:
[92mINFO [0m:      Received: train message 4e42a075-b5e2-4f02-8003-1b1bf212fa18
02/15/2025 02:12:47:INFO:Received: train message 4e42a075-b5e2-4f02-8003-1b1bf212fa18
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:13:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:19:INFO:
[92mINFO [0m:      Received: evaluate message b728fbf8-5123-46ec-8b4c-724229378514
02/15/2025 02:14:19:INFO:Received: evaluate message b728fbf8-5123-46ec-8b4c-724229378514
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:14:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:14:58:INFO:
[92mINFO [0m:      Received: train message 52548df3-db81-4a4b-a805-8058fbbb8f6a
02/15/2025 02:14:58:INFO:Received: train message 52548df3-db81-4a4b-a805-8058fbbb8f6a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:15:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:16:13:INFO:
[92mINFO [0m:      Received: evaluate message a9a0bf0c-f8fd-4c64-8fd5-9525ed9e1e6f
02/15/2025 02:16:13:INFO:Received: evaluate message a9a0bf0c-f8fd-4c64-8fd5-9525ed9e1e6f

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:16:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:17:06:INFO:
[92mINFO [0m:      Received: train message 460a5c3a-f301-48de-a141-d582c2b99f80
02/15/2025 02:17:06:INFO:Received: train message 460a5c3a-f301-48de-a141-d582c2b99f80
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:17:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:18:13:INFO:
[92mINFO [0m:      Received: evaluate message 08765fea-7674-4082-94f4-775be9d3f864
02/15/2025 02:18:13:INFO:Received: evaluate message 08765fea-7674-4082-94f4-775be9d3f864
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:18:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:19:17:INFO:
[92mINFO [0m:      Received: train message a81cce90-1255-4dfb-8d06-40a4c7b40068
02/15/2025 02:19:17:INFO:Received: train message a81cce90-1255-4dfb-8d06-40a4c7b40068
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 02:19:52:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:43:INFO:
[92mINFO [0m:      Received: evaluate message 44b96c6f-876b-4cc2-ae73-b69124f7d9f9
02/15/2025 02:20:43:INFO:Received: evaluate message 44b96c6f-876b-4cc2-ae73-b69124f7d9f9

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 02:20:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 02:20:54:INFO:
[92mINFO [0m:      Received: reconnect message 20cef9ee-45a3-489e-8942-b2895c99bad9
02/15/2025 02:20:54:INFO:Received: reconnect message 20cef9ee-45a3-489e-8942-b2895c99bad9
02/15/2025 02:20:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 02:20:54:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}



Final client history:
{'loss': [1.1048949821746563, 1.058297781948003, 1.073536047626045, 1.0794401189328358, 1.0624398717291192, 1.0663490521432462, 1.084899220455429, 1.0904746844117952, 1.0864726188501592, 1.0798291421849995, 1.1115349400686603, 1.103044621034746, 1.116660057752742, 1.1377133023841386, 1.128898848538477, 1.1403872948125342, 1.1293059780413142, 1.0979660454907392, 1.1280860309492713, 1.1358843801821275, 1.1230867100097501, 1.146551494769886, 1.1539860687934391, 1.130802183387986, 1.1230713471572227, 1.1482624578420029, 1.1216932629308634, 1.1380312521071803, 1.123537093983487, 1.1309204178922712], 'accuracy': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'auc': [0.6039493601988546, 0.6592898585275673, 0.6796047355616176, 0.688934429076983, 0.6989737888556129, 0.7029845268076778, 0.7077708789694589, 0.7141464679877055, 0.7174096229873748, 0.7189342707334716, 0.7225786822832252, 0.7230588130435662, 0.7279665346430993, 0.7288580968430808, 0.7315537826052624, 0.7319334837926648, 0.7327109513760451, 0.7341651890740173, 0.7369281271760532, 0.7389188304666188, 0.7398096702884789, 0.7414557365007032, 0.7421860551729056, 0.7431052490625214, 0.7440100152693738, 0.7433413448645787, 0.7439213485431502, 0.7433264294647299, 0.743611414956241, 0.7439199896894039], 'precision': [0.24999954080217573, 0.40000145617924, 0.4094970100542235, 0.4137946343759352, 0.42509441204576004, 0.42292583117517785, 0.42885343277504, 0.43280613314584093, 0.43236138851020667, 0.43542869999179207, 0.43119103268078185, 0.4356507848200995, 0.43641850107198915, 0.4343043935814031, 0.43829954636296675, 0.43524557756441695, 0.44123257602366733, 0.4497765319981863, 0.4394212930607599, 0.4411501956668974, 0.4462095527523425, 0.44259957755545254, 0.4380206988847102, 0.4459254269216412, 0.44983855100136216, 0.44633074950875073, 0.4544006590587122, 0.45387956839934324, 0.4516222187500172, 0.4516982630849909], 'recall': [0.4988272087568413, 0.5058639562157936, 0.5129007036747459, 0.5168100078186083, 0.5261923377638781, 0.5238467552775606, 0.5293197810789679, 0.5332290852228303, 0.5332290852228303, 0.5324472243940579, 0.5308835027365129, 0.5324472243940579, 0.5340109460516028, 0.5332290852228303, 0.5363565285379203, 0.5347928068803753, 0.5394839718530101, 0.5402658326817826, 0.5379202501954652, 0.5394839718530101, 0.5418295543393276, 0.5410476935105551, 0.5379202501954652, 0.5433932759968726, 0.54573885848319, 0.5426114151681001, 0.544175136825645, 0.5480844409695075, 0.5449569976544175, 0.5465207193119624], 'f1': [0.3330718973441611, 0.38101002254991995, 0.4069695203710913, 0.42286108227761027, 0.44515138344730637, 0.4528939593228807, 0.45662074055580004, 0.46180937044810966, 0.46481428696006255, 0.472223426639122, 0.46521778429433247, 0.4731887028852204, 0.47306090132615675, 0.4688633657173529, 0.474813791161583, 0.4697073477802556, 0.4778850185403391, 0.4892411315796606, 0.4766230033291213, 0.4779442681175915, 0.48503830013615207, 0.4800228018806917, 0.47442302966211497, 0.484346953368893, 0.4892166530514037, 0.48497701128301257, 0.4940774609803316, 0.4937111810290798, 0.4914386995927841, 0.49115732872559625]}

