nohup: ignoring input
02/15/2025 10:51:33:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 10:51:33:DEBUG:ChannelConnectivity.IDLE
02/15/2025 10:51:33:DEBUG:ChannelConnectivity.CONNECTING
02/15/2025 10:51:33:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739645493.732836 2282759 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/15/2025 10:52:07:INFO:
[92mINFO [0m:      Received: train message 7e0c7aee-8302-44ac-a0eb-2e574f44b679
02/15/2025 10:52:07:INFO:Received: train message 7e0c7aee-8302-44ac-a0eb-2e574f44b679
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:52:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:53:32:INFO:
[92mINFO [0m:      Received: evaluate message 36f9b93f-eadf-4231-a6f8-79b80d745b7f
02/15/2025 10:53:32:INFO:Received: evaluate message 36f9b93f-eadf-4231-a6f8-79b80d745b7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:53:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:54:12:INFO:
[92mINFO [0m:      Received: train message c9ae1fe8-7747-46fe-beab-4e84ff239fbf
02/15/2025 10:54:12:INFO:Received: train message c9ae1fe8-7747-46fe-beab-4e84ff239fbf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:55:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:55:37:INFO:
[92mINFO [0m:      Received: evaluate message 31a6faf8-788a-474d-a66b-ee8f9e06dcd7
02/15/2025 10:55:37:INFO:Received: evaluate message 31a6faf8-788a-474d-a66b-ee8f9e06dcd7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:55:41:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:56:21:INFO:
[92mINFO [0m:      Received: train message 766ac1e3-a52f-431d-a47e-dbd29b0969ce
02/15/2025 10:56:21:INFO:Received: train message 766ac1e3-a52f-431d-a47e-dbd29b0969ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:57:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:57:49:INFO:
[92mINFO [0m:      Received: evaluate message d607f76f-1e24-42f3-9e02-5c7f000ae772
02/15/2025 10:57:49:INFO:Received: evaluate message d607f76f-1e24-42f3-9e02-5c7f000ae772
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:57:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:58:23:INFO:
[92mINFO [0m:      Received: train message 9ccb1d22-01e6-4644-87d3-91b29af4aa49
02/15/2025 10:58:23:INFO:Received: train message 9ccb1d22-01e6-4644-87d3-91b29af4aa49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:59:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:59:32:INFO:
[92mINFO [0m:      Received: evaluate message 27b0aee7-4d26-45e1-b821-f13ee8733e18
02/15/2025 10:59:32:INFO:Received: evaluate message 27b0aee7-4d26-45e1-b821-f13ee8733e18
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:59:35:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:00:23:INFO:
[92mINFO [0m:      Received: train message cdc5d483-f2ea-4e67-9cf4-b630747e3c73
02/15/2025 11:00:23:INFO:Received: train message cdc5d483-f2ea-4e67-9cf4-b630747e3c73
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:01:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:01:31:INFO:
[92mINFO [0m:      Received: evaluate message 66655a73-2e80-4aff-87a1-f765a3204bb5
02/15/2025 11:01:31:INFO:Received: evaluate message 66655a73-2e80-4aff-87a1-f765a3204bb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:01:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:02:10:INFO:
[92mINFO [0m:      Received: train message c50cf3b1-9e5b-46b6-b01f-412a06071ee8
02/15/2025 11:02:10:INFO:Received: train message c50cf3b1-9e5b-46b6-b01f-412a06071ee8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:02:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:03:29:INFO:
[92mINFO [0m:      Received: evaluate message 9d927d4d-5dbf-4510-98b1-7c5ea62d8a46
02/15/2025 11:03:29:INFO:Received: evaluate message 9d927d4d-5dbf-4510-98b1-7c5ea62d8a46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:03:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:04:13:INFO:
[92mINFO [0m:      Received: train message 21244bb2-ce08-4378-a0dd-93c872bc00a3
02/15/2025 11:04:13:INFO:Received: train message 21244bb2-ce08-4378-a0dd-93c872bc00a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:05:00:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:05:39:INFO:
[92mINFO [0m:      Received: evaluate message d53fd1e0-1474-4660-8475-58fa5a454c5e
02/15/2025 11:05:39:INFO:Received: evaluate message d53fd1e0-1474-4660-8475-58fa5a454c5e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144], 'accuracy': [0.506645817044566], 'auc': [0.7056891333322408], 'precision': [0.3993532547885127], 'recall': [0.506645817044566], 'f1': [0.3855554490777106]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269], 'accuracy': [0.506645817044566, 0.5285379202501954], 'auc': [0.7056891333322408, 0.732304691606418], 'precision': [0.3993532547885127, 0.4299345130435604], 'recall': [0.506645817044566, 0.5285379202501954], 'f1': [0.3855554490777106, 0.4662334545136853]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:05:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:06:05:INFO:
[92mINFO [0m:      Received: train message b2049136-b7de-4e2f-aa0d-23efbf02c9a2
02/15/2025 11:06:05:INFO:Received: train message b2049136-b7de-4e2f-aa0d-23efbf02c9a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:06:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:07:35:INFO:
[92mINFO [0m:      Received: evaluate message e9e0e4c9-2e51-4ded-83f1-071c044aa46c
02/15/2025 11:07:35:INFO:Received: evaluate message e9e0e4c9-2e51-4ded-83f1-071c044aa46c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:07:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:08:13:INFO:
[92mINFO [0m:      Received: train message a5d6137b-9c71-4140-9184-16ca71b1ef7f
02/15/2025 11:08:13:INFO:Received: train message a5d6137b-9c71-4140-9184-16ca71b1ef7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:08:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:09:35:INFO:
[92mINFO [0m:      Received: evaluate message 24f6e83d-21d3-4887-a00c-6c2790a483e0
02/15/2025 11:09:35:INFO:Received: evaluate message 24f6e83d-21d3-4887-a00c-6c2790a483e0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:09:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:10:14:INFO:
[92mINFO [0m:      Received: train message 1df19a1b-f1ad-4979-bbfc-c674dcee0bab
02/15/2025 11:10:14:INFO:Received: train message 1df19a1b-f1ad-4979-bbfc-c674dcee0bab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:10:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:11:28:INFO:
[92mINFO [0m:      Received: evaluate message bca8d3de-6b0f-41c1-9339-731edb59e51d
02/15/2025 11:11:28:INFO:Received: evaluate message bca8d3de-6b0f-41c1-9339-731edb59e51d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:11:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:12:09:INFO:
[92mINFO [0m:      Received: train message 13ce36e6-2c8a-4f49-82a4-6816b974baa6
02/15/2025 11:12:09:INFO:Received: train message 13ce36e6-2c8a-4f49-82a4-6816b974baa6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:12:50:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:13:30:INFO:
[92mINFO [0m:      Received: evaluate message afcb6e29-1ceb-482e-88f5-fa9e5e23ca64
02/15/2025 11:13:30:INFO:Received: evaluate message afcb6e29-1ceb-482e-88f5-fa9e5e23ca64
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:13:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:13:53:INFO:
[92mINFO [0m:      Received: train message 62730a25-ea9b-42d3-8626-b88d16f1a3b5
02/15/2025 11:13:53:INFO:Received: train message 62730a25-ea9b-42d3-8626-b88d16f1a3b5

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:14:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:15:19:INFO:
[92mINFO [0m:      Received: evaluate message 594e2bfe-709e-4570-bab6-5a26a8def6ff
02/15/2025 11:15:19:INFO:Received: evaluate message 594e2bfe-709e-4570-bab6-5a26a8def6ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:15:22:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:15:46:INFO:
[92mINFO [0m:      Received: train message 03429a17-793f-4d8a-98a7-6b59863d937c
02/15/2025 11:15:46:INFO:Received: train message 03429a17-793f-4d8a-98a7-6b59863d937c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:16:31:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:17:22:INFO:
[92mINFO [0m:      Received: evaluate message cb78c88b-0cea-4a0c-806b-97609f2a4eb0
02/15/2025 11:17:22:INFO:Received: evaluate message cb78c88b-0cea-4a0c-806b-97609f2a4eb0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:17:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:17:56:INFO:
[92mINFO [0m:      Received: train message a4fe3b7f-5d6a-4ea3-9201-8a92ee941711
02/15/2025 11:17:56:INFO:Received: train message a4fe3b7f-5d6a-4ea3-9201-8a92ee941711
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:18:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:19:21:INFO:
[92mINFO [0m:      Received: evaluate message 3bc01de4-9558-4098-9579-cb585009f156
02/15/2025 11:19:21:INFO:Received: evaluate message 3bc01de4-9558-4098-9579-cb585009f156
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:19:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:19:58:INFO:
[92mINFO [0m:      Received: train message 3469dfe4-5ccb-4733-a85b-9f555ebf28f1
02/15/2025 11:19:58:INFO:Received: train message 3469dfe4-5ccb-4733-a85b-9f555ebf28f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:20:40:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:21:06:INFO:
[92mINFO [0m:      Received: evaluate message 5d82a159-69c3-4a6f-9359-969fb4320f38
02/15/2025 11:21:06:INFO:Received: evaluate message 5d82a159-69c3-4a6f-9359-969fb4320f38
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:21:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:21:52:INFO:
[92mINFO [0m:      Received: train message 1d8b5961-8564-4e14-ae6e-ee989e2cbab7
02/15/2025 11:21:52:INFO:Received: train message 1d8b5961-8564-4e14-ae6e-ee989e2cbab7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:22:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:23:16:INFO:
[92mINFO [0m:      Received: evaluate message d52a389f-6635-4916-a4f1-3a23d84def7d
02/15/2025 11:23:16:INFO:Received: evaluate message d52a389f-6635-4916-a4f1-3a23d84def7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:23:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:23:52:INFO:
[92mINFO [0m:      Received: train message c3618ee8-2738-43f9-9bb0-a7cca2532456
02/15/2025 11:23:52:INFO:Received: train message c3618ee8-2738-43f9-9bb0-a7cca2532456
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:24:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:25:14:INFO:
[92mINFO [0m:      Received: evaluate message 99d2130c-0598-4206-b32b-dada4da7659a
02/15/2025 11:25:14:INFO:Received: evaluate message 99d2130c-0598-4206-b32b-dada4da7659a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:25:17:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:25:40:INFO:
[92mINFO [0m:      Received: train message 0aa7280e-23b1-4135-a57e-48c16381fa10
02/15/2025 11:25:40:INFO:Received: train message 0aa7280e-23b1-4135-a57e-48c16381fa10
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:26:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:27:01:INFO:
[92mINFO [0m:      Received: evaluate message 192115e0-5039-4871-81a3-7e6c09b7a42f
02/15/2025 11:27:01:INFO:Received: evaluate message 192115e0-5039-4871-81a3-7e6c09b7a42f

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:27:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:27:47:INFO:
[92mINFO [0m:      Received: train message ba73d778-29e2-43b7-bcee-7bf0971caca4
02/15/2025 11:27:47:INFO:Received: train message ba73d778-29e2-43b7-bcee-7bf0971caca4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:28:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:29:17:INFO:
[92mINFO [0m:      Received: evaluate message bbde0105-8e11-4b7f-93a8-132b2872f1ae
02/15/2025 11:29:17:INFO:Received: evaluate message bbde0105-8e11-4b7f-93a8-132b2872f1ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:29:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:29:33:INFO:
[92mINFO [0m:      Received: train message 4f13cb1f-ea8d-4d3c-96ce-b0a42c7d44bd
02/15/2025 11:29:33:INFO:Received: train message 4f13cb1f-ea8d-4d3c-96ce-b0a42c7d44bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:30:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:31:14:INFO:
[92mINFO [0m:      Received: evaluate message bc495e1e-92a7-45d4-aaa6-7f41b00d889d
02/15/2025 11:31:14:INFO:Received: evaluate message bc495e1e-92a7-45d4-aaa6-7f41b00d889d

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:31:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:31:54:INFO:
[92mINFO [0m:      Received: train message eae8ab2c-185d-4f7a-bca3-740666f81eec
02/15/2025 11:31:54:INFO:Received: train message eae8ab2c-185d-4f7a-bca3-740666f81eec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:32:37:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:33:11:INFO:
[92mINFO [0m:      Received: evaluate message 13bd1a1d-4dce-43cc-8a29-7132e9b2baf2
02/15/2025 11:33:11:INFO:Received: evaluate message 13bd1a1d-4dce-43cc-8a29-7132e9b2baf2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:33:13:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:33:36:INFO:
[92mINFO [0m:      Received: train message 3955117e-392b-4375-9ed9-edf996080f78
02/15/2025 11:33:36:INFO:Received: train message 3955117e-392b-4375-9ed9-edf996080f78
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:34:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:35:11:INFO:
[92mINFO [0m:      Received: evaluate message 5de67603-8caa-4092-aa82-23cd8b4b73dd
02/15/2025 11:35:11:INFO:Received: evaluate message 5de67603-8caa-4092-aa82-23cd8b4b73dd

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:35:14:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:35:43:INFO:
[92mINFO [0m:      Received: train message 06b7b6a8-c19e-49d3-911a-041660ee141e
02/15/2025 11:35:43:INFO:Received: train message 06b7b6a8-c19e-49d3-911a-041660ee141e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:36:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:36:47:INFO:
[92mINFO [0m:      Received: evaluate message a4fe6f7d-e391-4b5c-9d8d-b74b70df89d2
02/15/2025 11:36:47:INFO:Received: evaluate message a4fe6f7d-e391-4b5c-9d8d-b74b70df89d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:36:49:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:37:34:INFO:
[92mINFO [0m:      Received: train message 0d97c99a-1e87-4a6e-9bad-20a06c8d6f0b
02/15/2025 11:37:34:INFO:Received: train message 0d97c99a-1e87-4a6e-9bad-20a06c8d6f0b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:38:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:39:01:INFO:
[92mINFO [0m:      Received: evaluate message f46b4afe-dafd-4c14-82da-26a456e6068f
02/15/2025 11:39:01:INFO:Received: evaluate message f46b4afe-dafd-4c14-82da-26a456e6068f

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:39:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:39:37:INFO:
[92mINFO [0m:      Received: train message e6bf65d0-a667-40aa-90e8-7f78cd504205
02/15/2025 11:39:37:INFO:Received: train message e6bf65d0-a667-40aa-90e8-7f78cd504205
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:40:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:40:59:INFO:
[92mINFO [0m:      Received: evaluate message a1a6863d-ae96-4c1a-9dfa-48d2c59766e1
02/15/2025 11:40:59:INFO:Received: evaluate message a1a6863d-ae96-4c1a-9dfa-48d2c59766e1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:41:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:41:21:INFO:
[92mINFO [0m:      Received: train message a2e7e0f4-1609-4006-91cc-41a05838fbf7
02/15/2025 11:41:21:INFO:Received: train message a2e7e0f4-1609-4006-91cc-41a05838fbf7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:42:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:42:50:INFO:
[92mINFO [0m:      Received: evaluate message 8a8b1eb9-1de1-42c5-b61d-a65a89c0b897
02/15/2025 11:42:50:INFO:Received: evaluate message 8a8b1eb9-1de1-42c5-b61d-a65a89c0b897

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:42:53:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:43:25:INFO:
[92mINFO [0m:      Received: train message 5fb49ff5-2176-4dfc-8bba-2865658d6191
02/15/2025 11:43:25:INFO:Received: train message 5fb49ff5-2176-4dfc-8bba-2865658d6191
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:44:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:44:51:INFO:
[92mINFO [0m:      Received: evaluate message 1ef97090-5e84-47cf-92f9-d1ce20ab9879
02/15/2025 11:44:51:INFO:Received: evaluate message 1ef97090-5e84-47cf-92f9-d1ce20ab9879
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:44:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:45:10:INFO:
[92mINFO [0m:      Received: train message b59008c0-a4cc-484a-9778-e1574bcbc7bd
02/15/2025 11:45:10:INFO:Received: train message b59008c0-a4cc-484a-9778-e1574bcbc7bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:45:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:46:40:INFO:
[92mINFO [0m:      Received: evaluate message b3043a05-3efe-417e-aba1-65b98968ad55
02/15/2025 11:46:40:INFO:Received: evaluate message b3043a05-3efe-417e-aba1-65b98968ad55

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:46:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:47:25:INFO:
[92mINFO [0m:      Received: train message e7392768-f4e6-47d1-b055-319ecf81c07a
02/15/2025 11:47:25:INFO:Received: train message e7392768-f4e6-47d1-b055-319ecf81c07a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:48:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:48:46:INFO:
[92mINFO [0m:      Received: evaluate message 4285644f-d24b-4f63-8fb9-6d2b090c4aec
02/15/2025 11:48:46:INFO:Received: evaluate message 4285644f-d24b-4f63-8fb9-6d2b090c4aec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:48:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:49:19:INFO:
[92mINFO [0m:      Received: train message a6f10c70-2d31-4d06-954f-e73471537df3
02/15/2025 11:49:19:INFO:Received: train message a6f10c70-2d31-4d06-954f-e73471537df3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:50:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:50:52:INFO:
[92mINFO [0m:      Received: evaluate message ec100211-2de3-43d9-a0e4-f82050f2be32
02/15/2025 11:50:52:INFO:Received: evaluate message ec100211-2de3-43d9-a0e4-f82050f2be32

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:50:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:50:55:INFO:
[92mINFO [0m:      Received: reconnect message 0dfb4c35-ad0d-4e5a-b3d2-9b9940fa28d5
02/15/2025 11:50:55:INFO:Received: reconnect message 0dfb4c35-ad0d-4e5a-b3d2-9b9940fa28d5
02/15/2025 11:50:55:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 11:50:55:INFO:Disconnect and shut down

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966, 1.0564935112297769], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003, 0.7894125548593478], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599, 0.5876472686112911], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808, 0.5344292164861765]}



Final client history:
{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966, 1.0564935112297769], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003, 0.7894125548593478], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599, 0.5876472686112911], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808, 0.5344292164861765]}

