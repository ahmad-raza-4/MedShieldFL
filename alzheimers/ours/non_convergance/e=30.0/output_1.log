nohup: ignoring input
02/15/2025 10:51:35:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/15/2025 10:51:35:DEBUG:ChannelConnectivity.IDLE
02/15/2025 10:51:35:DEBUG:ChannelConnectivity.CONNECTING
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739645495.374662 2282824 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
02/15/2025 10:51:35:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/15/2025 10:52:09:INFO:
[92mINFO [0m:      Received: train message 4e26a770-cf1b-4d17-9a10-94813375da86
02/15/2025 10:52:09:INFO:Received: train message 4e26a770-cf1b-4d17-9a10-94813375da86
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:52:54:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:53:18:INFO:
[92mINFO [0m:      Received: evaluate message 76df6791-2237-4820-9950-2f2e88cd4013
02/15/2025 10:53:18:INFO:Received: evaluate message 76df6791-2237-4820-9950-2f2e88cd4013
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:53:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:54:14:INFO:
[92mINFO [0m:      Received: train message f589aa6c-82d8-4890-9a78-0a8be0215310
02/15/2025 10:54:14:INFO:Received: train message f589aa6c-82d8-4890-9a78-0a8be0215310
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:55:02:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:55:39:INFO:
[92mINFO [0m:      Received: evaluate message cc68842b-0b05-4fd6-a074-fc8d2b5add50
02/15/2025 10:55:39:INFO:Received: evaluate message cc68842b-0b05-4fd6-a074-fc8d2b5add50
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:55:42:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:56:24:INFO:
[92mINFO [0m:      Received: train message b0fe102d-9108-4181-9ed3-7076366ca3d8
02/15/2025 10:56:24:INFO:Received: train message b0fe102d-9108-4181-9ed3-7076366ca3d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:57:10:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:57:45:INFO:
[92mINFO [0m:      Received: evaluate message a27bba55-d690-45c8-abd1-b55a3b8a7228
02/15/2025 10:57:45:INFO:Received: evaluate message a27bba55-d690-45c8-abd1-b55a3b8a7228
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:57:48:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:58:16:INFO:
[92mINFO [0m:      Received: train message 39598ca4-839a-46dd-9229-06ef4411a455
02/15/2025 10:58:16:INFO:Received: train message 39598ca4-839a-46dd-9229-06ef4411a455
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 10:59:05:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 10:59:47:INFO:
[92mINFO [0m:      Received: evaluate message 61d863c9-6116-497f-b5b0-399a2206fd37
02/15/2025 10:59:47:INFO:Received: evaluate message 61d863c9-6116-497f-b5b0-399a2206fd37
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 10:59:51:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:00:13:INFO:
[92mINFO [0m:      Received: train message 6abc93c1-5d69-4438-84e3-8a1630b15387
02/15/2025 11:00:13:INFO:Received: train message 6abc93c1-5d69-4438-84e3-8a1630b15387
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:00:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:01:44:INFO:
[92mINFO [0m:      Received: evaluate message dd44ce89-0c89-4156-a120-3e58cad2fd9b
02/15/2025 11:01:44:INFO:Received: evaluate message dd44ce89-0c89-4156-a120-3e58cad2fd9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:01:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:02:22:INFO:
[92mINFO [0m:      Received: train message 0ab35fe2-1d07-4355-844e-df4dd39227c8
02/15/2025 11:02:22:INFO:Received: train message 0ab35fe2-1d07-4355-844e-df4dd39227c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:03:04:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:03:44:INFO:
[92mINFO [0m:      Received: evaluate message 7b2b52c9-6429-4fa2-b89d-96ca42ce6743
02/15/2025 11:03:44:INFO:Received: evaluate message 7b2b52c9-6429-4fa2-b89d-96ca42ce6743
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:03:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:04:13:INFO:
[92mINFO [0m:      Received: train message bf45771c-46b9-4300-9b6e-a2ba531bd1f7
02/15/2025 11:04:13:INFO:Received: train message bf45771c-46b9-4300-9b6e-a2ba531bd1f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:05:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:05:41:INFO:
[92mINFO [0m:      Received: evaluate message 7b98d46b-6d1d-4a4c-bfd5-939875abcc77
02/15/2025 11:05:41:INFO:Received: evaluate message 7b98d46b-6d1d-4a4c-bfd5-939875abcc77
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144], 'accuracy': [0.506645817044566], 'auc': [0.7056891333322408], 'precision': [0.3993532547885127], 'recall': [0.506645817044566], 'f1': [0.3855554490777106]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269], 'accuracy': [0.506645817044566, 0.5285379202501954], 'auc': [0.7056891333322408, 0.732304691606418], 'precision': [0.3993532547885127, 0.4299345130435604], 'recall': [0.506645817044566, 0.5285379202501954], 'f1': [0.3855554490777106, 0.4662334545136853]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:05:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:06:15:INFO:
[92mINFO [0m:      Received: train message b5b8229f-f3bc-4698-8b69-4a8d0acaceec
02/15/2025 11:06:15:INFO:Received: train message b5b8229f-f3bc-4698-8b69-4a8d0acaceec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:06:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:07:37:INFO:
[92mINFO [0m:      Received: evaluate message 7dcf3097-0de0-4637-b2d7-b92c6d5bbc34
02/15/2025 11:07:37:INFO:Received: evaluate message 7dcf3097-0de0-4637-b2d7-b92c6d5bbc34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:07:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:08:13:INFO:
[92mINFO [0m:      Received: train message 71e6ce62-c991-4d13-a46f-f72304a6d080
02/15/2025 11:08:13:INFO:Received: train message 71e6ce62-c991-4d13-a46f-f72304a6d080
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:08:59:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:09:23:INFO:
[92mINFO [0m:      Received: evaluate message 53c4667a-3151-44c6-85bc-c19773a8f0f3
02/15/2025 11:09:23:INFO:Received: evaluate message 53c4667a-3151-44c6-85bc-c19773a8f0f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:09:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:10:10:INFO:
[92mINFO [0m:      Received: train message eb8b51c9-443c-45d0-b987-a893aac7967d
02/15/2025 11:10:10:INFO:Received: train message eb8b51c9-443c-45d0-b987-a893aac7967d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:10:55:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:11:32:INFO:
[92mINFO [0m:      Received: evaluate message bf0ee09d-845c-452f-8c8b-3ecf19ce0f1c
02/15/2025 11:11:32:INFO:Received: evaluate message bf0ee09d-845c-452f-8c8b-3ecf19ce0f1c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:11:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:12:01:INFO:
[92mINFO [0m:      Received: train message c514a28d-7ff1-4a43-97a5-6e4cd8211998
02/15/2025 11:12:01:INFO:Received: train message c514a28d-7ff1-4a43-97a5-6e4cd8211998
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:12:46:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:13:23:INFO:
[92mINFO [0m:      Received: evaluate message 6feb7f70-956b-4fc2-a0ff-b7a29bee4280
02/15/2025 11:13:23:INFO:Received: evaluate message 6feb7f70-956b-4fc2-a0ff-b7a29bee4280
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:13:26:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:13:57:INFO:
[92mINFO [0m:      Received: train message 6e8547df-4029-4549-a877-cdd876e50a16
02/15/2025 11:13:57:INFO:Received: train message 6e8547df-4029-4549-a877-cdd876e50a16

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:14:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:15:21:INFO:
[92mINFO [0m:      Received: evaluate message 01203a06-f989-4591-b4ad-0cc3606e8268
02/15/2025 11:15:21:INFO:Received: evaluate message 01203a06-f989-4591-b4ad-0cc3606e8268
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:15:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:15:58:INFO:
[92mINFO [0m:      Received: train message 1d5e76e7-69b8-4416-9bf4-ee6899ea4a0e
02/15/2025 11:15:58:INFO:Received: train message 1d5e76e7-69b8-4416-9bf4-ee6899ea4a0e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:16:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:17:14:INFO:
[92mINFO [0m:      Received: evaluate message 730a310c-731b-4a90-941d-7bbaf84697ef
02/15/2025 11:17:14:INFO:Received: evaluate message 730a310c-731b-4a90-941d-7bbaf84697ef
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:17:16:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:17:57:INFO:
[92mINFO [0m:      Received: train message 4283a757-b18f-418d-a65c-98ff34aeec35
02/15/2025 11:17:57:INFO:Received: train message 4283a757-b18f-418d-a65c-98ff34aeec35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:18:44:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:19:24:INFO:
[92mINFO [0m:      Received: evaluate message febae07b-b16b-448e-b9ee-89a26ae29671
02/15/2025 11:19:24:INFO:Received: evaluate message febae07b-b16b-448e-b9ee-89a26ae29671
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:19:28:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:19:43:INFO:
[92mINFO [0m:      Received: train message 380f9989-c014-4fb5-955d-8e9552123bfe
02/15/2025 11:19:43:INFO:Received: train message 380f9989-c014-4fb5-955d-8e9552123bfe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:20:27:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:21:17:INFO:
[92mINFO [0m:      Received: evaluate message 8abd6a1b-bc73-4ac4-a17d-e05e6edd314a
02/15/2025 11:21:17:INFO:Received: evaluate message 8abd6a1b-bc73-4ac4-a17d-e05e6edd314a
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:21:20:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:21:40:INFO:
[92mINFO [0m:      Received: train message 7bdcc334-3153-482e-94af-12a5e082631a
02/15/2025 11:21:40:INFO:Received: train message 7bdcc334-3153-482e-94af-12a5e082631a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:22:25:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:22:58:INFO:
[92mINFO [0m:      Received: evaluate message ef24581c-d7d0-431b-83ed-92613ea685a8
02/15/2025 11:22:58:INFO:Received: evaluate message ef24581c-d7d0-431b-83ed-92613ea685a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:23:01:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:23:37:INFO:
[92mINFO [0m:      Received: train message 979794ac-70dc-4a80-8669-6a9fae562dd6
02/15/2025 11:23:37:INFO:Received: train message 979794ac-70dc-4a80-8669-6a9fae562dd6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:24:23:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:25:17:INFO:
[92mINFO [0m:      Received: evaluate message ff01fb66-73de-4c2f-b05a-424797e086d2
02/15/2025 11:25:17:INFO:Received: evaluate message ff01fb66-73de-4c2f-b05a-424797e086d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:25:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:25:47:INFO:
[92mINFO [0m:      Received: train message ad97f9f3-a1ba-4948-af5c-ab868d9fa19e
02/15/2025 11:25:47:INFO:Received: train message ad97f9f3-a1ba-4948-af5c-ab868d9fa19e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:26:36:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:27:02:INFO:
[92mINFO [0m:      Received: evaluate message 02913f9d-5f2e-42dc-9167-36ec2fe1ec0f
02/15/2025 11:27:02:INFO:Received: evaluate message 02913f9d-5f2e-42dc-9167-36ec2fe1ec0f

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:27:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:27:51:INFO:
[92mINFO [0m:      Received: train message 6da82544-8381-4b3c-9107-787c06c3e6b3
02/15/2025 11:27:51:INFO:Received: train message 6da82544-8381-4b3c-9107-787c06c3e6b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:28:38:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:28:55:INFO:
[92mINFO [0m:      Received: evaluate message 0bac4a7f-d86e-4626-893e-379a9e88a821
02/15/2025 11:28:55:INFO:Received: evaluate message 0bac4a7f-d86e-4626-893e-379a9e88a821
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:28:58:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:29:53:INFO:
[92mINFO [0m:      Received: train message 9dfcca43-bfbe-48e1-8c45-f5dca3809cfc
02/15/2025 11:29:53:INFO:Received: train message 9dfcca43-bfbe-48e1-8c45-f5dca3809cfc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:30:39:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:31:17:INFO:
[92mINFO [0m:      Received: evaluate message 22e428d3-7655-4719-b23e-8b9480642d4c
02/15/2025 11:31:17:INFO:Received: evaluate message 22e428d3-7655-4719-b23e-8b9480642d4c

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:31:21:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:31:47:INFO:
[92mINFO [0m:      Received: train message 06301985-97b4-4cb5-b291-e94307658243
02/15/2025 11:31:47:INFO:Received: train message 06301985-97b4-4cb5-b291-e94307658243
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:32:33:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:33:15:INFO:
[92mINFO [0m:      Received: evaluate message 6e638999-469b-4d91-b3a4-2a882d357b4e
02/15/2025 11:33:15:INFO:Received: evaluate message 6e638999-469b-4d91-b3a4-2a882d357b4e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:33:18:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:33:46:INFO:
[92mINFO [0m:      Received: train message 256a377d-a57a-4341-a799-b2b04a96f17f
02/15/2025 11:33:46:INFO:Received: train message 256a377d-a57a-4341-a799-b2b04a96f17f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:34:32:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:35:06:INFO:
[92mINFO [0m:      Received: evaluate message 9bead0e1-3575-4327-b216-debef54d6d1d
02/15/2025 11:35:06:INFO:Received: evaluate message 9bead0e1-3575-4327-b216-debef54d6d1d

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:35:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:35:24:INFO:
[92mINFO [0m:      Received: train message 31a431bc-b281-4215-9360-64f905d3d123
02/15/2025 11:35:24:INFO:Received: train message 31a431bc-b281-4215-9360-64f905d3d123
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:36:06:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:37:04:INFO:
[92mINFO [0m:      Received: evaluate message f63a21dc-160e-4490-ab2e-ace3572ce986
02/15/2025 11:37:04:INFO:Received: evaluate message f63a21dc-160e-4490-ab2e-ace3572ce986
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:37:07:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:37:41:INFO:
[92mINFO [0m:      Received: train message 2d3d1c64-a5bb-4103-bf0e-8edfeb8e9b68
02/15/2025 11:37:41:INFO:Received: train message 2d3d1c64-a5bb-4103-bf0e-8edfeb8e9b68
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:38:24:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:38:45:INFO:
[92mINFO [0m:      Received: evaluate message 2e6f8890-92aa-4191-b2de-410eca45ccbe
02/15/2025 11:38:45:INFO:Received: evaluate message 2e6f8890-92aa-4191-b2de-410eca45ccbe

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:38:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:39:34:INFO:
[92mINFO [0m:      Received: train message d0ebb33f-674d-4838-9098-b6284a67782e
02/15/2025 11:39:34:INFO:Received: train message d0ebb33f-674d-4838-9098-b6284a67782e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:40:19:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:40:54:INFO:
[92mINFO [0m:      Received: evaluate message 444155b7-d1bf-434e-b35b-69ee2217b6a2
02/15/2025 11:40:54:INFO:Received: evaluate message 444155b7-d1bf-434e-b35b-69ee2217b6a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:40:56:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:41:27:INFO:
[92mINFO [0m:      Received: train message 7d248e24-718c-4685-bc5d-44e201ae54f3
02/15/2025 11:41:27:INFO:Received: train message 7d248e24-718c-4685-bc5d-44e201ae54f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:42:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:42:41:INFO:
[92mINFO [0m:      Received: evaluate message 6951d401-f38b-4566-b2a7-655620fc94be
02/15/2025 11:42:41:INFO:Received: evaluate message 6951d401-f38b-4566-b2a7-655620fc94be

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:42:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:43:22:INFO:
[92mINFO [0m:      Received: train message 72925a68-c8a4-47ec-9bb6-35edc8ce3160
02/15/2025 11:43:22:INFO:Received: train message 72925a68-c8a4-47ec-9bb6-35edc8ce3160
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:44:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:44:32:INFO:
[92mINFO [0m:      Received: evaluate message 42b4bd51-3c62-472a-8084-f9590124fb91
02/15/2025 11:44:32:INFO:Received: evaluate message 42b4bd51-3c62-472a-8084-f9590124fb91
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:44:34:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:45:25:INFO:
[92mINFO [0m:      Received: train message af553d7e-c1e8-4796-8452-5d8610ab5f49
02/15/2025 11:45:25:INFO:Received: train message af553d7e-c1e8-4796-8452-5d8610ab5f49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:46:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:46:42:INFO:
[92mINFO [0m:      Received: evaluate message 78e33616-a063-4f9d-86f4-bcbc7bb6a843
02/15/2025 11:46:42:INFO:Received: evaluate message 78e33616-a063-4f9d-86f4-bcbc7bb6a843

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:46:45:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:47:23:INFO:
[92mINFO [0m:      Received: train message 3724eb05-1d61-4f3d-89f2-16b9a8f79735
02/15/2025 11:47:23:INFO:Received: train message 3724eb05-1d61-4f3d-89f2-16b9a8f79735
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:48:09:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:48:41:INFO:
[92mINFO [0m:      Received: evaluate message d456f4a0-8985-4648-bfd1-c7419fc50d30
02/15/2025 11:48:41:INFO:Received: evaluate message d456f4a0-8985-4648-bfd1-c7419fc50d30
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:48:43:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:49:23:INFO:
[92mINFO [0m:      Received: train message 19725042-cdeb-4008-852a-a05579d639e2
02/15/2025 11:49:23:INFO:Received: train message 19725042-cdeb-4008-852a-a05579d639e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/15/2025 11:50:11:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:50:45:INFO:
[92mINFO [0m:      Received: evaluate message e40d9d38-57c3-415e-9adb-01529a85f7fe
02/15/2025 11:50:45:INFO:Received: evaluate message e40d9d38-57c3-415e-9adb-01529a85f7fe

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/15/2025 11:50:47:INFO:Sent reply
[92mINFO [0m:      
02/15/2025 11:50:55:INFO:
[92mINFO [0m:      Received: reconnect message 9cb60054-ec98-4ecc-89af-f17754845a54
02/15/2025 11:50:55:INFO:Received: reconnect message 9cb60054-ec98-4ecc-89af-f17754845a54
02/15/2025 11:50:55:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/15/2025 11:50:55:INFO:Disconnect and shut down

{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966, 1.0564935112297769], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003, 0.7894125548593478], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599, 0.5876472686112911], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808, 0.5344292164861765]}



Final client history:
{'loss': [1.0958042525984144, 1.0552909414781269, 1.1514449331143388, 1.1290359631751645, 1.1545505162978005, 1.0939165621273585, 1.1259826190589088, 1.1271796691594784, 1.1503272581324155, 1.1074568125957431, 1.1809134308111864, 1.1195454191099021, 1.130173770919081, 1.1296815082279381, 1.1121701037072875, 1.0912247090436593, 1.13829312252756, 1.0835983931226036, 1.0853095020140588, 1.1025898658362472, 1.0733265936887293, 1.104501843219441, 1.1184601575905126, 1.0942197366092614, 1.1004709717908625, 1.0857244437331794, 1.0716309078602795, 1.082462082298777, 1.0877046878212966, 1.0564935112297769], 'accuracy': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7056891333322408, 0.732304691606418, 0.7446955562088431, 0.7499562852395555, 0.7537794808401792, 0.7572304976912001, 0.7597990005872022, 0.7632216165193144, 0.7649061366860211, 0.767057224009712, 0.7670168216238608, 0.7703316373298406, 0.7718320726722752, 0.7730709553628233, 0.7749154668315283, 0.7754639341114764, 0.7763529885289324, 0.7780815128358947, 0.7791575068789753, 0.7810208675963811, 0.782135210939445, 0.7832476240009322, 0.7834835404632632, 0.7842866662449622, 0.7857674700263091, 0.7860689024961041, 0.7877859382275603, 0.7874950406085465, 0.7884393158477003, 0.7894125548593478], 'precision': [0.3993532547885127, 0.4299345130435604, 0.4278693312535895, 0.4393257069142997, 0.4300335575751085, 0.4608460150683551, 0.453540235141392, 0.45179465936578095, 0.43891799252244196, 0.45545315655812374, 0.574575609066659, 0.5884704466144658, 0.5946698320978453, 0.4603276706675955, 0.5974533675862839, 0.5679971067811871, 0.5983274569220747, 0.5915903686281746, 0.6111847646674448, 0.5899106495769582, 0.6182782306176574, 0.6151380082867871, 0.5740255851479713, 0.5813328745107706, 0.5952895510362282, 0.5980455586722274, 0.6019571195875933, 0.5957767663927925, 0.580208245686599, 0.5876472686112911], 'recall': [0.506645817044566, 0.5285379202501954, 0.5301016419077405, 0.5363565285379203, 0.5316653635652854, 0.5519937451133698, 0.5504300234558248, 0.5488663017982799, 0.5387021110242377, 0.5512118842845973, 0.5363565285379203, 0.547302580140735, 0.5519937451133698, 0.5551211884284597, 0.5543393275996873, 0.5598123534010946, 0.5574667709147771, 0.5691946833463644, 0.5660672400312744, 0.5676309616888194, 0.5715402658326818, 0.5746677091477717, 0.563721657544957, 0.5754495699765442, 0.5723221266614542, 0.5762314308053167, 0.5770132916340891, 0.5770132916340891, 0.5723221266614542, 0.5809225957779516], 'f1': [0.3855554490777106, 0.4662334545136853, 0.45346952601508816, 0.4772795082916233, 0.4615234039758262, 0.5009202493268031, 0.49247141639546194, 0.4901462300071532, 0.47199692996755216, 0.4951658540185778, 0.46542640986894546, 0.4856683117352225, 0.4945526832673471, 0.5002680062978325, 0.4974540982833978, 0.5062640070842945, 0.4963553794389774, 0.5196896772165693, 0.5161681220143959, 0.5141811548647023, 0.5259692248307127, 0.521405686889413, 0.5050373813945037, 0.5233854052407987, 0.5174447827293518, 0.5265774449078122, 0.5294166893142008, 0.5262906616084266, 0.518691006996808, 0.5344292164861765]}

