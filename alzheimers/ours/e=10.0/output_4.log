nohup: ignoring input
02/05/2025 10:01:54:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:01:54:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:01:54:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 10:01:54:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778514.375418 1722825 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:02:17:INFO:
[92mINFO [0m:      Received: train message 29d4aa94-2ce7-46a7-97e7-0e47b42f3ab8
02/05/2025 10:02:17:INFO:Received: train message 29d4aa94-2ce7-46a7-97e7-0e47b42f3ab8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:03:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:01:INFO:
[92mINFO [0m:      Received: evaluate message 69032a0c-2adf-4c6a-9172-7ed2f0d854d7
02/05/2025 10:04:01:INFO:Received: evaluate message 69032a0c-2adf-4c6a-9172-7ed2f0d854d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:04:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:31:INFO:
[92mINFO [0m:      Received: train message c6f2d9a4-94a5-4d06-8ffe-5da823ec9bb9
02/05/2025 10:04:31:INFO:Received: train message c6f2d9a4-94a5-4d06-8ffe-5da823ec9bb9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:05:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:32:INFO:
[92mINFO [0m:      Received: evaluate message dbb434f3-0225-432c-ab77-122191e7d778
02/05/2025 10:06:32:INFO:Received: evaluate message dbb434f3-0225-432c-ab77-122191e7d778
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:52:INFO:
[92mINFO [0m:      Received: train message bf6fc361-87e7-441e-92ef-4256bc38cfe6
02/05/2025 10:06:52:INFO:Received: train message bf6fc361-87e7-441e-92ef-4256bc38cfe6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:05:INFO:
[92mINFO [0m:      Received: evaluate message 010025ba-f2aa-46e8-8198-0c04a943ae51
02/05/2025 10:09:05:INFO:Received: evaluate message 010025ba-f2aa-46e8-8198-0c04a943ae51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:34:INFO:
[92mINFO [0m:      Received: train message 66f2bfe3-b932-490f-890f-8f87953cfd48
02/05/2025 10:09:34:INFO:Received: train message 66f2bfe3-b932-490f-890f-8f87953cfd48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:27:INFO:
[92mINFO [0m:      Received: evaluate message e824cf97-abcb-487e-b338-d8ccb9c0e006
02/05/2025 10:11:27:INFO:Received: evaluate message e824cf97-abcb-487e-b338-d8ccb9c0e006
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:46:INFO:
[92mINFO [0m:      Received: train message 8fe02e18-13e4-44d6-adf6-829f03a7e8c7
02/05/2025 10:11:46:INFO:Received: train message 8fe02e18-13e4-44d6-adf6-829f03a7e8c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:56:INFO:
[92mINFO [0m:      Received: evaluate message 2b21aed8-bf07-4a7a-8f50-3a5847f6a967
02/05/2025 10:13:56:INFO:Received: evaluate message 2b21aed8-bf07-4a7a-8f50-3a5847f6a967
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:50:INFO:
[92mINFO [0m:      Received: train message 3690fcfd-ec18-44ec-a417-83ecdb4e7eb4
02/05/2025 10:14:50:INFO:Received: train message 3690fcfd-ec18-44ec-a417-83ecdb4e7eb4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:15:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:08:INFO:
[92mINFO [0m:      Received: evaluate message a0ef28f1-edde-4861-920f-abc86094bcdb
02/05/2025 10:16:08:INFO:Received: evaluate message a0ef28f1-edde-4861-920f-abc86094bcdb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:22:INFO:
[92mINFO [0m:      Received: train message 6a094194-6484-4dd2-b9e4-6e8b9669f705
02/05/2025 10:17:22:INFO:Received: train message 6a094194-6484-4dd2-b9e4-6e8b9669f705
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:18:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:37:INFO:
[92mINFO [0m:      Received: evaluate message 802228a4-fab5-4a08-87f3-151de51dfeab
02/05/2025 10:18:37:INFO:Received: evaluate message 802228a4-fab5-4a08-87f3-151de51dfeab
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945], 'accuracy': [0.5113369820172009], 'auc': [0.7085531489885228], 'precision': [0.40347364107321304], 'recall': [0.5113369820172009], 'f1': [0.39881455974124325]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902], 'accuracy': [0.5113369820172009, 0.5293197810789679], 'auc': [0.7085531489885228, 0.7350934873405456], 'precision': [0.40347364107321304, 0.43385134376460194], 'recall': [0.5113369820172009, 0.5293197810789679], 'f1': [0.39881455974124325, 0.4722552092863052]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:52:INFO:
[92mINFO [0m:      Received: train message 4a1e8289-eae8-4867-8a36-10cc07dc88e9
02/05/2025 10:19:52:INFO:Received: train message 4a1e8289-eae8-4867-8a36-10cc07dc88e9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:21:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:39:INFO:
[92mINFO [0m:      Received: evaluate message 2cf14508-0b86-4c87-9267-d16912ca9b0f
02/05/2025 10:22:39:INFO:Received: evaluate message 2cf14508-0b86-4c87-9267-d16912ca9b0f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:22:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:07:INFO:
[92mINFO [0m:      Received: train message bb1d9cd4-0db2-41c3-8a5e-81805425a341
02/05/2025 10:23:07:INFO:Received: train message bb1d9cd4-0db2-41c3-8a5e-81805425a341
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:23:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:02:INFO:
[92mINFO [0m:      Received: evaluate message e8e19c4f-7ca8-42d8-ba79-488c867d750e
02/05/2025 10:25:02:INFO:Received: evaluate message e8e19c4f-7ca8-42d8-ba79-488c867d750e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:25:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:27:INFO:
[92mINFO [0m:      Received: train message e4950b36-c921-4d56-ada2-ac8cee644c4b
02/05/2025 10:25:27:INFO:Received: train message e4950b36-c921-4d56-ada2-ac8cee644c4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:12:INFO:
[92mINFO [0m:      Received: evaluate message 34cd9903-b934-4551-a0cd-5f81fb493a58
02/05/2025 10:27:12:INFO:Received: evaluate message 34cd9903-b934-4551-a0cd-5f81fb493a58
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:08:INFO:
[92mINFO [0m:      Received: train message 817ab3f5-230f-4ed3-874a-bf87172ca4b3
02/05/2025 10:28:08:INFO:Received: train message 817ab3f5-230f-4ed3-874a-bf87172ca4b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:29:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:34:INFO:
[92mINFO [0m:      Received: evaluate message 68873bd7-edf3-47c6-8a79-af949955a02d
02/05/2025 10:29:34:INFO:Received: evaluate message 68873bd7-edf3-47c6-8a79-af949955a02d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:10:INFO:
[92mINFO [0m:      Received: train message f2723e2f-45e8-4673-ad60-b73f0ba6d434
02/05/2025 10:30:10:INFO:Received: train message f2723e2f-45e8-4673-ad60-b73f0ba6d434

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:15:INFO:
[92mINFO [0m:      Received: evaluate message 34ab143d-f373-48f6-b4d6-bbe2d675a8c6
02/05/2025 10:32:15:INFO:Received: evaluate message 34ab143d-f373-48f6-b4d6-bbe2d675a8c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:19:INFO:
[92mINFO [0m:      Received: train message c967fa3e-52b7-4387-9814-bd130523a52a
02/05/2025 10:33:19:INFO:Received: train message c967fa3e-52b7-4387-9814-bd130523a52a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:34:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:45:INFO:
[92mINFO [0m:      Received: evaluate message eddaf73e-76cf-4e2a-a23a-2239c83d2f92
02/05/2025 10:34:45:INFO:Received: evaluate message eddaf73e-76cf-4e2a-a23a-2239c83d2f92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:35:INFO:
[92mINFO [0m:      Received: train message 5ff3bc34-5be4-421b-8eb2-e8f7f54ae1b4
02/05/2025 10:35:35:INFO:Received: train message 5ff3bc34-5be4-421b-8eb2-e8f7f54ae1b4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:36:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:12:INFO:
[92mINFO [0m:      Received: evaluate message b88eac49-4d6d-4530-b7f1-66ba3a58c612
02/05/2025 10:37:12:INFO:Received: evaluate message b88eac49-4d6d-4530-b7f1-66ba3a58c612
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:26:INFO:
[92mINFO [0m:      Received: train message 372ac3d6-300d-4caf-83f9-2087e2175551
02/05/2025 10:38:26:INFO:Received: train message 372ac3d6-300d-4caf-83f9-2087e2175551
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:39:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:03:INFO:
[92mINFO [0m:      Received: evaluate message de584b89-c2cc-48a6-a10c-60e9f49898c8
02/05/2025 10:40:03:INFO:Received: evaluate message de584b89-c2cc-48a6-a10c-60e9f49898c8
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:40:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:59:INFO:
[92mINFO [0m:      Received: train message fcb782ca-8450-4a63-ad08-a10c29853c23
02/05/2025 10:40:59:INFO:Received: train message fcb782ca-8450-4a63-ad08-a10c29853c23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:25:INFO:
[92mINFO [0m:      Received: evaluate message 6d695c16-cd5e-4a32-b332-9565f6956823
02/05/2025 10:42:25:INFO:Received: evaluate message 6d695c16-cd5e-4a32-b332-9565f6956823
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:14:INFO:
[92mINFO [0m:      Received: train message fd450711-edb9-4112-aa81-a907db39c956
02/05/2025 10:43:14:INFO:Received: train message fd450711-edb9-4112-aa81-a907db39c956
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:44:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:12:INFO:
[92mINFO [0m:      Received: evaluate message 7faf2120-bfa0-4f5f-9fdd-7d4e2c209057
02/05/2025 10:45:12:INFO:Received: evaluate message 7faf2120-bfa0-4f5f-9fdd-7d4e2c209057
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:45:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:54:INFO:
[92mINFO [0m:      Received: train message c5e82de3-e33a-4a5a-bc9e-e39c8db26814
02/05/2025 10:45:54:INFO:Received: train message c5e82de3-e33a-4a5a-bc9e-e39c8db26814
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:19:INFO:
[92mINFO [0m:      Received: evaluate message 1a4da55a-c103-4a11-bf98-7838952c1794
02/05/2025 10:47:19:INFO:Received: evaluate message 1a4da55a-c103-4a11-bf98-7838952c1794

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:59:INFO:
[92mINFO [0m:      Received: train message 40cbfb93-1789-43a1-8476-7970e21d58cd
02/05/2025 10:47:59:INFO:Received: train message 40cbfb93-1789-43a1-8476-7970e21d58cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:48:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:24:INFO:
[92mINFO [0m:      Received: evaluate message 14522122-4622-4e88-b9ea-f5227bdee6a4
02/05/2025 10:50:24:INFO:Received: evaluate message 14522122-4622-4e88-b9ea-f5227bdee6a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:06:INFO:
[92mINFO [0m:      Received: train message c048fb51-6bea-4c56-b393-f1184a775027
02/05/2025 10:51:06:INFO:Received: train message c048fb51-6bea-4c56-b393-f1184a775027
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:51:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:08:INFO:
[92mINFO [0m:      Received: evaluate message 8fb4b4a1-f204-4fdd-9c1e-e14eac064a80
02/05/2025 10:53:08:INFO:Received: evaluate message 8fb4b4a1-f204-4fdd-9c1e-e14eac064a80

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:08:INFO:
[92mINFO [0m:      Received: train message 96142f15-da55-4136-a9ca-3acb19275435
02/05/2025 10:54:08:INFO:Received: train message 96142f15-da55-4136-a9ca-3acb19275435
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:28:INFO:
[92mINFO [0m:      Received: evaluate message d333ee13-98d3-482e-a8e6-2ed78207be00
02/05/2025 10:56:28:INFO:Received: evaluate message d333ee13-98d3-482e-a8e6-2ed78207be00
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:56:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:49:INFO:
[92mINFO [0m:      Received: train message b184f920-a25c-464f-a158-5f6d887b7655
02/05/2025 10:56:49:INFO:Received: train message b184f920-a25c-464f-a158-5f6d887b7655
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:57:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:58:INFO:
[92mINFO [0m:      Received: evaluate message 609c15fa-d353-4d8a-9910-380c381949e8
02/05/2025 10:59:58:INFO:Received: evaluate message 609c15fa-d353-4d8a-9910-380c381949e8

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:00:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:55:INFO:
[92mINFO [0m:      Received: train message 0d437630-12f3-445f-bd2c-ba9f8b71fcf3
02/05/2025 11:00:55:INFO:Received: train message 0d437630-12f3-445f-bd2c-ba9f8b71fcf3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:02:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:07:INFO:
[92mINFO [0m:      Received: evaluate message 3921756b-e522-42c2-aced-e5e682a45712
02/05/2025 11:03:07:INFO:Received: evaluate message 3921756b-e522-42c2-aced-e5e682a45712
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:12:INFO:
[92mINFO [0m:      Received: train message ccb3a688-a15c-4386-9f0d-fce5c5c1b49c
02/05/2025 11:04:12:INFO:Received: train message ccb3a688-a15c-4386-9f0d-fce5c5c1b49c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:05:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:39:INFO:
[92mINFO [0m:      Received: evaluate message a0eaff05-5c8d-4542-ac20-6ba3f2a4a0e1
02/05/2025 11:06:39:INFO:Received: evaluate message a0eaff05-5c8d-4542-ac20-6ba3f2a4a0e1

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:06:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:05:INFO:
[92mINFO [0m:      Received: train message dcb5b73a-24bc-42eb-b952-acfd9a0412c0
02/05/2025 11:08:05:INFO:Received: train message dcb5b73a-24bc-42eb-b952-acfd9a0412c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:09:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:50:INFO:
[92mINFO [0m:      Received: evaluate message cc85ff84-e036-4d22-858b-13e4ddf91afa
02/05/2025 11:10:50:INFO:Received: evaluate message cc85ff84-e036-4d22-858b-13e4ddf91afa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:10:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:32:INFO:
[92mINFO [0m:      Received: train message 8431e8c6-c111-4fde-bc30-6a76ef048748
02/05/2025 11:12:32:INFO:Received: train message 8431e8c6-c111-4fde-bc30-6a76ef048748
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:13:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:41:INFO:
[92mINFO [0m:      Received: evaluate message 97a0dad9-a9ae-4e4c-9c7e-f5696af0a854
02/05/2025 11:14:41:INFO:Received: evaluate message 97a0dad9-a9ae-4e4c-9c7e-f5696af0a854

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:14:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:15:44:INFO:
[92mINFO [0m:      Received: train message ad28c2c1-6dcc-499b-ae34-d79ec4c2c57a
02/05/2025 11:15:44:INFO:Received: train message ad28c2c1-6dcc-499b-ae34-d79ec4c2c57a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:16:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:54:INFO:
[92mINFO [0m:      Received: evaluate message 6937d13a-bdda-41d9-b295-d5123004a67c
02/05/2025 11:17:54:INFO:Received: evaluate message 6937d13a-bdda-41d9-b295-d5123004a67c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:19:13:INFO:
[92mINFO [0m:      Received: train message f7a92860-09d6-429b-91a8-1dafed7c1811
02/05/2025 11:19:13:INFO:Received: train message f7a92860-09d6-429b-91a8-1dafed7c1811
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:20:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:15:INFO:
[92mINFO [0m:      Received: evaluate message b09461d5-2553-459b-ad97-9a102f4007c6
02/05/2025 11:21:15:INFO:Received: evaluate message b09461d5-2553-459b-ad97-9a102f4007c6

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:21:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:40:INFO:
[92mINFO [0m:      Received: train message 95099a08-db1c-43d8-9f8a-b4d938a30bbd
02/05/2025 11:21:40:INFO:Received: train message 95099a08-db1c-43d8-9f8a-b4d938a30bbd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:36:INFO:
[92mINFO [0m:      Received: evaluate message 7750bb6b-3fb5-43e7-82b7-5ed4473d0292
02/05/2025 11:23:36:INFO:Received: evaluate message 7750bb6b-3fb5-43e7-82b7-5ed4473d0292
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:39:INFO:
[92mINFO [0m:      Received: train message 7bc1cf0c-4dbb-4142-a5a8-cad83575c93b
02/05/2025 11:24:39:INFO:Received: train message 7bc1cf0c-4dbb-4142-a5a8-cad83575c93b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:25:INFO:
[92mINFO [0m:      Received: evaluate message bd9a2199-9ed1-42e1-9020-27b251fa76cd
02/05/2025 11:26:25:INFO:Received: evaluate message bd9a2199-9ed1-42e1-9020-27b251fa76cd

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:33:INFO:
[92mINFO [0m:      Received: reconnect message 6ec5588e-6564-4d6b-bae9-075850abdf47
02/05/2025 11:26:33:INFO:Received: reconnect message 6ec5588e-6564-4d6b-bae9-075850abdf47
02/05/2025 11:26:33:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:26:33:INFO:Disconnect and shut down

{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576, 1.0347204484261043], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504, 0.7977737627253754], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071, 0.595632848171531], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902, 0.5462812197574198]}



Final client history:
{'loss': [1.0969858964594945, 1.050336043241902, 1.144624180771393, 1.1154123269272, 1.1477992487102864, 1.0758866047933755, 1.1110736242116104, 1.1091698019927698, 1.1305510425493064, 1.0906751916872701, 1.1679614120577348, 1.095669831728171, 1.1095380756983635, 1.1079163421309488, 1.0868579101711628, 1.0730584302015655, 1.1115415927206194, 1.0599474385438998, 1.0705202612399682, 1.0865612880059572, 1.0486578852800394, 1.084744617471106, 1.081270793250429, 1.065619055930295, 1.0714870652153308, 1.0619428360527432, 1.0579087673173089, 1.0533572934958224, 1.0767547506387576, 1.0347204484261043], 'accuracy': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'auc': [0.7085531489885228, 0.7350934873405456, 0.7478333950211565, 0.7537665008048988, 0.7582617425510005, 0.7630563765811681, 0.7653647733525079, 0.769640021767352, 0.771636952398261, 0.7737791645124287, 0.7746449064619101, 0.7778917663971608, 0.779487392529315, 0.7809571736572559, 0.7833506920759905, 0.7836475786939228, 0.784733637348576, 0.7865631120130656, 0.786733021326078, 0.7886882324938745, 0.7895122290660017, 0.7907754487119264, 0.791184858075052, 0.7921011923944533, 0.793341896088612, 0.7938998077031043, 0.7954928049528104, 0.7962339415100597, 0.7979266183661504, 0.7977737627253754], 'precision': [0.40347364107321304, 0.43385134376460194, 0.4299049100810858, 0.44491300462264466, 0.4321060542720169, 0.4597224466799581, 0.4555593949890854, 0.45545522186133514, 0.5849028522427754, 0.6054576088273188, 0.5823908757175856, 0.5268938135612521, 0.603944893338426, 0.6122123632082181, 0.5789230886516712, 0.5887539692038942, 0.5704001114709152, 0.6010965675547959, 0.594529087976968, 0.5884726952447187, 0.6046947213234481, 0.5898596373655013, 0.5769990322033756, 0.6098400243500441, 0.5752796672118611, 0.6009265224271907, 0.6106305970963777, 0.6087850809995173, 0.569888176506071, 0.595632848171531], 'recall': [0.5113369820172009, 0.5293197810789679, 0.5316653635652854, 0.5410476935105551, 0.5332290852228303, 0.5488663017982799, 0.5535574667709148, 0.5527756059421423, 0.5449569976544175, 0.5613760750586395, 0.544175136825645, 0.5551211884284597, 0.5613760750586395, 0.5676309616888194, 0.565285379202502, 0.5707584050039093, 0.565285379202502, 0.5754495699765442, 0.5738858483189992, 0.5777951524628616, 0.5777951524628616, 0.5770132916340891, 0.5715402658326818, 0.5863956215793589, 0.5793588741204065, 0.5887412040656763, 0.5863956215793589, 0.5879593432369038, 0.5723221266614542, 0.5910867865519938], 'f1': [0.39881455974124325, 0.4722552092863052, 0.4585647116310328, 0.4834263803103398, 0.4644119051622756, 0.4995717749608185, 0.49419450888518424, 0.4941387243816717, 0.48062354070835656, 0.507210474803362, 0.4724222245435365, 0.4948825637171529, 0.5050243699037229, 0.5143435204476023, 0.5107948150100066, 0.5183995755090968, 0.506653187949343, 0.527311483653469, 0.5256401766926598, 0.525322387520407, 0.5336993408464595, 0.5225891367857811, 0.5176429494148099, 0.5374942228916959, 0.5278068320662135, 0.540162707699297, 0.5368244863781604, 0.5377124635650309, 0.5166371496313902, 0.5462812197574198]}

F0000 00:00:1738783660.318465 3502365 timer_manager.cc:69] Check failed: check_result.has_value() ERROR: More than one MainLoop is running.
*** Check failure stack trace: ***
