nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/11/2025 11:31:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/11/2025 11:31:31:DEBUG:ChannelConnectivity.IDLE
02/11/2025 11:31:31:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/11/2025 11:39:15:INFO:
[92mINFO [0m:      Received: train message 5dd229e7-9ba9-4e1c-8999-de9070d51bf6
02/11/2025 11:39:15:INFO:Received: train message 5dd229e7-9ba9-4e1c-8999-de9070d51bf6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 11:42:35:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:09:21:INFO:
[92mINFO [0m:      Received: evaluate message fb727a47-4f7b-48d3-9e0f-01e7eefd76a6
02/11/2025 12:09:21:INFO:Received: evaluate message fb727a47-4f7b-48d3-9e0f-01e7eefd76a6
[92mINFO [0m:      Sent reply
02/11/2025 12:13:16:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:14:13:INFO:
[92mINFO [0m:      Received: train message c66f1f77-949d-43ef-9072-2172fd7e38d3
02/11/2025 12:14:13:INFO:Received: train message c66f1f77-949d-43ef-9072-2172fd7e38d3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:17:20:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:43:59:INFO:
[92mINFO [0m:      Received: evaluate message bd068304-32f6-404e-937d-2f97c4708665
02/11/2025 12:43:59:INFO:Received: evaluate message bd068304-32f6-404e-937d-2f97c4708665
[92mINFO [0m:      Sent reply
02/11/2025 12:48:16:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:48:32:INFO:
[92mINFO [0m:      Received: train message 879528ce-59eb-4212-b351-b95456051b89
02/11/2025 12:48:32:INFO:Received: train message 879528ce-59eb-4212-b351-b95456051b89
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:51:40:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:22:18:INFO:
[92mINFO [0m:      Received: evaluate message 469d5a49-16a4-424c-98dd-17f2afc670d7
02/11/2025 13:22:18:INFO:Received: evaluate message 469d5a49-16a4-424c-98dd-17f2afc670d7
[92mINFO [0m:      Sent reply
02/11/2025 13:26:31:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:26:57:INFO:
[92mINFO [0m:      Received: train message aa751086-6715-446b-aad6-898b48071199
02/11/2025 13:26:57:INFO:Received: train message aa751086-6715-446b-aad6-898b48071199
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:29:55:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:56:56:INFO:
[92mINFO [0m:      Received: evaluate message 201fc209-9b6f-45fa-b324-dcd8775aa934
02/11/2025 13:56:56:INFO:Received: evaluate message 201fc209-9b6f-45fa-b324-dcd8775aa934
[92mINFO [0m:      Sent reply
02/11/2025 14:01:00:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:01:27:INFO:
[92mINFO [0m:      Received: train message d6eb6ae1-f87d-4e28-9273-dfbea7dae93b
02/11/2025 14:01:27:INFO:Received: train message d6eb6ae1-f87d-4e28-9273-dfbea7dae93b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:04:44:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:31:43:INFO:
[92mINFO [0m:      Received: evaluate message 96e926ee-7ff2-4897-9a2f-3c3b2dfa1a38
02/11/2025 14:31:43:INFO:Received: evaluate message 96e926ee-7ff2-4897-9a2f-3c3b2dfa1a38
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 14:35:57:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:36:26:INFO:
[92mINFO [0m:      Received: train message 2d541d49-95df-4098-9a5a-d2c775922214
02/11/2025 14:36:26:INFO:Received: train message 2d541d49-95df-4098-9a5a-d2c775922214
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:39:38:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:06:29:INFO:
[92mINFO [0m:      Received: evaluate message 585d0f64-8456-41e3-9152-7b6ef3e2bf11
02/11/2025 15:06:29:INFO:Received: evaluate message 585d0f64-8456-41e3-9152-7b6ef3e2bf11
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:10:35:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:11:12:INFO:
[92mINFO [0m:      Received: train message c0bfd2b7-4b13-48c2-8254-33f3e98b5b6c
02/11/2025 15:11:12:INFO:Received: train message c0bfd2b7-4b13-48c2-8254-33f3e98b5b6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:14:17:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:41:02:INFO:
[92mINFO [0m:      Received: evaluate message b188b728-5a16-43ad-8130-b2a8460a5173
02/11/2025 15:41:02:INFO:Received: evaluate message b188b728-5a16-43ad-8130-b2a8460a5173
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192], 'accuracy': [0.25654450261780104], 'auc': [0.5097526219963764], 'precision': [0.25488184534737934], 'recall': [0.25654450261780104], 'f1': [0.19218762720861446]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877], 'accuracy': [0.25654450261780104, 0.29883205799436163], 'auc': [0.5097526219963764, 0.5400268636526322], 'precision': [0.25488184534737934, 0.31012842353432646], 'recall': [0.25654450261780104, 0.29883205799436163], 'f1': [0.19218762720861446, 0.20880157085225062]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:44:57:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:45:25:INFO:
[92mINFO [0m:      Received: train message c9a3b8aa-0724-4e54-a96b-287f2b590b00
02/11/2025 15:45:25:INFO:Received: train message c9a3b8aa-0724-4e54-a96b-287f2b590b00
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:47:53:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:15:29:INFO:
[92mINFO [0m:      Received: evaluate message 4c0a7de1-3d92-44df-a824-cfc82189cecb
02/11/2025 16:15:29:INFO:Received: evaluate message 4c0a7de1-3d92-44df-a824-cfc82189cecb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:19:21:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:20:30:INFO:
[92mINFO [0m:      Received: train message 8c4eafd1-ee47-40de-bb9d-0f2ec69e0de7
02/11/2025 16:20:30:INFO:Received: train message 8c4eafd1-ee47-40de-bb9d-0f2ec69e0de7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 16:23:47:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:53:29:INFO:
[92mINFO [0m:      Received: evaluate message 94844793-2db9-47c7-9d2b-fadbe306982c
02/11/2025 16:53:29:INFO:Received: evaluate message 94844793-2db9-47c7-9d2b-fadbe306982c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:57:29:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:57:48:INFO:
[92mINFO [0m:      Received: train message 7d82371a-de4a-41c6-aedb-9d3580b5a81c
02/11/2025 16:57:48:INFO:Received: train message 7d82371a-de4a-41c6-aedb-9d3580b5a81c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:00:46:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:26:32:INFO:
[92mINFO [0m:      Received: evaluate message 7343ea4c-ea44-458d-bdfe-115168447cea
02/11/2025 17:26:32:INFO:Received: evaluate message 7343ea4c-ea44-458d-bdfe-115168447cea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 17:30:39:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:31:02:INFO:
[92mINFO [0m:      Received: train message 2eb480b3-5e26-4261-948d-dd0805a7eb47
02/11/2025 17:31:02:INFO:Received: train message 2eb480b3-5e26-4261-948d-dd0805a7eb47
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:33:30:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:01:03:INFO:
[92mINFO [0m:      Received: evaluate message 4ee243cb-f442-4def-ae1d-46034c3e111c
02/11/2025 18:01:03:INFO:Received: evaluate message 4ee243cb-f442-4def-ae1d-46034c3e111c

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:05:00:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:05:15:INFO:
[92mINFO [0m:      Received: train message 4d984f67-cce4-480b-91d7-1465c65e88b1
02/11/2025 18:05:15:INFO:Received: train message 4d984f67-cce4-480b-91d7-1465c65e88b1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:08:12:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:34:07:INFO:
[92mINFO [0m:      Received: evaluate message fb99153a-23c0-40ea-9204-20fc887e5228
02/11/2025 18:34:07:INFO:Received: evaluate message fb99153a-23c0-40ea-9204-20fc887e5228
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:37:56:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:38:43:INFO:
[92mINFO [0m:      Received: train message 36743b13-be72-41f6-966a-84252d56c788
02/11/2025 18:38:43:INFO:Received: train message 36743b13-be72-41f6-966a-84252d56c788
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:41:57:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:10:09:INFO:
[92mINFO [0m:      Received: evaluate message 0ce6cd80-6a2b-46a2-a66c-48cb110673f4
02/11/2025 19:10:09:INFO:Received: evaluate message 0ce6cd80-6a2b-46a2-a66c-48cb110673f4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:13:50:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:14:54:INFO:
[92mINFO [0m:      Received: train message 57680c99-cb28-43ce-ad67-d68136246535
02/11/2025 19:14:54:INFO:Received: train message 57680c99-cb28-43ce-ad67-d68136246535
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:18:05:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:44:10:INFO:
[92mINFO [0m:      Received: evaluate message 52bad68d-0f46-40c9-8cad-823c4e3e9b59
02/11/2025 19:44:10:INFO:Received: evaluate message 52bad68d-0f46-40c9-8cad-823c4e3e9b59

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:48:08:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:48:45:INFO:
[92mINFO [0m:      Received: train message e6a92fd1-3d87-45b8-b52a-04581da7057f
02/11/2025 19:48:45:INFO:Received: train message e6a92fd1-3d87-45b8-b52a-04581da7057f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:52:01:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:19:53:INFO:
[92mINFO [0m:      Received: evaluate message 8e5682af-5c6f-46e1-b02c-df8c54783a6c
02/11/2025 20:19:53:INFO:Received: evaluate message 8e5682af-5c6f-46e1-b02c-df8c54783a6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:23:45:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:24:16:INFO:
[92mINFO [0m:      Received: train message 61cfe5d3-2b69-4276-b09e-3484d0598ab4
02/11/2025 20:24:16:INFO:Received: train message 61cfe5d3-2b69-4276-b09e-3484d0598ab4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 20:27:25:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:53:03:INFO:
[92mINFO [0m:      Received: evaluate message 80f84952-8d71-4a10-99df-e7e35af8c4c0
02/11/2025 20:53:03:INFO:Received: evaluate message 80f84952-8d71-4a10-99df-e7e35af8c4c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:56:41:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:57:50:INFO:
[92mINFO [0m:      Received: train message 90a9e694-c87b-44b8-9639-e1900ad5d0f5
02/11/2025 20:57:50:INFO:Received: train message 90a9e694-c87b-44b8-9639-e1900ad5d0f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:00:50:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:29:14:INFO:
[92mINFO [0m:      Received: evaluate message 6c67fbfc-a2e5-48be-b37e-36678a24ef79
02/11/2025 21:29:14:INFO:Received: evaluate message 6c67fbfc-a2e5-48be-b37e-36678a24ef79
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 21:33:22:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:33:49:INFO:
[92mINFO [0m:      Received: train message e5763873-110e-46c3-994b-aaf020c8bce1
02/11/2025 21:33:49:INFO:Received: train message e5763873-110e-46c3-994b-aaf020c8bce1
Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:36:49:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:03:01:INFO:
[92mINFO [0m:      Received: evaluate message 7266ecae-7a4b-4abf-8d9d-0cd30325365f
02/11/2025 22:03:01:INFO:Received: evaluate message 7266ecae-7a4b-4abf-8d9d-0cd30325365f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:07:05:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:07:43:INFO:
[92mINFO [0m:      Received: train message f6ef22aa-5c75-4456-826b-65c8e90ccc55
02/11/2025 22:07:43:INFO:Received: train message f6ef22aa-5c75-4456-826b-65c8e90ccc55
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:10:49:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:39:01:INFO:
[92mINFO [0m:      Received: evaluate message bfa76eb1-eeb4-4cb7-beb7-ac83ea87ae27
02/11/2025 22:39:01:INFO:Received: evaluate message bfa76eb1-eeb4-4cb7-beb7-ac83ea87ae27
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:43:00:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:43:30:INFO:
[92mINFO [0m:      Received: train message 694f6a7c-1bd7-44a7-ae21-4ad45ab386e8
02/11/2025 22:43:30:INFO:Received: train message 694f6a7c-1bd7-44a7-ae21-4ad45ab386e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:46:38:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:12:22:INFO:
[92mINFO [0m:      Received: evaluate message e6c899e9-9acd-4458-9896-cae7feb96330
02/11/2025 23:12:22:INFO:Received: evaluate message e6c899e9-9acd-4458-9896-cae7feb96330
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:16:24:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:16:58:INFO:
[92mINFO [0m:      Received: train message e96abae7-86aa-433f-8e4a-59d9d1ce206c
02/11/2025 23:16:58:INFO:Received: train message e96abae7-86aa-433f-8e4a-59d9d1ce206c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:20:00:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:46:55:INFO:
[92mINFO [0m:      Received: evaluate message 82eebaa8-32d3-4695-80fe-5c5f92e3b495
02/11/2025 23:46:55:INFO:Received: evaluate message 82eebaa8-32d3-4695-80fe-5c5f92e3b495
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:51:12:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:51:44:INFO:
[92mINFO [0m:      Received: train message dd0d7b58-09f3-445b-b0c9-61ae042fa1b6
02/11/2025 23:51:44:INFO:Received: train message dd0d7b58-09f3-445b-b0c9-61ae042fa1b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:54:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:22:08:INFO:
[92mINFO [0m:      Received: evaluate message 75c12bf7-2f77-4e84-ae3b-67d0e3298834
02/12/2025 00:22:08:INFO:Received: evaluate message 75c12bf7-2f77-4e84-ae3b-67d0e3298834

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 00:26:04:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:26:20:INFO:
[92mINFO [0m:      Received: train message 84727d49-c2d7-4a38-9a54-43b9e1d089e2
02/12/2025 00:26:20:INFO:Received: train message 84727d49-c2d7-4a38-9a54-43b9e1d089e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:28:51:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:56:40:INFO:
[92mINFO [0m:      Received: evaluate message b96b03ed-e698-4d15-bb30-aa31e04a0536
02/12/2025 00:56:40:INFO:Received: evaluate message b96b03ed-e698-4d15-bb30-aa31e04a0536
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:00:40:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:01:11:INFO:
[92mINFO [0m:      Received: train message 933d228f-040e-4072-9928-420002c67c67
02/12/2025 01:01:11:INFO:Received: train message 933d228f-040e-4072-9928-420002c67c67
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:04:05:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:30:25:INFO:
[92mINFO [0m:      Received: evaluate message 61a00121-ad14-4c95-90f6-8813bbfce819
02/12/2025 01:30:25:INFO:Received: evaluate message 61a00121-ad14-4c95-90f6-8813bbfce819

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:34:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:35:03:INFO:
[92mINFO [0m:      Received: train message 3a627c35-81ad-42f6-a0f1-8faf97e1fe6a
02/12/2025 01:35:03:INFO:Received: train message 3a627c35-81ad-42f6-a0f1-8faf97e1fe6a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:38:01:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:03:48:INFO:
[92mINFO [0m:      Received: evaluate message ae7cdea0-5460-44aa-9745-548ddcb5b95e
02/12/2025 02:03:48:INFO:Received: evaluate message ae7cdea0-5460-44aa-9745-548ddcb5b95e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:07:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:08:38:INFO:
[92mINFO [0m:      Received: train message ac309977-5688-4302-b71d-840917e0b9c9
02/12/2025 02:08:38:INFO:Received: train message ac309977-5688-4302-b71d-840917e0b9c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:11:37:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:39:41:INFO:
[92mINFO [0m:      Received: evaluate message 0c148e96-6e87-4d77-b3d2-395890affbf2
02/12/2025 02:39:41:INFO:Received: evaluate message 0c148e96-6e87-4d77-b3d2-395890affbf2

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:43:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:44:06:INFO:
[92mINFO [0m:      Received: train message fe6d36e3-dc18-49bc-adf9-2e90637d5a2b
02/12/2025 02:44:06:INFO:Received: train message fe6d36e3-dc18-49bc-adf9-2e90637d5a2b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:47:01:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:13:28:INFO:
[92mINFO [0m:      Received: evaluate message 03d278c1-6953-41e5-814b-1cdd2c899d08
02/12/2025 03:13:28:INFO:Received: evaluate message 03d278c1-6953-41e5-814b-1cdd2c899d08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:17:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:17:57:INFO:
[92mINFO [0m:      Received: train message e1d3b74d-a5c7-452b-850f-821e75aea6b9
02/12/2025 03:17:57:INFO:Received: train message e1d3b74d-a5c7-452b-850f-821e75aea6b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:20:47:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:47:56:INFO:
[92mINFO [0m:      Received: evaluate message 314514d7-1588-4275-b747-e5e5048ba596
02/12/2025 03:47:56:INFO:Received: evaluate message 314514d7-1588-4275-b747-e5e5048ba596

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:52:12:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:52:40:INFO:
[92mINFO [0m:      Received: train message 2ddb0207-6930-4729-8af2-333a03d4c7a7
02/12/2025 03:52:40:INFO:Received: train message 2ddb0207-6930-4729-8af2-333a03d4c7a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:55:49:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:25:15:INFO:
[92mINFO [0m:      Received: evaluate message 408aa1dc-0a42-4542-a27e-fe5c37066484
02/12/2025 04:25:15:INFO:Received: evaluate message 408aa1dc-0a42-4542-a27e-fe5c37066484
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 04:29:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:29:53:INFO:
[92mINFO [0m:      Received: train message 3610b868-7fd6-413b-aaaf-22c0b40d0204
02/12/2025 04:29:53:INFO:Received: train message 3610b868-7fd6-413b-aaaf-22c0b40d0204

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 04:33:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:01:45:INFO:
[92mINFO [0m:      Received: evaluate message 0279f9d7-30e2-4c35-86b7-b4bc90a7e97b
02/12/2025 05:01:45:INFO:Received: evaluate message 0279f9d7-30e2-4c35-86b7-b4bc90a7e97b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 05:05:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:05:54:INFO:
[92mINFO [0m:      Received: reconnect message d59f595a-9725-459f-9935-541d96a6eb0f
02/12/2025 05:05:54:INFO:Received: reconnect message d59f595a-9725-459f-9935-541d96a6eb0f
02/12/2025 05:05:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/12/2025 05:05:54:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}



Final client history:
{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}

