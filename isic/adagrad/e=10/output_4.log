nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/11/2025 11:33:50:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/11/2025 11:33:50:DEBUG:ChannelConnectivity.IDLE
02/11/2025 11:33:50:DEBUG:ChannelConnectivity.CONNECTING
02/11/2025 11:33:50:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739302430.547455 1488276 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/11/2025 11:39:15:INFO:
[92mINFO [0m:      Received: train message b95b2cee-bbf6-468e-a779-b93bbd301581
02/11/2025 11:39:15:INFO:Received: train message b95b2cee-bbf6-468e-a779-b93bbd301581
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 11:51:08:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:09:32:INFO:
[92mINFO [0m:      Received: evaluate message 5372639b-da7c-4c76-93a8-98791cfe0800
02/11/2025 12:09:32:INFO:Received: evaluate message 5372639b-da7c-4c76-93a8-98791cfe0800
[92mINFO [0m:      Sent reply
02/11/2025 12:13:33:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:14:18:INFO:
[92mINFO [0m:      Received: train message fa3c943c-b8e6-4d07-b6de-8df270dd202c
02/11/2025 12:14:18:INFO:Received: train message fa3c943c-b8e6-4d07-b6de-8df270dd202c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:26:01:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:44:07:INFO:
[92mINFO [0m:      Received: evaluate message 069c1a58-84d5-4d23-ae37-8335c1b517a3
02/11/2025 12:44:07:INFO:Received: evaluate message 069c1a58-84d5-4d23-ae37-8335c1b517a3
[92mINFO [0m:      Sent reply
02/11/2025 12:48:15:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:48:49:INFO:
[92mINFO [0m:      Received: train message df961b4c-061c-467b-bcc9-5920be6cb7bd
02/11/2025 12:48:49:INFO:Received: train message df961b4c-061c-467b-bcc9-5920be6cb7bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:00:26:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:22:23:INFO:
[92mINFO [0m:      Received: evaluate message c9de47bd-2c03-4878-aa7b-068ac451b703
02/11/2025 13:22:23:INFO:Received: evaluate message c9de47bd-2c03-4878-aa7b-068ac451b703
[92mINFO [0m:      Sent reply
02/11/2025 13:26:27:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:26:53:INFO:
[92mINFO [0m:      Received: train message 2be5d84d-2e74-4249-8c61-c8a9ef30e650
02/11/2025 13:26:53:INFO:Received: train message 2be5d84d-2e74-4249-8c61-c8a9ef30e650
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:38:10:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:56:41:INFO:
[92mINFO [0m:      Received: evaluate message 973494c4-875e-47f5-9b1c-f298ff0e806f
02/11/2025 13:56:41:INFO:Received: evaluate message 973494c4-875e-47f5-9b1c-f298ff0e806f
[92mINFO [0m:      Sent reply
02/11/2025 14:00:19:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:01:30:INFO:
[92mINFO [0m:      Received: train message b1fe8ceb-f0a9-4919-bf3f-0953a15717dd
02/11/2025 14:01:30:INFO:Received: train message b1fe8ceb-f0a9-4919-bf3f-0953a15717dd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:12:43:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:31:48:INFO:
[92mINFO [0m:      Received: evaluate message 9084097e-8be5-48d1-b632-06eb6a0dfda9
02/11/2025 14:31:48:INFO:Received: evaluate message 9084097e-8be5-48d1-b632-06eb6a0dfda9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 14:35:58:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:36:30:INFO:
[92mINFO [0m:      Received: train message f8717405-4d5c-4a89-86ad-ae1da4698062
02/11/2025 14:36:30:INFO:Received: train message f8717405-4d5c-4a89-86ad-ae1da4698062
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:47:52:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:06:43:INFO:
[92mINFO [0m:      Received: evaluate message 468a6d3b-b4e7-4775-aa0a-6b5dab66a026
02/11/2025 15:06:43:INFO:Received: evaluate message 468a6d3b-b4e7-4775-aa0a-6b5dab66a026
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:10:36:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:11:06:INFO:
[92mINFO [0m:      Received: train message d2b478bc-78b4-466d-a508-c9d68e5f46af
02/11/2025 15:11:06:INFO:Received: train message d2b478bc-78b4-466d-a508-c9d68e5f46af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:22:18:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:41:07:INFO:
[92mINFO [0m:      Received: evaluate message 80ba15fc-6b6c-4b35-a8d8-988e710fd6a6
02/11/2025 15:41:07:INFO:Received: evaluate message 80ba15fc-6b6c-4b35-a8d8-988e710fd6a6
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192], 'accuracy': [0.25654450261780104], 'auc': [0.5097526219963764], 'precision': [0.25488184534737934], 'recall': [0.25654450261780104], 'f1': [0.19218762720861446]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877], 'accuracy': [0.25654450261780104, 0.29883205799436163], 'auc': [0.5097526219963764, 0.5400268636526322], 'precision': [0.25488184534737934, 0.31012842353432646], 'recall': [0.25654450261780104, 0.29883205799436163], 'f1': [0.19218762720861446, 0.20880157085225062]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:44:50:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:45:42:INFO:
[92mINFO [0m:      Received: train message abc155c3-7687-4edf-9a5c-a478e12940c4
02/11/2025 15:45:42:INFO:Received: train message abc155c3-7687-4edf-9a5c-a478e12940c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:57:21:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:15:39:INFO:
[92mINFO [0m:      Received: evaluate message f2aeb838-7d66-45b9-9497-bbd28edd6f6d
02/11/2025 16:15:39:INFO:Received: evaluate message f2aeb838-7d66-45b9-9497-bbd28edd6f6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:19:31:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:20:11:INFO:
[92mINFO [0m:      Received: train message 69c81563-ae98-4c84-a52a-7ab1079ca38e
02/11/2025 16:20:11:INFO:Received: train message 69c81563-ae98-4c84-a52a-7ab1079ca38e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 16:32:27:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:53:25:INFO:
[92mINFO [0m:      Received: evaluate message 2ba1a2be-ae83-4c25-b519-bca85ac628d6
02/11/2025 16:53:25:INFO:Received: evaluate message 2ba1a2be-ae83-4c25-b519-bca85ac628d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:57:10:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:57:54:INFO:
[92mINFO [0m:      Received: train message f746d4b0-acd4-4e55-b378-1afea1d9774c
02/11/2025 16:57:54:INFO:Received: train message f746d4b0-acd4-4e55-b378-1afea1d9774c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:09:17:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:26:32:INFO:
[92mINFO [0m:      Received: evaluate message 9bae86f9-d0c0-4539-89a6-b253a08e2b06
02/11/2025 17:26:32:INFO:Received: evaluate message 9bae86f9-d0c0-4539-89a6-b253a08e2b06
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 17:30:13:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:31:19:INFO:
[92mINFO [0m:      Received: train message 9745d2df-88b9-4efd-b1c6-a363e0801680
02/11/2025 17:31:19:INFO:Received: train message 9745d2df-88b9-4efd-b1c6-a363e0801680
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:42:49:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:00:45:INFO:
[92mINFO [0m:      Received: evaluate message 1142448f-512d-4c78-aa9e-04c0546d6238
02/11/2025 18:00:45:INFO:Received: evaluate message 1142448f-512d-4c78-aa9e-04c0546d6238

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:04:01:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:05:29:INFO:
[92mINFO [0m:      Received: train message 601ff539-ff05-48a1-a653-5f696c24c2ea
02/11/2025 18:05:29:INFO:Received: train message 601ff539-ff05-48a1-a653-5f696c24c2ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:16:41:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:34:16:INFO:
[92mINFO [0m:      Received: evaluate message c0376360-6bfa-4976-8f83-3b96644bb70c
02/11/2025 18:34:16:INFO:Received: evaluate message c0376360-6bfa-4976-8f83-3b96644bb70c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:38:07:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:38:38:INFO:
[92mINFO [0m:      Received: train message d5cd54bd-f7ca-4607-b664-bda3c8c34258
02/11/2025 18:38:38:INFO:Received: train message d5cd54bd-f7ca-4607-b664-bda3c8c34258
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:50:38:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:10:09:INFO:
[92mINFO [0m:      Received: evaluate message d09d4ecd-3098-42ea-a474-fe8109bc11d5
02/11/2025 19:10:09:INFO:Received: evaluate message d09d4ecd-3098-42ea-a474-fe8109bc11d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:14:09:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:14:48:INFO:
[92mINFO [0m:      Received: train message 1105201e-cfee-477b-9fef-b4474db1d169
02/11/2025 19:14:48:INFO:Received: train message 1105201e-cfee-477b-9fef-b4474db1d169
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:26:38:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:44:10:INFO:
[92mINFO [0m:      Received: evaluate message 47ee1346-bd9a-4ad4-ac1c-c14b3f4d7702
02/11/2025 19:44:10:INFO:Received: evaluate message 47ee1346-bd9a-4ad4-ac1c-c14b3f4d7702

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:48:11:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:48:30:INFO:
[92mINFO [0m:      Received: train message 035f6a34-6f6b-4385-ac0f-2ed5b5f9e8de
02/11/2025 19:48:30:INFO:Received: train message 035f6a34-6f6b-4385-ac0f-2ed5b5f9e8de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:59:45:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:19:34:INFO:
[92mINFO [0m:      Received: evaluate message fe159a05-c4cb-4c35-9375-fc96cb45ab75
02/11/2025 20:19:34:INFO:Received: evaluate message fe159a05-c4cb-4c35-9375-fc96cb45ab75
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:22:36:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:24:03:INFO:
[92mINFO [0m:      Received: train message 191854c9-1f13-46f2-a7a2-61e627d73e7f
02/11/2025 20:24:03:INFO:Received: train message 191854c9-1f13-46f2-a7a2-61e627d73e7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 20:35:16:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:53:18:INFO:
[92mINFO [0m:      Received: evaluate message ca8beb18-dc8e-41bd-9af1-41b3c7c367b5
02/11/2025 20:53:18:INFO:Received: evaluate message ca8beb18-dc8e-41bd-9af1-41b3c7c367b5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:57:17:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:57:35:INFO:
[92mINFO [0m:      Received: train message cfb68d6b-4988-432e-a8a1-4febf3f4a26c
02/11/2025 20:57:35:INFO:Received: train message cfb68d6b-4988-432e-a8a1-4febf3f4a26c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:09:01:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:29:17:INFO:
[92mINFO [0m:      Received: evaluate message 300f0cf4-6237-44e0-a7d9-dcf633f5460b
02/11/2025 21:29:17:INFO:Received: evaluate message 300f0cf4-6237-44e0-a7d9-dcf633f5460b

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 21:33:20:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:34:05:INFO:
[92mINFO [0m:      Received: train message 12728ec9-4d11-4b9c-8f8a-295f34017644
02/11/2025 21:34:05:INFO:Received: train message 12728ec9-4d11-4b9c-8f8a-295f34017644
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:46:30:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:03:13:INFO:
[92mINFO [0m:      Received: evaluate message 78af7215-a1e6-4463-aab9-9a91b7cecae0
02/11/2025 22:03:13:INFO:Received: evaluate message 78af7215-a1e6-4463-aab9-9a91b7cecae0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:07:02:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:07:43:INFO:
[92mINFO [0m:      Received: train message 337f5629-6486-443f-a206-c0c8e7f4cf01
02/11/2025 22:07:43:INFO:Received: train message 337f5629-6486-443f-a206-c0c8e7f4cf01
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:19:37:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:38:46:INFO:
[92mINFO [0m:      Received: evaluate message f7dfb9ba-ab13-4bdc-a7e7-4a4a2b131758
02/11/2025 22:38:46:INFO:Received: evaluate message f7dfb9ba-ab13-4bdc-a7e7-4a4a2b131758
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:42:07:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:43:26:INFO:
[92mINFO [0m:      Received: train message 594b659a-60c4-4ae8-be7a-cdadd19abaf2
02/11/2025 22:43:26:INFO:Received: train message 594b659a-60c4-4ae8-be7a-cdadd19abaf2

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:54:50:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:12:33:INFO:
[92mINFO [0m:      Received: evaluate message aec5217b-146c-441c-b3c3-e3f73695e64a
02/11/2025 23:12:33:INFO:Received: evaluate message aec5217b-146c-441c-b3c3-e3f73695e64a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:16:14:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:17:02:INFO:
[92mINFO [0m:      Received: train message f5fdce98-a96c-4a71-a13b-c46535382d9b
02/11/2025 23:17:02:INFO:Received: train message f5fdce98-a96c-4a71-a13b-c46535382d9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:28:12:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:47:02:INFO:
[92mINFO [0m:      Received: evaluate message f97c03c7-3040-44d1-971b-1406764d7b14
02/11/2025 23:47:02:INFO:Received: evaluate message f97c03c7-3040-44d1-971b-1406764d7b14
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:50:31:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:51:39:INFO:
[92mINFO [0m:      Received: train message b7e5b386-c29c-4646-9254-cfb3dd464df8
02/11/2025 23:51:39:INFO:Received: train message b7e5b386-c29c-4646-9254-cfb3dd464df8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:02:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:21:54:INFO:
[92mINFO [0m:      Received: evaluate message 37daffed-e0af-4b81-9553-f5ed3abb6b43
02/12/2025 00:21:54:INFO:Received: evaluate message 37daffed-e0af-4b81-9553-f5ed3abb6b43
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 00:25:16:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:26:38:INFO:
[92mINFO [0m:      Received: train message a8f435b1-9fcb-43f7-a782-d8ac295e5183
02/12/2025 00:26:38:INFO:Received: train message a8f435b1-9fcb-43f7-a782-d8ac295e5183
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:37:51:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:56:33:INFO:
[92mINFO [0m:      Received: evaluate message 5a395449-2bcc-4cd7-b861-53036eba4c38
02/12/2025 00:56:33:INFO:Received: evaluate message 5a395449-2bcc-4cd7-b861-53036eba4c38
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 00:59:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:01:17:INFO:
[92mINFO [0m:      Received: train message 8943a52d-d9fd-41bf-823a-7d043360b989
02/12/2025 01:01:17:INFO:Received: train message 8943a52d-d9fd-41bf-823a-7d043360b989
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:13:12:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:30:33:INFO:
[92mINFO [0m:      Received: evaluate message c9acd983-da4e-46d9-a0d8-62d1bb44d22b
02/12/2025 01:30:33:INFO:Received: evaluate message c9acd983-da4e-46d9-a0d8-62d1bb44d22b

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:34:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:35:08:INFO:
[92mINFO [0m:      Received: train message a2909509-cd29-46b0-b7c5-8ef58921ec96
02/12/2025 01:35:08:INFO:Received: train message a2909509-cd29-46b0-b7c5-8ef58921ec96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:46:11:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:03:44:INFO:
[92mINFO [0m:      Received: evaluate message 38dadfd8-de56-43f1-b8de-7d6b4be464f6
02/12/2025 02:03:44:INFO:Received: evaluate message 38dadfd8-de56-43f1-b8de-7d6b4be464f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:06:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:08:30:INFO:
[92mINFO [0m:      Received: train message 5eb2cd7a-e268-4c81-beb8-ed44a49f31e6
02/12/2025 02:08:30:INFO:Received: train message 5eb2cd7a-e268-4c81-beb8-ed44a49f31e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:19:52:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:39:26:INFO:
[92mINFO [0m:      Received: evaluate message 1fb272b3-890a-4be5-a438-a4414f19807b
02/12/2025 02:39:26:INFO:Received: evaluate message 1fb272b3-890a-4be5-a438-a4414f19807b

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:42:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:44:18:INFO:
[92mINFO [0m:      Received: train message 47a38e2e-ee3d-4f79-af1b-c98c24667515
02/12/2025 02:44:18:INFO:Received: train message 47a38e2e-ee3d-4f79-af1b-c98c24667515
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:55:36:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:13:28:INFO:
[92mINFO [0m:      Received: evaluate message aa7b5ea3-ab69-46a2-b513-cc18043b9548
02/12/2025 03:13:28:INFO:Received: evaluate message aa7b5ea3-ab69-46a2-b513-cc18043b9548
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:17:04:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:18:12:INFO:
[92mINFO [0m:      Received: train message 7df933f6-7045-4fc1-bdc0-48e7fbaf5dcb
02/12/2025 03:18:12:INFO:Received: train message 7df933f6-7045-4fc1-bdc0-48e7fbaf5dcb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:29:58:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:47:58:INFO:
[92mINFO [0m:      Received: evaluate message 09d0438f-a728-4e5d-95e3-08ba306e4a40
02/12/2025 03:47:58:INFO:Received: evaluate message 09d0438f-a728-4e5d-95e3-08ba306e4a40

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:51:19:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:52:42:INFO:
[92mINFO [0m:      Received: train message 69c97ee8-7161-45ef-bc72-ee9ab1ee65ce
02/12/2025 03:52:42:INFO:Received: train message 69c97ee8-7161-45ef-bc72-ee9ab1ee65ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 04:03:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:25:10:INFO:
[92mINFO [0m:      Received: evaluate message 87300444-135f-41b9-bd5a-71c8aed2c80c
02/12/2025 04:25:10:INFO:Received: evaluate message 87300444-135f-41b9-bd5a-71c8aed2c80c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 04:28:41:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:29:36:INFO:
[92mINFO [0m:      Received: train message ab638dc2-9a21-4ff6-9a98-befcf907a920
02/12/2025 04:29:36:INFO:Received: train message ab638dc2-9a21-4ff6-9a98-befcf907a920

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 04:40:36:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:01:49:INFO:
[92mINFO [0m:      Received: evaluate message a6561a23-f89c-4586-a098-0d5871065dc7
02/12/2025 05:01:49:INFO:Received: evaluate message a6561a23-f89c-4586-a098-0d5871065dc7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 05:05:26:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:05:54:INFO:
[92mINFO [0m:      Received: reconnect message 817fcac4-5267-4ed2-b8ce-05507e9fe5b2
02/12/2025 05:05:54:INFO:Received: reconnect message 817fcac4-5267-4ed2-b8ce-05507e9fe5b2
02/12/2025 05:05:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/12/2025 05:05:54:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}



Final client history:
{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}

