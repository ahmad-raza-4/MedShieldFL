nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/11/2025 11:35:17:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/11/2025 11:35:17:DEBUG:ChannelConnectivity.IDLE
02/11/2025 11:35:17:DEBUG:ChannelConnectivity.CONNECTING
02/11/2025 11:35:17:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739302517.754866 1489714 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/11/2025 11:39:02:INFO:
[92mINFO [0m:      Received: train message 2be78491-a3da-4eaa-8eec-84ce6bbe5b41
02/11/2025 11:39:02:INFO:Received: train message 2be78491-a3da-4eaa-8eec-84ce6bbe5b41
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 11:56:30:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:09:21:INFO:
[92mINFO [0m:      Received: evaluate message 489172dd-4162-404a-9436-f4bb6fba1774
02/11/2025 12:09:21:INFO:Received: evaluate message 489172dd-4162-404a-9436-f4bb6fba1774
[92mINFO [0m:      Sent reply
02/11/2025 12:13:12:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:14:20:INFO:
[92mINFO [0m:      Received: train message b697e340-7a15-4325-9c96-7d66d8c2108d
02/11/2025 12:14:20:INFO:Received: train message b697e340-7a15-4325-9c96-7d66d8c2108d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:31:24:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:44:12:INFO:
[92mINFO [0m:      Received: evaluate message 5410c5c8-4e1b-4846-b747-5934b811aff2
02/11/2025 12:44:12:INFO:Received: evaluate message 5410c5c8-4e1b-4846-b747-5934b811aff2
[92mINFO [0m:      Sent reply
02/11/2025 12:48:14:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:48:49:INFO:
[92mINFO [0m:      Received: train message 10815bf7-3cef-4cab-a269-66f9aa1a5de9
02/11/2025 12:48:49:INFO:Received: train message 10815bf7-3cef-4cab-a269-66f9aa1a5de9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:05:29:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:22:29:INFO:
[92mINFO [0m:      Received: evaluate message a3b5ba74-23ae-437b-8e5b-7b374b063d48
02/11/2025 13:22:29:INFO:Received: evaluate message a3b5ba74-23ae-437b-8e5b-7b374b063d48
[92mINFO [0m:      Sent reply
02/11/2025 13:26:32:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:27:04:INFO:
[92mINFO [0m:      Received: train message 92ee7194-e8b1-489d-b5e8-1220fe5de9a1
02/11/2025 13:27:04:INFO:Received: train message 92ee7194-e8b1-489d-b5e8-1220fe5de9a1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:43:45:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:56:46:INFO:
[92mINFO [0m:      Received: evaluate message 6c01cce9-28d1-43e1-a993-a93c9cd96cba
02/11/2025 13:56:46:INFO:Received: evaluate message 6c01cce9-28d1-43e1-a993-a93c9cd96cba
[92mINFO [0m:      Sent reply
02/11/2025 14:00:40:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:01:17:INFO:
[92mINFO [0m:      Received: train message 2bd98686-d776-4ac4-a9f2-f527bdf7bd85
02/11/2025 14:01:17:INFO:Received: train message 2bd98686-d776-4ac4-a9f2-f527bdf7bd85
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:17:55:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:31:43:INFO:
[92mINFO [0m:      Received: evaluate message 2d9c55d7-2ea7-4eab-9c8f-bf4d52a4d913
02/11/2025 14:31:43:INFO:Received: evaluate message 2d9c55d7-2ea7-4eab-9c8f-bf4d52a4d913
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 14:35:56:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:36:35:INFO:
[92mINFO [0m:      Received: train message 04106b7a-f518-41e1-b21d-8c161305e82d
02/11/2025 14:36:35:INFO:Received: train message 04106b7a-f518-41e1-b21d-8c161305e82d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:53:10:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:06:40:INFO:
[92mINFO [0m:      Received: evaluate message 76e90284-3e55-4d0c-8575-c5b6f002edd3
02/11/2025 15:06:40:INFO:Received: evaluate message 76e90284-3e55-4d0c-8575-c5b6f002edd3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:10:34:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:11:19:INFO:
[92mINFO [0m:      Received: train message f354518a-99af-4366-9ff4-152bacbe3d94
02/11/2025 15:11:19:INFO:Received: train message f354518a-99af-4366-9ff4-152bacbe3d94
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:28:01:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:40:53:INFO:
[92mINFO [0m:      Received: evaluate message 55bc3ec0-5291-4872-833c-b3ec9c67c07a
02/11/2025 15:40:53:INFO:Received: evaluate message 55bc3ec0-5291-4872-833c-b3ec9c67c07a
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192], 'accuracy': [0.25654450261780104], 'auc': [0.5097526219963764], 'precision': [0.25488184534737934], 'recall': [0.25654450261780104], 'f1': [0.19218762720861446]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877], 'accuracy': [0.25654450261780104, 0.29883205799436163], 'auc': [0.5097526219963764, 0.5400268636526322], 'precision': [0.25488184534737934, 0.31012842353432646], 'recall': [0.25654450261780104, 0.29883205799436163], 'f1': [0.19218762720861446, 0.20880157085225062]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:44:06:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:45:44:INFO:
[92mINFO [0m:      Received: train message a404a107-8dff-4c05-9a59-b21ff8a6b939
02/11/2025 15:45:44:INFO:Received: train message a404a107-8dff-4c05-9a59-b21ff8a6b939
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 16:02:46:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:15:36:INFO:
[92mINFO [0m:      Received: evaluate message bd29461a-d3b7-497e-a56b-dbe70ec6cefc
02/11/2025 16:15:36:INFO:Received: evaluate message bd29461a-d3b7-497e-a56b-dbe70ec6cefc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:19:27:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:20:30:INFO:
[92mINFO [0m:      Received: train message ac9a0f7b-2703-48de-9848-970aaa03c959
02/11/2025 16:20:30:INFO:Received: train message ac9a0f7b-2703-48de-9848-970aaa03c959
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 16:38:54:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:53:25:INFO:
[92mINFO [0m:      Received: evaluate message 858e2d04-80b1-4b3b-92f8-badb5895bd73
02/11/2025 16:53:25:INFO:Received: evaluate message 858e2d04-80b1-4b3b-92f8-badb5895bd73
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:57:08:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:58:02:INFO:
[92mINFO [0m:      Received: train message 26795172-d47e-4187-ab38-88192694386d
02/11/2025 16:58:02:INFO:Received: train message 26795172-d47e-4187-ab38-88192694386d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:14:28:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:26:41:INFO:
[92mINFO [0m:      Received: evaluate message abbe03b0-3db0-4fbb-8e26-edbd90b44822
02/11/2025 17:26:41:INFO:Received: evaluate message abbe03b0-3db0-4fbb-8e26-edbd90b44822
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 17:30:34:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:31:17:INFO:
[92mINFO [0m:      Received: train message 4134a698-7bbd-4bee-b77b-fd5173f0f7e1
02/11/2025 17:31:17:INFO:Received: train message 4134a698-7bbd-4bee-b77b-fd5173f0f7e1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:48:52:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:00:51:INFO:
[92mINFO [0m:      Received: evaluate message cf532a50-841d-4c5e-9979-16bb94ae3522
02/11/2025 18:00:51:INFO:Received: evaluate message cf532a50-841d-4c5e-9979-16bb94ae3522

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:04:35:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:05:13:INFO:
[92mINFO [0m:      Received: train message 9207b589-0328-4ea5-94d0-64d514896eae
02/11/2025 18:05:13:INFO:Received: train message 9207b589-0328-4ea5-94d0-64d514896eae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:21:47:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:34:14:INFO:
[92mINFO [0m:      Received: evaluate message c9708de3-7a48-4282-aa8b-ec56a05dbcc8
02/11/2025 18:34:14:INFO:Received: evaluate message c9708de3-7a48-4282-aa8b-ec56a05dbcc8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:38:02:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:38:48:INFO:
[92mINFO [0m:      Received: train message cb95c855-4d62-4857-96d4-b8181fcd197e
02/11/2025 18:38:48:INFO:Received: train message cb95c855-4d62-4857-96d4-b8181fcd197e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:56:13:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:10:09:INFO:
[92mINFO [0m:      Received: evaluate message 10e149ac-c171-4f5c-99ec-9983baad0701
02/11/2025 19:10:09:INFO:Received: evaluate message 10e149ac-c171-4f5c-99ec-9983baad0701
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:14:10:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:14:42:INFO:
[92mINFO [0m:      Received: train message cdb493b8-a4be-40ee-9fbc-467fd8bcf3a9
02/11/2025 19:14:42:INFO:Received: train message cdb493b8-a4be-40ee-9fbc-467fd8bcf3a9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:31:57:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:44:14:INFO:
[92mINFO [0m:      Received: evaluate message fb5fcb6b-9885-4898-8bac-128620cbe616
02/11/2025 19:44:14:INFO:Received: evaluate message fb5fcb6b-9885-4898-8bac-128620cbe616

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:48:13:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:48:21:INFO:
[92mINFO [0m:      Received: train message a7b64101-9d6e-4b18-bd5e-9ed1531db605
02/11/2025 19:48:21:INFO:Received: train message a7b64101-9d6e-4b18-bd5e-9ed1531db605
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 20:04:54:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:19:55:INFO:
[92mINFO [0m:      Received: evaluate message 42d9d36d-d825-4e68-8bbe-fb88a9e23fee
02/11/2025 20:19:55:INFO:Received: evaluate message 42d9d36d-d825-4e68-8bbe-fb88a9e23fee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:23:36:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:24:19:INFO:
[92mINFO [0m:      Received: train message 10f77681-22e7-469f-b120-9b322ed37cf3
02/11/2025 20:24:19:INFO:Received: train message 10f77681-22e7-469f-b120-9b322ed37cf3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 20:40:57:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:53:18:INFO:
[92mINFO [0m:      Received: evaluate message b6b88506-467b-449c-b17d-a54452690e42
02/11/2025 20:53:18:INFO:Received: evaluate message b6b88506-467b-449c-b17d-a54452690e42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:57:18:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:57:47:INFO:
[92mINFO [0m:      Received: train message 6b1165e3-2274-4961-b5b0-465ff9d2d74a
02/11/2025 20:57:47:INFO:Received: train message 6b1165e3-2274-4961-b5b0-465ff9d2d74a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:14:45:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:29:07:INFO:
[92mINFO [0m:      Received: evaluate message 29e15c2d-e0b0-4446-b2d2-84d3690b0178
02/11/2025 21:29:07:INFO:Received: evaluate message 29e15c2d-e0b0-4446-b2d2-84d3690b0178

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 21:33:10:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:33:59:INFO:
[92mINFO [0m:      Received: train message 78b3aa9d-27a9-4b52-bfc8-247d57a71d82
02/11/2025 21:33:59:INFO:Received: train message 78b3aa9d-27a9-4b52-bfc8-247d57a71d82
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:51:13:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:02:58:INFO:
[92mINFO [0m:      Received: evaluate message 8eb16c0a-0356-444f-8c87-b341a3181452
02/11/2025 22:02:58:INFO:Received: evaluate message 8eb16c0a-0356-444f-8c87-b341a3181452
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:06:28:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:07:48:INFO:
[92mINFO [0m:      Received: train message f9d0c964-ba50-4078-b663-c2c0d1f6ff24
02/11/2025 22:07:48:INFO:Received: train message f9d0c964-ba50-4078-b663-c2c0d1f6ff24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:24:36:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:39:01:INFO:
[92mINFO [0m:      Received: evaluate message 67e4ae95-21e5-4113-b491-7c72cd3007a6
02/11/2025 22:39:01:INFO:Received: evaluate message 67e4ae95-21e5-4113-b491-7c72cd3007a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:42:50:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:43:11:INFO:
[92mINFO [0m:      Received: train message 4498fc0c-e943-4452-99b4-35651d01b11e
02/11/2025 22:43:11:INFO:Received: train message 4498fc0c-e943-4452-99b4-35651d01b11e

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:59:48:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:12:30:INFO:
[92mINFO [0m:      Received: evaluate message cf5b4d23-9ab3-47a6-a2f6-1bf1256aba7a
02/11/2025 23:12:30:INFO:Received: evaluate message cf5b4d23-9ab3-47a6-a2f6-1bf1256aba7a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:16:29:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:16:54:INFO:
[92mINFO [0m:      Received: train message cea679ab-b10b-4db5-bcc4-16935a1b3434
02/11/2025 23:16:54:INFO:Received: train message cea679ab-b10b-4db5-bcc4-16935a1b3434
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:34:29:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:46:49:INFO:
[92mINFO [0m:      Received: evaluate message 8da9c4f5-42a8-4c72-9e64-17bd33665f0a
02/11/2025 23:46:49:INFO:Received: evaluate message 8da9c4f5-42a8-4c72-9e64-17bd33665f0a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:50:53:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:51:24:INFO:
[92mINFO [0m:      Received: train message 15a0b705-d766-4a03-8c5e-113b29b11de9
02/11/2025 23:51:24:INFO:Received: train message 15a0b705-d766-4a03-8c5e-113b29b11de9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:08:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:22:07:INFO:
[92mINFO [0m:      Received: evaluate message 22c354c5-8d32-48ea-aeb6-0bd80388099f
02/12/2025 00:22:07:INFO:Received: evaluate message 22c354c5-8d32-48ea-aeb6-0bd80388099f
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 00:26:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:26:34:INFO:
[92mINFO [0m:      Received: train message 5e99d359-7f93-400c-9790-89a8fac33f56
02/12/2025 00:26:34:INFO:Received: train message 5e99d359-7f93-400c-9790-89a8fac33f56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:44:29:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:56:38:INFO:
[92mINFO [0m:      Received: evaluate message bc19d8e8-a6fa-4197-8186-674305964aa8
02/12/2025 00:56:38:INFO:Received: evaluate message bc19d8e8-a6fa-4197-8186-674305964aa8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:00:32:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:01:20:INFO:
[92mINFO [0m:      Received: train message f1db162b-306a-443f-a804-d78fd0c0894a
02/12/2025 01:01:20:INFO:Received: train message f1db162b-306a-443f-a804-d78fd0c0894a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:18:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:30:32:INFO:
[92mINFO [0m:      Received: evaluate message 44fe7de8-8589-4e62-9967-1f558ce349d2
02/12/2025 01:30:32:INFO:Received: evaluate message 44fe7de8-8589-4e62-9967-1f558ce349d2

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:34:33:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:34:46:INFO:
[92mINFO [0m:      Received: train message 4de6e767-b8c6-46c4-9005-369f009e8bea
02/12/2025 01:34:46:INFO:Received: train message 4de6e767-b8c6-46c4-9005-369f009e8bea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:51:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:04:04:INFO:
[92mINFO [0m:      Received: evaluate message fa3af95e-e372-45ec-9c49-a4f11c9576c8
02/12/2025 02:04:04:INFO:Received: evaluate message fa3af95e-e372-45ec-9c49-a4f11c9576c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:08:06:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:08:39:INFO:
[92mINFO [0m:      Received: train message 073778df-0729-440e-ae04-f091e8471c96
02/12/2025 02:08:39:INFO:Received: train message 073778df-0729-440e-ae04-f091e8471c96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:25:36:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:39:46:INFO:
[92mINFO [0m:      Received: evaluate message 8199642a-9e7c-4d6f-a8bb-0a09e2ec1a02
02/12/2025 02:39:46:INFO:Received: evaluate message 8199642a-9e7c-4d6f-a8bb-0a09e2ec1a02

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:43:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:44:01:INFO:
[92mINFO [0m:      Received: train message acdbdcdf-7bf4-47e8-8808-e686e32beb37
02/12/2025 02:44:01:INFO:Received: train message acdbdcdf-7bf4-47e8-8808-e686e32beb37
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:00:50:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:13:25:INFO:
[92mINFO [0m:      Received: evaluate message 394cb53e-0934-4c3f-8de2-8620b7b9fbeb
02/12/2025 03:13:25:INFO:Received: evaluate message 394cb53e-0934-4c3f-8de2-8620b7b9fbeb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:17:38:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:18:12:INFO:
[92mINFO [0m:      Received: train message 07c56311-815f-4ed8-b846-736337481df5
02/12/2025 03:18:12:INFO:Received: train message 07c56311-815f-4ed8-b846-736337481df5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:35:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:47:52:INFO:
[92mINFO [0m:      Received: evaluate message 375924d5-f5a3-4907-8ffb-79125c39b0bb
02/12/2025 03:47:52:INFO:Received: evaluate message 375924d5-f5a3-4907-8ffb-79125c39b0bb

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:52:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:52:40:INFO:
[92mINFO [0m:      Received: train message 0685b49a-3eb1-4251-8289-04ec98b23398
02/12/2025 03:52:40:INFO:Received: train message 0685b49a-3eb1-4251-8289-04ec98b23398
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 04:09:34:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:25:18:INFO:
[92mINFO [0m:      Received: evaluate message 50ee2859-5857-4396-bfa5-011fe7487d54
02/12/2025 04:25:18:INFO:Received: evaluate message 50ee2859-5857-4396-bfa5-011fe7487d54
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 04:29:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:29:42:INFO:
[92mINFO [0m:      Received: train message 197d3fd1-402f-4011-b9ad-80505f90f88c
02/12/2025 04:29:42:INFO:Received: train message 197d3fd1-402f-4011-b9ad-80505f90f88c

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 04:46:52:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:01:37:INFO:
[92mINFO [0m:      Received: evaluate message 1a532db8-70da-4a4e-86e4-184c25b48473
02/12/2025 05:01:37:INFO:Received: evaluate message 1a532db8-70da-4a4e-86e4-184c25b48473
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 05:05:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:05:54:INFO:
[92mINFO [0m:      Received: reconnect message b39fc4cb-8c75-4dd4-90ee-9145e90e01e8
02/12/2025 05:05:54:INFO:Received: reconnect message b39fc4cb-8c75-4dd4-90ee-9145e90e01e8
02/12/2025 05:05:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/12/2025 05:05:54:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}



Final client history:
{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}

