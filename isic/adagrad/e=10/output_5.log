nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/11/2025 11:32:02:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/11/2025 11:32:02:DEBUG:ChannelConnectivity.IDLE
02/11/2025 11:32:02:DEBUG:ChannelConnectivity.CONNECTING
02/11/2025 11:32:02:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739302322.765974 1487250 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/11/2025 11:39:07:INFO:
[92mINFO [0m:      Received: train message 3c4d58c7-f7db-4922-b320-6841a5852f38
02/11/2025 11:39:07:INFO:Received: train message 3c4d58c7-f7db-4922-b320-6841a5852f38
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 11:44:35:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:09:19:INFO:
[92mINFO [0m:      Received: evaluate message 5dcfd71a-7d08-46ab-9562-ab24c7333257
02/11/2025 12:09:19:INFO:Received: evaluate message 5dcfd71a-7d08-46ab-9562-ab24c7333257
[92mINFO [0m:      Sent reply
02/11/2025 12:13:15:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:14:13:INFO:
[92mINFO [0m:      Received: train message 70fdd095-f6a5-4d6f-9f17-6c08c2c3f24e
02/11/2025 12:14:13:INFO:Received: train message 70fdd095-f6a5-4d6f-9f17-6c08c2c3f24e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:19:21:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:44:12:INFO:
[92mINFO [0m:      Received: evaluate message 3df0d210-0d25-4b0b-ae03-eb17bdb59e8a
02/11/2025 12:44:12:INFO:Received: evaluate message 3df0d210-0d25-4b0b-ae03-eb17bdb59e8a
[92mINFO [0m:      Sent reply
02/11/2025 12:48:16:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:48:43:INFO:
[92mINFO [0m:      Received: train message 734ae612-6234-4b34-a646-3e3b61921181
02/11/2025 12:48:43:INFO:Received: train message 734ae612-6234-4b34-a646-3e3b61921181
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:53:38:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:22:23:INFO:
[92mINFO [0m:      Received: evaluate message 1dbfa511-4dda-42d2-9a14-3c37c66c552b
02/11/2025 13:22:23:INFO:Received: evaluate message 1dbfa511-4dda-42d2-9a14-3c37c66c552b
[92mINFO [0m:      Sent reply
02/11/2025 13:26:22:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:27:07:INFO:
[92mINFO [0m:      Received: train message ee6464aa-24ce-464a-9e49-f6076dc7cb1f
02/11/2025 13:27:07:INFO:Received: train message ee6464aa-24ce-464a-9e49-f6076dc7cb1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:32:09:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:56:54:INFO:
[92mINFO [0m:      Received: evaluate message 1096b2a5-4a3b-48ae-b47d-282958bc0a1e
02/11/2025 13:56:54:INFO:Received: evaluate message 1096b2a5-4a3b-48ae-b47d-282958bc0a1e
[92mINFO [0m:      Sent reply
02/11/2025 14:00:47:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:01:21:INFO:
[92mINFO [0m:      Received: train message 19459ab1-9dcd-42f2-9b3b-1f321b1fd720
02/11/2025 14:01:21:INFO:Received: train message 19459ab1-9dcd-42f2-9b3b-1f321b1fd720
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:06:21:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:31:42:INFO:
[92mINFO [0m:      Received: evaluate message 40b0f865-887b-4bcd-b057-af058983b2a3
02/11/2025 14:31:42:INFO:Received: evaluate message 40b0f865-887b-4bcd-b057-af058983b2a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 14:35:50:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:36:21:INFO:
[92mINFO [0m:      Received: train message e83cdb0e-62ae-4316-a452-0c774b01dbf5
02/11/2025 14:36:21:INFO:Received: train message e83cdb0e-62ae-4316-a452-0c774b01dbf5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:41:08:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:06:29:INFO:
[92mINFO [0m:      Received: evaluate message b5749a22-5bc8-4d89-a3cf-334e5fb3fc56
02/11/2025 15:06:29:INFO:Received: evaluate message b5749a22-5bc8-4d89-a3cf-334e5fb3fc56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:10:04:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:11:19:INFO:
[92mINFO [0m:      Received: train message b46503c6-f68c-41b5-8d5a-9caab8e5274f
02/11/2025 15:11:19:INFO:Received: train message b46503c6-f68c-41b5-8d5a-9caab8e5274f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:16:24:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:41:16:INFO:
[92mINFO [0m:      Received: evaluate message 13c62477-c9fb-447f-bbd8-f8bb5bdb8781
02/11/2025 15:41:16:INFO:Received: evaluate message 13c62477-c9fb-447f-bbd8-f8bb5bdb8781
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192], 'accuracy': [0.25654450261780104], 'auc': [0.5097526219963764], 'precision': [0.25488184534737934], 'recall': [0.25654450261780104], 'f1': [0.19218762720861446]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877], 'accuracy': [0.25654450261780104, 0.29883205799436163], 'auc': [0.5097526219963764, 0.5400268636526322], 'precision': [0.25488184534737934, 0.31012842353432646], 'recall': [0.25654450261780104, 0.29883205799436163], 'f1': [0.19218762720861446, 0.20880157085225062]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:44:59:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:45:37:INFO:
[92mINFO [0m:      Received: train message 681edf52-ed66-44f5-a284-68cea57b8339
02/11/2025 15:45:37:INFO:Received: train message 681edf52-ed66-44f5-a284-68cea57b8339
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:50:39:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:15:41:INFO:
[92mINFO [0m:      Received: evaluate message 374d3e0d-5b10-4ca6-931e-41d90c7fe19c
02/11/2025 16:15:41:INFO:Received: evaluate message 374d3e0d-5b10-4ca6-931e-41d90c7fe19c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:19:48:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:20:34:INFO:
[92mINFO [0m:      Received: train message 9f238b91-7905-4b4e-b70b-8207dbedc1cf
02/11/2025 16:20:34:INFO:Received: train message 9f238b91-7905-4b4e-b70b-8207dbedc1cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 16:26:02:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:53:25:INFO:
[92mINFO [0m:      Received: evaluate message e33c6adc-3311-4973-8873-f8c124c4a28e
02/11/2025 16:53:25:INFO:Received: evaluate message e33c6adc-3311-4973-8873-f8c124c4a28e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:57:26:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:57:45:INFO:
[92mINFO [0m:      Received: train message 037e5e0e-5b55-4650-9cd2-90ebdd9c9363
02/11/2025 16:57:45:INFO:Received: train message 037e5e0e-5b55-4650-9cd2-90ebdd9c9363
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:02:37:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:26:38:INFO:
[92mINFO [0m:      Received: evaluate message 9d653714-c37f-46a5-af11-4b598141cded
02/11/2025 17:26:38:INFO:Received: evaluate message 9d653714-c37f-46a5-af11-4b598141cded
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 17:30:45:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:31:17:INFO:
[92mINFO [0m:      Received: train message e0cf5210-8fd9-47be-a07f-0ba24cf2faa8
02/11/2025 17:31:17:INFO:Received: train message e0cf5210-8fd9-47be-a07f-0ba24cf2faa8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:36:32:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:00:51:INFO:
[92mINFO [0m:      Received: evaluate message 3efd9c99-ae6f-41d6-840a-cdd700fbeb90
02/11/2025 18:00:51:INFO:Received: evaluate message 3efd9c99-ae6f-41d6-840a-cdd700fbeb90

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:04:53:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:05:31:INFO:
[92mINFO [0m:      Received: train message b7392a3f-1844-42e0-b4db-61dedd7218ae
02/11/2025 18:05:31:INFO:Received: train message b7392a3f-1844-42e0-b4db-61dedd7218ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:10:43:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:34:14:INFO:
[92mINFO [0m:      Received: evaluate message 8bf47f10-57ed-4f7c-a9fa-998478fa9674
02/11/2025 18:34:14:INFO:Received: evaluate message 8bf47f10-57ed-4f7c-a9fa-998478fa9674
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:38:15:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:38:38:INFO:
[92mINFO [0m:      Received: train message 5ec4cb38-c95d-4030-a7dd-a4ada8994bad
02/11/2025 18:38:38:INFO:Received: train message 5ec4cb38-c95d-4030-a7dd-a4ada8994bad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:43:55:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:10:12:INFO:
[92mINFO [0m:      Received: evaluate message f0239f89-550f-4d9f-aace-461760b202ce
02/11/2025 19:10:12:INFO:Received: evaluate message f0239f89-550f-4d9f-aace-461760b202ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:13:54:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:14:47:INFO:
[92mINFO [0m:      Received: train message 06b92385-05c7-43ef-b705-736157f746d2
02/11/2025 19:14:47:INFO:Received: train message 06b92385-05c7-43ef-b705-736157f746d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:19:55:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:44:03:INFO:
[92mINFO [0m:      Received: evaluate message 712dff73-cc40-4877-ae9b-d058f5f3581c
02/11/2025 19:44:03:INFO:Received: evaluate message 712dff73-cc40-4877-ae9b-d058f5f3581c

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:47:56:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:48:44:INFO:
[92mINFO [0m:      Received: train message 26c9c3e0-2fe6-4132-addb-211158d46bfd
02/11/2025 19:48:44:INFO:Received: train message 26c9c3e0-2fe6-4132-addb-211158d46bfd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:54:10:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:19:54:INFO:
[92mINFO [0m:      Received: evaluate message 161b8cdb-e88c-422f-9257-bc631bde3bfa
02/11/2025 20:19:54:INFO:Received: evaluate message 161b8cdb-e88c-422f-9257-bc631bde3bfa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:23:44:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:23:57:INFO:
[92mINFO [0m:      Received: train message 7a3c678e-d1a4-4ec3-a666-0299634a13a0
02/11/2025 20:23:57:INFO:Received: train message 7a3c678e-d1a4-4ec3-a666-0299634a13a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 20:28:42:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:53:16:INFO:
[92mINFO [0m:      Received: evaluate message 376be35c-2d87-4bca-a878-da7df64d07da
02/11/2025 20:53:16:INFO:Received: evaluate message 376be35c-2d87-4bca-a878-da7df64d07da
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:57:11:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:57:46:INFO:
[92mINFO [0m:      Received: train message 3c91258c-10e0-4de1-9743-38009ddbef15
02/11/2025 20:57:46:INFO:Received: train message 3c91258c-10e0-4de1-9743-38009ddbef15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:02:41:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:29:07:INFO:
[92mINFO [0m:      Received: evaluate message 04d97f05-d7d8-48f6-9bc4-de9aec420548
02/11/2025 21:29:07:INFO:Received: evaluate message 04d97f05-d7d8-48f6-9bc4-de9aec420548
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 21:33:17:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:34:04:INFO:
[92mINFO [0m:      Received: train message be22dd76-b7ee-476a-9620-e987297180d5
02/11/2025 21:34:04:INFO:Received: train message be22dd76-b7ee-476a-9620-e987297180d5
Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:39:19:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:03:12:INFO:
[92mINFO [0m:      Received: evaluate message 6f1687ae-f666-41f6-84ce-3ad2e0cfca44
02/11/2025 22:03:12:INFO:Received: evaluate message 6f1687ae-f666-41f6-84ce-3ad2e0cfca44
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:07:15:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:07:38:INFO:
[92mINFO [0m:      Received: train message 89bbc127-cd56-4594-90b1-da2ade8e9f13
02/11/2025 22:07:38:INFO:Received: train message 89bbc127-cd56-4594-90b1-da2ade8e9f13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:12:42:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:39:04:INFO:
[92mINFO [0m:      Received: evaluate message 94aa66e1-274e-440a-a394-19b80807922a
02/11/2025 22:39:04:INFO:Received: evaluate message 94aa66e1-274e-440a-a394-19b80807922a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:43:00:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:43:34:INFO:
[92mINFO [0m:      Received: train message 3552c6a5-4be6-4dcb-b854-7eda99b6e010
02/11/2025 22:43:34:INFO:Received: train message 3552c6a5-4be6-4dcb-b854-7eda99b6e010
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:48:42:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:12:12:INFO:
[92mINFO [0m:      Received: evaluate message 0ce7eea7-4fe5-4e17-a2a7-4effeac85a8c
02/11/2025 23:12:12:INFO:Received: evaluate message 0ce7eea7-4fe5-4e17-a2a7-4effeac85a8c
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:15:33:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:17:03:INFO:
[92mINFO [0m:      Received: train message acf948f9-39a8-482e-a811-d5a36135c665
02/11/2025 23:17:03:INFO:Received: train message acf948f9-39a8-482e-a811-d5a36135c665
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:22:02:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:46:49:INFO:
[92mINFO [0m:      Received: evaluate message a463529f-d6bc-4ffe-b09b-6a0539085117
02/11/2025 23:46:49:INFO:Received: evaluate message a463529f-d6bc-4ffe-b09b-6a0539085117
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:50:58:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:51:32:INFO:
[92mINFO [0m:      Received: train message c266c36e-4a3d-425f-a1e8-06b335308c56
02/11/2025 23:51:32:INFO:Received: train message c266c36e-4a3d-425f-a1e8-06b335308c56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:56:33:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:22:05:INFO:
[92mINFO [0m:      Received: evaluate message 32937c32-9250-4d8c-8072-c1df7a141480
02/12/2025 00:22:05:INFO:Received: evaluate message 32937c32-9250-4d8c-8072-c1df7a141480

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 00:26:03:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:26:38:INFO:
[92mINFO [0m:      Received: train message 55d0592c-efce-4bb5-99cc-9ef65264e898
02/12/2025 00:26:38:INFO:Received: train message 55d0592c-efce-4bb5-99cc-9ef65264e898
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:31:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:56:54:INFO:
[92mINFO [0m:      Received: evaluate message 624042b5-2ec2-4508-bb09-21fc36c3b23c
02/12/2025 00:56:54:INFO:Received: evaluate message 624042b5-2ec2-4508-bb09-21fc36c3b23c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:00:47:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:01:20:INFO:
[92mINFO [0m:      Received: train message dfd9c505-7861-408a-a29e-67d29316eab8
02/12/2025 01:01:20:INFO:Received: train message dfd9c505-7861-408a-a29e-67d29316eab8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:06:38:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:30:33:INFO:
[92mINFO [0m:      Received: evaluate message e9961ed1-3bcb-4cf1-a130-c15dd2466ece
02/12/2025 01:30:33:INFO:Received: evaluate message e9961ed1-3bcb-4cf1-a130-c15dd2466ece

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:34:35:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:35:03:INFO:
[92mINFO [0m:      Received: train message befdd037-a959-48f2-9f39-6ea522f46ced
02/12/2025 01:35:03:INFO:Received: train message befdd037-a959-48f2-9f39-6ea522f46ced
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:40:11:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:04:01:INFO:
[92mINFO [0m:      Received: evaluate message b9c43638-d8e4-4c85-b76e-de9f258bcff3
02/12/2025 02:04:01:INFO:Received: evaluate message b9c43638-d8e4-4c85-b76e-de9f258bcff3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:08:03:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:08:37:INFO:
[92mINFO [0m:      Received: train message 19721406-4ce8-477f-9635-ecbfe2070d16
02/12/2025 02:08:37:INFO:Received: train message 19721406-4ce8-477f-9635-ecbfe2070d16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:13:49:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:39:25:INFO:
[92mINFO [0m:      Received: evaluate message 627d566a-6a71-45e7-9414-647479ffa3c9
02/12/2025 02:39:25:INFO:Received: evaluate message 627d566a-6a71-45e7-9414-647479ffa3c9

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:43:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:44:17:INFO:
[92mINFO [0m:      Received: train message d5cb2091-9b44-4d94-970d-e7d00f8a9d6b
02/12/2025 02:44:17:INFO:Received: train message d5cb2091-9b44-4d94-970d-e7d00f8a9d6b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:49:32:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:13:18:INFO:
[92mINFO [0m:      Received: evaluate message 3ddc08fc-3f54-4f03-9ad2-de02466d09a5
02/12/2025 03:13:18:INFO:Received: evaluate message 3ddc08fc-3f54-4f03-9ad2-de02466d09a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:17:22:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:17:54:INFO:
[92mINFO [0m:      Received: train message e591969d-05c5-4b6d-a79c-25865b2cd35d
02/12/2025 03:17:54:INFO:Received: train message e591969d-05c5-4b6d-a79c-25865b2cd35d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:22:36:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:47:46:INFO:
[92mINFO [0m:      Received: evaluate message 5ba0e75a-8cb1-4f95-841b-8fee624c04ad
02/12/2025 03:47:46:INFO:Received: evaluate message 5ba0e75a-8cb1-4f95-841b-8fee624c04ad

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:51:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:52:37:INFO:
[92mINFO [0m:      Received: train message fe4a1efb-4e24-4801-a839-3abea70054ac
02/12/2025 03:52:37:INFO:Received: train message fe4a1efb-4e24-4801-a839-3abea70054ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:57:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:25:18:INFO:
[92mINFO [0m:      Received: evaluate message a767d433-2e2f-426e-9d15-4d162fb412ce
02/12/2025 04:25:18:INFO:Received: evaluate message a767d433-2e2f-426e-9d15-4d162fb412ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 04:29:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:29:51:INFO:
[92mINFO [0m:      Received: train message 85b51d93-99f7-424d-9629-09a53639614e
02/12/2025 04:29:51:INFO:Received: train message 85b51d93-99f7-424d-9629-09a53639614e

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 04:35:12:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:01:37:INFO:
[92mINFO [0m:      Received: evaluate message 4764cbe8-e378-4fc9-9675-83d2cb2d8189
02/12/2025 05:01:37:INFO:Received: evaluate message 4764cbe8-e378-4fc9-9675-83d2cb2d8189
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 05:05:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:05:54:INFO:
[92mINFO [0m:      Received: reconnect message 1db3dcb5-1faf-4d2e-a673-b628206f6e97
02/12/2025 05:05:54:INFO:Received: reconnect message 1db3dcb5-1faf-4d2e-a673-b628206f6e97
02/12/2025 05:05:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/12/2025 05:05:54:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}



Final client history:
{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}

