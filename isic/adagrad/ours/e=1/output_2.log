nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/12/2025 06:52:59:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/12/2025 06:52:59:DEBUG:ChannelConnectivity.IDLE
02/12/2025 06:52:59:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739371979.334113 2078434 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/12/2025 06:57:37:INFO:
[92mINFO [0m:      Received: train message a4ab40c4-a7c6-4f11-968b-410b029a9d4f
02/12/2025 06:57:37:INFO:Received: train message a4ab40c4-a7c6-4f11-968b-410b029a9d4f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:18:02:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:34:19:INFO:
[92mINFO [0m:      Received: evaluate message 1edefc07-6ada-41aa-b2c3-df3c5b8d40e6
02/12/2025 07:34:19:INFO:Received: evaluate message 1edefc07-6ada-41aa-b2c3-df3c5b8d40e6
[92mINFO [0m:      Sent reply
02/12/2025 07:38:54:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:39:26:INFO:
[92mINFO [0m:      Received: train message 647a81cf-2094-4ca5-a8fc-e757b6dd7818
02/12/2025 07:39:26:INFO:Received: train message 647a81cf-2094-4ca5-a8fc-e757b6dd7818
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:57:49:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:14:28:INFO:
[92mINFO [0m:      Received: evaluate message 96316e90-a965-408a-a5e1-87aaad865b18
02/12/2025 08:14:28:INFO:Received: evaluate message 96316e90-a965-408a-a5e1-87aaad865b18
[92mINFO [0m:      Sent reply
02/12/2025 08:18:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:19:22:INFO:
[92mINFO [0m:      Received: train message 6eb78a8f-c42d-4833-8bca-d2df145a82be
02/12/2025 08:19:22:INFO:Received: train message 6eb78a8f-c42d-4833-8bca-d2df145a82be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 08:37:56:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:54:37:INFO:
[92mINFO [0m:      Received: evaluate message 7f1b9ab5-d7ac-4e11-afe5-90e824116eff
02/12/2025 08:54:37:INFO:Received: evaluate message 7f1b9ab5-d7ac-4e11-afe5-90e824116eff
[92mINFO [0m:      Sent reply
02/12/2025 08:59:01:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:59:33:INFO:
[92mINFO [0m:      Received: train message f7d58879-e4e4-4b33-b1e4-68be25b795fe
02/12/2025 08:59:33:INFO:Received: train message f7d58879-e4e4-4b33-b1e4-68be25b795fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:18:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:34:21:INFO:
[92mINFO [0m:      Received: evaluate message b6714389-5beb-4529-8c8f-2f8de8b7af81
02/12/2025 09:34:21:INFO:Received: evaluate message b6714389-5beb-4529-8c8f-2f8de8b7af81
[92mINFO [0m:      Sent reply
02/12/2025 09:38:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:39:19:INFO:
[92mINFO [0m:      Received: train message acaf768b-b123-4d49-b31d-ef5b94b3efda
02/12/2025 09:39:19:INFO:Received: train message acaf768b-b123-4d49-b31d-ef5b94b3efda
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:57:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:14:15:INFO:
[92mINFO [0m:      Received: evaluate message 6be29677-e31d-4035-badf-e27c85eeb5f4
02/12/2025 10:14:15:INFO:Received: evaluate message 6be29677-e31d-4035-badf-e27c85eeb5f4
[92mINFO [0m:      Sent reply
02/12/2025 10:18:37:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:19:13:INFO:
[92mINFO [0m:      Received: train message 440a3708-d117-4cd1-bdcf-0123db1cd5d1
02/12/2025 10:19:13:INFO:Received: train message 440a3708-d117-4cd1-bdcf-0123db1cd5d1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 10:37:52:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:54:16:INFO:
[92mINFO [0m:      Received: evaluate message cabe9a13-a795-44bd-b786-cffa3273284c
02/12/2025 10:54:16:INFO:Received: evaluate message cabe9a13-a795-44bd-b786-cffa3273284c
[92mINFO [0m:      Sent reply
02/12/2025 10:58:37:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:59:14:INFO:
[92mINFO [0m:      Received: train message 07a2c8a8-3f43-4964-b7b2-9eb7f53541c6
02/12/2025 10:59:14:INFO:Received: train message 07a2c8a8-3f43-4964-b7b2-9eb7f53541c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:18:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:34:14:INFO:
[92mINFO [0m:      Received: evaluate message 096a0396-c095-4feb-9358-e374def3886f
02/12/2025 11:34:14:INFO:Received: evaluate message 096a0396-c095-4feb-9358-e374def3886f
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124], 'accuracy': [0.23076923076923078], 'auc': [0.47906768884808504], 'precision': [0.22101253138913413], 'recall': [0.23076923076923078], 'f1': [0.19973020403381153]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868], 'accuracy': [0.23076923076923078, 0.2730567861457914], 'auc': [0.47906768884808504, 0.501405220341778], 'precision': [0.22101253138913413, 0.24545349552426635], 'recall': [0.23076923076923078, 0.2730567861457914], 'f1': [0.19973020403381153, 0.21604162997075227]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 11:38:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:39:00:INFO:
[92mINFO [0m:      Received: train message 9cccdb62-5e5e-4efd-b0b7-c9a071a62d56
02/12/2025 11:39:00:INFO:Received: train message 9cccdb62-5e5e-4efd-b0b7-c9a071a62d56
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:58:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:13:32:INFO:
[92mINFO [0m:      Received: evaluate message 571bffa0-ef9d-431d-beff-45c513822afe
02/12/2025 12:13:32:INFO:Received: evaluate message 571bffa0-ef9d-431d-beff-45c513822afe
[92mINFO [0m:      Sent reply
02/12/2025 12:17:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:18:25:INFO:
[92mINFO [0m:      Received: train message e79e0d9c-b8c6-4978-9c8b-dba2d468903a
02/12/2025 12:18:25:INFO:Received: train message e79e0d9c-b8c6-4978-9c8b-dba2d468903a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 12:37:35:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:52:57:INFO:
[92mINFO [0m:      Received: evaluate message c543b25f-cc68-4d62-93c9-6865bf1e154b
02/12/2025 12:52:57:INFO:Received: evaluate message c543b25f-cc68-4d62-93c9-6865bf1e154b
[92mINFO [0m:      Sent reply
02/12/2025 12:57:13:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:57:37:INFO:
[92mINFO [0m:      Received: train message 5a7db67a-113c-41a2-9404-2b7ee9b07806
02/12/2025 12:57:37:INFO:Received: train message 5a7db67a-113c-41a2-9404-2b7ee9b07806
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:16:38:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:32:19:INFO:
[92mINFO [0m:      Received: evaluate message c5dd2e0e-caac-4021-bc68-0c0e767ab2c2
02/12/2025 13:32:19:INFO:Received: evaluate message c5dd2e0e-caac-4021-bc68-0c0e767ab2c2
[92mINFO [0m:      Sent reply
02/12/2025 13:36:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:37:14:INFO:
[92mINFO [0m:      Received: train message 33e14c65-9e7b-4e98-824e-eb3999ef56b0
02/12/2025 13:37:14:INFO:Received: train message 33e14c65-9e7b-4e98-824e-eb3999ef56b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:56:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:11:13:INFO:
[92mINFO [0m:      Received: evaluate message 1431aed7-691b-451b-9492-2532198e82b4
02/12/2025 14:11:13:INFO:Received: evaluate message 1431aed7-691b-451b-9492-2532198e82b4

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 14:15:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:16:07:INFO:
[92mINFO [0m:      Received: train message dc8f9fd4-c1b0-4f3e-8e15-64ad270105bf
02/12/2025 14:16:07:INFO:Received: train message dc8f9fd4-c1b0-4f3e-8e15-64ad270105bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 14:35:18:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:50:14:INFO:
[92mINFO [0m:      Received: evaluate message c807ead6-f71f-4445-9233-5be6f8300607
02/12/2025 14:50:14:INFO:Received: evaluate message c807ead6-f71f-4445-9233-5be6f8300607
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 14:54:47:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:55:05:INFO:
[92mINFO [0m:      Received: train message 318df1db-4095-44fd-a9b9-161d530daa7e
02/12/2025 14:55:05:INFO:Received: train message 318df1db-4095-44fd-a9b9-161d530daa7e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:14:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:29:16:INFO:
[92mINFO [0m:      Received: evaluate message 3c60038d-585b-42be-86bd-21162f79c031
02/12/2025 15:29:16:INFO:Received: evaluate message 3c60038d-585b-42be-86bd-21162f79c031
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 15:33:35:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:33:58:INFO:
[92mINFO [0m:      Received: train message 450ef435-f31f-4f5f-afe1-c2ba226f19a5
02/12/2025 15:33:58:INFO:Received: train message 450ef435-f31f-4f5f-afe1-c2ba226f19a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:53:31:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:08:27:INFO:
[92mINFO [0m:      Received: evaluate message 1d0ddbca-72cf-4c68-92d6-b46d41b3e665
02/12/2025 16:08:27:INFO:Received: evaluate message 1d0ddbca-72cf-4c68-92d6-b46d41b3e665

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:12:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:13:31:INFO:
[92mINFO [0m:      Received: train message f8cfb847-2dde-4c1f-9ade-ec4b72ce726d
02/12/2025 16:13:31:INFO:Received: train message f8cfb847-2dde-4c1f-9ade-ec4b72ce726d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 16:33:18:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:47:55:INFO:
[92mINFO [0m:      Received: evaluate message b372bd45-5e85-4700-bbe6-02736b5c2185
02/12/2025 16:47:55:INFO:Received: evaluate message b372bd45-5e85-4700-bbe6-02736b5c2185
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:52:25:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:53:03:INFO:
[92mINFO [0m:      Received: train message 93a5d83d-1948-4276-9ce5-f903e6b4c813
02/12/2025 16:53:03:INFO:Received: train message 93a5d83d-1948-4276-9ce5-f903e6b4c813
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:11:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:26:40:INFO:
[92mINFO [0m:      Received: evaluate message c1c3ecc3-74cf-4d2f-9ad7-ef50146d02b0
02/12/2025 17:26:40:INFO:Received: evaluate message c1c3ecc3-74cf-4d2f-9ad7-ef50146d02b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 17:31:01:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:31:33:INFO:
[92mINFO [0m:      Received: train message 66b04547-81df-459a-8ef4-4958e9b2d2e9
02/12/2025 17:31:33:INFO:Received: train message 66b04547-81df-459a-8ef4-4958e9b2d2e9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:50:43:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:05:12:INFO:
[92mINFO [0m:      Received: evaluate message d771b961-e792-4ea6-8132-f79fc64e9fa5
02/12/2025 18:05:12:INFO:Received: evaluate message d771b961-e792-4ea6-8132-f79fc64e9fa5

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:09:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:10:28:INFO:
[92mINFO [0m:      Received: train message 014c827a-e036-40cc-a58e-663b0611a6bc
02/12/2025 18:10:28:INFO:Received: train message 014c827a-e036-40cc-a58e-663b0611a6bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 18:29:07:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:43:48:INFO:
[92mINFO [0m:      Received: evaluate message abbcffd6-0c45-4b86-9b63-1e567ff962dd
02/12/2025 18:43:48:INFO:Received: evaluate message abbcffd6-0c45-4b86-9b63-1e567ff962dd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:48:13:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:48:50:INFO:
[92mINFO [0m:      Received: train message 2fad84f5-c453-47cc-88ae-3a92d07cb031
02/12/2025 18:48:50:INFO:Received: train message 2fad84f5-c453-47cc-88ae-3a92d07cb031
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:07:23:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:22:19:INFO:
[92mINFO [0m:      Received: evaluate message 917247db-cca2-4e1c-b7b3-15512d9db837
02/12/2025 19:22:19:INFO:Received: evaluate message 917247db-cca2-4e1c-b7b3-15512d9db837
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 19:26:47:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:27:23:INFO:
[92mINFO [0m:      Received: train message f5a97481-baf3-43d4-9145-da12e045859e
02/12/2025 19:27:23:INFO:Received: train message f5a97481-baf3-43d4-9145-da12e045859e

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:46:05:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:01:11:INFO:
[92mINFO [0m:      Received: evaluate message d705e049-fc06-4690-9119-381a374a147a
02/12/2025 20:01:11:INFO:Received: evaluate message d705e049-fc06-4690-9119-381a374a147a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:05:29:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:06:11:INFO:
[92mINFO [0m:      Received: train message f8779a75-30f7-46ba-9db8-1edfd8ec9e6d
02/12/2025 20:06:11:INFO:Received: train message f8779a75-30f7-46ba-9db8-1edfd8ec9e6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:24:43:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:39:57:INFO:
[92mINFO [0m:      Received: evaluate message bd58e01b-fca6-4e64-a146-4af0779ec4ee
02/12/2025 20:39:57:INFO:Received: evaluate message bd58e01b-fca6-4e64-a146-4af0779ec4ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:44:10:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:45:04:INFO:
[92mINFO [0m:      Received: train message e03b611b-b173-4381-907a-04f82d37b589
02/12/2025 20:45:04:INFO:Received: train message e03b611b-b173-4381-907a-04f82d37b589
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 21:03:49:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:19:48:INFO:
[92mINFO [0m:      Received: evaluate message da7ec5f9-b6e4-4c68-a6fc-efc9035e82d8
02/12/2025 21:19:48:INFO:Received: evaluate message da7ec5f9-b6e4-4c68-a6fc-efc9035e82d8
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 21:24:08:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:24:44:INFO:
[92mINFO [0m:      Received: train message f9cc0a26-a55f-412e-b26e-2dd86950af3d
02/12/2025 21:24:44:INFO:Received: train message f9cc0a26-a55f-412e-b26e-2dd86950af3d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 21:43:13:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:01:48:INFO:
[92mINFO [0m:      Received: evaluate message eecb6c3e-969b-4eca-a407-632e3aee9a8f
02/12/2025 22:01:48:INFO:Received: evaluate message eecb6c3e-969b-4eca-a407-632e3aee9a8f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:06:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:07:15:INFO:
[92mINFO [0m:      Received: train message 94032726-20f3-44c5-80d3-609be9a05801
02/12/2025 22:07:15:INFO:Received: train message 94032726-20f3-44c5-80d3-609be9a05801
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 22:26:05:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:44:05:INFO:
[92mINFO [0m:      Received: evaluate message fd181a46-b3bd-45cb-adac-3570ecd2c222
02/12/2025 22:44:05:INFO:Received: evaluate message fd181a46-b3bd-45cb-adac-3570ecd2c222

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:48:35:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:49:28:INFO:
[92mINFO [0m:      Received: train message 5c4fec36-3fbc-4a63-b3dd-db67e7ab774c
02/12/2025 22:49:28:INFO:Received: train message 5c4fec36-3fbc-4a63-b3dd-db67e7ab774c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:07:25:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:24:28:INFO:
[92mINFO [0m:      Received: evaluate message 9ea489ac-8a75-4c3f-aecd-ba6fd7fc00f2
02/12/2025 23:24:28:INFO:Received: evaluate message 9ea489ac-8a75-4c3f-aecd-ba6fd7fc00f2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 23:28:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:29:03:INFO:
[92mINFO [0m:      Received: train message cbcc750b-a0df-4a19-b50f-7800b24d2780
02/12/2025 23:29:03:INFO:Received: train message cbcc750b-a0df-4a19-b50f-7800b24d2780
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:46:33:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:04:56:INFO:
[92mINFO [0m:      Received: evaluate message 2eefd1f8-8f98-4af0-ab45-8c647a10df4b
02/13/2025 00:04:56:INFO:Received: evaluate message 2eefd1f8-8f98-4af0-ab45-8c647a10df4b

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:09:09:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:09:45:INFO:
[92mINFO [0m:      Received: train message c9cf84ae-5c12-48ed-9acb-fdcb070d0702
02/13/2025 00:09:45:INFO:Received: train message c9cf84ae-5c12-48ed-9acb-fdcb070d0702
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 00:27:24:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:44:27:INFO:
[92mINFO [0m:      Received: evaluate message 2f30c209-2325-4164-8fc6-0a6c4c68fcb5
02/13/2025 00:44:27:INFO:Received: evaluate message 2f30c209-2325-4164-8fc6-0a6c4c68fcb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:48:42:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:49:14:INFO:
[92mINFO [0m:      Received: train message cbbfe4b4-a6b6-4396-815f-85ea3b51314f
02/13/2025 00:49:14:INFO:Received: train message cbbfe4b4-a6b6-4396-815f-85ea3b51314f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:08:10:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:24:28:INFO:
[92mINFO [0m:      Received: evaluate message 7ebe5a90-df41-4044-a895-33ea7b7aa289
02/13/2025 01:24:28:INFO:Received: evaluate message 7ebe5a90-df41-4044-a895-33ea7b7aa289

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 01:28:46:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:29:29:INFO:
[92mINFO [0m:      Received: train message bc5b86b7-ebd4-4642-88aa-2f2f9dffc94d
02/13/2025 01:29:29:INFO:Received: train message bc5b86b7-ebd4-4642-88aa-2f2f9dffc94d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:47:32:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:00:54:INFO:
[92mINFO [0m:      Received: evaluate message eb9d4d29-a50d-4419-ab06-47e7961485ce
02/13/2025 02:00:54:INFO:Received: evaluate message eb9d4d29-a50d-4419-ab06-47e7961485ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:04:47:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:05:57:INFO:
[92mINFO [0m:      Received: train message 75b4c865-8ea2-4900-a61e-df8b4f67dad4
02/13/2025 02:05:57:INFO:Received: train message 75b4c865-8ea2-4900-a61e-df8b4f67dad4

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 02:23:24:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:36:17:INFO:
[92mINFO [0m:      Received: evaluate message b5690551-b0b3-4ca7-8936-20cbd21ac1c0
02/13/2025 02:36:17:INFO:Received: evaluate message b5690551-b0b3-4ca7-8936-20cbd21ac1c0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:40:21:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:40:22:INFO:
[92mINFO [0m:      Received: reconnect message 3bec87df-455b-4e92-9dd0-56ed2272525e
02/13/2025 02:40:22:INFO:Received: reconnect message 3bec87df-455b-4e92-9dd0-56ed2272525e
02/13/2025 02:40:22:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/13/2025 02:40:22:INFO:Disconnect and shut down
