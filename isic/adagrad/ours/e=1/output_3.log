nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/12/2025 06:52:29:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/12/2025 06:52:29:DEBUG:ChannelConnectivity.IDLE
02/12/2025 06:52:29:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739371949.098570 2078144 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/12/2025 06:57:43:INFO:
[92mINFO [0m:      Received: train message 3d3c5e6d-a8a7-4129-ad76-22fccb21faa8
02/12/2025 06:57:43:INFO:Received: train message 3d3c5e6d-a8a7-4129-ad76-22fccb21faa8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:16:19:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:34:17:INFO:
[92mINFO [0m:      Received: evaluate message 8c4af73f-a75b-405b-80b7-06fc2caf19d1
02/12/2025 07:34:17:INFO:Received: evaluate message 8c4af73f-a75b-405b-80b7-06fc2caf19d1
[92mINFO [0m:      Sent reply
02/12/2025 07:38:54:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:39:28:INFO:
[92mINFO [0m:      Received: train message 3c9e1a07-d3e2-4746-9994-ec2821227c5f
02/12/2025 07:39:28:INFO:Received: train message 3c9e1a07-d3e2-4746-9994-ec2821227c5f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:56:11:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:14:28:INFO:
[92mINFO [0m:      Received: evaluate message 57cb7b60-efb8-4747-bd50-50fe5fcb312c
02/12/2025 08:14:28:INFO:Received: evaluate message 57cb7b60-efb8-4747-bd50-50fe5fcb312c
[92mINFO [0m:      Sent reply
02/12/2025 08:18:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:19:22:INFO:
[92mINFO [0m:      Received: train message 27beb86b-383b-4264-829e-a912cd28ce19
02/12/2025 08:19:22:INFO:Received: train message 27beb86b-383b-4264-829e-a912cd28ce19
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 08:36:02:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:54:35:INFO:
[92mINFO [0m:      Received: evaluate message be1cd703-5ac9-4c8c-beae-e4904d22b8c3
02/12/2025 08:54:35:INFO:Received: evaluate message be1cd703-5ac9-4c8c-beae-e4904d22b8c3
[92mINFO [0m:      Sent reply
02/12/2025 08:59:02:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:59:33:INFO:
[92mINFO [0m:      Received: train message efbde979-b16d-46de-80c3-ce7bc7bddd70
02/12/2025 08:59:33:INFO:Received: train message efbde979-b16d-46de-80c3-ce7bc7bddd70
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:16:33:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:34:22:INFO:
[92mINFO [0m:      Received: evaluate message eeb14fa3-bdec-45fa-8fcf-c0b8270014d9
02/12/2025 09:34:22:INFO:Received: evaluate message eeb14fa3-bdec-45fa-8fcf-c0b8270014d9
[92mINFO [0m:      Sent reply
02/12/2025 09:38:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:39:04:INFO:
[92mINFO [0m:      Received: train message ebd8ee90-de77-4b79-b4bf-2bbb86e5511a
02/12/2025 09:39:04:INFO:Received: train message ebd8ee90-de77-4b79-b4bf-2bbb86e5511a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:55:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:14:18:INFO:
[92mINFO [0m:      Received: evaluate message c0b6e60c-c647-4212-9800-8f4d7a659d0d
02/12/2025 10:14:18:INFO:Received: evaluate message c0b6e60c-c647-4212-9800-8f4d7a659d0d
[92mINFO [0m:      Sent reply
02/12/2025 10:18:38:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:19:04:INFO:
[92mINFO [0m:      Received: train message 8ed01a48-850d-407e-8c87-3afdec659456
02/12/2025 10:19:04:INFO:Received: train message 8ed01a48-850d-407e-8c87-3afdec659456
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 10:35:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:54:23:INFO:
[92mINFO [0m:      Received: evaluate message 9828d45b-c742-439e-be2c-228e22a8efca
02/12/2025 10:54:23:INFO:Received: evaluate message 9828d45b-c742-439e-be2c-228e22a8efca
[92mINFO [0m:      Sent reply
02/12/2025 10:58:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:59:04:INFO:
[92mINFO [0m:      Received: train message 01a22589-8dc5-4458-b6dd-3e1f0610f0eb
02/12/2025 10:59:04:INFO:Received: train message 01a22589-8dc5-4458-b6dd-3e1f0610f0eb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:16:11:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:33:59:INFO:
[92mINFO [0m:      Received: evaluate message 53e26bd9-b856-410b-bbe5-a0eaaf657901
02/12/2025 11:33:59:INFO:Received: evaluate message 53e26bd9-b856-410b-bbe5-a0eaaf657901
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124], 'accuracy': [0.23076923076923078], 'auc': [0.47906768884808504], 'precision': [0.22101253138913413], 'recall': [0.23076923076923078], 'f1': [0.19973020403381153]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868], 'accuracy': [0.23076923076923078, 0.2730567861457914], 'auc': [0.47906768884808504, 0.501405220341778], 'precision': [0.22101253138913413, 0.24545349552426635], 'recall': [0.23076923076923078, 0.2730567861457914], 'f1': [0.19973020403381153, 0.21604162997075227]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 11:38:04:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:39:01:INFO:
[92mINFO [0m:      Received: train message a3e3e867-fb1f-4747-9135-022a4290fa49
02/12/2025 11:39:01:INFO:Received: train message a3e3e867-fb1f-4747-9135-022a4290fa49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:56:18:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:13:18:INFO:
[92mINFO [0m:      Received: evaluate message 760c66e5-e79a-4f37-b768-d494707238a0
02/12/2025 12:13:18:INFO:Received: evaluate message 760c66e5-e79a-4f37-b768-d494707238a0
[92mINFO [0m:      Sent reply
02/12/2025 12:17:17:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:18:17:INFO:
[92mINFO [0m:      Received: train message 48d11838-d5fa-4a31-81c5-3a48b0559072
02/12/2025 12:18:17:INFO:Received: train message 48d11838-d5fa-4a31-81c5-3a48b0559072
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 12:35:28:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:52:37:INFO:
[92mINFO [0m:      Received: evaluate message 413824f1-af0c-4dcb-8590-c81cf467ff6b
02/12/2025 12:52:37:INFO:Received: evaluate message 413824f1-af0c-4dcb-8590-c81cf467ff6b
[92mINFO [0m:      Sent reply
02/12/2025 12:56:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:57:46:INFO:
[92mINFO [0m:      Received: train message 5d13a1cb-5853-4364-8139-4453b1d3dfc3
02/12/2025 12:57:46:INFO:Received: train message 5d13a1cb-5853-4364-8139-4453b1d3dfc3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:15:16:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:32:23:INFO:
[92mINFO [0m:      Received: evaluate message 2f6ac233-f1c3-4637-822f-ab2de3114ed8
02/12/2025 13:32:23:INFO:Received: evaluate message 2f6ac233-f1c3-4637-822f-ab2de3114ed8
[92mINFO [0m:      Sent reply
02/12/2025 13:36:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:37:12:INFO:
[92mINFO [0m:      Received: train message 231336c7-752e-495c-8e03-efb8d933860c
02/12/2025 13:37:12:INFO:Received: train message 231336c7-752e-495c-8e03-efb8d933860c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:54:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:11:21:INFO:
[92mINFO [0m:      Received: evaluate message 1e2c0811-b5fa-4cba-9b39-2bdbdd86982a
02/12/2025 14:11:21:INFO:Received: evaluate message 1e2c0811-b5fa-4cba-9b39-2bdbdd86982a

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 14:15:33:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:16:07:INFO:
[92mINFO [0m:      Received: train message 6506afd0-bdd8-403c-9755-53718f0fd9df
02/12/2025 14:16:07:INFO:Received: train message 6506afd0-bdd8-403c-9755-53718f0fd9df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 14:33:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:50:05:INFO:
[92mINFO [0m:      Received: evaluate message e5670b10-b2f3-4b74-9725-5072b064581c
02/12/2025 14:50:05:INFO:Received: evaluate message e5670b10-b2f3-4b74-9725-5072b064581c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 14:54:32:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:55:21:INFO:
[92mINFO [0m:      Received: train message 509f7d18-6fab-4c07-8aa3-8c5899ac7d67
02/12/2025 14:55:21:INFO:Received: train message 509f7d18-6fab-4c07-8aa3-8c5899ac7d67
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:12:47:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:29:00:INFO:
[92mINFO [0m:      Received: evaluate message 3ae749d8-538a-41bd-8cbb-449a4afb8c8d
02/12/2025 15:29:00:INFO:Received: evaluate message 3ae749d8-538a-41bd-8cbb-449a4afb8c8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 15:32:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:34:02:INFO:
[92mINFO [0m:      Received: train message 74419641-df07-4533-a7c9-5bf6eb612c46
02/12/2025 15:34:02:INFO:Received: train message 74419641-df07-4533-a7c9-5bf6eb612c46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:51:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:08:20:INFO:
[92mINFO [0m:      Received: evaluate message a409e1c3-69d7-4778-a72a-ff1a43028e61
02/12/2025 16:08:20:INFO:Received: evaluate message a409e1c3-69d7-4778-a72a-ff1a43028e61

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:12:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:13:30:INFO:
[92mINFO [0m:      Received: train message ab32cd3a-bd2d-4f5e-bcd4-4477d8e06b33
02/12/2025 16:13:30:INFO:Received: train message ab32cd3a-bd2d-4f5e-bcd4-4477d8e06b33
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 16:31:17:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:47:41:INFO:
[92mINFO [0m:      Received: evaluate message ce9cf10f-aad1-473a-9e4d-5b73b9e695fa
02/12/2025 16:47:41:INFO:Received: evaluate message ce9cf10f-aad1-473a-9e4d-5b73b9e695fa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:52:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:52:38:INFO:
[92mINFO [0m:      Received: train message 0eadfae8-2bca-419c-bc83-4bfb3343a3c4
02/12/2025 16:52:38:INFO:Received: train message 0eadfae8-2bca-419c-bc83-4bfb3343a3c4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:09:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:26:26:INFO:
[92mINFO [0m:      Received: evaluate message 72440d05-7ae7-4e4b-b42e-2c2cc88427af
02/12/2025 17:26:26:INFO:Received: evaluate message 72440d05-7ae7-4e4b-b42e-2c2cc88427af
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 17:30:52:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:31:33:INFO:
[92mINFO [0m:      Received: train message 4f345123-be1c-4d81-bfd5-77f694249c28
02/12/2025 17:31:33:INFO:Received: train message 4f345123-be1c-4d81-bfd5-77f694249c28
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:49:28:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:05:16:INFO:
[92mINFO [0m:      Received: evaluate message 0b80dd5a-11e3-456c-aa94-9c2bde570c32
02/12/2025 18:05:16:INFO:Received: evaluate message 0b80dd5a-11e3-456c-aa94-9c2bde570c32

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:10:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:10:36:INFO:
[92mINFO [0m:      Received: train message 20126d69-b727-4ca5-9900-3c5f59c66632
02/12/2025 18:10:36:INFO:Received: train message 20126d69-b727-4ca5-9900-3c5f59c66632
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 18:27:14:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:43:49:INFO:
[92mINFO [0m:      Received: evaluate message 3f5fa0e9-19ab-4979-8ab9-78f61ba5bea1
02/12/2025 18:43:49:INFO:Received: evaluate message 3f5fa0e9-19ab-4979-8ab9-78f61ba5bea1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:48:14:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:48:39:INFO:
[92mINFO [0m:      Received: train message 1c92207d-f3c2-493f-bd8e-9918a44f04e3
02/12/2025 18:48:39:INFO:Received: train message 1c92207d-f3c2-493f-bd8e-9918a44f04e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:05:45:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:22:24:INFO:
[92mINFO [0m:      Received: evaluate message b58bec5b-3d0e-4c84-bc5b-7d7e878ef8ca
02/12/2025 19:22:24:INFO:Received: evaluate message b58bec5b-3d0e-4c84-bc5b-7d7e878ef8ca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 19:26:49:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:27:17:INFO:
[92mINFO [0m:      Received: train message cd816a76-93d4-48bf-958b-b5627c46e7a2
02/12/2025 19:27:17:INFO:Received: train message cd816a76-93d4-48bf-958b-b5627c46e7a2

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:44:04:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:00:58:INFO:
[92mINFO [0m:      Received: evaluate message e2cb8171-d31c-43fb-8cd1-cfd8526411c2
02/12/2025 20:00:58:INFO:Received: evaluate message e2cb8171-d31c-43fb-8cd1-cfd8526411c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:05:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:06:05:INFO:
[92mINFO [0m:      Received: train message 6cf62923-42bc-47ac-8f1c-0b114592e95d
02/12/2025 20:06:05:INFO:Received: train message 6cf62923-42bc-47ac-8f1c-0b114592e95d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:23:03:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:40:09:INFO:
[92mINFO [0m:      Received: evaluate message 00649118-549e-4edb-bf74-954892fed544
02/12/2025 20:40:09:INFO:Received: evaluate message 00649118-549e-4edb-bf74-954892fed544
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:44:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:45:02:INFO:
[92mINFO [0m:      Received: train message ec0a5f17-98da-4867-8b3b-527203492263
02/12/2025 20:45:02:INFO:Received: train message ec0a5f17-98da-4867-8b3b-527203492263
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 21:02:03:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:19:38:INFO:
[92mINFO [0m:      Received: evaluate message 2dfdf2fa-dc43-49c9-8e5e-ec20121708d9
02/12/2025 21:19:38:INFO:Received: evaluate message 2dfdf2fa-dc43-49c9-8e5e-ec20121708d9
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 21:23:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:24:23:INFO:
[92mINFO [0m:      Received: train message 3999f474-e9d5-42b1-8b51-1629b75ee578
02/12/2025 21:24:23:INFO:Received: train message 3999f474-e9d5-42b1-8b51-1629b75ee578
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 21:41:04:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:01:44:INFO:
[92mINFO [0m:      Received: evaluate message b1df01f9-1919-4476-aeec-a2ce67c4ecc3
02/12/2025 22:01:44:INFO:Received: evaluate message b1df01f9-1919-4476-aeec-a2ce67c4ecc3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:06:22:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:07:13:INFO:
[92mINFO [0m:      Received: train message 96193fde-9813-40ae-9953-5d827e9ecd50
02/12/2025 22:07:13:INFO:Received: train message 96193fde-9813-40ae-9953-5d827e9ecd50
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 22:24:10:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:44:11:INFO:
[92mINFO [0m:      Received: evaluate message f3481847-b419-45b3-9db4-3becc34be373
02/12/2025 22:44:11:INFO:Received: evaluate message f3481847-b419-45b3-9db4-3becc34be373

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:48:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:49:16:INFO:
[92mINFO [0m:      Received: train message a531b52d-9492-46f3-8fb6-27ae05b83e3d
02/12/2025 22:49:16:INFO:Received: train message a531b52d-9492-46f3-8fb6-27ae05b83e3d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:05:58:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:24:25:INFO:
[92mINFO [0m:      Received: evaluate message 64463a1d-1050-4108-9df3-b13f10f2fe92
02/12/2025 23:24:25:INFO:Received: evaluate message 64463a1d-1050-4108-9df3-b13f10f2fe92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 23:28:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:29:07:INFO:
[92mINFO [0m:      Received: train message 684a81bc-31c8-42da-a2ef-72ca1ef82d43
02/12/2025 23:29:07:INFO:Received: train message 684a81bc-31c8-42da-a2ef-72ca1ef82d43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:45:46:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:04:46:INFO:
[92mINFO [0m:      Received: evaluate message afdb788f-5133-4f7c-b3ef-b0bdc37152f5
02/13/2025 00:04:46:INFO:Received: evaluate message afdb788f-5133-4f7c-b3ef-b0bdc37152f5

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:09:00:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:09:37:INFO:
[92mINFO [0m:      Received: train message f3f7905d-8515-4eda-a4d4-efc75d0278ab
02/13/2025 00:09:37:INFO:Received: train message f3f7905d-8515-4eda-a4d4-efc75d0278ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 00:26:05:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:44:15:INFO:
[92mINFO [0m:      Received: evaluate message 1d0d9927-8928-4f02-94b2-1e2d6b801690
02/13/2025 00:44:15:INFO:Received: evaluate message 1d0d9927-8928-4f02-94b2-1e2d6b801690
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:48:16:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:49:04:INFO:
[92mINFO [0m:      Received: train message 6fc05f97-7298-45e4-8182-9d1d9f1e2b15
02/13/2025 00:49:04:INFO:Received: train message 6fc05f97-7298-45e4-8182-9d1d9f1e2b15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:07:06:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:24:42:INFO:
[92mINFO [0m:      Received: evaluate message 01ea5d15-5081-4004-b6b5-16ea0b9c8542
02/13/2025 01:24:42:INFO:Received: evaluate message 01ea5d15-5081-4004-b6b5-16ea0b9c8542

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 01:28:56:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:29:25:INFO:
[92mINFO [0m:      Received: train message ccbb35de-7bb3-45b1-8440-54a6536851f1
02/13/2025 01:29:25:INFO:Received: train message ccbb35de-7bb3-45b1-8440-54a6536851f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:45:49:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:01:09:INFO:
[92mINFO [0m:      Received: evaluate message 486bbef9-d8f1-4069-8456-9b4241a3ca41
02/13/2025 02:01:09:INFO:Received: evaluate message 486bbef9-d8f1-4069-8456-9b4241a3ca41
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:05:22:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:05:50:INFO:
[92mINFO [0m:      Received: train message 345bc11b-2736-4b22-ab7d-ac821c9648ec
02/13/2025 02:05:50:INFO:Received: train message 345bc11b-2736-4b22-ab7d-ac821c9648ec

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 02:22:16:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:36:04:INFO:
[92mINFO [0m:      Received: evaluate message 9436005d-b0eb-492d-a485-b4b1d950aa77
02/13/2025 02:36:04:INFO:Received: evaluate message 9436005d-b0eb-492d-a485-b4b1d950aa77
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:39:58:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:40:22:INFO:
[92mINFO [0m:      Received: reconnect message 2bf58762-2018-47be-8ebf-65138fc17b68
02/13/2025 02:40:22:INFO:Received: reconnect message 2bf58762-2018-47be-8ebf-65138fc17b68
02/13/2025 02:40:22:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/13/2025 02:40:22:INFO:Disconnect and shut down
