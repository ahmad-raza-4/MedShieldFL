nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/12/2025 06:57:12:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/12/2025 06:57:12:DEBUG:ChannelConnectivity.IDLE
02/12/2025 06:57:12:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739372232.225839 2080446 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/12/2025 06:57:47:INFO:
[92mINFO [0m:      Received: train message fac31231-0008-4f12-a27a-7d1bc4827ee9
02/12/2025 06:57:47:INFO:Received: train message fac31231-0008-4f12-a27a-7d1bc4827ee9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:33:38:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:34:18:INFO:
[92mINFO [0m:      Received: evaluate message fe9c9bbd-df77-47aa-b4d7-7a4f5f452b71
02/12/2025 07:34:18:INFO:Received: evaluate message fe9c9bbd-df77-47aa-b4d7-7a4f5f452b71
[92mINFO [0m:      Sent reply
02/12/2025 07:38:52:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:39:21:INFO:
[92mINFO [0m:      Received: train message f3634c36-56fd-455e-86c8-5b61a69d5a3b
02/12/2025 07:39:21:INFO:Received: train message f3634c36-56fd-455e-86c8-5b61a69d5a3b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 08:13:49:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:14:15:INFO:
[92mINFO [0m:      Received: evaluate message e098be0e-1e0d-4115-a848-1896964dc821
02/12/2025 08:14:15:INFO:Received: evaluate message e098be0e-1e0d-4115-a848-1896964dc821
[92mINFO [0m:      Sent reply
02/12/2025 08:18:22:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:19:02:INFO:
[92mINFO [0m:      Received: train message 8eb4e6f2-993d-4c04-9784-cd0f5d51be7f
02/12/2025 08:19:02:INFO:Received: train message 8eb4e6f2-993d-4c04-9784-cd0f5d51be7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 08:53:58:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:54:16:INFO:
[92mINFO [0m:      Received: evaluate message b437d471-900d-4a26-b341-73b974b56c8b
02/12/2025 08:54:16:INFO:Received: evaluate message b437d471-900d-4a26-b341-73b974b56c8b
[92mINFO [0m:      Sent reply
02/12/2025 08:58:04:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:59:31:INFO:
[92mINFO [0m:      Received: train message d9d4d830-c629-484a-ad4a-6cfbaa2ac4f6
02/12/2025 08:59:31:INFO:Received: train message d9d4d830-c629-484a-ad4a-6cfbaa2ac4f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:33:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:34:28:INFO:
[92mINFO [0m:      Received: evaluate message 5cc3a119-7e02-4c2c-af05-afd3ad32aa5f
02/12/2025 09:34:28:INFO:Received: evaluate message 5cc3a119-7e02-4c2c-af05-afd3ad32aa5f
[92mINFO [0m:      Sent reply
02/12/2025 09:38:47:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:39:16:INFO:
[92mINFO [0m:      Received: train message 52c05ecf-422c-4cbd-ac52-51512ec4e013
02/12/2025 09:39:16:INFO:Received: train message 52c05ecf-422c-4cbd-ac52-51512ec4e013
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 10:13:41:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:14:20:INFO:
[92mINFO [0m:      Received: evaluate message 3e52774a-5d91-4e35-b14f-ef956e32493f
02/12/2025 10:14:20:INFO:Received: evaluate message 3e52774a-5d91-4e35-b14f-ef956e32493f
[92mINFO [0m:      Sent reply
02/12/2025 10:18:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:19:13:INFO:
[92mINFO [0m:      Received: train message 0a442739-0135-4256-be6b-b00d7f5162f0
02/12/2025 10:19:13:INFO:Received: train message 0a442739-0135-4256-be6b-b00d7f5162f0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 10:53:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:54:13:INFO:
[92mINFO [0m:      Received: evaluate message 22ac5e3d-f708-4594-8240-3a5c1163b94d
02/12/2025 10:54:13:INFO:Received: evaluate message 22ac5e3d-f708-4594-8240-3a5c1163b94d
[92mINFO [0m:      Sent reply
02/12/2025 10:58:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:59:01:INFO:
[92mINFO [0m:      Received: train message aea0add6-f263-463e-9791-a9873b0fce08
02/12/2025 10:59:01:INFO:Received: train message aea0add6-f263-463e-9791-a9873b0fce08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:33:37:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:34:03:INFO:
[92mINFO [0m:      Received: evaluate message 76059afe-8138-4479-9ab6-c21ae1125c10
02/12/2025 11:34:03:INFO:Received: evaluate message 76059afe-8138-4479-9ab6-c21ae1125c10
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124], 'accuracy': [0.23076923076923078], 'auc': [0.47906768884808504], 'precision': [0.22101253138913413], 'recall': [0.23076923076923078], 'f1': [0.19973020403381153]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868], 'accuracy': [0.23076923076923078, 0.2730567861457914], 'auc': [0.47906768884808504, 0.501405220341778], 'precision': [0.22101253138913413, 0.24545349552426635], 'recall': [0.23076923076923078, 0.2730567861457914], 'f1': [0.19973020403381153, 0.21604162997075227]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 11:38:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:38:54:INFO:
[92mINFO [0m:      Received: train message 99cef291-0e94-473e-8317-b0333d3fbc25
02/12/2025 11:38:54:INFO:Received: train message 99cef291-0e94-473e-8317-b0333d3fbc25
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 12:12:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:13:32:INFO:
[92mINFO [0m:      Received: evaluate message baa1d400-2dbb-41c3-a365-fc565380900d
02/12/2025 12:13:32:INFO:Received: evaluate message baa1d400-2dbb-41c3-a365-fc565380900d
[92mINFO [0m:      Sent reply
02/12/2025 12:17:54:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:18:19:INFO:
[92mINFO [0m:      Received: train message ba32f2f1-33d7-4561-93bd-4c1abf19c0fe
02/12/2025 12:18:19:INFO:Received: train message ba32f2f1-33d7-4561-93bd-4c1abf19c0fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 12:52:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:52:49:INFO:
[92mINFO [0m:      Received: evaluate message 47894947-519a-4370-86b9-4e126f37304d
02/12/2025 12:52:49:INFO:Received: evaluate message 47894947-519a-4370-86b9-4e126f37304d
[92mINFO [0m:      Sent reply
02/12/2025 12:57:05:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:57:46:INFO:
[92mINFO [0m:      Received: train message 365f6182-9c4e-4aff-b7e1-76c7aafc3fc0
02/12/2025 12:57:46:INFO:Received: train message 365f6182-9c4e-4aff-b7e1-76c7aafc3fc0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:31:45:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:32:22:INFO:
[92mINFO [0m:      Received: evaluate message ded414fa-3123-450b-82e4-cb74cdc922a6
02/12/2025 13:32:22:INFO:Received: evaluate message ded414fa-3123-450b-82e4-cb74cdc922a6
[92mINFO [0m:      Sent reply
02/12/2025 13:36:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:37:07:INFO:
[92mINFO [0m:      Received: train message 5503c688-1009-49e4-a40a-da7eff12b96a
02/12/2025 13:37:07:INFO:Received: train message 5503c688-1009-49e4-a40a-da7eff12b96a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 14:10:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:11:24:INFO:
[92mINFO [0m:      Received: evaluate message c2b0b5a7-abbb-4460-a92b-45bc96be8859
02/12/2025 14:11:24:INFO:Received: evaluate message c2b0b5a7-abbb-4460-a92b-45bc96be8859

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 14:15:32:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:16:05:INFO:
[92mINFO [0m:      Received: train message 1b1d5277-1d85-46c1-9ccb-21d13cc90440
02/12/2025 14:16:05:INFO:Received: train message 1b1d5277-1d85-46c1-9ccb-21d13cc90440
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 14:49:36:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:50:10:INFO:
[92mINFO [0m:      Received: evaluate message 9023d99b-8d64-428e-b25f-be0ce80ea0a2
02/12/2025 14:50:10:INFO:Received: evaluate message 9023d99b-8d64-428e-b25f-be0ce80ea0a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 14:54:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:55:11:INFO:
[92mINFO [0m:      Received: train message 609adc26-2d7e-434e-8c9e-91b04499d0f3
02/12/2025 14:55:11:INFO:Received: train message 609adc26-2d7e-434e-8c9e-91b04499d0f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:28:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:29:08:INFO:
[92mINFO [0m:      Received: evaluate message 8937aa2e-bdc1-42e3-9e1d-8902789f3313
02/12/2025 15:29:08:INFO:Received: evaluate message 8937aa2e-bdc1-42e3-9e1d-8902789f3313
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 15:33:26:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:34:10:INFO:
[92mINFO [0m:      Received: train message ecfa27a0-61eb-4f05-83f6-b512bf408411
02/12/2025 15:34:10:INFO:Received: train message ecfa27a0-61eb-4f05-83f6-b512bf408411
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 16:07:52:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:08:31:INFO:
[92mINFO [0m:      Received: evaluate message 64f1481f-d6a7-48d2-aa96-71c3b84db8df
02/12/2025 16:08:31:INFO:Received: evaluate message 64f1481f-d6a7-48d2-aa96-71c3b84db8df

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:12:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:13:20:INFO:
[92mINFO [0m:      Received: train message 10f6ae1c-3d65-48ba-8e26-d8ff43d1743e
02/12/2025 16:13:20:INFO:Received: train message 10f6ae1c-3d65-48ba-8e26-d8ff43d1743e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 16:47:16:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:47:54:INFO:
[92mINFO [0m:      Received: evaluate message 1ed818ab-fd99-4311-9667-5a061be1c701
02/12/2025 16:47:54:INFO:Received: evaluate message 1ed818ab-fd99-4311-9667-5a061be1c701
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:52:26:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:52:56:INFO:
[92mINFO [0m:      Received: train message 97966d06-6b13-4a85-bc6e-302c6f8dbd0f
02/12/2025 16:52:56:INFO:Received: train message 97966d06-6b13-4a85-bc6e-302c6f8dbd0f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:26:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:26:23:INFO:
[92mINFO [0m:      Received: evaluate message fc921c2d-16f9-4de5-b32f-724a5ff0c62e
02/12/2025 17:26:23:INFO:Received: evaluate message fc921c2d-16f9-4de5-b32f-724a5ff0c62e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 17:30:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:31:19:INFO:
[92mINFO [0m:      Received: train message 09fcc398-59cc-4d77-8f08-e85a3ca1915d
02/12/2025 17:31:19:INFO:Received: train message 09fcc398-59cc-4d77-8f08-e85a3ca1915d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 18:04:35:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:05:11:INFO:
[92mINFO [0m:      Received: evaluate message d8d34a87-fd8d-4c28-a278-5e70a66595f0
02/12/2025 18:05:11:INFO:Received: evaluate message d8d34a87-fd8d-4c28-a278-5e70a66595f0

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:09:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:10:18:INFO:
[92mINFO [0m:      Received: train message 9492bc9f-7128-40cb-a8df-9a5cb4fb3272
02/12/2025 18:10:18:INFO:Received: train message 9492bc9f-7128-40cb-a8df-9a5cb4fb3272
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 18:43:10:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:43:40:INFO:
[92mINFO [0m:      Received: evaluate message 8e6fbc13-756c-4dcd-89c8-72410a236746
02/12/2025 18:43:40:INFO:Received: evaluate message 8e6fbc13-756c-4dcd-89c8-72410a236746
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:48:05:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:48:50:INFO:
[92mINFO [0m:      Received: train message d8eab748-0e96-40ed-b06c-bd9e7344f1a4
02/12/2025 18:48:50:INFO:Received: train message d8eab748-0e96-40ed-b06c-bd9e7344f1a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:21:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:22:25:INFO:
[92mINFO [0m:      Received: evaluate message cc53f00c-ab7c-4348-a790-b5f51b88c65f
02/12/2025 19:22:25:INFO:Received: evaluate message cc53f00c-ab7c-4348-a790-b5f51b88c65f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 19:26:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:27:26:INFO:
[92mINFO [0m:      Received: train message c1541b0c-47d2-4f1d-ba9f-b389ecef7ac6
02/12/2025 19:27:26:INFO:Received: train message c1541b0c-47d2-4f1d-ba9f-b389ecef7ac6

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:00:40:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:01:21:INFO:
[92mINFO [0m:      Received: evaluate message 97244695-c729-4a14-89ec-0d378e43ece5
02/12/2025 20:01:21:INFO:Received: evaluate message 97244695-c729-4a14-89ec-0d378e43ece5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:05:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:06:25:INFO:
[92mINFO [0m:      Received: train message 5f5aee7e-045d-4763-8995-44c61c550e3e
02/12/2025 20:06:25:INFO:Received: train message 5f5aee7e-045d-4763-8995-44c61c550e3e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:39:32:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:40:07:INFO:
[92mINFO [0m:      Received: evaluate message c9fcc7f2-340f-4709-a810-63cbf3325bcf
02/12/2025 20:40:07:INFO:Received: evaluate message c9fcc7f2-340f-4709-a810-63cbf3325bcf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:44:24:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:44:53:INFO:
[92mINFO [0m:      Received: train message e9860529-4335-4bf6-970c-5034af1159e5
02/12/2025 20:44:53:INFO:Received: train message e9860529-4335-4bf6-970c-5034af1159e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 21:19:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:19:44:INFO:
[92mINFO [0m:      Received: evaluate message 47118e9e-f6be-46f0-80b4-1d08d9a445d6
02/12/2025 21:19:44:INFO:Received: evaluate message 47118e9e-f6be-46f0-80b4-1d08d9a445d6
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 21:24:06:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:24:43:INFO:
[92mINFO [0m:      Received: train message e93da4c6-339d-4fd9-8e21-a0d0f978e344
02/12/2025 21:24:43:INFO:Received: train message e93da4c6-339d-4fd9-8e21-a0d0f978e344
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 22:01:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:01:58:INFO:
[92mINFO [0m:      Received: evaluate message 6f8cbe06-8f66-4099-8182-135848ecfda3
02/12/2025 22:01:58:INFO:Received: evaluate message 6f8cbe06-8f66-4099-8182-135848ecfda3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:06:36:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:06:57:INFO:
[92mINFO [0m:      Received: train message f3ce7c47-bb0d-41f7-ad71-ecba7c8daa47
02/12/2025 22:06:57:INFO:Received: train message f3ce7c47-bb0d-41f7-ad71-ecba7c8daa47
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 22:43:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:44:02:INFO:
[92mINFO [0m:      Received: evaluate message 4f3985ea-36be-4a53-acc7-00d3d0ce58d5
02/12/2025 22:44:02:INFO:Received: evaluate message 4f3985ea-36be-4a53-acc7-00d3d0ce58d5

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:48:19:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:49:12:INFO:
[92mINFO [0m:      Received: train message c0880fb9-ef43-404a-b7f7-4272154fc16f
02/12/2025 22:49:12:INFO:Received: train message c0880fb9-ef43-404a-b7f7-4272154fc16f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:23:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:24:13:INFO:
[92mINFO [0m:      Received: evaluate message 9cec155d-b364-4a9a-b53c-73fe4d7ade43
02/12/2025 23:24:13:INFO:Received: evaluate message 9cec155d-b364-4a9a-b53c-73fe4d7ade43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 23:28:22:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:29:15:INFO:
[92mINFO [0m:      Received: train message bf1e0f22-0c78-4e4b-b41a-9175d304cdad
02/12/2025 23:29:15:INFO:Received: train message bf1e0f22-0c78-4e4b-b41a-9175d304cdad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 00:04:17:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:04:50:INFO:
[92mINFO [0m:      Received: evaluate message 39acf9cf-85f6-4640-a0f3-30e898bf1071
02/13/2025 00:04:50:INFO:Received: evaluate message 39acf9cf-85f6-4640-a0f3-30e898bf1071

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:09:06:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:09:45:INFO:
[92mINFO [0m:      Received: train message 367ddcbb-0f8b-4e8b-8cfa-d926912bc691
02/13/2025 00:09:45:INFO:Received: train message 367ddcbb-0f8b-4e8b-8cfa-d926912bc691
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 00:43:51:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:44:14:INFO:
[92mINFO [0m:      Received: evaluate message 10e570c4-f457-44ca-baea-466cb548f837
02/13/2025 00:44:14:INFO:Received: evaluate message 10e570c4-f457-44ca-baea-466cb548f837
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:48:13:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:49:04:INFO:
[92mINFO [0m:      Received: train message c14e1ea0-8335-4574-b480-46a19d1764c8
02/13/2025 00:49:04:INFO:Received: train message c14e1ea0-8335-4574-b480-46a19d1764c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:24:06:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:24:38:INFO:
[92mINFO [0m:      Received: evaluate message dca2a393-e26b-4c35-bb6e-12061b310699
02/13/2025 01:24:38:INFO:Received: evaluate message dca2a393-e26b-4c35-bb6e-12061b310699

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 01:28:55:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:29:30:INFO:
[92mINFO [0m:      Received: train message 09da67b3-2ab8-4e53-87bf-11f4c468c28e
02/13/2025 01:29:30:INFO:Received: train message 09da67b3-2ab8-4e53-87bf-11f4c468c28e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 02:00:29:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:01:12:INFO:
[92mINFO [0m:      Received: evaluate message 26497e30-1be5-4f4b-98d4-154b538e82c8
02/13/2025 02:01:12:INFO:Received: evaluate message 26497e30-1be5-4f4b-98d4-154b538e82c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:05:13:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:06:01:INFO:
[92mINFO [0m:      Received: train message 160258d0-ce6f-4db4-8391-ebf356a32f50
02/13/2025 02:06:01:INFO:Received: train message 160258d0-ce6f-4db4-8391-ebf356a32f50

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 02:35:42:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:36:14:INFO:
[92mINFO [0m:      Received: evaluate message 28c3b4ba-6f11-4bb6-9678-63b5f3d56a07
02/13/2025 02:36:14:INFO:Received: evaluate message 28c3b4ba-6f11-4bb6-9678-63b5f3d56a07
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:40:16:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:40:22:INFO:
[92mINFO [0m:      Received: reconnect message a792eebc-7906-4c74-a6f1-2ec96cd4aa98
02/13/2025 02:40:22:INFO:Received: reconnect message a792eebc-7906-4c74-a6f1-2ec96cd4aa98
02/13/2025 02:40:22:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/13/2025 02:40:22:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613, 1.577787735411298], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175, 0.6979438649202241], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876, 0.448109658694975], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136, 0.32303410546437183]}



Final client history:
{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613, 1.577787735411298], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175, 0.6979438649202241], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876, 0.448109658694975], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136, 0.32303410546437183]}

