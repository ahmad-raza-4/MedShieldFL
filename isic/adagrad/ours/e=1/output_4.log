nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/12/2025 06:51:05:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/12/2025 06:51:05:DEBUG:ChannelConnectivity.IDLE
02/12/2025 06:51:05:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739371865.396711 2077234 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/12/2025 06:57:33:INFO:
[92mINFO [0m:      Received: train message 029fcfd2-2313-4b3e-ad23-79b6363b5654
02/12/2025 06:57:33:INFO:Received: train message 029fcfd2-2313-4b3e-ad23-79b6363b5654
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:11:26:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:34:10:INFO:
[92mINFO [0m:      Received: evaluate message b8c36a95-1c69-4c55-a0dc-7a89e4c40fce
02/12/2025 07:34:10:INFO:Received: evaluate message b8c36a95-1c69-4c55-a0dc-7a89e4c40fce
[92mINFO [0m:      Sent reply
02/12/2025 07:38:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:39:25:INFO:
[92mINFO [0m:      Received: train message 57ec12f5-8e0a-4871-8d83-b642393327d4
02/12/2025 07:39:25:INFO:Received: train message 57ec12f5-8e0a-4871-8d83-b642393327d4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:52:04:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:14:26:INFO:
[92mINFO [0m:      Received: evaluate message 41397af3-895d-4ac1-9d87-39b0f9f179d0
02/12/2025 08:14:26:INFO:Received: evaluate message 41397af3-895d-4ac1-9d87-39b0f9f179d0
[92mINFO [0m:      Sent reply
02/12/2025 08:18:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:19:09:INFO:
[92mINFO [0m:      Received: train message 2d9a4a0c-dc59-4fb5-bc32-fedb427bd058
02/12/2025 08:19:09:INFO:Received: train message 2d9a4a0c-dc59-4fb5-bc32-fedb427bd058
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 08:31:33:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:54:35:INFO:
[92mINFO [0m:      Received: evaluate message 7e2e33f2-232f-4d28-b65b-e0c0fc8cff76
02/12/2025 08:54:35:INFO:Received: evaluate message 7e2e33f2-232f-4d28-b65b-e0c0fc8cff76
[92mINFO [0m:      Sent reply
02/12/2025 08:59:02:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:59:26:INFO:
[92mINFO [0m:      Received: train message e6b7e28b-3017-417c-9be6-aa7cf5889e05
02/12/2025 08:59:26:INFO:Received: train message e6b7e28b-3017-417c-9be6-aa7cf5889e05
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:11:54:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:34:30:INFO:
[92mINFO [0m:      Received: evaluate message a20f87e9-e4b1-4348-8f5e-0554557200e4
02/12/2025 09:34:30:INFO:Received: evaluate message a20f87e9-e4b1-4348-8f5e-0554557200e4
[92mINFO [0m:      Sent reply
02/12/2025 09:38:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:39:17:INFO:
[92mINFO [0m:      Received: train message 1c92e82a-4849-4186-8611-3daf21a3e17b
02/12/2025 09:39:17:INFO:Received: train message 1c92e82a-4849-4186-8611-3daf21a3e17b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:51:33:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:14:07:INFO:
[92mINFO [0m:      Received: evaluate message a2037c5a-7998-4b0d-b513-9592bcea4a1a
02/12/2025 10:14:07:INFO:Received: evaluate message a2037c5a-7998-4b0d-b513-9592bcea4a1a
[92mINFO [0m:      Sent reply
02/12/2025 10:18:25:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:19:11:INFO:
[92mINFO [0m:      Received: train message a21d9dbe-1557-4b74-8b72-37e39fd93777
02/12/2025 10:19:11:INFO:Received: train message a21d9dbe-1557-4b74-8b72-37e39fd93777
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 10:31:28:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:54:24:INFO:
[92mINFO [0m:      Received: evaluate message 04cfb97b-b059-4167-987f-3e0a8c8936b6
02/12/2025 10:54:24:INFO:Received: evaluate message 04cfb97b-b059-4167-987f-3e0a8c8936b6
[92mINFO [0m:      Sent reply
02/12/2025 10:58:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:59:14:INFO:
[92mINFO [0m:      Received: train message 387ecdf3-ebde-47f3-96d8-d79f69dc0cc1
02/12/2025 10:59:14:INFO:Received: train message 387ecdf3-ebde-47f3-96d8-d79f69dc0cc1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:11:50:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:34:13:INFO:
[92mINFO [0m:      Received: evaluate message 2301192c-63fe-40d6-bfd3-2582534f0bfc
02/12/2025 11:34:13:INFO:Received: evaluate message 2301192c-63fe-40d6-bfd3-2582534f0bfc
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124], 'accuracy': [0.23076923076923078], 'auc': [0.47906768884808504], 'precision': [0.22101253138913413], 'recall': [0.23076923076923078], 'f1': [0.19973020403381153]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868], 'accuracy': [0.23076923076923078, 0.2730567861457914], 'auc': [0.47906768884808504, 0.501405220341778], 'precision': [0.22101253138913413, 0.24545349552426635], 'recall': [0.23076923076923078, 0.2730567861457914], 'f1': [0.19973020403381153, 0.21604162997075227]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 11:38:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:39:02:INFO:
[92mINFO [0m:      Received: train message 341307cb-df3e-4dcb-86a8-abe45d022592
02/12/2025 11:39:02:INFO:Received: train message 341307cb-df3e-4dcb-86a8-abe45d022592
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:52:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:13:29:INFO:
[92mINFO [0m:      Received: evaluate message be8adb9c-6e48-41da-b0b8-772323a546c4
02/12/2025 12:13:29:INFO:Received: evaluate message be8adb9c-6e48-41da-b0b8-772323a546c4
[92mINFO [0m:      Sent reply
02/12/2025 12:17:52:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:18:17:INFO:
[92mINFO [0m:      Received: train message 48a7aa17-a053-4c78-a996-09c3450a42fd
02/12/2025 12:18:17:INFO:Received: train message 48a7aa17-a053-4c78-a996-09c3450a42fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 12:30:57:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:52:57:INFO:
[92mINFO [0m:      Received: evaluate message 0643a9ef-c573-4d5c-a880-68f6da8312d5
02/12/2025 12:52:57:INFO:Received: evaluate message 0643a9ef-c573-4d5c-a880-68f6da8312d5
[92mINFO [0m:      Sent reply
02/12/2025 12:57:13:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:57:28:INFO:
[92mINFO [0m:      Received: train message 0e52bde6-d75f-4d8e-9bcb-551571851c98
02/12/2025 12:57:28:INFO:Received: train message 0e52bde6-d75f-4d8e-9bcb-551571851c98
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:10:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:32:20:INFO:
[92mINFO [0m:      Received: evaluate message 111a175f-ae12-40d4-95d9-77829a846073
02/12/2025 13:32:20:INFO:Received: evaluate message 111a175f-ae12-40d4-95d9-77829a846073
[92mINFO [0m:      Sent reply
02/12/2025 13:36:41:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:37:14:INFO:
[92mINFO [0m:      Received: train message d6c3e8e6-ca45-47e1-8355-11d81e6253b2
02/12/2025 13:37:14:INFO:Received: train message d6c3e8e6-ca45-47e1-8355-11d81e6253b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:50:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:11:24:INFO:
[92mINFO [0m:      Received: evaluate message 363f969c-2838-4e03-b77e-4aa39d01453b
02/12/2025 14:11:24:INFO:Received: evaluate message 363f969c-2838-4e03-b77e-4aa39d01453b

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 14:15:34:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:15:57:INFO:
[92mINFO [0m:      Received: train message 70b52ab3-af08-4e29-aa21-b89e38c5aa7b
02/12/2025 14:15:57:INFO:Received: train message 70b52ab3-af08-4e29-aa21-b89e38c5aa7b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 14:28:46:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:50:04:INFO:
[92mINFO [0m:      Received: evaluate message 39ef59cc-1d99-4edb-8edb-0137c8c74229
02/12/2025 14:50:04:INFO:Received: evaluate message 39ef59cc-1d99-4edb-8edb-0137c8c74229
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 14:54:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:55:21:INFO:
[92mINFO [0m:      Received: train message 88ca4da0-2f14-4f6e-acd5-0cc86739d77c
02/12/2025 14:55:21:INFO:Received: train message 88ca4da0-2f14-4f6e-acd5-0cc86739d77c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:08:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:29:19:INFO:
[92mINFO [0m:      Received: evaluate message 1545ae4b-f0e5-4c64-b41b-6775228448fc
02/12/2025 15:29:19:INFO:Received: evaluate message 1545ae4b-f0e5-4c64-b41b-6775228448fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 15:33:36:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:33:57:INFO:
[92mINFO [0m:      Received: train message 18b25bb7-a00b-4343-8d97-a4222b3a22fc
02/12/2025 15:33:57:INFO:Received: train message 18b25bb7-a00b-4343-8d97-a4222b3a22fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:47:26:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:08:18:INFO:
[92mINFO [0m:      Received: evaluate message 380050e4-8535-424c-a18b-1338121f4091
02/12/2025 16:08:18:INFO:Received: evaluate message 380050e4-8535-424c-a18b-1338121f4091

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:12:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:13:20:INFO:
[92mINFO [0m:      Received: train message dc2e6eea-7256-4d5a-91df-cad23c743a15
02/12/2025 16:13:20:INFO:Received: train message dc2e6eea-7256-4d5a-91df-cad23c743a15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 16:26:26:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:47:41:INFO:
[92mINFO [0m:      Received: evaluate message 6823f4a8-79af-48e6-8a51-8567405de713
02/12/2025 16:47:41:INFO:Received: evaluate message 6823f4a8-79af-48e6-8a51-8567405de713
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:52:05:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:52:53:INFO:
[92mINFO [0m:      Received: train message d12a0e1b-f95c-4af9-8e5c-761af4f07196
02/12/2025 16:52:53:INFO:Received: train message d12a0e1b-f95c-4af9-8e5c-761af4f07196
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:05:50:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:26:38:INFO:
[92mINFO [0m:      Received: evaluate message de5a8dde-08a5-43c0-825b-f784930d8ea4
02/12/2025 17:26:38:INFO:Received: evaluate message de5a8dde-08a5-43c0-825b-f784930d8ea4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 17:31:01:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:31:37:INFO:
[92mINFO [0m:      Received: train message 3cf0cf6d-90d4-4db0-a176-06574032c5b7
02/12/2025 17:31:37:INFO:Received: train message 3cf0cf6d-90d4-4db0-a176-06574032c5b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:44:29:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:05:02:INFO:
[92mINFO [0m:      Received: evaluate message 4b864895-9155-4063-b973-6d51727ddaed
02/12/2025 18:05:02:INFO:Received: evaluate message 4b864895-9155-4063-b973-6d51727ddaed

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:09:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:10:24:INFO:
[92mINFO [0m:      Received: train message 0b65076f-678c-489a-99d5-47684cf669f9
02/12/2025 18:10:24:INFO:Received: train message 0b65076f-678c-489a-99d5-47684cf669f9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 18:23:09:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:43:40:INFO:
[92mINFO [0m:      Received: evaluate message b3b85baf-854e-4261-8923-9b6916c50b96
02/12/2025 18:43:40:INFO:Received: evaluate message b3b85baf-854e-4261-8923-9b6916c50b96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:48:06:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:48:39:INFO:
[92mINFO [0m:      Received: train message 3520e19e-824b-4938-abdc-ca8b4d79ca93
02/12/2025 18:48:39:INFO:Received: train message 3520e19e-824b-4938-abdc-ca8b4d79ca93
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:01:25:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:22:08:INFO:
[92mINFO [0m:      Received: evaluate message 289b89a4-d51f-4929-82d9-0426f62f2c31
02/12/2025 19:22:08:INFO:Received: evaluate message 289b89a4-d51f-4929-82d9-0426f62f2c31
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 19:26:10:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:27:20:INFO:
[92mINFO [0m:      Received: train message 8300c9e6-7e0c-45a5-8234-dc9406968852
02/12/2025 19:27:20:INFO:Received: train message 8300c9e6-7e0c-45a5-8234-dc9406968852

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:40:12:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:01:17:INFO:
[92mINFO [0m:      Received: evaluate message d7855930-a993-4886-bc86-9f081aa45840
02/12/2025 20:01:17:INFO:Received: evaluate message d7855930-a993-4886-bc86-9f081aa45840
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:05:43:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:06:24:INFO:
[92mINFO [0m:      Received: train message c1a69531-e770-43f6-9bf3-5c685bb7c5f8
02/12/2025 20:06:24:INFO:Received: train message c1a69531-e770-43f6-9bf3-5c685bb7c5f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:19:18:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:40:05:INFO:
[92mINFO [0m:      Received: evaluate message 0075cefc-1684-4aa3-950b-dc7667a16a7a
02/12/2025 20:40:05:INFO:Received: evaluate message 0075cefc-1684-4aa3-950b-dc7667a16a7a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:44:25:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:44:47:INFO:
[92mINFO [0m:      Received: train message fdfca55a-06f8-4719-903d-8838abeefef2
02/12/2025 20:44:47:INFO:Received: train message fdfca55a-06f8-4719-903d-8838abeefef2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:57:13:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:19:48:INFO:
[92mINFO [0m:      Received: evaluate message 11e5e311-6ca0-4f89-b8d0-357c009da73c
02/12/2025 21:19:48:INFO:Received: evaluate message 11e5e311-6ca0-4f89-b8d0-357c009da73c
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 21:24:07:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:24:27:INFO:
[92mINFO [0m:      Received: train message f9580547-32a5-4623-8939-b6ad37ee1e15
02/12/2025 21:24:27:INFO:Received: train message f9580547-32a5-4623-8939-b6ad37ee1e15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 21:36:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:02:00:INFO:
[92mINFO [0m:      Received: evaluate message 5743a972-a453-4d0c-aa21-81d1331220d5
02/12/2025 22:02:00:INFO:Received: evaluate message 5743a972-a453-4d0c-aa21-81d1331220d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:06:41:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:07:15:INFO:
[92mINFO [0m:      Received: train message 0f49aa76-0384-4d94-a771-1cdbc6131300
02/12/2025 22:07:15:INFO:Received: train message 0f49aa76-0384-4d94-a771-1cdbc6131300
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 22:19:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:44:19:INFO:
[92mINFO [0m:      Received: evaluate message 1be83c94-9a9d-4a6f-ac5d-db0a60116c41
02/12/2025 22:44:19:INFO:Received: evaluate message 1be83c94-9a9d-4a6f-ac5d-db0a60116c41

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:48:49:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:49:24:INFO:
[92mINFO [0m:      Received: train message 2e5edde5-e07a-44e0-a3d6-80928a42bedd
02/12/2025 22:49:24:INFO:Received: train message 2e5edde5-e07a-44e0-a3d6-80928a42bedd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:01:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:24:13:INFO:
[92mINFO [0m:      Received: evaluate message 3a0e1315-088b-458a-9aa7-acbfdfeaeb23
02/12/2025 23:24:13:INFO:Received: evaluate message 3a0e1315-088b-458a-9aa7-acbfdfeaeb23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 23:28:19:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:29:15:INFO:
[92mINFO [0m:      Received: train message 2ab25427-6152-4f62-8258-083f3e2e8f60
02/12/2025 23:29:15:INFO:Received: train message 2ab25427-6152-4f62-8258-083f3e2e8f60
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:41:51:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:04:58:INFO:
[92mINFO [0m:      Received: evaluate message 07447426-7e28-4451-ac77-89e015208272
02/13/2025 00:04:58:INFO:Received: evaluate message 07447426-7e28-4451-ac77-89e015208272

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:09:10:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:09:41:INFO:
[92mINFO [0m:      Received: train message e22d93e3-8bd6-4097-9a96-558eae39b45e
02/13/2025 00:09:41:INFO:Received: train message e22d93e3-8bd6-4097-9a96-558eae39b45e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 00:21:49:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:44:27:INFO:
[92mINFO [0m:      Received: evaluate message e8f8f4ed-4167-4ba7-8cdf-0dde8bdfd96b
02/13/2025 00:44:27:INFO:Received: evaluate message e8f8f4ed-4167-4ba7-8cdf-0dde8bdfd96b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:48:35:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:49:14:INFO:
[92mINFO [0m:      Received: train message 5505a804-95a3-4ddc-8327-2e9b8f9ba4f6
02/13/2025 00:49:14:INFO:Received: train message 5505a804-95a3-4ddc-8327-2e9b8f9ba4f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:01:50:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:24:40:INFO:
[92mINFO [0m:      Received: evaluate message 4aacbfdf-0220-4fc5-99f6-65732aac761c
02/13/2025 01:24:40:INFO:Received: evaluate message 4aacbfdf-0220-4fc5-99f6-65732aac761c

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 01:28:55:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:29:11:INFO:
[92mINFO [0m:      Received: train message c173df23-7379-488c-91d4-03849cbec144
02/13/2025 01:29:11:INFO:Received: train message c173df23-7379-488c-91d4-03849cbec144
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:40:55:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:00:50:INFO:
[92mINFO [0m:      Received: evaluate message f3ef89f7-abfc-4705-aeb2-2e8e7d624ca2
02/13/2025 02:00:50:INFO:Received: evaluate message f3ef89f7-abfc-4705-aeb2-2e8e7d624ca2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:05:04:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:05:58:INFO:
[92mINFO [0m:      Received: train message a1f85121-222f-4fc8-84a0-c0e5562c56b9
02/13/2025 02:05:58:INFO:Received: train message a1f85121-222f-4fc8-84a0-c0e5562c56b9

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 02:18:15:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:36:20:INFO:
[92mINFO [0m:      Received: evaluate message e55c10e8-af8a-4cbd-b536-8069b11415d9
02/13/2025 02:36:20:INFO:Received: evaluate message e55c10e8-af8a-4cbd-b536-8069b11415d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:40:21:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:40:22:INFO:
[92mINFO [0m:      Received: reconnect message 01ee0b84-f11c-46bd-badd-4192d83f67a3
02/13/2025 02:40:22:INFO:Received: reconnect message 01ee0b84-f11c-46bd-badd-4192d83f67a3
02/13/2025 02:40:22:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/13/2025 02:40:22:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613, 1.577787735411298], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175, 0.6979438649202241], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876, 0.448109658694975], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136, 0.32303410546437183]}



Final client history:
{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613, 1.577787735411298], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175, 0.6979438649202241], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876, 0.448109658694975], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136, 0.32303410546437183]}

